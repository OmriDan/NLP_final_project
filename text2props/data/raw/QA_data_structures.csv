question_type,points,question_translation_latex,multiple_choice_answer,answer_translation_latex,topics_covered
Multiple Choice,6,"\noindent Consider a red-black tree as studied in class and described in Cormen (all Nils are considered leaves and their color is black). Let the height of a node \( y \) be defined as the length of the longest path (counted in nodes) upwards to \( y \). The count does not include \( y \) and the discussed nil. Let \( x \) be a node that is removed (note: the intention is the node removed and not the value removed) in the delete operation (as studied in class in a binary search tree). We are interested in the height of \( x \) before the removal. Then:
\begin{enumerate}
    \item The tightest upper bound on the height of \( x \) is 0.
    \item The tightest upper bound on the height of \( x \) is 1.
    \item The tightest upper bound on the height of \( x \) is 2.
    \item The tightest upper bound on the height of \( x \) is 3.
    \item There is no upper bound on the height of \( x \).
\end{enumerate}",2,na,OTHER
Multiple Choice,3,"\noindent A family of functions \( H = \{h \mid h : M \to \{1, 2, \ldots, n\}\} \) is called universal if for all different \( x, y \) in \( M \), the number of functions \( h \) in \( H \) for which \( h(x) = h(y) \) is the size of \( H \) divided by \( n \).

A family of functions \( H = \{h \mid h : M \to \{1, 2, \ldots, n\}\} \) is called 2-universal if for all different \( x, y \) in \( M \), the pair \( (h(x), h(y)) \) is uniformly distributed over the set \(\{(i, j) \mid 1 \leq i, j \leq n\}\).

Is it true that every 2-universal family is universal?

1. Yes

2. No

",1,na,Hash tables
Multiple Choice,3,"\noindent A family of functions $H = \{h \mid h: M \to \{1,2,\ldots,n\}\}$ is called universal if for every distinct $x, y$ in $M$, the number of functions $h$ in $H$ for which $h(x) = h(y)$ is equal to the size of $H$ divided by $n$. \\
A family of functions $H = \{h \mid h: M \to \{1,2,\ldots,n\}\}$ is called almost-universal if for every distinct $x, y$ in $M$, if we choose $h$ uniformly from $H$, then the probability that $h(x) = h(y)$ is less than $\frac{2}{n}$. \\

\noindent Is it true that every universal family is almost-universal? \\
1. Yes \\
2. No",1,na,Hash tables
Multiple Choice,6,"Given an array of size \( n \) where all its values are from the set \(\{1,2,\ldots,n^{\log\log(n)}\}\). It is possible to sort the elements of the array:\\

1. In time \(\theta(n)\) on average\\
2. In time \(\theta(n)\) in the worst case\\
3. In time \(\theta(n\log(n))\) in the worst case\\
4. In time \(\theta(n\log\log(n))\) in the worst case\\
5. In time \(\theta\left(\frac{n\log(n)}{\log\log(n)}\right)\) in the worst case\\
6. In time \(\theta\left(\frac{n\log(n)}{\log\log(n)}\right)\) on average",4,na,"Complexity analysis,   Non-comparison based sorting (radix,   bucket,   counting)"
Multiple Choice,6,"
Given an array with \( n \) elements where \( n^{2001/2002} \) of the elements are identical. The array can be sorted in worst-case time (mark the tightest bound, assuming the RAM model):
\begin{enumerate}
    \item \( \Theta(n) \)
    \item \( \Theta(n^{1/2002}\log(n)) \)
    \item \( \Theta(n\log(n)^{1/2002}) \)
    \item \( \Theta(n^{2001/2002}\log(n)) \)
    \item \( \Theta(n\log(n)^{2001/2002}) \)
    \item \( \Theta(n\log(n)) \)
\end{enumerate}
",6,na,"Complexity analysis,   Quick-sort,   Selection and median-of-median algorithm"
Multiple Choice,5,"Given a queue of natural numbers \( Q_1 \) with \( n \) elements. We want to sort the elements in \( Q_1 \), meaning that at the end of the sorting algorithm's run, \( Q_1 \) will contain the same elements as at the beginning, only in sorted order. The sorting algorithm is allowed to use one additional queue \( Q_2 \) with \( O(n) \) elements and an additional \( O(1) \) memory. Then, there exists such a sorting algorithm as follows:\\
1. Its running time is \( O(n^3) \) and uses principles similar to Bubble Sort.\\
2. Its running time is \( O(n^2) \) and uses principles similar to Bubble Sort.\\
3. Its running time is \( O(n^3\log(n)) \) and uses principles similar to Quick Sort (meaning the version of Quick Sort with a running time of \( O(n\log(n)) \) in the worst case).\\
4. Its running time is \( O(n^2\log(n)) \) and uses principles similar to Quick Sort (meaning the version of Quick Sort with a running time of \( O(n\log(n)) \) in the worst case).\\
5. Its running time is \( O(n^2\log(n)) \) and uses principles similar to Merge Sort.\\
6. Its running time is \( O(n\log(n)) \) and uses principles similar to Merge Sort.",6,na,"Complexity analysis,   Quick-sort,   Selection and median-of-median algorithm,   OTHER"
Multiple Choice,na,"
Assume you have an algorithm capable of performing the Heapify operation (as defined in class) on a heap in time \( O(\log\log(n)) \). What will be the running time of the BuildHeap algorithm (as defined in class), if instead of using the standard Heapify algorithm as a black box, you use the new Heapify algorithm?

A. \(\Theta(n\log(n))\) 

B. \(\Theta(n\log\log(n))\) 

C. \(\Theta\left(\frac{n\log(n)}{\log\log(n)}\right)\) 

D. \(\Theta\left(\frac{n\log\log(n)}{\log(n)}\right)\) 

E. \(\Theta(n)\) 
",E,na,Complexity analysis
Multiple Choice,na,"
In class, we saw a binary search algorithm in a sorted array.
We will modify it as follows:
At each step, instead of checking only the value at index \( n/2 \), we will check the value at each of the indices \( n/k \), \( 2n/k \), \( 3n/k \), ..., \( (k-1)n/k \).  
If at one of them the value equals the value being searched for, we will stop and return its index.
Otherwise, we will apply the algorithm recursively only on the appropriate segment.
The runtime of this algorithm will be:
\begin{enumerate}
    \item \(\Theta(n)\) 
    \item \(\Theta(n\log(k))\) 
    \item \(\Theta\left(\frac{k\log(n)}{\log(k)}\right)\) 
    \item \(\Theta\left(\frac{\log(n)}{\log(k)}\right)\)
    \item \(\Theta\left(\frac{\log(n)}{k}\right)\)
\end{enumerate}
",C,na,Complexity analysis
Multiple Choice,na,"
Given the functions \( f(n) = 2^{(\log\log(n))^{0.5}} \) and \( g(n) = (\log\log(n))^2 \cdot n^{0.5} \), which of the following is true?
\begin{enumerate}
    \item \( f = \Theta(g) \)
    \item \( f = o(g) \)
    \item \( g = o(f) \)
    \item None of the above
\end{enumerate}
",2,na,Complexity analysis
Multiple Choice,na,"
We want to convert a binary search tree into a red-black search tree. Assuming the comparison model, this can be done in the best possible time:

A. \(\Theta(n)\) W.C., average \(\Theta(n)\).
B. \(\Theta(n)\) W.C., average \(\Theta(n \log(n))\).
C. \(\Theta(n \log(n))\) W.C., average \(\Theta(n)\).
D. \(\Theta(n \log(n))\) W.C., average \(\Theta(n \log(n))\).
E. None of the above.
",A,na,"Binary search trees,   OTHER"
Multiple Choice,na,"
Given two sorted arrays of size \( n \). No value repeats itself. We want to find their common median (i.e., an element with exactly \( n \) elements smaller than it in the combined arrays).

This can be done in W.C. time (best possible):

A. \(\Theta(1)\)

B. \(\Theta(\log(n))\)

C. \(\Theta(n)\)

D. \(\Theta(n \log(n))\)

E. \(\Theta(n^2)\)
",B,na,"Complexity analysis,   Selection and median-of-median algorithm"
Multiple choice with explanations,4,"\subsection*{A Ternary Tree}
A ternary tree is a tree in which each node has at most three children. A complete and balanced ternary tree, containing $N$ nodes, is given. The depth of a node is the number of edges between the node and the root (the depth of the root is 0). The height of a node is the number of edges between the node and the nearest leaf (the height of a leaf is 0). Let $D$ be the sum of the depths of all the nodes (including leaves), then:
\begin{enumerate}
    \item $D = \theta(N)$
    \item $D = \theta(N\log(N))$
    \item $D = \theta(N^2)$
    \item $D = \theta(N^2\log(N))$
    \item None of the above answers is correct.
\end{enumerate}",B,"
b. 
The depth of all the nodes is \(O(\log n)\). Therefore, the sum is \(O(n \log n)\).
On the other hand, the depth of all the leaves is \(\Omega(\log n)\) and there are \(\Omega(n)\) leaves, therefore the sum is also \(\Omega(n \log n)\).
","Binary search trees,   OTHER"
Multiple choice with explanations,4,"
A ternary tree is a tree in which each node has at most three children.
Given a full and balanced ternary tree containing \( N \) nodes.
The depth of a node is the number of edges between the root and the node (the depth of the root is 0).
The height of a node is the number of edges between the node and the closest leaf (the height of a leaf is 0). Let \( H \) be the sum of the heights of all nodes (including leaves), then:
a. \( H = \theta(N) \)
b. \( H = \theta(N \log(N)) \)
c. \( H = \theta(N^2) \)
d. \( H = \theta(N^2 \log(N)) \)
e. None of the above answers is correct.
",A,"
The calculation is similar to the calculation of the running time of Build-Heap. We obtain a geometric series of the form \(\Theta(n) \times \sum_{i} \left(\frac{i}{3^i}\right)\).
","Binary search trees,   OTHER"
Multiple choice with explanations,7,"
$T(n) = T(n/4) + T(3n/4) + 1$  
A. $T(n) = \theta(n)$  
B. $T(n) = \theta(n^{\log_3{4}})$  
C. $T(n) = \theta(n\log{n})$  
D. $T(n) = \theta(n^{0.75})$  
E. $T(n) = \theta(\log^2{n})$  
F. $T(n) = \theta(\log^{0.75}{n})$
",A,"
Part A.
",Complexity analysis
Multiple choice with explanations,7,"
If \(\log(f(n)) = \theta(\log(g(n)))\) then necessarily:
a. \(f(n) = \theta(g(n + 1))\)
b. \(f(n) = \theta(g(n/2))\) 
c. \(f(n) = \theta(g(n^{0.5}))\)
d. a + b
e. a + b + c
f. None of the above
",F,"
\subsection{Counterexample}
\( f(n) = n! \), \( g(n) = n^n \)
",Complexity analysis
Multiple choice with explanations,7,"
Given a binary search tree that supports order statistic operations. The height of the tree in edges is \( h \) and it contains \( n \) elements. An in-order traversal is performed on the tree, but instead of printing the keys, the value of the Size field is printed.
The maximum value will necessarily be printed in the location:
\begin{enumerate}
    \item First
    \item Last
    \item One of the first \( h \) places.
    \item One of the last \( h \) places.
    \item \( n/2 \)
    \item None of the above.
\end{enumerate}
",F,"
6. The maximum Size value belongs to the root. The root can be of any statistical order, therefore it can appear anywhere in the Inorder printout.
",Binary search trees
Multiple choice with explanations,7,"\noindent
Modify the QuickSort algorithm as follows: the pivot element will be selected by choosing the $n/4$ order statistic, using the Select function. The running time of the new algorithm will be:
\begin{itemize}
    \item[(a)] W.C.- $\Theta(n\log(n))$, Expected - $\Theta(n)$
    \item[(b)] W.C.- $\Theta(n^2)$, Expected - $\Theta(n^2)$ 
    \item[(c)] W.C.- $\Theta(n^2)$, Expected - $\Theta(n\log(n))$
    \item[(d)] W.C.- $\Theta(n\log(n))$, Expected - $\Theta(n\log(n))$
    \item[(e)] W.C.- $\Theta(n^{0.25}\log(n))$, Expected - $\Theta(n^{0.25}\log(n))$ 
    \item[(f)] W.C.- $\Theta(n^2\log(n))$, Expected - $\Theta(n^2\log(n))$
\end{itemize}",D,"The correct answer is D.\\

The runtime will be the same for the worst case and the average case – it will not actually be affected by the input or random choices.
The recurrence formula: \( T(n) = T\left(\frac{3n}{4}\right) + T\left(\frac{n}{4}\right) + \Theta(n) \) (rounded to integers).",Quick-sort
Multiple choice with explanations,3,"
The same data as in the previous question. What is the W.C. runtime of the new program? 

a. \(\theta(\log(n))\)

b. \(\theta(n)\)

c. \(\theta(n\log(n))\)

d. \(\theta(n^2)\)

e. None of the above
",B,"
b. Proof by induction.
","Complexity analysis,   Recursive algorithms"
Multiple choice with explanations,8,"
Solve the following recurrence formula:
\[ T(n) = T(c \cdot n) + T((1-c) \cdot n) + n \]
where \( c \) is a constant, \( 0 < c < 1 \).

\begin{enumerate}
    \item \( T(n) = \theta(n) \)
    \item \( T(n) = \theta(n \log n) \)
    \item \( T(n) = \theta(n^{1.5}) \)
    \item \( T(n) = \theta(n^2) \)
\end{enumerate}
",B,\noindent Proof by induction or by recursion tree.,Complexity analysis
Multiple choice with explanations,8,"What is the asymptotic relationship between the following functions:
If there are multiple correct answers, choose the tightest one.
\( f(n) = \frac{n}{\log(n)}, \quad g(n) = \frac{n^2}{(\log(n))^{2004}} \)
\begin{enumerate}
    \item \( f(n) = O(g(n)) \)
    \item \( f(n) = \Omega(g(n)) \)
    \item \( f(n) = \Theta(g(n)) \)
    \item \( f(n) = o(g(n)) \)
    \item \( f(n) = \omega(g(n)) \)
\end{enumerate}",D,"
For all \( \epsilon > 0 \), it holds that \(\log n = o(n)\).
",Complexity analysis
Multiple choice with explanations,9,"We modify the MergeSort algorithm so that if the array is sorted, the algorithm returns the array and does not make two additional recursive calls. Suppose we run the new algorithm on an array where each element appears exactly \( \frac{n}{\log n} \) times (therefore it contains \(\log n\) different elements). The runtime of the algorithm in the worst case (WC) is:
a. \(\Theta(n)\)  
b. \(\Theta(n \log \log n)\)  
c. \(\Theta(n \log n)\)  
d. \(\Theta(n^2)\)",C,"\noindent For the following input, the running time will be like in the original MergeSort algorithm: An array composed of $n/\log n$ parts, each part with $\log n$ values in descending order.","Complexity analysis,   Lower bound for comparison based sorting"
Multiple choice with explanations,9,"Given an Order-Statistics Tree as defined in class, with the additional field size (i.e., the size of a node x is the number of nodes, including x, in the subtree for which x is the root). In how much time can we print the k nodes in the tree with the largest size field? It is allowed to assume that $k < n^{0.5}$.
A. \(\Theta(n)\).
B. \(\Theta(n \log n)\).
C. \(\Theta(k \log n)\).
D. \(\Theta(k \log k)\).
E. \(\Theta(k)\).",D,"
Similar to printing the $k$ largest elements from a max heap, we will use an auxiliary heap, which will contain the candidates for printing. At each step, we will print the maximum of the auxiliary heap, remove it from the auxiliary heap, and insert its two children from the tree into the auxiliary heap.
",OTHER
Multiple choice with explanations,8,"\begin{quote}
We want a data structure that supports the operations Insert, Delete, and Find.

Two data structures are proposed:
The first supports all operations in W.C. time \( O(n) \) and in Amortized time per operation \( O(1) \).
The second supports all operations in W.C. time \( O(\log n) \) and in Amortized time per operation \( O(\log N) \).

We remind that \( n \) describes the number of elements in the structure, and \( N \) describes the number of operations performed on the structure.

The structure is needed for an algorithm that performs \( N \) operations on the structure. The structure is initialized to an empty state. At the beginning of the algorithm's run, \( N/10 \) insert operations are performed on it.

Which of the two structures is preferable in order to achieve the lowest W.C. time for the algorithm?
\begin{itemize}
    \item [a.] The first.
    \item [b.] The second.
    \item [c.] The answer for \( n = N/2 \) is different from the answer for \( n = N/4 \).
    \item [d.] None of the above answers.
\end{itemize}
\end{quote}",A,The first requires total time $O(N)$. The second requires total time $O(N \log N)$.,Amortized analysis
Multiple choice with explanations,9,"
We are interested in the following problem:
Input: An array of size $n$.
Question: Are there 2 values in the array such that the number of their occurrences together is exactly 2004?

We want an algorithm that will solve the problem in the shortest expected time.
It can be done in expected time:
A. $\Theta(\log(n))$
B. $\Theta(n)$ 
C. $\Theta(n\log(n))$
D. $\Theta(n^2)$
E. None of the above
",B,"\noindent
Insert all elements into a Hash table of size $n$ (collision resolution using linked lists) in expected time $O(n)$. Duplicate values will be stored in the same list element by a counter. \\
The counters are integers between $1$ and $n$, so they can be sorted in an array using counting sort (CountSort) in time $O(n)$. Now, with the help of two indices, one can traverse the array from the beginning and the end, and check if there are two elements in the array whose sum is $2004$. \\",Complexity analysis
Multiple choice with explanations,8,"The recurrence formula for the running time of the Select algorithm (finding any order statistic in an unsorted array) is: \( T(n) \leq T(n/5) + T(7n/10 + c) + O(n) \) (where \( c \) is some constant).

We change the Select operation so that in the first step the division is into triplets instead of quintets. The updated recurrence formula will be:
\begin{enumerate}
    \item \( T(n) \leq T(n/3) + T(n/3 + c') + O(n) \) (where \( c' \) is some constant).
    \item \( T(n) \leq T(n/3) + T(2n/3 + c') + O(n) \) (where \( c' \) is some constant).
    \item \( T(n) \leq T(2n/3) + T(2n/3 + c') + O(n) \) (where \( c' \) is some constant).
    \item \( T(n) \leq T(n/3) + T(5n/6 + c') + O(n) \) (where \( c' \) is some constant).
    \item \( T(n) \leq T(n/3) + T(7n/6 + c') + O(n) \) (where \( c' \) is some constant).
    \item \( T(n) \leq T(n/3) + T(7n/10 + c') + O(n) \) (where \( c' \) is some constant).
\end{enumerate}",B,"
Finding the median of medians of the triplets: \( EMBED \ \text{Equation.DSMT4} \). It is guaranteed that at least (almost) \( n/3 \) of the elements are eliminated for the next step.
",Selection and median-of-median algorithm
Multiple choice with explanations,9,"
What is the height of the decision tree of the HeapSort sorting algorithm on an array of size \( n \)?
A. \( \theta(n) \)
B. \( \theta(n \log n) \)
C. \( \theta(n^2) \)
D. \( \theta(n!) \)
E. \( \theta((n!) \log n) \)
F. The HeapSort sorting algorithm does not fit the comparison model, thus the question is meaningless.
",B,"
b. Because this is the worst case complexity of the number of comparisons in this algorithm.
","Complexity analysis,   Lower bound for comparison based sorting"
Multiple choice with explanations,8,"
In a max-heap, the median is necessarily located:

A. at the root.

B. at a depth of at most $\frac{\log(n)}{2} - 1$.

C. at a depth of at most $\frac{\log(n)}{2} + 1$.

D. in the two lowest layers.

E. none of the above.
",E,"\begin{itemize}
    \item[(e)] The median can be a direct child of the root (for example, if every right subtree is larger than every left subtree), and it can also be in one of the leaves (if the half smallest values are in the leaves).
\end{itemize}",Binary heaps
Multiple choice with explanations,8,"\noindent Let \( T \) be an AVL tree with \( n \) nodes. The height of a node is defined as the length of the longest path (in edges) from it to a leaf. For example, the height of a leaf is 0. Let \( m \) be the node that contains the minimum value in the tree. Then the height of \( m \) in \( T \) is: 

\begin{enumerate}
    \item necessarily \( 0 \).
    \item less than or equal to \( 1 \).
    \item exactly \( 1 \).
    \item less than or equal to \( 2 \).
    \item possibly \(\Theta(\log(n))\).
    \item possibly \(\Theta(n)\).
\end{enumerate}",B,"
b. The minimum has no left child. Therefore, it is a leaf or a node with a right child that must, according to the AVL definition, be a leaf.
","Balanced BST (AVL,   B)"
Multiple choice with explanations,9,"\noindent Given an array $A$ containing $n$ distinct (unsorted) elements. A min-heap $H$ is given, containing the same elements. We want to take the smallest $n^{0.5}$ elements and sort them in the lowest worst-case time. Let $a(n)$ be the cost of performing the operation (with the best algorithm you know) on $A$ and let $h(n)$ be the cost of performing the operation (with the best algorithm you know) on $H$. Then:
\begin{enumerate}
    \item The cost of execution for both algorithms is identical.
    \item The cost of execution for both algorithms is asymptotically the same, but the constant factor for $a(n)$ is lower.
    \item The cost of execution for both algorithms is asymptotically the same, but the constant factor for $h(n)$ is lower.
    \item $a(n) = o(h(n))$.
    \item $h(n) = o(a(n))$.
\end{enumerate}",5,"
5. Perform ORDER STAT on \( A \) and then sort. Cost: \(\Theta(A \log A)\). Perform \(\Theta(H \cdot \log H)\) DELETEMIN on \( H \). Cost: \(\Theta(H \cdot \log H)\).
","Arrays and linked lists,   Binary heaps"
Multiple Choice,8,"\begin{flushleft}
Given an array representing a maximum heap of size \( n \). QuickSort is applied to it. The pivot element at each stage is chosen to be the first in the subarray on which the algorithm is applied. The worst-case runtime is:
\begin{enumerate}
    \item \(\Theta(n)\)
    \item \(\Theta(n \log n)\)
    \item \(\Theta(n^2)\)
    \item \(\Theta(n^2 \log n)\)
    \item \(\Theta(n^3)\)
\end{enumerate}
\end{flushleft}",3,na,"I,   Binary heaps"
Multiple Choice,7,"
Let \( T(n) \) be a function satisfying:
\[ T(n) = T(c \cdot n) + T(d \cdot n) + n \]
where \( c > 0, d > 0 \), and \( c + d < 1 \).

\( T(n) = ? \)

A. \( \theta(\log(n)) \).

B. \( \theta(\max\{n^c, n^d\}) \).

C. \( \theta(n) \).

D. \( \theta(n\log(n)) \).

E. \( \theta(n^{1/(c+d)}) \).

F. None of the above.
",3,na,Complexity analysis
Multiple Choice,7,"
Given a binary search tree (not necessarily balanced) of size \( n \) and height \( h \), where each node has a field \( Su \) pointing to the successor, but this field is incorrect. We want to fix the tree, i.e., correct the \( Su \) field of all the nodes in minimal worst-case time. It can be done in:

A. \( \Theta(\log(n)) \) \\
B. \( \Theta(n) \) \\
C. \( \Theta(n\log(n)) \) \\
D. \( \Theta(n^2) \) \\
E. \( \Theta(nh^2) \) \\
F. None of the above
",2,na,Binary search trees
Multiple Choice,7,"
Claim: If \( f(n) = O(g(n)) \) then \( \sqrt{f(n)} = O(\sqrt{g(n)}) \).

a. The claim is always false

b. The claim is always true

c. The claim is sometimes true and sometimes false
",2,na,Complexity analysis
Multiple Choice,7,"
Claim: If we traverse a minimum heap (Min-Heap) in which all elements are distinct using LRD traversal (post-order), we will obtain a decreasing sequence of heap elements.

A. The claim is always false.

B. The claim is always true.

C. The claim is sometimes true and sometimes false.
",3,na,Binary heaps
Multiple choice with explanations,8,"
The recurrence formula \( T(n) = T(n/2) + T(n/2) + \theta(n) \) describes the W.C. running time of the following algorithms:
Note: For a specific algorithm, it is not enough that the complexity matches the solution of the recurrence formula. The recurrence formula must also fit the operation of the algorithm.
\begin{enumerate}
    \item QuickSort
    \item QuickSort where the pivot element is chosen to be the median using Select.
    \item Sorting by insertion into an AVL search tree and scanning the tree at the end.
    \item MergeSort
    \item b + d
    \item b + c + d
\end{enumerate}
",E,"
5. 
The recurrence formula describes both MergeSort and QuickSort, provided that the pivot element is chosen to be the median in linear time (for example, using Select). In standard QuickSort, the partitioning can be unbalanced, so the above recurrence formula is not suitable. Sorting using an AVL tree indeed takes \(O(n \log n)\), which is the solution of \(T\), but \(T\) does not fit to describe the runtime (for example, because it is not a recursive algorithm).
","Balanced BST (AVL,   B),   Quick-sort"
Multiple choice with explanations,8,"
Given a 2-3 search tree where the root has exactly two children: left and middle. Let \( X \) denote the number of nodes in the middle subtree and \( Y \) denote the number of nodes in the left subtree. Which of the following is necessarily true? (If more than one answer is correct, choose the tightest one).

\begin{itemize}
    \item[A.] \( X = Y \)
    \item[B.] \( X \leq 1.5Y \)
    \item[C.] \( X \leq 2Y \)
    \item[D.] \( X \leq 6Y \)
    \item[E.] \( X \leq 2006Y \)
    \item[F.] None of the above.
\end{itemize}
",F,"\textbf{6.}\\
It is possible for one subtree of each node to have exactly two children and the other subtree to have exactly three children. Therefore, if the height of the tree is $h$, then in one subtree the number of nodes is $2^h - 1$ and in the other $3^h - 1$. That is, in the first there are $2^h - 1$ nodes, and in the second more than $2^h - 1$ nodes.

","Balanced BST (AVL,   B)"
Multiple choice with explanations,8,"
Given a binary search tree (not necessarily balanced) with \( n \) nodes, where each node has two additional fields: Pred, Suc, which point to the predecessor and successor of the node. We want to implement the Delete operation on this tree in the best possible W.C. time, such that after the deletion all the fields in the tree are correct. The delete operation receives a pointer to the element intended for deletion. It is allowed to assume that all the regular fields exist, such as \texttt{Parent[x]}. The resulting time is:
\begin{enumerate}
    \item \(\Theta(1)\)
    \item \(\Theta(\log(n))\) independent of the height of the tree.
    \item \(\Theta(n)\) independent of the height of the tree.
    \item \(\Theta(h)\) where \( h \) is the height of the tree.
    \item \(\Theta(n\log(n))\)
    \item It is impossible to perform deletion on such a tree, because it is impossible to determine with certainty the new value for the Pred and Suc fields.
\end{enumerate}
",A,"
Let \( x \) be the deleted element. Remember its predecessor and successor in order to update their mutual pointers after deleting \( x \). If \( x \) has a single child or is a leaf – delete it in \( O(1) \) time as in a regular deletion. If \( x \) has two children – find its successor using the field \texttt{Suc} in \( O(\log n) \) time, and proceed as in a regular deletion: swap and delete in the new location. Total time - \( O(\log n) \).
","Complexity analysis,   Binary search trees"
Multiple choice with explanations,8,"\begin{itemize}
    \item What is the data structure with the worst-case search time?
    \item A. A hash table with collision resolution by linked lists.
    \item B. Perfect Hash
    \item C. A max heap implemented by an array.
    \item D. A+B
    \item E. A+C
    \item F. A+B+C
\end{itemize}",E,"\begin{itemize}
    \item[5.] The search time in the worst case for a hash table and the heap is $O(\log n)$. For a perfect hash, the search time is $O(1)$ in the worst case.
\end{itemize}","Binary search trees,   Balanced BST (AVL,   B),   Hash tables"
Multiple Choice,7,"
Let \( f(n) = (n!)^{2006} \),
\( g(n) = (n^{2006})! \).
We wish to compare the \(\log\) of the two functions. Which of the following is correct?

A. \(\log(f(n)) = o(\log(g(n)))\)

B. \(\log(f(n)) = \omega(\log(g(n)))\)

C. \(\log(f(n)) = \Theta(\log(g(n)))\)

D. None of the above.
",1,na,Complexity analysis
Multiple Choice,7,"
Solve the following recurrence relation:
\begin{equation}
T(n) = 3 \cdot T(n - 2) + 9
\end{equation}
\( T(1) = T(2) = 1 \)

What is \( T(n) \)?

a. \( \theta(n^3) \)

b. \( \theta(n^2) \)

c. \( \theta(3^n) \)

d. \( \theta(n^{1.5}) \)

e. \( \theta((3^{0.5})^n) \)
",5,na,Complexity analysis
Multiple Choice,8,"\textit{Given an empty AVL tree. We perform \( n \) Insert operations on it. Which of the following is true?}

\begin{enumerate}
    \item The total running time will be \(\theta(n)\) and the number of rotations is necessarily \(O(n)\).
    \item The total running time will be \(\theta(n \log n)\) and the number of rotations is necessarily \(O(n)\).
    \item The total running time will be \(\theta(n)\) and the number of rotations can be \(\Omega(n \log n)\).
    \item The total running time will be \(\theta(n \log n)\) and the number of rotations can be \(\Omega(n \log n)\).
\end{enumerate}",2,na,"Complexity analysis,   Balanced BST (AVL,   B)"
Multiple Choice,8,"
Given an AVL search tree that supports Order Statistic operations, meaning each node also has the Size field. An LRD (postorder) traversal is performed on the tree, but instead of printing the keys, the Size fields are printed. Which of the following is correct?
A. The first number printed is the smallest.
B. The last number printed is the largest.
C. The numbers will be printed in monotonically increasing order.
D. A+B
E. A+B+C
",4,na,"Balanced BST (AVL,   B)"
Multiple Choice,8,"
We are interested in the QuickSort sorting algorithm when the pivot element is chosen to be the first in the segment of the array on which the algorithm operates (and not chosen randomly).
What is the smallest depth of a leaf in the decision tree of this algorithm on an array of size \( n \)?
a. \( \theta(n) \) \\
b. \( \theta(n \log(n)) \) \\
c. \( \theta(n^2) \) \\
d. \( \theta(n!) \) \\
e. \( \theta((n!) \log(n)) \) \\
f. The above algorithm does not fit the comparison model, so the question is meaningless.
",2,na,"I,   Lower bound for comparison based sorting"
Multiple Choice,8,"
The median, using the function NewSelect. The running time of NewSelect on an array of size \(n\) is \(\Theta(n^{2006/2005})\) W.C.. The running time of the new QuickSort algorithm will be:
a. \(\Theta(n)\) W.C.
b. \(\Theta(n \log(n))\) W.C.
c. \(\Theta(n^2)\) W.C.
d. \(\Theta(n^{2006/2005})\) W.C.
e. \(\Theta(n^{2006/2005} \log(n))\) W.C.
f. \(\Theta(n^{2 \times 2006/2005})\) W.C.
",4,na,"Complexity analysis,   Binomial heaps,   Fibonacci heaps"
Multiple Choice,8,"\begin{itemize}
    \item[] Given \( n \) elements. We want to build a data structure from them. Which of the following structures will have the longest W.C. build time?
    \begin{enumerate}
        \item A hash table where collisions are resolved using linked lists.
        \item Perfect hash.
        \item Max-heap.
        \item AVL search tree.
        \item A+C
        \item A+B+C
    \end{enumerate}
\end{itemize}",2,na,"Binary search trees,   Balanced BST (AVL,   B),   Hash tables"
Multiple choice with explanations,9,"
Given a red-black search tree where at each node \( v \) there is a field \texttt{Size} containing the number of nodes in the subtree rooted at \( v \) (including \( v \)). A post-order traversal (LRD) is performed on the tree, but instead of printing the keys, the \texttt{Size} fields are printed. 
(Reminder: A post-order traversal is a traversal where first a recursive post-order traversal is performed on the left subtree, then the right subtree, and finally the value at the node itself is printed.)

The question: Which of the following is true?
A. The first number printed is the smallest.
B. The last number printed is the largest.
C. The numbers are printed in monotonically increasing order.
D. The sum of the second-to-last number printed and the third-to-last number printed is one less than the last number printed.
E. A + B
F. A + B + C
",E,"
a. True because the first node doesn't have a son (i.e., it is a leaf), otherwise its son would have been traversed before it.

b. True because a parent is traversed after its children, thus the last node to be traversed must be an ""orphan"", that is, the root of the tree.

c. and d. are not true, for example, if we take a complete tree of height 2.
",OTHER
Multiple choice with explanations,9,"
We run the QuickSort algorithm. At each stage, the pivot element is chosen to be the median. However, the median is found by a new function called NewSelect, whose worst-case running time on an array of size \(n\) is \(O(n^{1.5})\). The worst-case running time of the QuickSort algorithm we defined will be:
\begin{enumerate}
    \item \(O(n)\)
    \item \(O(n \log(n))\)
    \item \(O(n^2)\)
    \item \(O(n^{1.5})\)
    \item \(O(n^{1.5} \log(n))\)
    \item \(O(n^3)\)
\end{enumerate}
",D,"
The total running time is \( T(n) = 2T(n/2) + n^{3/2} \), and according to the master method, for example, we get \(\mathcal{D}\).
","Complexity analysis,   Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,9,"\begin{quote}
Given \( n \) numbers. We want to build a data structure from them. Which of the following structures will have the longest worst-case build time?
\begin{enumerate}
    \item A hash table where collisions are resolved using linked lists.
    \item Perfect Hash
    \item Max Heap
    \item AVL Search Tree
    \item 1 + 3
    \item 1 + 2 + 3
\end{enumerate}
\end{quote}",B,"b.
Balanced search tree requires a build time of $O(n\log n)$.
Data structures A and C require a build time of $O(n)$.
However, Perfect Hash may require unbounded time: each time, we randomly choose a hash function and test it, and may fail an unlimited number of times. Therefore, the worst-case build time is unbounded.","Binary search trees,   Balanced BST (AVL,   B),   Hash tables,   Binary heaps"
Multiple choice with explanations,7,"\text{Given a max-heap of size } n. \text{ The keys of } $\frac{n}{4}$  of the leaves in the heap have been changed, so now the heap may no longer be valid. We want to make the heap valid with minimal worst-case running time. Assume there is access to the elements of the heap (i.e., the heap is not a black box). The worst-case running time will be: \\
a. \ $\theta(\log(n))$ \\
b. \ $\theta(\log(n)^2)$ \\
c. \ $\theta(n)$ \\
d. \ $\theta(n\log(n)$) \\
e. \ $\theta(n^3)$ \\
f. \ \text{None of the above.}",C,"
The time required to access each of the nodes in our list is \( O(n) \), therefore a faster solution is not possible. In \( O(n \log n) \) time, it is possible to perform BuildHeap again on all the elements.
","Complexity analysis,   Binary heaps"
Multiple choice with explanations,7,"
We perform an LDR scan on a binary search tree of size \( n \) (not necessarily balanced), but at each step, instead of printing the key of the current node \( x \), we print the key of its parent. If \( x \) is the root of the tree, we do not print at that step. Which of the following is true?
A. Every key in the tree, except for the keys of the leaves and the root, is printed twice. \\
B. The values that are printed are in monotonically increasing order. \\
C. The number of keys that are printed twice is \( \Theta(n) \) for every tree. \\
D. A + C \\
E. A + B + C \\
F. None of the above.
",F,"
a. False, because any node that has a single child will be printed only once. b. False, for example the tree with root 3, left child 1 and right child of the left child 2. It will print 3 first and then 1. c. False, for example for a tree where each node is either a leaf or has a single child.
",Binary search trees
Multiple choice with explanations,7,"
Given a data structure \( A \) containing \( n \) keys. We want to find a pointer to an element whose key is the median of the keys, in the fastest worst-case time. For this purpose, it is preferable that \( A \) be:

a. A hash table of size \( n \) with collision resolution using 2-3 trees.

b. A doubly linked list sorted in ascending order.

c. An AVL tree with support for order-statistic operations.

d. A hash table of size \( n \) with collision resolution using doubly linked lists, and it is known that the values are the integers from 1 to \( n \).

e. a + d

f. b + c
",C,"The correct answer is c.

a – We need to scan all elements in the trees in the table, hence it will take \(O(n)\).

b – To reach the element in the middle of the list, we need to traverse half of it, so it will take \(O(n/2) = O(n)\).

c – Finding the order statistic \(n/2\) can be done in \(O(n)\).

d – The median is the \(n/2\)-th number. Finding the cell takes \(O(n/2) = O(n)\). Searching in the list takes \(O(n)\). Total \(O(n)\).","Binary search trees,   Balanced BST (AVL,   B),   Hash tables"
Multiple choice with explanations,7,"\text{What is the lower bound in the comparison model of the worst-case runtime of a sorting algorithm, if it is known that }$ n^{0.5}$ \text{ of the smaller elements are already in the correct place?}
\begin{enumerate}
    \item $\theta(\log(n))$
    \item $\theta(n\log(n))$
    \item $\theta(n)$
    \item $\theta(n^{1.5})$
    \item $\theta(n\sqrt{\log(n)})$
    \item \text{None of the above.}
\end{enumerate}",B,"
b. We still need to sort an array of size \( k \). Sorting an array of size \( k \) under the comparison model takes \( O(k \log k) \). But \( O(k \log k) \)
","Complexity analysis,   Lower bound for comparison based sorting"
Amortized analysis,na,"Given a node \( v \) in a red-black tree. \( P_1 \) and \( P_2 \) are the lengths of the shortest path and longest path from \( v \) to a leaf, respectively. Provide a lower bound on the ratio between \( P_1 \) and \( P_2 \). Justify your answer.",na,\textit{Maximum twice the length because there is the same black length and no two consecutive reds},"Balanced BST (AVL,   B)"
Recursive algorithms,9,"We will discuss the implementation of a red-black tree where all the leaves are black (in the CORMEN implementation as taught in class, there is a layer of NILs that serves as the 'leaf layer' and is entirely black). What is the minimum ratio of black nodes to red nodes in a valid red-black tree?

\begin{itemize}
    \item[(A)] 1 black to 1 red.
    \item[(B)] 2 blacks to 1 red.
    \item[(C)] 5 blacks to 2 reds.
    \item[(D)] 1 black to 2 reds.
    \item[(E)] The ratio can be as small as we want.
\end{itemize}
",b,"\text{2 blacks for 1 red. Each red node in the tree has two children, both black thanks to the NIL layer.}","Balanced BST (AVL,   B)"
Recursive algorithms,9,"\textbf{In an open hash table (chaining), we replace the linked lists of elements in each bucket with closed hash tables. The worst-case runtime for an operation on the hash table, where \( N \) is the number of elements and \( B \) is the number of buckets, is:}

\begin{enumerate}
    \item \( O(1) \)
    \item \( O(\log N) \)
    \item \( O(N) \)
    \item \( O(N \log N) \)
    \item \( O(B/N) \)
\end{enumerate}

\textbf{Brief justification:}",c,"\textit{O(N), in the worst case the elements will collide in two hash tables.}",Hash tables
Recursive algorithms,9,"\text{Given an algorithm that receives an array of size } n \text{ of different unsorted numbers, along with an integer } k \text{ between } 1 \text{ and } n\text{. The algorithm prints the } k \text{ smallest numbers in the array in ascending order. A lower bound on the worst-case runtime of the algorithm (valid for any } k \leq n\text{) is:}
\text{a. } \Omega(n)
\text{b. } \Omega(k \log n)
\text{c. } \Omega(n \log k)
\text{d. } \Omega(n \log n)
\text{e. Answers a and b are correct}

\text{Reason:} ",e,"\text{The answer of the Lord is correct.} \\
\text{A is correct because it is clear that the algorithm must read the entire input in time } \Omega(n), \text{ because there is no way to know whether a number needs to be printed or not without reading it.} \\
\text{B is correct: by contradiction, suppose there exists an algorithm } A \text{ that solves the problem in time } o(k \log n) \text{ for some value of } k. \text{ According to A, it must be } k=\Omega(n/\log n), \text{ otherwise the algorithm } A \text{ does not read the entire input. We proved in class that sorting } k \text{ elements takes } \Omega(k \log k)=\Omega(k \log(n/\log n))=\Omega(k \log n) \text{ time, contradicting the assumption.} \\
\text{C is not correct: it is possible to find the } k\text{-th largest element using Select in time } O(n), \text{ extract the } k-1 \text{ smaller elements in time } O(n), \text{ and then sort the } k \text{ elements in time } O(k\log k). \text{ Total time is } O(n+k\log k). \text{ For, say, } k=n/\log n \text{ the running time is thus } o(n\log n) \text{ i.e., } O(n). \\
\text{D is not correct for the same reason that C is not correct.}",Lower bound for comparison based sorting
Amortized analysis,na,"\text{Given an array } A[1..n] \text{ containing } n \text{ elements, all different from each other. A pair of indices } (i, j) \text{ is called an inversion if } i < j \text{ and } A[i] > A[j].

\text{Assume that the set of numbers is } \{1, 2, \ldots, n\}. \text{ Which array contains the highest number of inversions? How many inversions does it contain?}",na,"\textit{In a reversely sorted array, each such pair is an inversion. The number of such pairs is } $\theta(n^2)$.",Arrays and linked lists
Amortized analysis,na,"Suppose that \( A[6] \) contains the smallest element in the array (\( A[6]=1 \) in the example above), and \( A[4] \) contains the second largest element (\( A[4]=2 \) in the example above). How many inversions \( (i,j) \) exist such that \( j=4 \) or \( j=6 \)?",na,3+5=8,Arrays and linked lists
Amortized analysis,na,"The keys \( k_1, k_2, \ldots, k_n \) are inserted in a certain order into a hash table of size \( m \) using linear probing with the hash function \( h(x) \). After the insertion, the \( i \)-th position is empty. Is there a different order of key insertion where the \( i \)-th position in the table would be occupied?

If so, provide an example where such a situation occurs. If not, prove that such a situation is not possible.",na,"\textit{Assume by contradiction that there is an insertion order in which cell $i$ is occupied (in the original it was empty). There is no element mapped directly to cell $i$, otherwise, the cell would have been occupied in the original insertion. This means there is a contiguous segment immediately before position $i$ of length $x$, to which at least $x+1$ elements were mapped (otherwise, there would not have been overflow to position $i$). Those $x+1$ elements were mapped to the same locations in the original insertion, and they are $x$ cells. Each element occupies its own cell, it is impossible to place $x+1$ elements in $x$ cells, and hence the contradiction.}",Hash tables
Amortized analysis,10,"\textit{We insert into a binomial heap (initially empty) the sequence of numbers 1, 2, \ldots, n in ascending order. For two binomial trees in the heap created, with ranks i, j such that i < j, what is the relation between all the values in the tree of rank i and all the values in the tree of rank j? Explain.}",na,"All the nodes in the tree with rank \( j \) were inserted before all the nodes in the tree with rank \( i \). Therefore, the nodes in the tree with rank \( j \) are smaller than all the nodes in the tree with rank \( i \).","Binomial heaps,   Fibonacci heaps"
Amortized analysis,na,"\documentclass{article}
\usepackage[utf8]{inputenc}
\begin{document}

In this problem, we will sort strings in lexicographic order (dictionary order). A string is a list of characters over a known alphabet. For the sake of simplicity, assume all strings have the same length $t$. A string $s_1$ comes before a string $s_2$ in order if the first character in which they differ is smaller in $s_1$ than in $s_2$. For example, if $s_1=\text{CGTAA}$ and $s_2=\text{CGTCA}$ then $s_1$ comes before $s_2$. Assume that each character in every string can be accessed in constant time. Denote $n$ as the number of strings, and $m$ as the total length of all strings. Note that $m=n \cdot t$. Assume that the strings are given in an array $A$, that is $A[i]$ is a pointer to the $i+1$ string in the array. (Indices in $A$ start from 0.)

The following algorithm, which uses the routines $\text{find-median-char}$ and $\text{partition}$, sorts the strings at locations $k_1,k_1+1,\ldots,k_2$ in array $A$ according to the lexicographic order of the suffixes of these strings starting at position $j$, using comparisons between pairs of characters only. (The suffix of a string consists of the string's characters starting from a certain position $j$ to the end of the string.) To sort all the strings lexicographically, perform $\text{string-sort}(0,n-1,0)$. (The indices of the characters in the strings also start from 0.)

\begin{verbatim}
string-sort(k1,k2,j):
   C \leftarrow \text{find-median-char}(k1,k2,j)
   (p,q) \leftarrow \text{partition}(k1,k2,C,j)
   \text{If} (k1 < p) \text{string-sort}(k1,p-1,j);
   \text{If} (q < k2) \text{string-sort}(q+1,k2,j);
   \text{If} (j < t-1) \text{and} (p < q) \text{string-sort}(p,q,j+1);
\end{verbatim}

The routine $\text{find-median-char}(k1,k2,j)$ returns the median character in the set of characters at position $j$ of the strings at locations $k_1,\ldots,k_2$ in $A$. 

What is the running time of the algorithm as a function of $m$ and $n$, assuming that $\text{find-median-char}(k1,k2,j)$ and $\text{partition}(k1,k2,C,j)$ run in time $O(k_2-k_1)$? Prove your answer.

\end{document}",na,"\begin{quote}
The first two lines require time proportional to the size of the array we are currently dealing with. The first two recursive calls operate on an array whose size is at most half of the original size. The last line advances the letter \( j \) within the strings from \( p \) to \( q \); it is called up to \( t \) times for each string. From an analysis similar to Quicksort, plus \( O(nt) \) work, we get \( O(m+n\log n) \).
\end{quote}","Selection and median-of-median algorithm,   Recursive algorithms,   Quick-sort"
Amortized analysis,na,"\textit{A tournament containing \( n \) elements is a perfect binary tree of depth \( i \) where \( i = \lceil \log_2(n) \rceil \) (at distance \( k \) from the root there are exactly \( 2^k \) nodes). The elements are placed at the leaves. In each internal node, we place the smallest of the elements located in the children of that node. A leaf that does not contain a real element contains a dummy element denoted by \( \infty \), which is greater than any real element. We represent the tree using an array like a binary heap, where the children of the node represented by cell \( i \) are represented by the cells \( 2i+1 \) and \( 2i+2 \).

Given \( n \) elements, we construct a tournament containing them as follows. We allocate an array of appropriate size and initialize it entirely with the value \( \infty \). Then we insert the elements one by one. To add an element to the tournament, we place it in the next available leaf from left to right and update, if necessary, the elements located in the ancestors of that leaf.}

\textit{What is the worst-case runtime of this algorithm?}",na,"\text{For each element, we ascend in the worst case up to the root.} \\
\text{Therefore, the running time is } O(n\log n).","Complexity analysis,   Binary search trees"
Amortized analysis,na,"\[
\text{A tournament containing } n \text{ elements is a perfect binary tree at depth } i \text{ where } i = \lceil \log_2(n) \rceil \text{ (at distance } k \text{ from the root, there are exactly } 2^k \text{ nodes). The elements are placed at the leaves. In each internal node, we place the smallest of the elements located in the children of that node. A leaf that does not contain a real element contains an imaginary element denoted by } \infty \text{, which is larger than any real element.}
\]

\[
\text{We represent the tree with an array like a binary heap, where the children of the node represented by cell } i \text{ are represented by cells } 2i+1 \text{ and } 2i+2.
\]

\[
\text{Given } n \text{ elements, we construct a tournament containing them as follows. We allocate an array of suitable size and initialize it entirely with the value } \infty. \text{ We then insert the elements one by one. To add an element to the tournament, we place it at the next free leaf from left to right and update, if necessary, the elements located at the ancestors of that leaf.}
\]

\[
\text{Now assume that we do not know in advance the number of elements } n \text{ in the tournament, and they are given to us one by one. Since the size of the tournament is not known, we initially allocate an array of size 3 suitable for a tournament with at most 2 elements. If more than 2 elements arrive, we allocate an array of size 7 suitable for a tournament with at most 4 elements, copy the first two elements into it, and continue inserting the subsequent elements. We will continue in this manner; after more than } 2^i \text{ elements arrive, we will have a tournament suitable for at most } 2^{i+1} \text{ elements into which we will insert the next elements. Once more than } 2^{i+1} \text{ elements arrive, we will allocate a tournament double in size, copy the already inserted elements into it, and continue to insert the subsequent elements. Each element's insertion will be performed by the algorithm described above. What is the complexity of each insertion, in the worst-case and amortized?}
\]",na,"\textit{In the worst case, we will need to double the array and adjust its contents accordingly; the running time for this operation will be \(O(n)\). Under amortized analysis, we can charge the running time of doubling the array to the preceding operations that did not involve doubling the array, similar to the array doubling analysis taught in class. However, it is still possible that each element will need to percolate up to the root, resulting in an amortized running time per operation of \(O(\log n)\).}","Complexity analysis,   Binary search trees"
Amortized analysis,na,"\begin{itemize}
    \item In this question, we analyze a new deterministic algorithm for the Selection problem. The algorithm selects the k-th largest element from a set A consisting of n different elements.
    \item If \( n < 100 \), the algorithm sorts the n elements and returns the k-th largest element.
    \item Otherwise, the algorithm divides the elements into quartets. The algorithm sorts each quartet and selects the third largest element from it. (If the quartet is \( a<b<c<d \), then the selected element is \( c \).) We denote the selected elements as set B.
    \item Among the \( m = n/4 \) elements of B, the algorithm selects, via a recursive call, the \((2/5)m\)-th largest element. We denote this element as \( x \). (\( x \) is greater than or equal to \((2/5)m\) elements of \( B \).)
    \item The element \( x \) is now compared with all elements of \( A \). We denote by \( A_1 \) the elements of \( A \) that are less than or equal to \( x \), and by \( A_2 \) the elements of \( A \) that are greater than \( x \). We also denote \( r = |A_1| \).
    \item If \( k \leq r \), the algorithm finds, via a recursive call, the k-th largest element in \( A_1 \) and returns it. Otherwise, the algorithm finds, via a recursive call, the \((k-r)\)-th largest element in \( A_2 \) and returns it.
    \item We can ignore partition problems. (It's convenient to assume that \( n \) is divisible by 4, that \( m \) is divisible by 5, and so on.)
    \item What is the largest \( a \) for which it always holds that \( r \geq an + O(1) \)? Prove your answer.
\end{itemize}",na,\(\frac{2}{5} \times \frac{3}{4} = \frac{3}{10}\),Selection and median-of-median algorithm
Amortized analysis,na,"\documentclass{article}
\begin{document}

In this question, we analyze a new deterministic algorithm for the Selection problem. The algorithm selects the k-th largest element from a set $A$ with $n$ distinct elements.

If $n<100$, the algorithm sorts the $n$ elements and returns the k-th largest element.

Otherwise, the algorithm divides the elements into quartets. The algorithm sorts each quartet and chooses from it the third largest element. (If the quartet is $a<b<c<d$, then the chosen element is $c$.) Let $B$ be the set of chosen elements.

Among the $m=n/4$ elements of $B$, the algorithm selects, by a recursive call, the $(2/5)m$-th largest element. Let this element be $x$. ($x$ is greater than or equal to $(2/5)m$ of the elements of $B$.)

The element $x$ is now compared to all the elements of $A$. Let $A_1$ be the elements of $A$ that are less than or equal to $x$, and $A_2$ be the elements of $A$ that are greater than $x$. Also, let $r=|A_1|$.

If $k\leq r$, the algorithm finds, by a recursive call, the k-th largest element in $A_1$ and returns it. Otherwise, the algorithm finds, by a recursive call, the $(k−r)$-th largest element in $A_2$ and returns it.

You can ignore partition issues. (It is possible, when convenient, to assume that $n$ is divisible by 4, that $m$ is divisible by 5, and so on.)
What is the smallest $b$ such that $r\leq bn+O(1)$ always holds? Prove your answer.

\end{document}",na,\( 1 - \frac{3}{5} \cdot \frac{2}{4} = \frac{7}{10} \),Selection and median-of-median algorithm
Amortized analysis,na,"
In this question, we will analyze a new deterministic algorithm for the selection problem. The algorithm selects the k-th largest element from a set \( A \) with \( n \) distinct elements.

If \( n < 100 \), the algorithm sorts the \( n \) elements and returns the k-th largest element.

Otherwise, the algorithm divides the elements into quartets. The algorithm sorts each quartet and selects the third largest element from it. (If the quartet is \( a < b < c < d \), then the selected element is \( c \).) We denote by \( B \) the set of selected elements.

Among the \( m = n/4 \) elements of \( B \), the algorithm selects, through a recursive call, the \((2/5)m\)-th largest element. We denote this element by \( x \). (\( x \) is greater than or equal to \((2/5)m\) elements of \( B \).)

The element \( x \) is now compared to all elements of \( A \). We denote by \( A_1 \) the elements of \( A \) that are less than or equal to \( x \), and by \( A_2 \) the elements of \( A \) that are greater than \( x \). We also denote \( r = |A_1| \).

If \( k \leq r \), the algorithm finds, through a recursive call, the k-th largest element in \( A_1 \) and returns it. Otherwise, the algorithm finds, through a recursive call, the \((k−r)\)-th largest element in \( A_2 \) and returns it.

We can ignore partitioning problems. (It is possible, when convenient, to assume that \( n \) is divisible by 4, that \( m \) is divisible by 5, and so on.)
Write a recurrence relation for the running time of the algorithm on a set with \( n \) elements.",na,"\text{The runtime consists of linear work on the array, a recursive call on B of size } n/4, \text{ and a recursive call on A1 or A2 whose size we have bounded in the previous sections. In total, } T(n)=O(n)+T(n/4)+T(7n/10).","Selection and median-of-median algorithm,   Recursive algorithms,   Complexity analysis"
Amortized analysis,na,"\documentclass{article}
\begin{document}

In this question, we will analyze a new deterministic algorithm for the Selection problem. The algorithm selects the k-th largest element from a set A of n different elements.

If \( n < 100 \), the algorithm sorts the n elements and returns the k-th largest element.

Otherwise, the algorithm divides the elements into quartets. The algorithm sorts each quartet and selects from it the third largest element. (If the quartet is \( a < b < c < d \), then the selected element is \( c \).) Let B denote the set of selected elements.

Among the \( m = n/4 \) elements of B, the algorithm selects, by a recursive call, the (2/5)m-largest element. Let this element be denoted by x. (x is greater than or equal to \((2/5)m\) of the elements of B.)

The element x is now compared to all the elements of A. Let \( A_1 \) denote the elements of A that are less than or equal to x, and let \( A_2 \) denote the elements of A that are greater than x. Also denote \( r = |A_1| \).

If \( k \leq r \), the algorithm finds, by a recursive call, the k-th largest element in \( A_1 \) and returns it. Otherwise, the algorithm finds, by a recursive call, the (k-r)-th largest element in \( A_2 \) and returns it.

Division issues can be ignored. (It is possible to assume for convenience that n is divisible by 4, m is divisible by 5, and so on.)
What is the runtime of the algorithm (as a function of n)? (An asymptotic answer in terms of \( O(.) \) is sufficient.)

\end{document}",na,"\text{Since } \frac{1}{4} + \frac{7}{10} < 1, \text{ the analysis is similar to that of the selection algorithm - } O(n).","Selection and median-of-median algorithm,   Recursive algorithms,   Complexity analysis"
Recursive algorithms,9,"\textbf{Given} an input consisting of $n$ distinct arbitrary integers, we are interested in sorting it. \textbf{It is also given} that the number of inversion pairs, denoted by $\frac{n(n-1)}{2}$, the number of number pairs that are in reverse relative order, is $I = \frac{n(n-1)}{2} - 5n$. A pair of elements $a_i$ and $a_j$ is inverted if $a_i < a_j$ and $i > j$.

Choose the most correct answer (it is possible that among different options, the strongest correct one should be chosen).

a. A lower bound on the sorting time of the input is $\Omega(n \log n)$.
b. The input can be sorted in $O(n)$ time.
c. The input can be sorted in $O(n \log \log n)$ time.
d. The input can be sorted in $O(n \log(n))$ time.
e. Answers a and d are correct.
f. No answer is correct.",b,"\text{The answer is B. We will reverse the array, and we will find that the number of arrangements and the inverse arrangements have swapped. That means, currently the number of pairs in reverse order is } 5n. \text{ We will sort using finger trees and get } O(n + n \log(m/n)) = O(n + n \log 5) = O(n).",Lower bound for comparison based sorting
Recursive algorithms,6,"\textit{Suppose that the data structure of SUFFIX TREE is updated so that the LABEL of each edge is only one character (that is, instead of an edge with the label ""ck ... c1"" there will be k edges with the labels ""c1"", ""c2"", ..., ""ck""). And it is allowed for an internal node to have one child. What is the worst-case space complexity for this data structure for strings of length n?}

A. \(O(\log n)\)

B. \(O(n)\)

C. \(O(n \log n)\)

D. \(O(n^2)\)

E. \(O(n^3)\)

F. None of the answers are correct",d,"\text{The answer is D. The worst-case scenario can occur when no ""letter"" repeats twice, and it is required that the alphabet be larger than } n. \text{ In this case, we will get a star tree shape around the root, where each edge coming out of the root points to a leaf representing a different and unique substring. Therefore, at least } O(n(n+1)/2)=O(n^2) \text{ space is required.}",Suffix trees
Recursive algorithms,9,"\[
Let \, T(n) \, be \, the \, recurrence \, relation \, T(n) = aT(n/a) + f(n) \\
where \, a \, is \, an \, integer \, greater \, than \, 1, \, and \, f(n) \, satisfies \, f(n) = \Theta(n(\log^k(n))) \, for \, an \, integer \, k \, greater \, than \, 1. \, Find \, a \, tight \, bound \, for \, T(n).
\]

\begin{enumerate}
    \item \Theta (n \log n)
    \item \Theta (n \log^k \log n)
    \item \Theta (n \log n / \log k)
    \item \Theta (n (\log^k n))
    \item \Theta (n (\log^{(k+1)} n))
    \item None \, of \, the \, above \, is \, correct
\end{enumerate}",d,"The answer is D.  
According to case 3 of the father's method.","Complexity analysis,   Recursive algorithms"
Amortized analysis,na,"In the string $S$, the letter $A$ appears one million times, the letter $B$ 100,000 times, and the letter $C$ 1,000 times. Draw the Huffman tree of the string and calculate the encoding length of the string according to this tree.",na,"The encoding length is equal to:  
\(10^6 + 2 \times 10^5 + 2 \times 10^3 = 1202000\)",OTHER
Amortized analysis,na,"Mapping \( n \) elements using a universal hash function family to a table with \( m \) cells while resolving collisions by chaining. What is the expected number of pairs \((x, y)\) such that \(x\) and \(y\) are elements of the input, and both map to the same cell?",na,"Answer: \(\Theta(\sqrt[3]{n})\). We number the elements from 1 to \(n\). Let \(X_{i,j}\) be a random variable that equals 1 if element number \(i\) collides with element number \(j\), and equals 0 otherwise. Let \(X\) be the sum of these variables. Then \(X\) equals the number of collisions. According to the linearity of expectation, \( \mathbb{E}[X] = \sum_{i<j} \mathbb{E}[X_{i,j}] \). The probability that two specific elements collide is \( \frac{1}{\text{number of slots}} \). Therefore, \( \mathbb{E}[X_{i,j}] = \frac{1}{m} \) and thus \( \mathbb{E}[X] = \binom{n}{2} \cdot \frac{1}{m} \).",Hash tables
Arrays and linked lists,10,"\begin{itemize}
\item Given the following recurrence formula:
\[ T(N) = 
\begin{cases} 
8T(\lfloor n/4 \rfloor) & \text{if } n>1 \\
1 & \text{if } n=1 
\end{cases}
\]

A student claimed that the formula satisfies \( T(n)=O(n) \) and provided the following proof:
\begin{enumerate}
    \item[i.] For \( n=1 \) the claim holds immediately.
    \item[ii.] For \( 1<n \) we will prove by induction.
    \begin{enumerate}
        \item[a.] Assume \( T(i) < ci \) for all \( i < n-1 \).
        \item[b.] Substitute the recurrence formula with assumption a and get: \( T(n) \leq 8(cn/4)+n \).
        \item[c.] From b we get \( T(n)=O(n) \).
    \end{enumerate}
\end{enumerate}

Choose the most correct answer:
\begin{enumerate}
    \item[a.] The student was correct in the claim and in the proof.
    \item[b.] The student was wrong in the claim and in the proof. The incorrect part of the proof is ii.c.
    \item[c.] The student was wrong in the claim and in the proof. The incorrect part of the proof is i.
    \item[d.] The student was correct in the claim and wrong in the proof.
    \item[e.] The claim is incorrect but if we replace the number 8 with 4 in the recurrence formula, the claim would be correct.
    \item[f.] Answers b and c are correct.
\end{enumerate}
\end{itemize}",F,ו. Answers B+C are correct,Complexity analysis
Recursive algorithms,9,"\text{We implemented a binary search tree (not necessarily balanced) on } n \text{ elements. Let } k \text{ denote the number of elements in the tree for which the find operation takes constant time (i.e., independent of } n\text{). What is } k\text{?} \\
\text{A. } k=1 \\
\text{B. } k=O(1) \\
\text{C. } k=O(\log n) \\
\text{D. } k=\lfloor n/2 \rfloor \\
\text{E. } k=O(n) \\
\text{F. Insufficient data to determine} \\
\text{Justify your answer.}",B,\textbf{Answer B:} It is not possible for there to be more than a fixed number of nodes at a given depth because each node has at most 2 children.,Binary search trees
Recursive algorithms,10,"\textbf{We want to color the nodes of a full binary search tree (i.e., each node has two children) in red and black, so that the coloring adheres to the rules of a red-black tree. Which of the following conditions is a sufficient condition for such a coloring to exist?}
A. All leaves are at the same distance from the root.  
B. The largest distance from the root to a leaf is at most twice the smallest distance from the root to a leaf.  
C. The depth of the right subtree of the root is at most twice the depth of the left subtree of the root.  
D. Answers A and B are correct.  
E. Answers A and C are correct.  
F. No answer is correct.",A,"\textbf{Answer A:} We will paint the entire tree black. It is possible to construct a tree that contradicts B, therefore D is not correct.",Binary search trees
Recursive algorithms,9,"\text{We are given two 2-4+ trees, one with height } h_1 \text{ and the other with height } h_2. \text{ The heights } h_1 \text{ and } h_2 \text{ are known to us, and we also know that } h_1 > h_2, \text{ and that all elements in the first tree (height } h_1) \text{ are smaller than all elements in the second tree. We want to merge the two trees into one. In what running time can this be done? Choose the tightest answer.} \\
\text{a. } O(h_1) \\
\text{b. } O(h_2) \\
\text{c. } O(h_1h_2) \\
\text{d. } O(h_1/h_2) \\
\text{e. } O(h_1+h_2) \\
\text{f. } O(h_1-h_2) \\
\text{Explain your answer.}",F,\textbf{Answer V:} We will hang the small tree on the rightmost path of the large tree and correct upwards.,"Balanced BST (AVL,   B)"
Recursive algorithms,10,"\begin{itemize}
    \item In this question, we discuss a tree with $100 < n$ nodes where the degree of the root is 2. 
    \item We define the imbalance of the tree as the ratio between the depth of the left subtree of the root and the depth of the right subtree of the root (ratio = the first number divided by the second number).
    \item In which of the following trees can we achieve the greatest imbalance:
    \begin{enumerate}
        \item A tree representing a red-black tree.
        \item A tree representing a 2-4 tree where the degree of the root is 2.
        \item A tree representing a find-union structure (with rank by union) where the degree of the root is 2.
        \item A tree representing a find-union structure without rank by union where the degree of the root is 2.
        \item Answers (a) and (b) are correct.
        \item Answers (c) and (d) are correct.
    \end{enumerate}
\end{itemize}",D,\textbf{Answer D: This is the only tree where there is no depth balance.},Union-Find
Recursive algorithms,9,"Let \( A \) be an array of \( N \) numbers. Let the number at the i-th position be denoted as \( A_i \). We say there is an inversion between \( A_i \) and \( A_j \) if \( i < j \) and \( A_i > A_j \). Let \( I \) denote the number of inversions in the array.

What is the runtime of the most efficient algorithm that takes \( A \) and returns the number of inversions, i.e., \( I \)? Choose the tightest upper bound for the runtime of the algorithm:

A. \( O(I) \)  
B. \( O(I + N) \)  
C. \( O(N \log N) \)  
D. \( O(\min\{I+N, N \log N\}) \)  
E. \( O(N + N \log (I/N)) \)  
F. None of the above.

Justify your answer.",E,"\textbf{Solution H'.} In the lecture, we saw insertion sorting using TREE FINGER. The overall runtime is $O(N+N\log(I/N))$ and for each insertion $O(\log(I_k))$, where $I_k$ is the number of inversions up to the $k$-th element. Two things need to be noted: the adjustments for a balanced tree in a red-black tree require a total linear time, and we have already considered the cost of the search. It is possible to perform the SIZE field update during the ascent in the search (using update debt memorization), and therefore it does not add to the cost.",Complexity analysis
Amortized analysis,2,"The deterministic ""select"" algorithm operates as follows:
a. The array is divided into groups of size 5.
b. Each group is sorted.
c. A pivot is chosen as the median of the medians.
d. A ""Partition"" is performed with the chosen pivot.
Assume that instead of groups of size 5 we take groups of size \(\log(n)\). What will be the running time of the algorithm?
What is the cost of step b?",na,\text{Cost} = \Theta(n \log \log n): \text{Explanation: Each set of size} \log n \text{ is sorted in } \Theta (\log n  \cdot  \log \log n) \text{ time. There are } \frac{n}{\log n} \text{ such sets.},Selection and median-of-median algorithm
Amortized analysis,8,"The ""select"" deterministic algorithm operates as follows:
a. The array is divided into groups of size 5.
b. Each group is sorted.
c. A pivot is chosen as the median of the medians.
d. A ""Partition"" is performed with the chosen pivot.
Suppose that instead of groups of size 5, we take groups of size \(\log(n)\). What will be the runtime of the algorithm? What is the total cost of the algorithm?",na,"\text{Cost: } O(n \log \log n). \text{ Explanation: It can be shown that the pivot is larger (and smaller) than at least } \frac{n}{4} \text{ elements, and therefore the recurrence formula is } T(n) \leq C \cdot n \log \log n + T\left(\frac{n}{\log n}\right) + T\left(\frac{3n}{4}\right). \text{ By induction it can be shown that this results in } T(n) = \Theta(n \log \log n).",Selection and median-of-median algorithm
Amortized analysis,2,"\textbf{Reminder:} In perfect hash, we choose a random universal hash function to map a given space \( U \) of size \( n \) to an array of size \( n \). Let \( n_i \) be the number of elements mapped to cell \( i \). Then, a random universal hash function is used to map the elements mapped to cell \( i \) to another array of size \( n_i^2 \).

\textbf{Question:} 

Define: A nearly universal family of functions is a family of functions from space \( U \) to \(\{0, \ldots, m-1\}\) such that for any pair of keys \( k_1, k_2 \) in \( U \) the condition \( \Pr(h[k_1] = h[k_2]) \leq 1/m^{0.5} \) holds. Suppose we have a nearly universal family when \( |U| = n \). We take a random function from the family and use it to map the elements of \( U \) to an array of size \( m \). What is the best upper bound on the expected number of collisions if \( m = n^4 \)?",na,"\text{Answer = 0.5. There are } \frac{n^2}{2} \text{ possible collisions and the probability for each is } \frac{1}{\sqrt{m}} = \frac{1}{n^2}, \text{ so by linearity of expectation we get 0.5.}",Hash tables
Arrays and linked lists,9,"\textbf{Given an input sequence of} $N$ \textbf{distinct real numbers. The first} $0.99N$ \textbf{numbers in the sequence are given in a random arbitrary order (any order is equally likely). The last} $0.01N$ \textbf{numbers satisfy:} 

\textbf{A. They are all smaller than all of the first} $0.99N$ \textbf{numbers in the sequence.}

\textbf{B. They are sorted in ascending order.}

\textbf{Insert the numbers, in their given order, using the insert operation as learned in class, into a regular binary search tree (BST) (not necessarily balanced).}

\textbf{Choose the most accurate statement and provide a convincing argument:}

\textbf{A. The expected total cost of all insertions is} $\Theta(N \log N)$ \textbf{and the maximum cost of a single insertion in the sequence is} $\Theta(N)$

\textbf{B. The expected total cost of all insertions is} $\Theta(N^2)$ \textbf{and the maximum cost of a single insertion in the sequence is} $\Theta(N)$

\textbf{C. The expected total cost of all insertions is} $\Theta(N \log N)$ \textbf{and the maximum cost of a single insertion in the sequence is} $\Theta(\log N)$

\textbf{D. The expected total cost of all insertions is} $\Theta(N^2)$ \textbf{and the maximum cost of a single insertion in the sequence is} $\Theta(\log N)$

\textbf{E. The expected total cost of all insertions is} $\Theta(N)$ \textbf{and the maximum cost of a single insertion in the sequence is} $\Theta(\log N)$

\textbf{F. None of the answers are correct}",B,"\textbf{Answer:} \\
b. The expected total cost of all the revenues is \(\Theta(N^2)\). \\
The maximum cost of a single revenue in the series is \(\Theta(N)\).",Binary search trees
Arrays and linked lists,7,"Given a pseudo code for a sorting algorithm named Min-Max-Sort. The algorithm takes as input an array \( A \) of \( n \) numbers, and two indices \( p, q \) that satisfy \( 1 \leq p \) and \( p \leq q \leq n \). The algorithm sorts the subarray \( A[p \ldots q] \). 
To sort the entire array \( A \), call Min-Max-Sort(A,1,n).

Min-Max-Sort(A,p,q)
\{
(r,s) \leftarrow \text{rearrange}(A,p,q)
// Rearranges the array A[p,q] such that all occurrences of the minimum are in positions A[p \ldots r-1],
// all occurrences of the maximum are in positions A[s+1 \ldots q], and the rest in A[r \ldots s], and returns r, s.
If(r \leq s)
\quad Min-Max-Sort(A,r,s)
\}

Example for the Rearrange algorithm:
Assume the array \( A \) is: 23 20 3 10 2 10 4 5 1 1
and call Rearrange(A,3,8), then a possible result is: 23 20 10 10 3 4 5 2 1 1
The returned indices are (4,6).
For every subarray \( A[p \ldots q] \), the procedure Rearrange runs in linear time in the size of \( A[p \ldots q] \).
Assume that there are \( k \) distinct values in the array \( A \).
What is the time complexity of the algorithm as a function of \( n \) and \( k \) in the worst case, when running Min-Max-Sort(A,1,n)?

A. \(\Theta(n \log(\frac{n}{k}))\)  
B. \(\Theta(n \log k)\)  
C. \(\Theta(n \log n)\)  
D. \(\Theta(nk)\)  
E. \(\Theta(n^2)\)  
F. None of the answers are correct",D,\textbf{Answer: D},Complexity analysis
Arrays and linked lists,9,"\begin{enumerate}
    \item If the array size is 1, stop.
    \item Divide the array into \(k\) parts of equal size (for simplicity, assume \(n\) is an integer power of \(k\)).
    \item Recursively sort each part using \(k\)-Merge-Sort.
    \item Merge the \(k\) parts into one sorted array (the merging is described below).
\end{enumerate}

\textbf{Merging \(k\) sorted arrays into one sorted array:}
\begin{enumerate}
    \item Keep a pointer to the beginning of each array.
    \item Insert the values pointed to into a binary minimum heap, along with the pointers, with the key being the value of the pointer.
    \item Perform \(n\) times:
    \begin{enumerate}
        \item Extract the minimum from the heap (denote the value as \(x\) from array \(A_i\)).
        \item Place \(x\) in the unified sorted array as the next value.
        \item Advance the pointer in array \(A_i\) and insert the value it points to into the heap. (If \(x\) is the last value in \(A_i\), do not insert another value into the heap).
    \end{enumerate}
\end{enumerate}

Choose the tightest upper bound possible for the running time of the \(k\)-Merge-Sort algorithm:
\begin{enumerate}
    \item \(O\left(\frac{n \log n}{\log k}\right)\)
    \item \(O(n \log n \cdot \log k)\)
    \item \(O(n \log k)\)
    \item \(O(n \log n)\)
    \item \(O(n (\log n)^k)\)
    \item None of the above
\end{enumerate}",D,\textbf{Answer: D},Complexity analysis
Arrays and linked lists,5,"\textbf{In this question, we deal with ordered pairs of integers }$(x,y)$\textbf{. Two pairs }$(a,b)$\textbf{ and }$(c,d)$\textbf{ are called swapped pairs if }$a=d$\textbf{ and }$c=b$\textbf{. For example: }$(10,17)$\textbf{ and }$(17,10)$\textbf{ are swapped pairs. We have }$M$\textbf{ distinct pairs of numbers }$(x,y)$\textbf{ given as input. We want to write an algorithm to find if among the }$M$\textbf{ pairs there is at least one pair of swapped pairs. We are interested in an algorithm whose expected running time, for any input, is as low as possible. What is the expected running time of the algorithm?}

\textbf{A. }$\theta(\log n)$\textbf{ on average}
\textbf{B. }$\theta(n)$\textbf{ on average}
\textbf{C. }$\theta(n\log n)$\textbf{ on average}
\textbf{D. }$\theta(n(\log n)^2)$\textbf{ on average}
\textbf{E. }$\theta(n^2)$\textbf{ on average}
\textbf{F. None of the answers is correct}",B,\textbf{Answer: B},Complexity analysis
Arrays and linked lists,9,"\textbf{The following question pertains to the Fibonacci heap data structure as studied and analyzed in class. As a reminder, the operations supported by this data structure are:} make-heap \textbf{(creating an empty heap),} insert \textbf{(inserting a new element into an existing heap),} meld \textbf{(merging two heaps),} decrease-key \textbf{(decreasing the key of a given element),} find-min \textbf{(finding the element in the heap with the minimum key), and} delete-min \textbf{(deleting the element with the minimum key from the heap).} \\
\textbf{n operations are performed on a collection of Fibonacci heaps. Initially, the collection is empty. All the heaps are created by make-heap operations that create empty heaps. Then elements are inserted into the heaps, merged, keys are decreased, elements are deleted, and so on.} \\
\textbf{Choose the most correct statement:} \\
\text{a. The amortized cost of each meld operation is} \, O(\log n) \\
\text{\hspace{1em} A meld operation may have a true cost of} \, \omega(n). \\
\text{b. The amortized cost of each meld operation is} \, O(1) \\
\text{\hspace{1em} A meld operation may have a true cost of} \, \omega(n). \\
\text{c. The amortized cost of each meld operation is} \, O(1) \\
\text{\hspace{1em} A meld operation may have a true cost of} \, \omega(\log n). \\
\text{d. The amortized cost of each meld operation is} \, O(1) \\
\text{\hspace{1em} A meld operation may have a true cost of} \, O(1). \\
\text{e. The amortized cost of each meld operation is} \, O(\log n) \\
\text{\hspace{1em} A meld operation may have a true cost of} \, O(\log n). \\
\text{f. The amortized cost of each meld operation is} \, O(n) \\
\text{\hspace{1em} A meld operation may have a true cost of} \, \omega(n). \\
",D,\textbf{Answer: D},"Binomial heaps,   Fibonacci heaps"
Arrays and linked lists,6,"The given minimal heap H contains n distinct data elements. We are interested in taking the smallest root(n) elements and sorting them in the lowest possible worst-case time. The cost of performing the operation using the best algorithm you know on H is:  
a. O(root(n))  
b. O(root(n)logn)  
c. O(n)  
d. O(nlogn)  
e. O(n root(n))  
f. O(n root(n)logn)",B,\textbf{Answer: B},Quick-sort
Amortized analysis,2,"The deterministic ""select"" algorithm, which we studied in class, works as follows:
a. The array is divided into groups of size 5
b. Sort each group
c. Choose a pivot as the median of the medians (recursively)
d. Perform ""partition"" with the chosen pivot
e. Recursively apply the algorithm on the relevant part.

Assume that instead of groups of size 5, we take groups of size 3. 
In this question, we will deal with the running time of the new algorithm with groups of size 3.
What is the cost of step b? 
Cost:
Explanation:",na,"\text{Cost: } O(N) \\
\text{Explanation: Sorting each triplet is fixed, and there are } N/3 \text{ triplets.}",Selection and median-of-median algorithm
Amortized analysis,2,"\textbf{The deterministic ""select"" algorithm, which we studied in class, operates as follows:}  
a. The array is divided into groups of size 5  
b. Each group is sorted  
c. A pivot is chosen as the median of the medians (recursively)  
d. A ""partition"" is performed with the chosen pivot  
e. The algorithm is applied recursively on the relevant part.  

\textbf{Assume that instead of groups of size 5, we take groups of size 3.}  
In this question, we will discuss the running time of the new algorithm with groups of size 3.  
Let \( T(N) \) denote the running time of the algorithm on an array of size \( N \). What is the cost of step c, expressed in terms of \( T \)?  
\textbf{Cost:}  
\textbf{Explanation:}",na,"\text{Cost: } T(N/3) \\
\text{Explanation: Looking for a median among } N/3 \text{ numbers}",Selection and median-of-median algorithm
Amortized analysis,6,"\textit{Given an array of size \( n \), divided into \( n/k \) segments, each of length \( k \). It is known that for any two of the \( k \) segments, either all the elements in the first segment are smaller than all the elements in the second segment, or all the elements in the first segment are larger than all the elements in the second segment.
We are interested in a sorting algorithm for arrays of this type, suitable for the comparison model, as well as a lower bound for sorting algorithms in the comparison model for arrays of this type.
Propose the best possible lower bound (as a function of \( n \) and \( k \)) and prove it.}",na,"The number of possible arrangements is \((k!)^{(n/k)} \cdot (n/k)!\).
Taking the log gives:
\(n \log k + \frac{n}{k} \log \frac{n}{k}\).",Lower bound for comparison based sorting
Amortized analysis,6,"\[
\text{Given an array of size } n, \text{ divided into } \frac{n}{k} \text{ segments, each of length } k. \text{ It is known that for any two of the } k \text{ segments, either all the elements in the first segment are smaller than all the elements in the second segment, or all the elements in the first segment are larger than all the elements in the second segment.}
\]
\[
\text{We are interested in a sorting algorithm for arrays of this type, suitable for the comparison model, and also in a lower bound for sorting algorithms in the comparison model for arrays of this type.}
\]
\[
\text{Propose a more efficient algorithm, if it is known that the number of distinct values in each segment is at most } \log k. \text{ What is the running time of the algorithm?}
\]",na,\text{Sorting each segment can now be done in }O(k\log\log k)\text{ time.},Lower bound for comparison based sorting
Recursive algorithms,10,"\begin{enumerate}
    \item A Fibonacci heap is given as learned in class. It is given that it is empty. We perform n insert operations and then two del-min operations. You need to choose the answer that contains:
    \begin{enumerate}
        \item The worst-case running time of the first min-del operation among the two.
        \item The worst-case running time of the second min-del operation among the two.
        \item Justify your answer.
    \end{enumerate}
    
\end{enumerate}

\begin{itemize}
    \item [A.] First w.c \(O(1)\) Second w.c \(O(1)\)
    \item [B.] First w.c \(O(\log n)\) Second w.c \(O(\log n)\)
    \item [C.] First w.c \(O(n)\) Second w.c \(O(n)\)
    \item [D.] First w.c \(O(\log n)\) Second w.c \(O(1)\)
    \item [E.] First w.c \(O(n)\) Second w.c \(O(\log n)\)
    \item [F.] First w.c \(O(n)\) Second w.c \(O(1)\)
\end{itemize}",E,"\textbf{Answer: h}
After one min-del operation, there will be trees up to degree \(\log n\), at most one of each degree.
Therefore, the successive linking in the next phase will take at most \(\log n\). The first operation
will take \(O(n)\) because each element is in its own tree.","Binomial heaps,   Fibonacci heaps"
Recursive algorithms,3,"The students were asked to design a new deterministic (non-random) sorting algorithm using comparisons and construct its comparison tree. The number of items to sort is \(N\) and they are all different from each other.

Student A proposed Algorithm A and constructed Tree A for it. The number of leaves in the tree is \(2^{(N \log \log N)}\).  
Student A proposed Algorithm B and constructed Tree B for it. The number of leaves in the tree is \(2^N\).  
Student A proposed Algorithm C and constructed Tree C for it. The number of leaves in the tree is \(N!\).

The three trees were examined and found to accurately reflect the algorithms. It is not known whether the algorithms are correct. Choose the correct answer and justify your answer:

Statement: The number of comparisons in the worst case of Algorithm C is \(O(N \log N)\).  
a. The statement is always true  
b. The statement is not necessarily true",B,"\textbf{Answer: B} \newline
The comparison tree can be balanced or unbalanced, and therefore it is not possible to bound the running time of algorithm G with \(O(N \log N)\).",Lower bound for comparison based sorting
Recursive algorithms,4,"The students were asked to design a new deterministic sorting algorithm (not random) using comparisons and to build its comparison tree. The number of elements to sort is \( N \) and they are all different from each other.

Student A proposed Algorithm A and built Tree A for it. The number of leaves in the tree is \( 2^{N \log \log N} \).  
Student A proposed Algorithm B and built Tree B for it. The number of leaves in the tree is \( 2^N \).  
Student A proposed Algorithm C and built Tree C for it. The number of leaves in the tree is \( N! \).

The three trees were examined and found to accurately reflect the algorithms. It is not known whether the algorithms are correct. Choose the correct answer and justify your choice:

A. Algorithm A is an incorrect algorithm.  
B. Algorithm A may be correct.",A,"\textbf{Answer: A}\\
Algorithm A' is incorrect because there are not enough leaves in its comparison tree.",Lower bound for comparison based sorting
Recursive algorithms,10,"Two binary heaps are given: heap 1 with \( n_1 \) elements, and heap 2 with \( n_2 \) elements. 
Also, it is given that \( n_1 = n_2^2 \). 
Each heap is maintained in an infinite array. What is the best running time for merging the two heaps, that is, an operation that results in one array (it can be one of the heap arrays, there is no need to preserve the original heaps) containing all the elements, such that they represent a heap?
Choose the correct answer and justify your answer. 
A. \( O(\log(n_1) + \log(n_2)) \) 
B. \( O(n_2) \)
C. \( O(n_1) \) 
D. \( O(n_1 + n_2) \) 
E. \( O(n_2 \log n_1) \) 
F. \( O(n_1 \log n_2) \) ",E,\textbf{Answer:} We will insert the elements of the smaller heap into the larger heap one by one.,Binary heaps
Recursive algorithms,10,"\begin{enumerate}
    \item To remind you, the select(k) selection algorithm we learned in class operates as follows:
    \begin{enumerate}
        \item Divide the given array into groups of five, namely into disjoint arrays each with five elements.
        \item Find the medians of each of the fives.
        \item Find the median of medians recursively.
        \item Use the median of medians as a pivot to partition the array.
        \item Apply the algorithm recursively to the appropriate part.
    \end{enumerate}
    A student implemented the algorithm we learned in class. Let's assume a mistake occurred in the implementation of step 2 of the algorithm, so that for epsilon of the fives, the algorithm chooses an element from the five that is not necessarily the median. For example, if epsilon=1/2, the algorithm will choose any element among the five in at most half of the fives, and accordingly, in at least half of the fives, the algorithm will correctly choose the median. Apart from this, the implementation of the algorithm is correct.
    What is the highest bound on epsilon such that the incorrect implementation still returns a correct answer in linear time with respect to the input size, among the following options? Justify your answer.
\end{enumerate}

\begin{itemize}
    \item[a.] $\epsilon = 0$ meaning the algorithm works correctly and in linear time only if the correct medians are always chosen in step 2.
    \item[b.] $\epsilon \leq 0.1$
    \item[c.] $\epsilon \leq 0.2$
    \item[d.] $\epsilon \leq 0.4$
    \item[e.] $\epsilon \leq 0.9$
    \item[f.] $\epsilon \leq 1$ meaning the algorithm will work correctly and in linear time even if in each of the fives a non-median element is chosen.
\end{itemize}",C,"\text{Answer: C} \\
\text{The runtime is } T(n) = O(n) + T(n/5) + T(n - 3n/10 + 2\epsilon/5) \text{ thus answer C} \\
\text{is the largest such that the runtime is linear. The result of the minimization holds in any case.}",Selection and median-of-median algorithm
Recursive algorithms,3,"\noindent We want to implement a dictionary with the operations Search(), Delete(), Insert() on elements whose keys are integers. We will choose one of the implementations as we have seen them in class. Determine whether the statement is true or false and justify. 

\noindent Statement: 

\noindent \quad If it is important to us that the worst-case runtime for each operation is as low as possible (asymptotically), implementing with a hash table is preferable over implementing with a balanced binary search tree.

\noindent a. True

\noindent b. False",B,"\textbf{Answer: B} A hash table asymptotically improves the expected running time, but not the other two parameters, compared to a balanced binary search tree.",Hash tables
Recursive algorithms,3,"We want to implement a dictionary with the operations Search(), Delete(), and Insert() on elements whose keys are integers. We will choose one of the implementations, as we saw them in class. Determine for the statement whether it is true or false and justify.

Statement:
If we care about minimizing memory space (asymptotically), implementation with a hash table is preferable over implementation with a balanced binary search tree.
a. True
b. False",B,"\textbf{Answer: B} \\
A hash table asymptotically improves the expected runtime, but not the other two parameters compared to a balanced binary search tree.",Hash tables
Recursive algorithms,10,"This question refers to an implementation given in class of the union-find data structure that supports the operations find(), union(), make-set(). We are given that in every union operation, it receives two roots and the size of the smaller set among the two being merged is at most 21. The running time of each operation in the worst case should be as good as possible. \( n \) denotes the number of elements in the structure. Choose from the options what the running times of the operations are in the worst case, and justify your answer:

a. The operations find(), union(), make-set() are all in time \( O(1) \).
b. The make-set() operation is in time \( O(1) \), the union(), find() operations are in time \( O(\alpha(n)) \).
c. The make-set(), union() operations are in time \( O(1) \), the find() operation is in time \( O(\log n) \).
d. The make-set() operation is in time \( O(1) \), the union(), find() operations are in time \( O(\log n) \).
e. The union(), find(), make-set() operations are all in time \( O(\log n) \).
f. The make-set(), union(), find() operations are all in time \( O(n) \).",A,"\textbf{Answer: A} \\
The depth of the small tree in the union is bounded by a constant.",Union-Find
Amortized analysis,4,"\textit{A revolutionary computer architecture has been invented that includes a revolutionary operation called COMP that takes} \( \sqrt{N} \) \textit{keys appearing consecutively somewhere in the array and sorts them in ascending order in that place. The COMP operation takes} \( O(1) \) \textit{time to execute. Show how you can efficiently sort} \( N \) \textit{keys. What is the complexity of the algorithm? Hint: Given an array of size} \( N \), \textit{consider how you can rearrange the array so that} \( 0.5 \times \sqrt{N} \) \textit{smallest keys are sorted at the beginning of the array.} 

\textbf{Complexity:}

\textbf{Explanation:}",na,"\textbf{Complexity:} \(O(N)\) \\
We will perform this process on every \(\frac{\sqrt{N}}{2}\) in the array from the left end to the finish. \\
Each such process is bounded by \(O(\sqrt{N})\) and it is performed \(2\sqrt{N}\) times, therefore the total runtime is linear.",Complexity analysis
Amortized analysis,2,"\textit{We will implement an n-bit binary counter. The counter is initialized to the value 0...00. The Inc operation advances the counter, i.e., increases its value by 1 as follows. The operation searches sequentially for the rightmost 0, turns it into a 1, and turns all the 1s to its right into 0s. If the state of the counter is 1...11, then advancing it will change its state to 00...0. For example, if n=5 and the counter state is currently 00111, then after the Inc operation, the counter state will be 01000. After another Inc operation, the counter state will be 01001. The cost of an operation is defined as the number of bits whose value is changed by the operation. For example, the cost of advancing the counter in the example from 00111 to 01000 is 4, since 4 bits changed their value. The answers to the following sections should be exact, not asymptotic in terms of O(). What is the cost of the Inc operation in the worst case? Explain.}",na,"The answer is: \( N \) because it is the number of bits in the numerator, and they can all change in one operation.",Complexity analysis
Amortized analysis,5,"\textbf{We will implement a binary counter with $n$ bits. The counter is initialized to the value $0 \ldots 00$. The Inc operation advances the counter, meaning it increases its value by 1 as follows. The operation sequentially searches for the rightmost 0, turns it into a 1, and converts all 1's to its right into 0's. If the counter's state is $1 \ldots 11$, then advancing it will change its state to $00 \ldots 0$. For example, if $n=5$ and the counter state is currently $00111$, then after the Inc operation the counter state will be $01000$. After another Inc operation, the counter state will be $01001$. The cost of an operation is defined as the number of bits whose value is changed by the operation. For instance, the cost of advancing the counter in the example from $00111$ to $01000$ is 4, as 4 bits changed their value. The answers to the following sections need to be precise, not asymptotic in terms of $O(.)$. We will define a Double-Inc operation that calls for two consecutive Inc operations. What is the cost of the Double-Inc operation in the worst case? Explain.}",na,"\textbf{Answer:} n+1 \\
Any two operations necessarily include an operation where only the first bit changes, and therefore in the worst case, it's n+1.",Complexity analysis
Amortized analysis,5,"\text{We implement a binary counter with } n \text{ bits. The counter is initialized to the value } 0...00. \text{ The operation} \\
\text{Inc advances the counter, meaning it increases its value by 1 in the following manner. The operation} \\
\text{systematically searches for the rightmost 0, changes it to 1, and changes all the 1's to its right to 0.} \\
\text{If the state of the counter is } 1...11, \text{ then advancing it will change its state to } 00...0. \\
\text{For example, if } n = 5 \text{ and the current state of the counter is } 00111, \text{ then after the Inc operation} \\
\text{the state of the counter will be } 01000. \text{ After an additional Inc operation, the state of the counter will be} \\
01001. \text{ The cost of an operation is defined as the number of bits whose value is changed by the operation.} \\
\text{For example, the cost of advancing the counter in the example from } 00111 \text{ to } 01000 \text{ is } 4, \text{ since} \\
4 \text{ bits changed their value.} \\
\text{The answers to the following sections need to be precise, not asymptotic in terms of } O(.). \\
\text{Define Double-Inc as an operation that calls two consecutive Inc operations.} \\
\text{What is the amortized cost of the Double-Inc operation? Explain.}",na,"\textbf{Answer: 4} It is exactly the same analysis, except for the two operations, and therefore 4. Indeed, in every two operations, there will be two bits that change from 0 to 1 (two different ones!) - they rise by two, and we place two coins for flipping from 1 to 0 in the future. Anyone who has changed from 1 to 0 has already been funded in the past.",Amortized analysis
Recursive algorithms,9,"\textbf{Define a d-ary minimum heap:} Each node is smaller than its children, and each node has d children (except for a few nodes in the last and penultimate levels). All levels are full, except possibly the last one, which is filled from the left. For \(d=2\), this is the binary heap that was taught in class. We encountered the d-ary heap in homework.

\textbf{Formally:} Define a data structure Q with two fields. \(Q.size\) is the number of elements in the structure. \(Q.A\) is the array representing the heap. The root is in position 0. The children of node \(i\) are in positions \(di+1\) to \(di+d\) consecutively.

\textbf{Define the d-ary heap construction process:} Traverse the levels from the last level to the top, and at each level from right to left, perform heapify-down on each node. This process is identical to building a binary heap for \(d=2\).

\textbf{Formally:} Assume the array \(Q.A\) is initialized with the input values and the size field as well, then the process is:

\begin{verbatim}
BuildHeap(Q)
for i=Q.size-1 to 0 do
    heapify_down(Q, i)

heapify_down(Q, i)
min = i
j=di+1
while (j > min(Q.size, d(i+1)+1))
    if (Q.A[j] > Q.A[min])
        min = j
    j++
if (min > i)
    swap(Q.A[i], Q.A[min])
    heapify_down(Q, min)
\end{verbatim}

What is the worst-case runtime of the construction process we defined?
A. \(\Theta(n)\)
B. \(\Theta(n\log{d})\)
C. \(\Theta(n\log{n}/\log{d})\)
D. \(\Theta(n\log{n})\)
E. \(\Theta(nd)\)
F. \(\Theta(nd\log{n}/\log{d})\)

\textbf{Brief explanation:}",F,"\textbf{Answer:} \\
We note that in the worst case, we will perform \( n \) iterations, where in each iteration we go through \( d \) elements.",Binary heaps
Arrays and linked lists,9,"Define an infinite binary array, like the binary counter we saw in class, which supports the operation add(k), receiving a positive integer parameter k and adding k to the value n represented in the binary counter. For example, a call add(1), meaning with parameter k=1, is exactly the increment operation of the binary counter. What is the best time complexity in which the operation add(k) can be implemented as a function of k and n?
a. In the worst case O(logn) and amortized O(logk)
b. In the worst case O(log(n+k)) and amortized O(logk)
c. In the worst case O(log(n+k)) and amortized O(logn)
d. In the worst case O(lognlogk) and amortized O(logn)
e. In the worst case O(lognk) and amortized O(logn)
f. In the worst case O(n) and amortized O(k)",B,\textbf{Answer: B},Amortized analysis
Recursive algorithms,9,"\textbf{How can we detect a substring that repeats exactly twice in a suffix tree?} \\
A. A leaf with exactly two possible paths to it. \\
B. A node with exactly two children. \\
C. An edge whose label is exactly two characters long. \\
D. A node whose subtree has a size of exactly two. \\
E. A node in whose subtree there are exactly two leaves. \\
F. This problem cannot be efficiently solved using a suffix tree.",E,"\textbf{Answer: h} \\
In other words, there are exactly two clauses that contain this substring.",Suffix trees
Arrays and linked lists,3,"\begin{quote}
B-tree search trees, as we learned in class, are defined with a parameter \(d\), such that every internal node, except for the root, has between \(d\) and \(2d\) children. For example, 2-4 trees, 3-6 trees, 1024-2048 trees. We will change the definition of the structure so that the number of allowed children will be between \(d_1\) and \(d_2\) for two natural numbers, where it is not necessarily the case that \(d_2=2d_1\). Apart from this change and adjustments derived from it, the rest of the implementation remains unchanged. Are 9-17 trees possible? 
A. Yes 
B. No
\end{quote}",A,"Answer: \\
A - Yes","Balanced BST (AVL,   B)"
Arrays and linked lists,3,"\textit{B-tree} search trees as we learned in class are defined with parameter \( d \), such that each internal node, except for the root, has between \( d \) and \( 2d \) children. For example, \( 2\text{-}4 \) trees, \( 3\text{-}6 \) trees, \( 1024\text{-}2042 \) trees. We will change the definition of the structure so that the number of allowed children is between \( d_1 \) and \( d_2 \) for two natural numbers, when \( d_2=2d_1 \) does not necessarily hold. Except for this change and adjustments derived from it, the rest of the implementation remains the same.
Are \( 9\text{-}99 \) trees possible?
A. Yes
B. No",A,"\textbf{Answer:} \\
A - Yes","Balanced BST (AVL,   B)"
Arrays and linked lists,9,"\textit{Given a lazy binomial heap with $n$ elements, we deleted the minimum element (via the del-min operation), and the number of trees in the heap increased. An upper bound on the difference between the new number of trees and the old number of trees is (give the tightest bound):}
\begin{enumerate}[label=\alph*.]
  \item $n$
  \item $\log n \cdot \log n$
  \item $\log n$
  \item $n/2$
  \item $1$
  \item \textit{none of the above}
\end{enumerate}",C,\textbf{Answer: C},"Binomial heaps,   Fibonacci heaps"
Arrays and linked lists,5,"\begin{itemize}
    \item Define a binary counter with the operation increment(i). The counter is implemented by an infinite array of bits, starting from index 0, where the least significant bit is in position 0 (defined as: the right side of the array). The operation increment(i) increases the value in the counter by 2^i.
    \item The implementation of the operation: starting from position i in the array, change all consecutive 1s starting at i and extending to the left to 0s, and change the first 0 to the left of the consecutive ones to 1 (pseudo-code follows). For example, if the array contains the value ...01101011101 and increment(2) is performed, then the new value will be ...01101100001.
    \item Assume we started with a counter whose value is 0 and performed n Increment(j) operations with arbitrary values of j. After these, increment(i) is performed. What is the worst-case time for this increment(i) operation?
\end{itemize}

\begin{verbatim}   
Increment(A, i)
j \leftarrow i
While (A[j] = 1)
    A[j] \leftarrow 0
    j \leftarrow j+1
A[j] \leftarrow 1
\end{verbatim}

Choose the correct answer (among different times, choose the tightest one).
\begin{enumerate}
    \item O(1)
    \item O(\log i)
    \item O(\log n - \log i)
    \item O(\log n)
    \item O(n-i)
    \item O(n)
\end{enumerate}",F,"\textbf{Answer:} \\
6. \(O(n)\)",Complexity analysis
Recursive algorithms,5,"Define a binary counter with the operation increment(i). The counter is implemented by an infinite array of bits, starting from index 0, with the least significant bit at position 0 (defined as the right side of the array). The operation increment(i) increases the value on the counter by 2^i. The implementation of the operation is as follows: Starting from position i in the array, change all consecutive 1s starting at i and extending left to 0, and change the first 0 to the left of these consecutive 1s to 1 (pseudocode follows). For example: if the array contains the value ...01101011101 and increment(2) is performed, the new value will be ...01101100001. Assume that we started with a counter with value 0 and performed n Increment(j) operations with arbitrary values of j. After that, perform increment(i). What is the amortized time for an increment(j) operation? Let n denote the number of increment(j) operations performed on the structure (running Increment operations with arbitrary values of j).

Increment(A, i) 
j ← i
While (A[j] = 1)
A[j] ← 0
j ← j+1
A[j] ← 1

Choose the most correct answer (among different times, choose the one that is the tightest). Justify your answer.
a. O(1)
b. O(logi)
c. O(logn-logi)
d. O(logn)
e. O(n-i)
f. O(n)",A,"\textbf{Answer:}  
A. O(1)  
A situation can be created where there are n consecutive ones, and then the increment from the start of the sequence will take linear time in n.  
Amortized runtime can be provided as we did in class. We will place a coin on each lit one, and it will fund its conversion to zero. In each such operation, there is one time a conversion from 0 to 1, and then we place a coin.",Amortized analysis
Recursive algorithms,5,"\textit{A graph} \(G = (V, E)\) \textit{contains vertices} \(v_1, \ldots, v_n\) \textit{and} \(m\) \textit{edges, which are pairs} \((v_i, v_j)\). \textit{Assume that each vertex and edge can be accessed in} \(O(1)\). \textit{Let} \(A\) \textit{be an algorithm that uses the data structure union-find. The algorithm is as follows:}
1. \textit{For each vertex from} \(i=1\) \textit{perform} \(\text{make-set}(v_i)\)
2. \textit{For each edge: check if} \(\text{find}(v_i) = \text{find}(v_j)\). \textit{If not, perform} \(\text{Union}(v_i, v_j)\).
\textit{Give a tight upper bound, as accurate as possible, on the number of union operations performed during the execution of algorithm} \(A\). \textit{The bound should be exact (not asymptotic)}
a. \(1\)
b. \(\log n\)
c. \(n/2\)
d. \(n-1\)
e. \(n\) 
f. \(m\)",D,"\textbf{Answer:} \\
\textbf{d.} \quad n-1",Union-Find
Arrays and linked lists,5,"\[
\text{Given a red-black tree with pointers to the maximum and minimum, two additional operations are added:}
\]

\[
\text{max-Insert – Insert an element (guaranteed to be larger than all other elements of the tree) as the right child of the maximum (a pointer to the maximum is given). Update the pointer to the new element inserted as the maximum.}
\]

\[
\text{delete-min – Find the successor of the minimum (search for the successor from the minimum position by the successor operation learned in class), delete the minimum from its position (given pointer). Update the pointer to point to the successor.}
\]

\[
\text{Of course, after each deletion or insertion, red-black tree adjustments are made, as learned.}
\]

\[
\text{The following sequence of operations is performed: Given a sorted array of size n and an empty red-black tree, perform max-insert on the array elements from smallest to largest. Then, perform delete-min until the tree is empty.}
\]

\[
\text{Provide the tightest possible bound for the runtime of the sequence of operations. Justify your answer.}
\]

\begin{enumerate}
    \item \(\mathcal{O}(1)\)
    \item \(\mathcal{O}(\log\log n)\)
    \item \(\mathcal{O}(\log n)\)
    \item \(\mathcal{O}(n)\)
    \item \(\mathcal{O}(n \log n)\)
    \item \(\mathcal{O}(n^2)\)
\end{enumerate}",D,"\textbf{Answer:} \\
d. \( O(n) \) \\
\textbf{Brief Justification:} The repairs may rise up to the root, and the height of the tree is bounded by \( \log n \). Each operation will take constant amortized time. The search following the minimum will take constant time.","Balanced BST (AVL,   B)"
Arrays and linked lists,5,"\textbf{Reminder of the select(i) algorithm for finding the i-th largest element:}
1. Partition the array into groups of five.
2. Sort each group of five.
3. Find the median of the medians of the groups of five using select (denote it by x).
4. Perform a partition on the array according to x (denote m as the position where x arrives).
5. If m=i, return x. Otherwise, if m<i continue recursively on the left part with i, otherwise continue recursively on the right part with m-i.

A student implemented the algorithm but mistakenly found the median of the maximum elements at step 3. Instead of finding the median of the medians of the groups of five, they took the maximum from each group and found the median among them using a recursive call to select.
What is the runtime and does the algorithm always produce a correct result? Choose the tightest bound. Justify.

A. O(n) and the algorithm is correct \\
B. O(n\log n) and the algorithm is correct \\
C. O(n^2) and the algorithm is correct \\
D. O(n) and the algorithm is incorrect \\
E. O(n\log n) and the algorithm is incorrect \\
F. O(n^2) and the algorithm is incorrect \\

\textbf{Brief explanation:}",C,"\textbf{Answer:}  
\textbf{c.} $O(n^2)$ and the algorithm is correct.  
From each quintet we select the maximum, so we can say with certainty that at least $1/10$ are larger than the pivot, so we continue recursively on at most $9/10$.  
Solution by induction.",Selection and median-of-median algorithm
Arrays and linked lists,5,"\text{Define an almost universal family of functions as follows:} \\
\text{For any two distinct values from the domain of } n \text{ keys, the probability of randomly selecting a function from the family such that it returns the same value for both is bounded by } m^{-2/3}. \\
\text{The minimum table size } m \text{ such that if we randomly choose a function, the expected number of collisions is bounded above by 122 is } n^3. \\
\text{Let } X \text{ denote the table size } (n^3). \text{ A student constructed a table of size } 8X. \\
\text{In expectation, after how many trials can a function be found for which there are no collisions? Choose the tightest upper bound. Justify your answer.} \\
\text{a. } 1 \\
\text{b. } \frac{8}{7} \\
\text{c. } \frac{4}{3} \\
\text{d. } 2 \\
\text{e. } 4 \\
\text{f. } 8 \\
",B,"\textbf{Answer:} \\
b. $8/7$ \\
\textbf{Justification:} \\
One must put into the equations the expected number of collisions: $n$ over $2$ divided by $m$ to the power of $-2/3$ and see when it comes out less than $1/2$. \\
According to Markov's inequality, the expectation is $1/2$. If we enter $8X$ into the equation instead of $X$, we will find that the number of collisions is smaller by a factor of $4$ (due to the power of $2/3$). From this, it is eight times further from the expectation, and therefore the probability is $1/8$, and the probability of success is $7/8$. The expectation of a geometric variable is $8/7$.",Hash tables
Arrays and linked lists,5,"\begin{quote}
Given an array of $n$ integers between $1$ and $n^2$. Which of the following sorting methods will be the fastest asymptotically? 
A. Counting sort 
B. Radix sort 
C. Quicksort 
D. Merge sort 
E. Counting sort and radix sort will achieve the same asymptotic running time, which is the best running time 
F. Quicksort and merge sort will achieve the same asymptotic running time, which is the best running time
\end{quote}",B,"\textbf{Answer:} \\
b. Radix sort","Non-comparison based sorting (radix,   bucket,   counting)"
Recursive algorithms,3,"\textbf{A ""nearly sorted"" array is defined as follows: each element in the array is within a distance of up to logn places from its sorted position in the array.}
\textbf{What can be said about the running time of finding the median element in the array? Mark A true / B false and justify your answer.}
\textbf{It is possible to find the median using a binary search in the array, and therefore the running time is O(logn)}
\textbf{A. True}
\textbf{B. False}
\textbf{Justification:} ",B,"\textbf{Answer:} \\
(b) False \\
Binary search will not help – the array is not sorted.",Complexity analysis
Recursive algorithms,3,"\textit{Define a ""nearly sorted"" array as follows: each element in the array is within a distance of at most} \(\log n\) \textit{positions from its location in a sorted array. What can be said about the running time for finding the median in the array? Mark A correct / B incorrect and justify your answer. It is necessary to go through all the elements to find the median, therefore the running time is} \(\Omega(n)\) \textit{for finding the median.} 
A. Correct 
B. Incorrect 
Justification:",B,"\textbf{Answer:} \\
b. False \\
This is not correct - the additional information narrows the search range.",Complexity analysis
Recursive algorithms,3,"Define an ""almost sorted"" array as follows: each element in the array is up to logn positions away from its position in a sorted array.
What can be said about the runtime for finding the median element in the array? Mark A for correct or B for incorrect and justify your answer.
It is possible to reduce the search space to O(logn) possibilities and find the median in linear time, thus the search time is O(logn).
A. Correct
B. Incorrect
Justification: ",A,"\textbf{Answer:} \\
a. Correct \\
The median does not deviate more than \(\log n\)",Complexity analysis
Recursive algorithms,3,"\textbf{Define an ""almost sorted"" array as follows: each element is at a distance of up to \(\log n\) places from its location in a sorted array. What can be said about the running time for finding the median in the array? Mark A' true / B' false and justify your answer. It is necessary to go through at least \(\Omega(\log n)\) elements in order to find the median, and thus the running time is \(\Omega(\log n)\).} \\
\textbf{A. True} \\
\textbf{B. False} \\
Justification:",A,"\textbf{Answer:} \\
a. True \\
The elements are not sorted.",Lower bound for comparison based sorting
Recursive algorithms,3,"\begin{enumerate}
    \item Answer true or false to the following statement. (Mark 'a' for true / 'b' for false)
    \item Given only pre-order and post-order traversals, a general binary tree can be uniquely reconstructed.
    \begin{itemize}
        \item a. True
        \item b. False
    \end{itemize}
    \item Brief justification:
\end{enumerate}",B,"\textbf{Answer:} \\
b. Incorrect \\
\textbf{Brief explanation:} A counterexample - X is the root, in one tree Y is the right child, and in another tree Y is the left child.",Binary search trees
Arrays and linked lists,3,"\textbf{Answer true or false to the following statement. (Select A for true / B for false)}
\textbf{Given only a pre-order traversal, a binary search tree can be uniquely reconstructed.}
A. True
B. False
\textbf{Brief explanation:}",A,"\textbf{Answer:} \\
1. True",Binary search trees
Multiple choice with explanations,5,"Given a hash table implemented using the chaining method. The hash is composed of an array with 1,000 cells and currently contains about a million elements. Three operations are performed on it in sequence:
First operation: Adding an element that doesn't exist. Second operation: Deleting a random element that exists in the hash.
Third operation: Adding the element that was deleted in the second operation.
Let the expected running time of operation \(i\) be \(t_i\). What is the ratio between the running times of the 3 operations (assuming a new element is always added to the end of the chain)?
A. \(t_1=t_2=t_3\)
B. \(t_2 \approx 2t_3\), \(t_1=t_2\)
C. \(t_2 \approx t_3\), \(t_1 \approx 2t_2\)
D. \(t_2 \approx 2t_1\), \(t_1=t_3\)
E. None of the answers are correct

Explanation (required):",d,na,Hash tables
Multiple choice with explanations,5,"\textbf{Reminder for the SELECT(i) algorithm to find the i-th largest element:}
1. Divide the array into groups of five.
2. Sort each group.
3. Find the median of the medians of the groups by select (denote it by x).
4. Perform a partition on the array according to x (denote m the position where x arrived).
5. If i=m, return x. Otherwise, if i<m continue recursively on the left part with i, otherwise continue recursively on the right part with i-m.

\textbf{A student implemented the algorithm but made the following changes in steps 1-3 of the algorithm:}
1. Instead of groups of five, he worked with groups of seven.
2. In step 3, by mistake, he found the 5/7-th element, meaning instead of finding the median of the medians of the groups of seven, he took the 5/7-th element among the medians, which he found by select.

\textbf{What is the recurrence relation describing the running time?}
A. It is not possible to run the algorithm on groups of seven.
B. \( T(n) \le T\left(\frac{17n}{21}\right) + O(n) \)
C. \( T(n) \le T\left(\frac{25n}{49}\right) + T\left(\frac{4n}{7}\right) + O(n) \)
D. \( T(n) \le T\left(\frac{41n}{49}\right) + T\left(\frac{n}{7}\right) + O(n) \)
E. \( T(n) \le T\left(\frac{25n}{49}\right) + T\left(\frac{n}{7}\right) + O(n) \)
F. \( T(n) \le T\left(\frac{41n}{49}\right) + T\left(\frac{4n}{7}\right) + O(n) \)
G. None of the answers is correct

\textbf{Explanation (required):}",d,na,Selection and median-of-median algorithm
Multiple choice with explanations,5,"\begin{itemize}
    \item When performing delete on $v$, an internal node in a binary heap, the following actions are taken:
    \begin{enumerate}
        \item Swap $v$ with the rightmost leaf.
        \item Delete the rightmost leaf.
    \end{enumerate}
    What is the next step in the algorithm:
    \begin{enumerate}[label=\alph*.]
        \item This is not the implementation of delete in a binary heap.
        \item In a binary heap, only the minimum element can be deleted.
        \item Always perform Heapify-Up on $v$.
        \item Always perform Heapify-Down on $v$.
        \item Sometimes perform Heapify-Up on $v$ and sometimes Heapify-Down.
        \item None of the answers is correct.
    \end{enumerate}
    Explanation (required):
\end{itemize}",e,na,Binary heaps
Multiple choice with explanations,6,"\textbf{You were asked to implement the following data structure, assuming that all keys are distinct from each other and $n$ is the number of elements in the data structure:}

\begin{itemize}
    \item \textbf{Insert(a)} - Insert a key $a$ in amortized time $O(\log n)$.
    \item \textbf{Delete(a)} - Delete a key $a$ in amortized time $O(\log n)$.
    \item \textbf{Find(a)} - Find a key $a$ in amortized time $O(\log n)$.
    \item \textbf{Count(a,b)} - Return the number of keys with values between $a$ and $b$.
\end{itemize}

\textbf{Let $x_{a,b}$ be the number of keys between $a$ and $b$. Find the most efficient implementation (in terms of amortized runtime) for Count without affecting the runtimes of the other operations.}

\begin{enumerate}
    \item[(a)] The data structure can be implemented using a red-black tree. Count can be implemented in time $O(x_{a,b} \log n)$ by finding $a$ and applying successor until finding $b$.
    \item[(b)] The data structure can be implemented using a red-black tree. Count can be implemented in time $O(x_{a,b})$ by finding $a$ and applying successor until finding $b$.
    \item[(c)] The data structure can be implemented using a red-black tree with size fields for each node. Count can be implemented in time $O(\log n + x_{a,b})$ by finding $a$ and $b$ and performing a binary search between them in the tree.
    \item[(d)] The data structure can be implemented using a red-black tree with size fields for each node. Count can be implemented in time $O(\log n)$ by finding $a$ and $b$ and looking at the appropriate nodes along the path from the root to $a$ and from the root to $b$.
    \item[(e)] The data structure can be implemented using a red-black tree. Count can be implemented in time $O(n)$ by sorting the tree and finding $a$ and $b$ in the sorted array.
    \item[(f)] The data structure can be implemented using a red-black tree. Count can be implemented in time $O(n \log n)$ by sorting the tree and finding $a$ and $b$ in the sorted array.
    \item[(g)] None of the answers are correct.
\end{enumerate}

\textbf{Explanation including full implementation:}",d,na,"Amortized analysis,   Balanced BST (AVL,   B)"
Multiple choice with explanations,5,"\documentclass{article}
\begin{document}
The exam booklets are identified by the booklet number, which is a natural number between 1 and N. After grading the exams, the lecturers sorted the booklets so that booklet 1 is at the top of the stack and booklet N is at the bottom. On the way to the office, the booklets fell and got mixed in random order. Fortunately, the top √N booklets (indices 1 to √N) remained sorted. Unfortunately, all the bottom booklets (indices √N + 1 to N) are mixed in random order. This is how they arrived at the office.

To overcome the mishap and allow quick access to the booklets, the secretariat inserted them into a binary search tree by performing N insert operations according to the order the booklets arrived (the top = 1 first, and so on). Choose the most correct answer.
The worst-case search time in the worst order in the tree is (give the most tightly bound correct):
A. O(N).
B. O(log N).
C. O(N).
D. O(√log N).
E. No answer is correct.

Draw the tree and provide an explanation (mandatory):
\end{document}",c,na,Binary search trees
Multiple choice with explanations,3,"A Smart-Red-Black-Tree is a regular red-black tree that maintains a special pointer to the last node inserted into the tree (which is updated in O(1) work for each insert operation).

In addition to the regular insert operation, the tree is equipped with a successor-insert operation, which is a regular insert except that the search for the insertion point starts from the LAST node (which is reached via direct access through the special pointer). The tree is constructed by performing N regular insert operations of elements 1, 2, ..., N in increasing order. Choose the most correct answer (among different times, choose the tightest correct one).

A. The tree construction time is O(n \log n).
B. The tree construction time is O(n).
C. The tree construction time is O(n^2).
D. If the order of elements were arbitrary, the cost would be O(n \log n), but in the current order, the cost is O(n).

Short justification (required):",a,na,"Balanced BST (AVL,   B)"
Multiple choice with explanations,6,"\textbf{Given a Union-Find data structure according to the definitions learned in class. Additionally, for each root \(x\), a field \texttt{size} is maintained that indicates the number of elements in the set represented by \(x\). The implementation of the operations on the structure is identical to what was learned in class, but it does not include path compression in \texttt{FIND}. Also, the implementation of the \texttt{LINK} operation is different. Here is the pseudocode for the new operation:}

\begin{verbatim}
LINK (x, y)
If (x.size < y.size)
    Parent[x] = y
    y.size = y.size + x.size
Else
    Parent[y] = x
    x.size = x.size + y.size
\end{verbatim}

\textbf{In other words, the operation hangs the root of the smaller set on the root of the larger set and updates the \texttt{size} field accordingly. What is the worst-case runtime of the \texttt{FIND} operation (on the new implementation of \texttt{LINK} and the additional data in the question)? Choose the tightest bound.}

\begin{enumerate}
    \item O(1)
    \item O(\alpha(n))
    \item O(\log^* n)
    \item O(\log n)
    \item O(n)
    \item None of the answers is correct.
\end{enumerate}",d,na,"Complexity analysis,   Union-Find"
Multiple choice with explanations,na,"We want to color the nodes of a full binary search tree (i.e., every non-leaf node has exactly two children) in red and black, so that the coloring satisfies the rules of a red-black tree.
For each of the following statements, state whether it is true or false. Justify your answer.
A. If all the leaves are at the same distance from the root, then such a coloring necessarily exists. True / False.",1,"\textit{Reasoning: In this case, it is a whole entity. Coloring all the nodes in black is a legal coloring.}","Binary search trees,   Balanced BST (AVL,   B)"
Multiple choice with explanations,na,"\text{We want to color the vertices of a full binary search tree (i.e., every non-leaf vertex has exactly two children) in red and black so that the coloring satisfies the rules of a red-black tree. Write for each of the following claims whether it is true or not true. Justify your answer.} \\
\text{c. If the depth of the right subtree of the root is at most twice the depth of the left subtree of the root. True / Not true}",False,"\textit{Explanation: There can be a very large left subtree, and then we won't be able to ensure that the black height is the same across all paths.}","Binary search trees,   Balanced BST (AVL,   B)"
Multiple choice with explanations,6,"\textbf{Previous question:}

\textbf{Question 7 (6 points):}
The average search time over all elements when the order of notebooks from Set B is random, in the tree from the previous question is (give the lowest correct one):

\textbf{a.} $O(\sqrt{N})$.

\textbf{b.} $O(\log N)$.

\textbf{c.} $O(N)$.

\textbf{d.} $O(\sqrt{N} + \log N)$.

\textbf{e.} $O(\sqrt{N}\log N)$.",b,na,Binary search trees
Multiple choice with explanations,6,"\text{In class, we learned about a priority queue that allows insert and deletemin operations. We learned that it can be efficiently implemented within an array using a binary heap (we performed these operations in } O(\log n) \text{ where } n \text{ is the size of the queue). \text{ In this question, we are interested in a new structure, a protection queue, which in addition to the previous operations also allows delete-order(k) that removes the k-th largest element (k=1 is the minimum). We aim to perform this task in a structure residing in an array (like the binary heap) or arrays.}

\text{If } k \text{ is not known in advance but it is known that } k < \log n, \text{ then it is possible to implement the structure and perform insert and deletemin in } O(\log n) \text{ as well as:}

\text{a. Perform delete-order(k) in } O((\log n)^2). \\
\text{b. Perform delete-order(k) in } O(\log n \log \log n). \\
\text{c. Perform delete-order(k) in } O(\log n). \\
\text{d. Perform delete-order(k) in } O(n). 

\text{Brief justification and explanation of the implementation or algorithm (mandatory):}",c,na,"Complexity analysis,   Arrays and linked lists,   Binary heaps"
Closed question with short explanations,na,"\begin{enumerate}
    \item Given an array containing n elements. The number of distinct elements is k (𝑛 ≤ 𝑘).
    \item Sort the following asymptotic orders without explanation (𝑛 ≤ 𝑘):
    \begin{itemize}
        \item n \log n,
        \item n,
        \item n \log k,
        \item n + k \log k,
        \item n^2
    \end{itemize}
\end{enumerate}",na,"n^2, n\log{n}, n\log{k}, n+k\log{k}, n",Complexity analysis
Closed question with short explanations,na,"\text{Given an array containing } n \text{ elements. The number of distinct elements is } k \, (k \leq n). \\
\text{d. Assume the elements are integers between } 1 \text{ and } n^4. \text{ Propose a deterministic algorithm to sort the array.}",na,We will use the count sort algorithm as taught in class. \(O(n)\).,"Non-comparison based sorting (radix,   bucket,   counting),   OTHER"
Multiple Choice,5,"\textbf{The ""d-ary heap insertion position problem"" is the following problem:} Given a d-ary heap, represented by an array as learned in class. The root is at position 0, and the children of index i are at positions i·d+1, … , i·d+d. Additionally, a key is given that is not in the heap. We need to find the index where the key would be inserted at the end of the insertion process (the heap should not be modified). The insertion process is identical to the insertion process in a binary heap, except the parent of node i is located at the floor of (i-1)/d. It can be assumed that log and exponentiation operations take constant time.

What is the best running time to solve it?
A. \(O(1)\) 
B. \(O(d \log \log n)\) 
C. \(O(\log \log n - \log \log d)\) 
D. \(O(n \log \log d)\) 
E. \(O(\log \log n)\) 
F. \(O(\log n / \log d)\)",C,na,Binary heaps
Multiple choice with explanations,5,"The problem of ""finding the degree of a Fibonacci heap insertion tree"" is as follows: Given a Fibonacci heap represented by a list of tree roots, where the degree is given at the root (as we learned in class). Also, a key is given that is not in the heap. You need to find what the degree of the tree to which the key would belong at the end of the insertion process would be (the heap should not be altered).

What is the best running time for the solution?  
A. \(O(1)\)  
B. \(O(\log \log n)\)  
C. \(O(\log n)\)  
D. \(O(\log^2 n)\)  
E. \(O(n)\)  
F. \(O(n \log n)\)  ",A,na,"Binomial heaps,   Fibonacci heaps"
Multiple Choice,5,"\textbf{A student analyzed the running time of the binary counter data structure, as learned in the practice session.} \\
\text{Recall, the counter holds a binary number (initialized to 0) in an infinite array from index 0 (Least} \\
\text{significant bit, defined as the right side). The increment operation increases the counter value by 1 by} \\
\text{changing the contiguous 1s to the right of the first 0 to 0, and the first adjacent 0 to 1.} \\
\text{The student proposed the following potential function: the number of contiguous ones to the right} \\
\text{of the first 0. For example, if the stored number is 100100111011.. the potential value is 2.} \\
\text{The student used the formula learned in class to calculate amortized time: actual + } \\
\text{amortized = delta potential. What result will be obtained? Let n denote the number of increment operations.} \\
\text{A. Amortized time } O(1) \text{ for the increment operation.} \\
\text{B. Amortized time } O(1) \text{ for the increment operation when the value before the operation is odd, and } O(\log n) \text{ when the value before the operation is even.} \\
\text{C. Amortized time } O(1) \text{ for the increment operation when the value before the operation is even, and } O(\log n) \text{ when the value before the operation is odd.} \\
\text{D. Amortized time } O(\log n) \text{ for the increment operation.} \\
\text{E. Amortized time } O(n) \text{ for the increment operation.} \\
\text{F. The function is not valid for amortized time analysis.}",B,na,"Amortized analysis,   OTHER"
Multiple Choice,5,"\textbf{A student analyzed the running time of a binary counter data structure, as learned in the exercise.} 

\textbf{As a reminder, the counter holds a binary number (initialized to 0) in an infinite array from index 0 (least bit significant, defined as the right side). The increment operation increases the value in the counter by 1 by changing the consecutive 1s to the right of the first 0, and the first 0 adjacent to them to 1.}

\textbf{The student proposed the following potential function: [number of ones] minus [number of zeros up to the leftmost one]. For example, if the number stored is 100100111011.., the potential value is 2 (7 minus 5).}

\textbf{The student used the formula learned in class to calculate amortized time: actual + delta potential = amortized. What result will be obtained? Let n denote the number of increment operations.}

\begin{enumerate}
\item Amortized time of \(O(1)\) for an increment operation.
\item Amortized time of \(O(1)\) for an increment operation when the value before the operation is odd, and \(O(\log n)\) when the value before the operation is even.
\item Amortized time of \(O(1)\) for an increment operation when the value before the operation is even, and \(O(\log n)\) when the value before the operation is odd.
\item Amortized time of \(O(\log n)\) for an increment operation.
\item Amortized time of \(O(n)\) for an increment operation.
\item The function is invalid for analyzing amortized time.
\end{enumerate}",F,na,"Amortized analysis,   OTHER"
Multiple Choice,5,"\[
\text{Graph } G=(V,E) \text{ contains vertices } v_1, \ldots, v_n \text{ and } m \text{ edges } (m > n), \text{ which are pairs } (v_i,v_j). \text{ Assume that each vertex and edge can be accessed in } O(1). \text{ Let algorithm } A \text{ use the Union-Find data structure. The implementation of each set is by a linked list containing the members of the set (with a pointer to the head of the list). Each element points to the representative of the set. Make-set creates a list with one element, pointing to the representative (in time } O(1)). \text{ Find is in } O(1) \text{ because there is a pointer to the representative. The Union operation links the lists (in time } O(1)) \text{ and updates all members of the smaller set to point to the representative of the larger set in linear time relative to the number of elements in the smaller set (the structure was presented in the tutorial). The size of the set is stored in the representative and updated in Union.}
\]

\noindent
\text{Algorithm A:}
\begin{enumerate}
    \item \text{For each vertex } i = 1 \text{ to } n, \text{ perform Make-set}(v_i).
    \item \text{Iterate over each edge and check if } \text{find}(v_i) = \text{find}(v_j). \text{ If not, perform Union}(v_i, v_j).
\end{enumerate}

\noindent
\text{What is the worst-case running time for the union operation?}

\begin{enumerate}[label=\alph*.]
    \item O(\alpha(n))
    \item O(\alpha(m))
    \item O(\log n)
    \item O(\log m)
    \item O(n)
    \item O(m)
\end{enumerate}",E,na,"Complexity analysis,   Union-Find"
Multiple choice with explanations,5,"Given a red-black tree with a pointer to the maximum and minimum and two additional operations are added:
Insert-max – Insert an element (guaranteed to be larger than all other elements in the tree) as a child of the maximum. Update the pointer to the maximum.
Delete-min– Remove the minimum from its position. Update the pointer to the minimum by searching for the minimum element from the root.
Of course, after each deletion or insertion, corrections are made to the red-black tree, as learned.
Execute the following series of operations: Given a sorted array of size \( n \) and an empty red-black tree, perform Insert-max on the array elements from smallest to largest. Then, perform Delete-min until the tree is empty.
Provide the tightest possible bound on the runtime of the series of operations.
a. \( O(1) \)
b. \( O(\log \log n) \)
c. \( O(\log n) \)
d. \( O(n) \)
e. \( O(n \log n) \)
f. \( O(n^2) \)",E,na,"Complexity analysis,   OTHER"
Multiple choice with explanations,5,"\begin{enumerate}
    \item Reminder for the SELECT(i) algorithm to find the i-th largest element:
    \begin{enumerate}
        \item Divide the array into chunks of five.
        \item Sort each chunk of five.
        \item Find the median of the medians of the five-element groups using select and denote it as x.
        \item Perform a partition on the array based on x, marking m as the position to which x arrived.
        \item If m=i, return x. Otherwise, if i<m, continue recursively on the left side (elements smaller than x) with i; otherwise, continue recursively on the right side (elements larger than x) with i-m.
    \end{enumerate}
    A student implemented the algorithm, but in step 3 mistakenly found the maximum of the medians. That is, instead of finding the median of the medians of the five-element groups, he took the maximum among the medians of the groups, found by passing linearly over the medians. What is the runtime and does the algorithm guarantee a correct result?
    \item[(A)] O(n), and the algorithm is correct.
    \item[(B)] O(n \log n), and the algorithm is correct.
    \item[(C)] O(n^2), and the algorithm is correct.
    \item[(D)] O(n), and the algorithm is incorrect.
    \item[(E)] O(n \log n), and the algorithm is incorrect.
    \item[(F)] O(n^2), and the algorithm is incorrect.
\end{enumerate}",C,na,"Selection and median-of-median algorithm,   Complexity analysis"
Multiple Choice,5,"\textit{We define a family of functions (mapping $n$ keys to a table of size $m$) as almost universal: \newline
For any two distinct values from the domain of $n$ keys, the probability of randomly selecting a function from the family such that it returns the same value (among $m$ values) for both is bounded by $m^{-1/3}$. \newline
What is the minimum table size $m$ such that if we choose a function randomly from an almost universal family, the expected number of collisions is bounded by $\frac{1}{2}$? \newline
A. $n^{1/3}$ \newline
B. $n^{2/3}$ \newline
C. $n^{4/3}$ \newline
D. $n^2$ \newline
E. $n^3$ \newline
F. $n^6$}",F,na,"Hash tables,   OTHER"
Multiple choice with explanations,5,"\[
\text{Define a family of functions (mapping n keys to a table of size m) that is almost universal:}
\]
\[
\text{For any two different values from the domain of n keys, the probability of randomly selecting a function }
\]
\[
\text{from the family that returns the same value (out of m values) for both is bounded by } m^{-1/3}.
\]
\[
\text{Let } X \text{ denote the minimum table size such that if we randomly select a function from an almost universal family,}
\]
\[
\text{the expected number of collisions is bounded by 1/2.}
\]
\[
\text{A student constructed a table of size } 8X. \text{ After how many expected trials will you find a function with no collisions?}
\]
\[
\text{Choose the tightest upper bound.}
\]
\[
\text{A. 1}
\]
\[
\text{B. \frac{8}{7}}
\]
\[
\text{C. \frac{4}{3}}
\]
\[
\text{D. 2}
\]
\[
\text{E. 4}
\]
\[
\text{F. 8}
\]",C,na,"Hash tables,   OTHER"
Multiple Choice,5,"\begin{quote}
Define a structure of a dictionary containing $n$ words. The dictionary is divided into $n/k$ pages, with each page containing $k$ words (assume $n/k$ is an integer). The words are sorted such that every word on page $i$ is lexicographically before all the words on page $i+1$, but within each page, the words are not sorted. The pages are placed in an array, providing direct access to them, and each page is also an array of words, allowing direct access to them as well. Assume all words are distinct from each other and a comparison between two words takes $O(1)$ time. We want to implement a search operation for a word in the entire dictionary. What size of $k$ will allow the best asymptotic runtime?

\begin{enumerate}
    \item $k = \log n$
    \item $k = \frac{n}{\log n}$
    \item $k = 1$
    \item $k = \sqrt{n}$
    \item Answers \textbf{a} and \textbf{c} are equivalent and will provide the best asymptotic runtime.
    \item The size of $k$ is irrelevant, search time for a word will be the same for any value of $k$.
\end{enumerate}
\end{quote}",E,na,"Complexity analysis,   Arrays and linked lists,   OTHER"
Multiple choice with explanations,5,"\begin{quote}
We define the structure of a dictionary containing \(n\) words. The dictionary is divided into \(n/k\) pages, with \(k\) words on each page (assume \(n/k\) is an integer). The words are sorted such that every word on page \(i\) is lexicographically before all the words on page \(i+1\), but within each page, the words are not sorted. The pages are stored in an array, so there is direct access to them, and each page is an array of words, which are also directly accessible. Assume all the words are distinct from each other and comparing two words takes \(O(1)\). We want to implement a search operation for a word in the entire dictionary. What value of \(k\) will allow for the best asymptotic running time, assuming additionally that the words within each page are lexicographically sorted?
\begin{enumerate}
    \item \(k = \log n\)
    \item \(k = \frac{n}{\log n}\)
    \item \(k = 1\)
    \item \(k = \sqrt{n}\)
    \item Options a and c are equivalent and will yield the best asymptotic running time.
    \item The size of \(k\) is not important; the running time for a word search will be the same for every value of \(k\).
\end{enumerate}
\end{quote}",F,na,"Complexity analysis,   Arrays and linked lists,   OTHER"
Multiple choice with explanations,3,"\textbf{Define an ""almost sorted"" array as follows: every element in the array is at a distance of up to $\log n$ positions from its location in a sorted array. It can be assumed that the elements are distinct from each other. We will discuss the runtime of sorting the array. Mark True/False: The array can be sorted in time $O(n \log \log n)$. Add a brief explanation.}",1,na,"Complexity analysis,   Arrays and linked lists,   Lower bound for comparison based sorting"
Multiple choice with explanations,3,"\begin{quote}
Define an ""almost sorted"" array as follows: each element in the array is at a distance of up to $\log n$ places from its position in a sorted array. You can assume that the elements are distinct from each other. We will deal with the running time of sorting the array.

Mark correct / incorrect: The array can be sorted in time $O(n \log n)$.
Add a brief explanation.
\end{quote}",1,na,"Complexity analysis,   Arrays and linked lists,   Lower bound for comparison based sorting"
Multiple choice with explanations,3,"\textbf{Define an ""almost sorted"" array as follows: each element in the array is within a distance of at most $\log n$ places from its position in a sorted array. It can be assumed that the elements are distinct. We will discuss the runtime of sorting the array. Mark true / false: It is possible to reduce to the problem of sorting a general array and therefore the lower bound for sorting the array is $\Omega(n \log n)$. Add a short justification.}",False,na,"Complexity analysis,   Arrays and linked lists,   Lower bound for comparison based sorting"
Multiple choice with explanations,3,"\text{Write whether the following statement is true/false (assuming that the elements are distinct from each other):}\\
\text{Given 2 min-heaps of n elements each (each represented in an array), they can be combined into one min-heap of 2n elements in } O(n).\\
\text{Add a short explanation.}",1,na,"Complexity analysis,   Binary heaps"
Multiple choice with explanations,3,"\begin{quote}
State whether the following claim is true or false (it can be assumed that the elements are distinct from each other):
Given 2 binomial heaps, each with $n$ elements, they can be merged into a single binomial heap with $2n$ elements in $O(n)$.
Add a brief justification.
\end{quote}",1,na,"Complexity analysis,   Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,3,"\textbf{Write whether the following statement is true / false (it can be assumed that the elements are distinct from each other):} Given 2 binomial heaps of $n$ elements each, they can be combined into one binomial heap of $2n$ elements in $O(\log n)$. \textbf{Add a brief justification.}",1,na,"Complexity analysis,   Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,3,"\textbf{Write whether the following claim is true/false (it can be assumed that the elements are distinct from each other):} Given 2 red-black trees each with \( n \) elements, it is possible to join them into a red-black tree with \( 2n \) elements in \( O(\log n) \). \textbf{Add a brief argument.}",False,na,"Complexity analysis,   OTHER"
Multiple choice with explanations,5,"In the course, there are \(N\) students. The grades awarded in the course are any real numbers, and no two grades are identical. The instructors proposed four optional data structures for submission to the secretariat: 
1) A regular binary search tree with a size field. 
2) A red-black tree with a size field. 
3) A Fibonacci heap. 
4) A binary heap.

The secretariat must perform \(\sqrt{N}\) order statistics searches (such as finding the median score, the 100th largest score, etc.). The structures cannot be changed, and the auxiliary memory available for use is \(O(1)\).

We are interested in a structure that will minimize the asymptotic complexity of the secretariat's tasks (but not including the complexity of building the data structure by the instructors):

a. A Fibonacci heap and a red-black tree with a size field will deliver the best performance and will allow execution in \(\Theta(\log N)\).
b. The best performance that can be achieved on a regular binary search tree with a size field will cost \(\Theta(N \log N)\).
c. A binary heap will allow the task to be performed in \(\Theta(N \log \log N)\).
d. A red-black tree with a size field will deliver the best performance and will allow execution in \(\mathcal{O}(\sqrt{N} \log N)\).
e. A binary search tree with a size field and a red-black tree with a size field will deliver the best performance and will allow execution in \(\Theta(N)\).
f. None of the answers are correct.",D,na,"Complexity analysis,   Binary search trees,   Binomial heaps,   Fibonacci heaps,   OTHER"
Multiple choice with explanations,3,"In a course, there are \(N\) students. The grades awarded in the course are any real numbers, and no two grades are identical to each other. The lecturers proposed 4 optional data structures for submission to the secretariat:
1) A regular binary search tree with a size field. 2) A red-black tree with a size field. 3) A Fibonacci heap. 4) A binary heap.

The secretariat needs to perform \(\sqrt{N}\) order statistics searches (like finding the median score, the 100th largest score, etc.). The structures cannot be changed, and the auxiliary memory that can be used is \(O(1)\).

We are interested in a structure that minimizes the asymptotic complexity of performing the task by the secretariat (but not including the complexity of constructing the data structure by the lecturers).

Answer true/false:
In the above problem, in the worst case, the performance of a binary search tree with a size field will be asymptotically the same as those of a binary search tree without a size field.
A. True
B. False",A,na,"Complexity analysis,   Binary search trees"
Multiple choice with explanations,6,"In the course, there are \(N\) students. The grades awarded in the course are any real numbers, and no two grades are identical to each other. The lecturers proposed 4 optional data structures for submission to the secretariat: 1) A regular binary search tree with a size field. 2) A red-black tree with a size field. 3) A Fibonacci heap. 4) A binary heap.

The secretariat needs to perform \(\sqrt{N}\) order statistics searches (such as finding the median grade, the 100th largest grade, etc.). The structures cannot be modified, and the auxiliary memory available for use is \(O(1)\).

We are interested in a structure that will reduce the asymptotic complexity of performing the task by the secretariat (but not including the complexity of building the data structure by the lecturers).

Now we want to perform a series of \(\Theta(N)\) order statistics requests. To ease the task, the university administration is willing to provide the secretariat with the series of requests for order statistics searches in sorted order. For example, the administration will submit to the secretariat the list (1, 3, 5, 7, \ldots, N) or the list (1, 2, 3, \ldots, N/4). Among the four structures, the one through which the best time can be achieved is:
a. Fibonacci heap.
b. Red-black tree with a size field.
c. Binary search tree with a size field.
d. b+c
e. a+c
f. a+b+c.",D,na,"Complexity analysis,   Binary search trees,   Binomial heaps,   Fibonacci heaps,   OTHER"
Multiple choice with explanations,5,"\textbf{Reminder for the SELECT(i) algorithm for finding the i-th largest element:}
1. Divide the array into groups of five.
2. Sort each group of five.
3. Find the median of the medians of the groups using select (let's call it x).
4. Perform a partition on the array based on x (let m be the position to which x moves).
5. If i=m, return X. Otherwise, if i<m, continue recursively on the left part with i, else continue recursively on the right part with i-m.
\[ \text{A student implemented the algorithm, but in step 3 accidentally found the } 2/5 \text{ element. That is, instead of finding the median of the medians of the quintets, he took the } 2/5 \text{ element among the medians of the quintets, which he found using select. What is the recurrence formula that describes the running time?}

\text{a.} \ T(n) \leq T(9n/10) + O(n) \\
\text{b.} \ T(n) \leq T(3n/5) + T(3n/5) + O(n) \\
\text{c.} \ T(n) \leq T(7n/10) + O(n) \\
\text{d.} \ T(n) \leq T(19n/25) + T(n/5) + O(n) \\
\text{e.} \ T(n) \leq T(3n/5) + T(n/5) + O(n) \\
\text{f.} \ T(n) \leq T(9/25n) + T(n/5) + O(n) \\
\text{g.} \ \text{None of the answers is correct} \]",D,na,"Complexity analysis,   Selection and median-of-median algorithm"
Multiple choice with explanations,5,"\textbf{Reminder for the SELECT(i) algorithm for finding the i-th largest element:}
1. Divide the array into groups of five.
2. Sort each group of five.
3. Find the median of the medians of the groups using select (let's call it x).
4. Perform a partition on the array based on x (let m be the position to which x moves).
5. If i=m, return X. Otherwise, if i<m, continue recursively on the left part with i, else continue recursively on the right part with i-m.
A student implemented the algorithm, but in step 3, he mistakenly found the 2/5-th element. In other words, instead of finding the median of the medians of the groups, he selected the 2/5-th element among the medians, which he found using select. What is the recurrence formula describing the running time?

What is the running time and does the algorithm definitely produce the correct result?
A. O(n), and the algorithm is correct.
B. O(n \log n), and the algorithm is correct.
C. O(n^2), and the algorithm is correct.
D. O(n), and the algorithm is incorrect.
E. O(n \log n), and the algorithm is incorrect.
F. O(n^2), and the algorithm is incorrect.
G. The running time is infinite and the algorithm is incorrect.",A,na,"Complexity analysis,   Selection and median-of-median algorithm"
Multiple choice with explanations,5,"\textbf{A counter is a data structure that holds a non-negative integer and supports the Increment operation. A call to the Increment operation increases the value of the counter by 1. The counter is implemented using an array initialized to zeros, holding the counter's value in binary representation. The Increment implementation is performed by iterating over the array from the least significant bit (LSB) in ascending order: for each bit encountered with a value of 1, it is changed to 0. The first bit with a value of 0 encountered is changed to 1, and then the process stops. It is assumed that the size of the array is sufficient to hold the counter's value.}

\textbf{Assume that the current counter value is $b$, and $m$ Increment calls are made from this state (where $b$ is not necessarily fixed!). What is the total runtime of these $m$ calls?}

\begin{itemize}
    \item (a) The runtime is $O(m)$.
    \item (b) The runtime is $O(b)$.
    \item (c) Given that $n = \Omega(b)$, the runtime is $O(m)$.
    \item (d) Given that $m = \Omega(b)$, the runtime is $O(m)$. If $m = o(b)$, any sequence of $m$ calls will run in time $\Theta(b)$.
    \item (e) None of the answers are correct.
\end{itemize}",C,na,"Complexity analysis,   Arrays and linked lists,   OTHER"
Multiple choice with explanations,5,"\textbf{A counter is a data structure that holds a non-negative number and supports the Increment operation. A call to the Increment operation increases the value of the counter by 1. A counter is implemented by an array initialized to zeros and holds the counter's value in binary representation. The implementation of Increment is done by traversing the array from the LSB in ascending order: every bit encountered with a value of 1 is changed to 0, and the first bit with a value of 0 that is encountered is changed to 1, and then the process stops. It can be assumed that the array size is sufficient to hold the counter's value.}

\textbf{An additional operation, k-Increment, is added to the counter structure, which increases the current counter value by k, where k is a positive integer. (k should not be considered constant, but rather an additional parameter of the problem!) k-Increment is implemented by k consecutive calls to Increment.}

\textbf{In this question, it is assumed that the counter is initialized to 0. What is the amortized time complexity of the k-Increment function? (The answer may depend on the parameter k, the number of function calls so far, m, and/or the counter's final value, n.)}
\begin{enumerate}
    \item The amortized time complexity is \(O(1)\), as in the case of the Increment operation.
    \item The amortized time complexity is \(O(\log n)\), where \(n\) is the final counter value, as in the case of the Increment operation.
    \item The amortized time complexity is \(O(\log m)\), where \(m\) is the number of function calls.
    \item The amortized time complexity is \(O(k)\).
    \item The amortized time complexity is \(O(k \log n)\), where \(n\) is the final counter value.
    \item None of the answers is correct.
\end{enumerate}",D,na,"Amortized analysis,   Arrays and linked lists,   OTHER"
Multiple choice with explanations,5,"Given a comparison tree suitable for an algorithm that sorts $n$ numbers, what is the minimum depth of a leaf in the tree?
\begin{enumerate}
    \item Every path between the root and a leaf in such a tree is of length \(\Omega(n \log n)\).
    \item While the maximum depth is \(\Omega(n \log n)\), the minimum depth can be \(O(1)\).
    \item The minimum depth of a leaf in the tree is \(O(1)\).
    \item The minimum depth of a leaf in the tree is \(\Omega(\log n)\).
    \item The minimum depth of a leaf in the tree is \(\Omega(n)\).
    \item If there is a leaf at depth \(o(n \log n)\), there must be another leaf at depth \(\omega(n \log n)\), which means that the worst-case running time of the sorting algorithm discussed in the question is necessarily \(\omega(n \log n)\).
\end{enumerate}",E,na,"Binary search trees,   Lower bound for comparison based sorting"
Multiple choice with explanations,5,"
\textbf{The following is the pseudocode for the Huffman algorithm. What is the runtime of the algorithm when the priority queue $Q$ is implemented with a binomial heap using lazy meld? Choose the most accurate answer.}

\textbf{Huffman}($C$) \quad /* $C$ is an array containing the alphabet */
\begin{itemize} \item $n \gets |C|$ \item \textbf{for} $i \gets 1$ \textbf{to} $n$ \begin{itemize} \item $z \gets C[i]$ \item \textbf{insert}($z, Q$) \end{itemize} \item \textbf{for} $i \gets 1$ \textbf{to} $n-1$ \begin{itemize} \item \textbf{do new}($z$) \item \textbf{left}($z$) $\gets x \gets$ \textbf{delete-min}($Q$) \item \textbf{right}($z$) $\gets y \gets$ \textbf{delete-min}($Q$) \item $f(z) \gets f(x) + f(y)$ \item \textbf{insert}($z, Q$) \end{itemize} \item \textbf{return} \textbf{delete-min}($Q$) \end{itemize}
\begin{enumerate}
    \item All the delete-min(Q) operations in the algorithm cost $O(\log n)$.
    \item All the delete-min(Q) operations in the algorithm cost $O(n)$ and none of them can be said to be $o(n)$.
    \item All the delete-min(Q) operations in the algorithm cost $O(\log n)$ except for the first two, which are $\Theta(n)$.
    \item All the delete-min(Q) operations in the algorithm cost $O(\log n)$ except for the first one, which is $\Theta(n)$.
    \item All the delete-min(Q) operations in the algorithm cost $O(1)$ except for the first one, which is $\Theta(\log n)$.
    \item Incorrect answer.
\end{enumerate}",D,na,"Complexity analysis,   Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,5,"A new algorithm named new-rand-partition receives an array A with indices from 1 to n. It performs:
1. Randomly select an integer between 1 and n (inclusive) without repetition and store it in variable x. Let A[x] be denoted by p.
2. Perform a partition according to the pivot element p. Denote by m the position of the element p in the array at the end of the process.
3. If m<n/6 or m>5n/6, return the array to its state before step 2 and go back to section 1. Otherwise, finish.

Assume that in section 1, each time a different number is drawn (i.e., drawing without repetition).
What is the number of comparisons the algorithm makes in the worst case?
a. \(O(1)\)
b. \(O(\log n)\)
c. \(O(n)\)
d. \(O(n \log n)\)
e. \(O(n^2)\)
f. The number of comparisons the algorithm makes in the worst case is not bounded.",E,na,"Complexity analysis,   Selection and median-of-median algorithm"
Multiple choice with explanations,5,"\begin{enumerate}
    \item Given an array with \( n \) integers in the range from 1 to \( \lfloor n \log n \rfloor \). Choose the most correct answer.
    \begin{enumerate}
        \item It is possible to sort using count-sort in \( O(n) \) time in the worst case.
        \item It is possible to sort using radix-sort with base \( \sqrt{n} \) in \( O(n) \) time in the worst case.
        \item It is possible to sort using merge-sort in \( O(n) \) time in the worst case.
        \item It is possible to sort using quick-sort in \( O(n) \) time in the worst case.
        \item Answers (a) and (b).
        \item None of the answers is correct.
    \end{enumerate}
\end{enumerate}",B,na,"Complexity analysis,   Non-comparison based sorting (radix,   bucket,   counting)"
Multiple choice with explanations,5,"\begin{quote}
In the homework, we encountered a d-ary heap. This is a heap that is implemented like a binary heap, with the difference that each node—except for leaves and at most one node in the second-to-last layer—has d children. The heap is represented using an array. The root is at position 0, and the children of index i are at positions di+1, ..., di+d. Note that d is not constant, but a parameter in the question.

\texttt{Huffman(C)} \\
$n \leftarrow |C|$ \\
$Q \leftarrow C$ \\
\textbf{for} $i \leftarrow 1$ \textbf{to} $n-1$ \\
\hspace{1em} \textbf{do} \textbf{new}$(z)$ \\
\hspace{2em} \textbf{left}$(z) \leftarrow x \leftarrow \textbf{delete-min}(Q)$ \\
\hspace{2em} \textbf{right}$(z) \leftarrow y \leftarrow \textbf{delete-min}(Q)$ \\
\hspace{2em} $f(z) \leftarrow f(x) + f(y)$ \\
\hspace{2em} \textbf{insert}$(z,Q)$ \\
\textbf{return} \textbf{delete-min}$(Q)$

What is the running time of Huffman's algorithm, when the priority queue $Q$ is implemented using a d-ary heap? \\
(A) $O(n)$ \\
(B) $O(n \log_d n)$ \\
(C) $O(n \log n)$ \\
(D) $O(n d)$ \\
(E) $O(n d \log_d n)$ \\
(F) $O(n^2)$
\end{quote}",E,na,"Complexity analysis,   Binary heaps"
Multiple choice with explanations,5,"\textbf{Given the following Compressed Suffix Tree structure (each edge can contain more than one character):}
O 
/ \      
 O O
/ \      
 O O
/ \      
 O O
/ \      
 O O

\textbf{Which strings can match it?}

a. ABCD  
b. ABCA  
c. ABAB  
d. ABBB  
e. AAAA 
f. c and d",E,na,Suffix trees
Multiple choice with explanations,5,"\[
\text{Given a binary search tree with pointers to the maximum and minimum, and with two additional operations: }
\]

\text{Insert-max – Insert an element (guaranteed to be larger than the other elements in the tree) as the right child of the maximum (a pointer to the maximum is given). Update the pointer to the maximum to the newly inserted element.}

\text{Delete-min – Find the successor of the minimum (search for the successor from the minimum position using the successor operation learned in class), delete the minimum from its position (a pointer is given). Update the pointer to the minimum to point to the successor.}

\text{What is the worst-case running time for the Insert-max operation:}

\text{a. }\ O(1)

\text{b. }\ O(\log \log n)

\text{c. }\ O(\log n)

\text{d. }\ O(n)

\text{e. }\ O(n \log n)

\text{f. }\ O(n^2)
",A,na,"Complexity analysis,   Binary search trees"
Multiple choice with explanations,5,"\text{Given a set } D \text{ of } n \text{ distinct elements and a family of universal functions } H \text{ from } D \text{ to a domain of size } m \text{ for which the collision probability is } \Pr[h(k_1) = h(k_2)] \leq 1/m \text{ for all } k_1 \neq k_2. \text{ It is given that } m = (n/ \log n). \text{ Assume that } h \text{ is chosen randomly, and we are interested in a function } h \text{ for which the number of collisions is less than } n \log n. \text{ Choose the tightest upper bound on the expected number of draws until a function satisfying the condition is drawn:} 
\text{a. } 1 
\text{b. } 2 
\text{c. } \log n 
\text{d. } n / 2 
\text{e. } n 
\text{f. The expected number of draws is unbounded.}",B,na,"Hash tables,   OTHER"
Multiple choice with explanations,3,"\textbf{Is the following statement true or false?} 
(Assume that the elements are distinct from one another). 
Given a red-black tree, a binary heap containing the same elements can be produced in O(n). 
Provide a brief justification.",1,na,"Binary heaps,   OTHER"
Multiple choice with explanations,3,"\textbf{Is the following statement true or false?} (Assume the elements are distinct from one another.) Given a binary heap, it is possible to create a sorted array containing the same elements in $O(n)$. \textbf{Add a short argument.}",False,na,"Binary heaps,   Arrays and linked lists"
Multiple choice with explanations,3,\textit{Is the following claim true or false? (Assume the elements are distinct from each other). A binary tree (not necessarily a search tree) can be uniquely reconstructed when the post-order traversal and the height of each element in the tree are given. Add a brief explanation.},False,na,Binary search trees
Multiple choice with explanations,3,\textbf{Is the following claim true or false?} (Assume that the elements are distinct from each other.) A binary tree (not necessarily a search tree) can be uniquely reconstructed when given an inorder traversal and the height of each element in the tree. Provide a brief explanation.,1,na,Binary search trees
Multiple choice with explanations,3,"\textbf{Is the following claim true or false?} 
(Assume that the elements are distinct from one another.)
A binary tree (not necessarily a search tree) can be uniquely reconstructed given an inorder traversal and the knowledge of whether each element is the root and whether it is a leaf.
Provide a brief justification.",False,na,Binary search trees
Multiple choice with explanations,3,"\textit{Is the following claim true or false? (Assume the elements are distinct from each other). In a Fibonacci heap, during a single decrease-key operation, $\Theta(n)$ cuts can occur (for any $n$). Add a brief explanation.}",1,na,"Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,3,"\textit{Is the following claim true or false? (Assume the elements are distinct from each other). Given 2 binomial heaps (not lazy) each containing n elements, they can be merged into a single binomial heap with 2n elements in $O(\log n)$. Provide a short justification.}",1,na,"Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,3,"\begin{quote}
Is the following statement true or false? (Assume that the elements are distinct from one another). In a lazy binomial heap, in one del-min operation more than $n/2$ linkings can occur (for any even $n$). Add a brief justification.
\end{quote}",1,na,"Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,3,"\textbf{Is the following claim true or false?} 
(Assume that the elements are distinct from one another). 
In a binary heap (with more than 10 elements), the second largest element (the largest one except for the maximum) is necessarily a leaf.
Provide a short justification.",False,na,Binary heaps
Multiple choice with explanations,5,"\textbf{Given an implementation of a queue using two stacks $S_1$ and $S_2$, which are initialized as empty:}

\textbf{Enqueue(x):}
\begin{verbatim}
S1.push(x)
\end{verbatim}

\textbf{Dequeue():}
\begin{verbatim}
If (S2.empty())
    While (not(S1.empty()))
        x = S1.pop()
        S2.push(x)
return S2.pop()
\end{verbatim}

A student analyzed the amortized running time using two potential functions: $f_1 = |S_1| + |S_2|$ and $f_2 = |S_1| - |S_2|$ (where $|S_i|$ represents the number of elements in stack $S_i$). What is the amortized running time of dequeue according to each function:

a. $O(1)$ according to both functions.  
b. $O(1)$ according to $f_1$ and $O(n)$ according to $f_2$.  
c. $O(1)$ according to $f_2$ and $O(n)$ according to $f_1$.  
d. $O(n)$ according to both functions.  
e. One of the functions is invalid for amortized running time analysis and according to the other $O(1)$.  
f. One of the functions is invalid for amortized running time analysis and according to the other $O(n)$.",F,na,"Amortized analysis,   OTHER"
Multiple choice with explanations,5,"\text{Given an array with } n \text{ integers in the range 1 to } n\text{. Choose the most correct answer.} \\ 
\text{A. It is possible to sort using count-sort in } O(n) \text{ time in the worst case.} \\
\text{B. It is possible to sort using radix-sort in base 2 in } O(n) \text{ time in the worst case.} \\
\text{C. It is possible to sort using merge-sort in } O(n) \text{ time in the worst case.} \\
\text{D. It is possible to sort using quick-sort in } O(n) \text{ time in the worst case.} \\
\text{E. Answers A and B.} \\
\text{F. None of the answers are correct.}",A,na,"Complexity analysis,   Non-comparison based sorting (radix,   bucket,   counting)"
Multiple choice with explanations,5,"\textbf{Huffman}($C$)
\begin{itemize} \item $n \gets |C|$ \item $Q \gets C$ \item \textbf{for} $i \gets 1$ \textbf{to} $n-1$ \begin{itemize} \item \textbf{do new}($z$) \item \textbf{left}($z$) $\gets x \gets$ \textbf{delete-min}($Q$) \item \textbf{right}($z$) $\gets y \gets$ \textbf{delete-min}($Q$) \item $f(z) \gets f(x) + f(y)$ \item \textbf{insert}($z, Q$) \end{itemize} \item \textbf{return} \textbf{delete-min}($Q$) \end{itemize}
\begin{enumerate}
\item What is the running time of Huffman's algorithm when the priority queue \( Q \) is implemented using a Fibonacci heap?
\begin{enumerate}
\item O(\log n)
\item O(n)
\item O(n \log n)
\item O(n \log^2 n)
\item O(n^2)
\item O(n^2 \log n)
\end{enumerate}
\end{enumerate}",C,na,"Complexity analysis,   Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,5,"Given a binary search tree with pointers to the maximum and minimum, and two additional operations have been added:
Insert-max – Insert an element (guaranteed to be larger than the rest of the tree elements) as the right child of the maximum (given a pointer to the maximum). Update the pointer to the maximum to the newly inserted element.
Delete-min – Find the successor of the minimum (search the successor from the minimum position using the successor operation learned in class), delete the minimum from its position (given a pointer). Update the pointer to the minimum to point to the successor.
The following series of operations are performed: Given a sorted array of size \( n \) and an empty binary search tree, perform Insert-max on the array elements from smallest to largest. Then, perform Delete-min until the tree is empty.
What is the worst-case running time for the described series of operations?
A. \( O(1) \)
B. \( O(\log \log n) \)
C. \( O(\log n) \)
D. \( O(n) \)
E. \( O(n \log n) \)
F. \( O(n^2) \)",D,na,"Complexity analysis,   Binary search trees"
Multiple choice with explanations,5,"Given a binary search tree with pointers to the maximum and minimum, two additional operations are introduced:

Insert-max – insert an element (guaranteed to be larger than the other elements of the tree) as the right child of the maximum (a pointer to the maximum is provided). Update the pointer to the maximum to the newly inserted element.

Delete-min – Find the successor of the minimum (search the successor from the minimum position using the successor operation learned in class), remove the minimum from its position (a pointer is provided). Update the pointer to the minimum to point to the successor.

The following sequence of operations is performed: Given a sorted array of size \( n \) and an empty binary search tree, perform Insert-max on the elements of the array from smallest to largest. Then, perform Delete-min until the tree is empty.

Select the tightest valid bound for the height of the tree throughout the sequence of operations. 
A. \( 1 \)
B. \( \log_2 \log_2 n \)
C. \( \log_2 n \)
D. \( \frac{n}{2} \)
E. \( n \)
F. \( n \log_2 n \)",E,na,"Complexity analysis,   Binary search trees"
Multiple choice with explanations,5,"Insert a series of n elements in some order into a binomial heap (not lazy). Then perform n delete-min operations from the heap.
In this question, we will count the number of linkings that occurred, i.e., the number of times a tree was attached to another tree.
What is the number of linkings that occurred in the worst case of all insert operations together, and of all delete-min operations together?
a. $O(\log n)$ for insert and $O(\log n)$ for delete-min  
b. $O(n)$ for insert and $O(1)$ for delete-min  
c. $O(n)$ for insert and $O(n \log n)$ for delete-min  
d. $O(n)$ for insert and $O(n)$ for delete-min  
e. $O(n \log n)$ for insert and $O(n)$ for delete-min  
f. $O(n \log n)$ for insert and $O(n \log n)$ for delete-min  ",C,na,"Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,5,"Given an array of integers \(A\), we are required to create an array \(B\) containing, for each element \(i\), the minimum in the range \(A[i], A[i+1], \ldots, A[i + \lfloor \sqrt{n} \rfloor]\).
What is the tightest upper bound for creating the array \(B\):

A. \(O(\sqrt{n} \log n)\)  
B. \(O\left(\frac{n}{\log n}\right)\)  
C. \(O(n \log n)\)  
D. \(O(n \log^2 n)\)  
E. \(O(n \sqrt{n})\)  
F. \(O(n^2)\)",C,na,"Complexity analysis,   Arrays and linked lists"
Multiple choice with explanations,5,"Given an array of integers \(A\), we are required to create an array \(B\) which contains for each element \(i\) the minimum in the range \(A[i], A[i+1], \ldots, A[i + \lfloor\sqrt{n}\rfloor]\).
Given any indices \(n \le i < j \le 1\), what is the tightest upper bound (in the worst case) for finding the smallest element in the array located between indices \(i\) and \(j\)? We need to find the minimum value and its exact position in the array.
A. \(O(1)\)
B. \(O(\log n)\)
C. \(O(\sqrt{n})\)
D. \(O(n)\)
E. \(O(n \log n)\)
F. \(O(n \sqrt{n})\)",C,na,"Complexity analysis,   Arrays and linked lists"
Multiple choice with explanations,3,"\text{Is the following claim true or false? (Assume that the elements are distinct from each other.) Given two binary heaps, they can be melded in } O(n) \text{. Provide a brief justification.}",1,na,Binary heaps
Multiple choice with explanations,3,"Is the following statement true or false? (Assume that the elements are distinct from each other). Given a binary heap, it is possible to produce a sorted array containing the log n smallest elements in the heap in O(log n * log log n) time. Add a short explanation.",1,na,"Binary heaps,   Arrays and linked lists"
Multiple choice with explanations,3,\textit{Is the following statement true or false? (Assume that the elements are distinct.) A binary search tree can be reconstructed from an unordered list of its elements and a sorted list of the leaves in the tree. Provide a short explanation.},False,na,Binary search trees
Multiple choice with explanations,3,"\textit{Is the following claim true or false? (Assume that the elements are distinct from each other). In a Fibonacci heap, during a dec-key operation, it is possible that 0 deletions of elements occur. Add a brief explanation.}",1,na,"Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,3,"\text{Is the following statement true or false? (Assume the elements are distinct from each other.) Given 2 Fibonacci heaps, each with } n \text{ elements, they can be combined into one Fibonacci heap with } 2n \text{ elements in } O(\log n). \text{ Add a short justification.}",1,na,"Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,3,"\text{Is the following claim true or false? }
\text{(Assume that the elements are distinct from one another). }
\text{In a lazy binomial heap, exactly n linkings can occur in one del-min operation (for every n). }
\text{Add a brief argument.}",False,na,"Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,3,"\textbf{Is the following claim true or false?} (Assume that the elements are distinct from one another). In a binary heap (with more than 10 elements), the largest value after the maximum can be a child of the root. \textbf{Provide a brief justification.}",False,na,Binary heaps
Multiple choice with explanations,5,"Given a data structure with \( n \) numbers and a parameter \( k \) (where \( k \) is not fixed, but part of the input). We want to find the \( k \)-th largest element and the \( (n-k) \)-th largest element.
What will be the best runtime for a binary search tree with a size field in the nodes (size = the number of nodes for which this node is the root)?
a. \( O(1) \)
b. \( O(\log n) \)
c. \( O(\min(\log n, k)) \)
d. \( O(k + \log n) \)
e. \( O(k \log n) \)
f. \( O(n) \)",B,na,"Complexity analysis,   Binary search trees"
Multiple choice with explanations,5,"
Given an array of size $n$, in which every value appears exactly twice, the most efficient algorithm for sorting the array in the comparison model operates in the worst-case time:
\begin{enumerate}
\item $\Omega(n \log n)$
\item $\Omega(n^3)$
\item $\Omega(n^3 \log n)$
\item $\Omega(n^3 \log^2 n)$
\item $\Omega(n^3 \log^3 n)$
\item None of the answers are correct
\end{enumerate}",C,na,"Complexity analysis,   Arrays and linked lists,   Lower bound for comparison based sorting"
Multiple choice with explanations,5,"\textbf{Given a Hash table (with a Hash function chosen randomly from a universal set). In this table, collisions are resolved as follows:} \\
Every element mapped to a non-empty cell is stored in a shared red-black tree for the entire table. A search is carried out by checking the appropriate cell (using the Hash function), and if the cell is not empty and the desired element is not there, we search in the tree. \\
Let \( B \) be the size of the table, \( N \) the number of elements, and \( \alpha = \frac{N}{B} \).

\textbf{The expected time for an unsuccessful search is:}  
a. \( \Theta(1+\alpha) \) on average if \( B= \Theta(N) \). \\
b. \( \Theta(1) \) on average if \( N^2 = B \). \( \Theta(\log N) \) on average if \( 10N = B \). \\
c. \( \Theta(1) \) in the worst case if \( N^2=B \) (the worst case is over all sequences of \( N \) elements and function choices); \( \Theta(\log N) \) in the worst case if \( 10N=B \). \\
d. \( \Theta(\frac{\log(N-B)}{B}) \) \\
e. \( \Theta(N) \) in the worst case. \\
f. None of the answers are correct.",B,na,"Complexity analysis,   Hash tables,   OTHER"
Multiple choice with explanations,5,"\[
\text{Given a hash table (with a hash function chosen randomly from a universal set). In this table, collisions are resolved as follows:}
\]
\[
\text{Every element mapped to a non-empty cell is stored in a red-black tree shared by the entire table. A search is performed by checking the appropriate cell (using the hash function), and if the cell is not empty and the desired element is not there, the search is performed in the tree.}
\]
\[
\text{Let } B \text{ be the size of the table, } N \text{ the number of elements, and } \alpha = \frac{N}{B}.
\]
\[
\text{The successful search time is bounded by:}
\]
\begin{enumerate}
    \item \( O(1) \)
    \item \( O(\log \log n) \)
    \item \( O(\log n) \)
    \item \( O(n) \)
    \item \( O(n \log n) \)
    \item \( O(n^2) \)
\end{enumerate}",C,na,"Complexity analysis,   Hash tables,   OTHER"
Multiple choice with explanations,5,"A given array $A$ of size $n$ is ""almost sorted,"" meaning that each element is at most $\log n$ positions away from its place in the sorted array.

We want the most efficient (in W.C.) algorithm to sort array $A$. This can be done in time:
\begin{enumerate}
    \item $O(\log n)$
    \item $O(n)$
    \item $O(n \log \log n)$
    \item $O(n \log n)$
    \item $O(n \log^2 n)$
    \item $O(n^2)$
\end{enumerate}",C,na,Arrays and linked lists
Multiple choice with explanations,5,"Two Fibonacci heaps, \( H_1 \) and \( H_2 \), are given. Let \( R_1 \) and \( R_2 \) denote the number of roots of \( H_1 \) and \( H_2 \) respectively. The heaps \( H_1 \) and \( H_2 \) are united using the union algorithm of a binomial heap to form a heap \( H \). Let \( R \) be the number of roots of \( H \). We want to know the relation between \( R_1 + R_2 \) and \( R \).

a. \( R = R_1 + R_2 \) always.

b. \( R \neq R_1 + R_2 \) always.

c. \( R \leq R_1 + R_2 \) always.

d. \( R \geq R_1 + R_2 \) always.

e. It is not possible to know.

f. None of the answers is correct.",A,na,"Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,5,"Given an array with numbers, the task is to find the longest contiguous subarray whose sum of elements is exactly equal to $0$. It is given that all the numbers are $1$ or $-1$. What is the best runtime for solving this problem?
\begin{enumerate}
\item $O(1)$
\item $O(\log \log n)$
\item $O(\log n)$
\item $O(n)$
\item $O(n \log n)$
\item $O(n^2)$
\end{enumerate}",D,"\documentclass{article}
\usepackage[utf8]{inputenc}

\begin{document}

We record an array of partial sums from the first position to the last. The list can be sorted in $O(n)$ since all the sums are between $-n$ to $n$. We look for a pair in the sorted list whose partial sum is identical, and for which the sum between the two elements is exactly 0. We search for the pair whose distance between its elements is the greatest.

\end{document}",Arrays and linked lists
Multiple choice with explanations,3,"\textit{Is the following claim true or false? (Assume the elements are distinct from each other). Given an algorithm that, given an element in an array, returns the answer of what its position would have been in the array if it were sorted (order), in time $O(\log n)$. Then it is possible to sort an unsorted array using the algorithm in $O(n)$. Add a short justification.}",False,na,"Arrays and linked lists,   Lower bound for comparison based sorting"
Multiple choice with explanations,3,"\textbf{Is the following claim true or false?} 
(Assume that the elements are distinct from one another).
Given an array where for every pair of elements at indices \(i\) and \(j\) where \(i + \log n = j\), it holds that the smaller element is at index \(i\). Then the array can be sorted in \(O(n)\).
\textbf{Provide a short justification.}",False,na,"Arrays and linked lists,   Lower bound for comparison based sorting"
Multiple choice with explanations,3,"\textbf{Is the following claim true or false?} (Assume the elements are distinct from each other). Given an array, it is known that after sorting the array, the difference between every two consecutive elements will be some constant (the value of the constant is unknown). Then the array can be sorted in \(O(n)\). Add a brief justification.",1,na,"Arrays and linked lists,   Lower bound for comparison based sorting"
Multiple choice with explanations,3,"\textit{Is the following claim true or false? (Assume the elements are distinct from one another). In a suffix tree, there are at most $n$ internal nodes, where $n$ is the length of the string. Add a short explanation.}",1,na,Suffix trees
Multiple choice with explanations,3,\textit{Is the following statement true or false? (Assume that the elements are distinct from each other.) It is possible to reconstruct a binary search tree from an in-order traversal when the root element is known. Add a brief explanation.},False,na,Binary search trees
Closed question with short explanations,3,"\textit{Given a series of n elements, what is the time complexity for finding the k smallest elements in the series (for k < n).}",na,"\textbf{Time complexity:} \(O(n)\) \\
\textbf{Brief explanation:} Using a selection algorithm we saw in class and then partition.",Selection and median-of-median algorithm
Multiple choice with explanations,2,"A sloppy minimum heap is a data structure similar to a heap, which instead of returning the minimum, returns one of the k smallest elements in the heap. Additionally, in the delete operation, it deletes the same element that it returned as the approximate minimum. The operations supported by the data structure (ADT) are:
\begin{itemize}
    \item \texttt{Insert(x)} - insertion of a new element with key \(x\).
    \item \texttt{FindApproxMin()} - return an approximate minimum (an element among the k smallest elements).
    \item \texttt{DeleteApproxMin()} - deletion of the approximate minimum (deletion of the same element that if we performed \texttt{FindApproxMin()} we would receive).
\end{itemize}
For \(k=1\) can a sloppy heap be implemented so that all operations in the ADT take amortized time \(O(1)\)?",False,"\noindent No. Explanation: No, because then we could reach a contradiction to the lower bound of sorting.","Amortized analysis,   Binary heaps"
Multiple choice with explanations,5,"A sloppy minimum heap is a data structure similar to a heap, where instead of returning the minimum element, it returns one of the K smallest elements in the heap. Additionally, in the delete operation, it deletes the element that was returned as the approximate minimum. The operations supported by the ADT are:
• \( \text{Insert}(x) \) - insertion of a new element with key \( x \).
• \( \text{FindApproxMin}() \) - return an approximate minimum (an element among the k smallest elements).
• \( \text{DeleteApproxMin}() \) - delete the approximate minimum (delete the same element that would be obtained by doing \( \text{FindApproxMin} \)).

For \( k=n/100 \), is it possible to implement the sloppy heap such that all operations in the ADT take amortized \( O(1) \) time?",1,"Yes. Explanation: Find the n/200 smallest elements – denote this set by S.
During the next n/200 operations, return an element that hasn't been removed from set S (note that there must necessarily be such an element). After n/200, rebuild (find the set S again according to the elements currently in the data structure). Note that the element returned by the data structure is always among the n/100 smallest.","Amortized analysis,   Binary heaps"
Closed question with short explanations,3,"\begin{enumerate}
    \item Given a valid binary min-heap with $n$ elements, at which levels in the tree can the second smallest element be located?
    \item What is the minimum number of elements you would need to read in order to guarantee finding the second smallest element?
    (For the purpose of numbering, the level of the root is 0)
\end{enumerate}",na,"\textit{Levels: At level 1. \\
Minimum number of places: 2, the minimum element can only be at level 1 and at level 1 there are two elements.}",Binary heaps
Closed question with short explanations,2,"Given a valid binary minimum heap of n elements, on which levels of the tree can the third smallest element be placed?",na,\text{Levels: Level 1 or 2.},Binary heaps
Multiple choice with explanations,7,"Given an array of size \( n \) of elements \((\text{key}, \text{value})\) where the number of distinct keys in the array is \( k \) (i.e., if we convert the keys in the array into a set, the set will be of size \( k \)). We want to sort the array in the comparison model so that the sorting will be stable.
Reminder: A sort is called stable if it maintains the order of elements with the same key after sorting.
Note: Without addressing stability in the explanation of the solution, only partial credit will be given.
a. When \( k = \sqrt{n} / \log n \), the array can be stably sorted in time \( O(\sqrt{n} \log n) \) using a balanced search tree.
b. When \( k = \log n \), the array can be stably sorted in time \( O(n \log \log \log n) \) using a balanced search tree.
c. When \( k = n / \log n \), the array can be stably sorted with complexity \( O(n) \) on average using a hash table.
d. When \( k = n \), the array can be stably sorted with complexity \( O(n \log \log n) \) on average using a hash table.",c,"\textbf{Solution:} c. \textbf{Explanation:} Initialize a hash table $A$ of size at least $k$ (with the goal that for each key, it will hold a pointer to a linked list of the elements in the array with the same key, in the order they appear in the array). Additionally, initialize an array $B$ of size $k$. Traverse the original array in order, and for each element, search $A$ by $key$. If it exists, append the element to the end of the linked list that the element points to. If it does not exist, add the key, initialize a linked list for that key, and insert the element there. Additionally, during traversal, if when searching for the key it is not found, add the key to array $B$. Sort the array $B$ in time $O((n\log n) \cdot \log(n\log n)) = O(n)$. Traverse $B$ in order, search for the corresponding linked list in the hash table, and append the linked list elements in order to the original array (or to a new output array).","Arrays and linked lists,   Lower bound for comparison based sorting"
Multiple choice with explanations,4,"\begin{quote}
After looking at the hash table using the chaining method, we thought it would be a good idea to change our choice of data structure (from a linked list to another data structure) to achieve better asymptotic complexity for insertion, search, and deletion from the hash table, even in the worst case (for any load factor), and on average (for load factor > 1). We denote load factor = \(\alpha\). Which data structure should we choose?

A. Binary heap. \\
B. Perfect hash table. \\
C. Array-based list that doubles in size. \\
D. Balanced search tree, such as AVL.
\end{quote}",d,"\textbf{d. Explanation:} We want to choose a balanced tree that will give us \(O(\log \alpha)\) expected per operation, and \(O(\log n)\) in the worst case.","Arrays and linked lists,   Balanced BST (AVL,   B),   Hash tables,   Binary heaps"
Multiple choice with explanations,4,"After looking at the hash table using the Chaining method, we thought it would be advisable to change our data structure choice (from a linked list to another data structure) in order to achieve better asymptotic complexity for insertion, search, and deletion from the hash table, even in the worst case (for any load factor), and also on average (for a load factor > 1). Let us denote load factor = \(\alpha\). What is the average complexity of insertion/search/deletion, depending on \(\alpha = \text{load factor}\)? (The tightest answer.)

A. \(O(\log \log \alpha)\)
B. \(O(\alpha \log \alpha)\)
C. \(O(\alpha)\)
D. \(O(\log \alpha)\)",d,"\textbf{D. Explanation}: For a balanced tree, the average complexity of an operation will be the expectation of (log of the number of elements in the cell). From the reminder, we obtain that the above expression is less than or equal to the log of (the expected number of elements in the cell). As is known, the expected number of elements in the cell is \(\alpha\), and therefore we obtain that the average complexity is \(\log \alpha\). The expectation is over the hash functions from the universal family.","Complexity analysis,   Arrays and linked lists,   Balanced BST (AVL,   B),   Hash tables,   Binary heaps"
Multiple choice with explanations,9,"\begin{quote}
In class, we learned about a deterministic \textit{select} that performed a \textit{partition} using a \textit{pivot} chosen by the median-of-medians algorithm. Assume that instead of the aforementioned \textit{pivot}, you received a new algorithm called \textit{GoodPivot} that takes an array containing \( n \) keys and returns the value of the \( 0.05n \)-th smallest element. For example, on an array of size 1000, it will return the value of the 50th smallest key. \textit{GoodPivot} does not call \textit{select} (unlike the median-of-medians algorithm). Assume the running time of \textit{GoodPivot} is bounded by \( c_1 \times n \), and the running time of \textit{partition} is bounded by \( c_2 \times n \). We are interested in the cost of \textit{select} using \textit{GoodPivot}. Choose the tightest answer. The running time of the algorithm is bounded by:
a. \( cn \)
b. \( c n \log n \)
c. \( cn^2 \)
Provide a proof by induction and the lowest value of \( c \) (as a function of \( c_1, c_2 \), and the problem data).
\end{quote}",a,"The tightest bound is: A. The value c is: 20. Proof: Induction - \( T(n) < (c_1+c_2)n + t(0.95n) \) 
\( T(n) < (c_1+c_2)n + 0.95 \times 20 (c_1+c_2)n = 20(c_1+c_2)n \)",Selection and median-of-median algorithm
Multiple choice with explanations,5,"In order to improve \texttt{quicksort}, a new function \texttt{GreatPivot} has been created. It receives an array containing $n$ keys and returns a random value that is uniformly distributed among all key values that are greater than or equal to the $\lceil 0.05n \rceil$-th largest and less than or equal to the $\lfloor 0.95n \rfloor$-th largest element. The \texttt{partition} function performs the partitioning using this value. In other words, unlike \texttt{randpartition}, which selects a pivot randomly from all elements, this choice is restricted to only 90\% of the elements mentioned above.

The cost of \texttt{greatpivot} is $O(1)$, and the cost of \texttt{partition} is linear. The time complexity of \texttt{quicksort} when using \texttt{partition} and \texttt{GreatPivot} is, in the worst case:
\begin{enumerate}
  \item $\Theta(n)$
  \item $\Theta(0.9n)$
  \item $\Theta(n \log n)$
  \item $\Theta(n^2)$
\end{enumerate}",c,"The complexity is: \( \theta(n \log n) \). Proof:  
The depth of the tree is bounded from above by \(\log_{1/0.95} n\) and from below by \(\log_{1/0.05} n\). That is, between these depths, the width of the layer is partial. Therefore, the bounds on the total width of all layers are \(n \log_{1/0.95} n\) and \(n \log_{1/0.05} n\).",Quick-sort
Closed question with short explanations,6,"\documentclass{article}
\begin{document}

In class, we learned about Union-Find implemented with linked lists. In this implementation, the \textit{union} operation was costly, and the \textit{find} was inexpensive. 

Reminder: In the union operation, the shorter list was merged into the longer one. Each list had a leader to whom all the elements of the list pointed. In the union operation, the algorithm traversed all the elements in the shorter list and updated their pointer to the leader of the longer list. The find operation returned the leader in $O(1)$ time.

We will deal with careless Find-Union. In it, merging the shorter list into the longer one is emphasized only if a certain condition concerning the two lists $A$ and $B$ holds. Otherwise, one is arbitrarily merged into the other.

If the ""emphasis"" condition is $5 < |\text{size}(A) - \text{size}(B)|$:

1. What is the maximum cost of the \textit{union} operation? (Counted by the number of elements changing groups)
2. What will be the complexity of $n$ \textit{union} operations? (Order of magnitude)

\end{document}",na,"\text{Maximum cost for a union operation (number of elements crossing a set): } \frac{n}{2} + 2 \\
\text{Justification:} \\
\text{Merge } \frac{n}{2} - 2 \text{ with } \frac{n}{2} + 2 \\
\text{Cost of } n \text{ union operations (order of magnitude): } O(n \log n) \\
\text{Justification: For each element, each transfer increases its group by a factor of at least 1.5 (if size > 10), so the number of times it transfers is at most } 10 + \log_{\text{base } 1.5} n",Union-Find
Closed question with short explanations,6,"\begin{itemize}
    \item In class, we learned about Union-Find implemented with linked lists. In this implementation, the union operation was costly and the find operation was cheap.
    \item Reminder: In the union operation, the shorter list was merged into the longer one. Each list had a leader to which all elements of the list pointed. In the union operation, the algorithm iterated over all the elements of the shorter list and updated their pointer to the leader of the longer list. The find operation returned the leader in \( O(1) \) time.
    \item We will discuss a careless Union-Find, where merging the shorter list into the longer one is enforced only if a certain condition regarding the two lists \( A \) and \( B \) is met. Otherwise, one is arbitrarily merged into the other.
    \item If the condition for ""enforcement"" is \( \frac{\text{size}(A)}{\text{size}(B)} > 5 \) or \( \frac{\text{size}(A)}{\text{size}(B)} < \frac{1}{5} \):
    \begin{enumerate}
        \item What is the maximum cost of a union operation? (Counted by the number of elements switching groups)
        \item What will be the complexity of \( n \) union operations? (Order of magnitude)
    \end{enumerate}
\end{itemize}",na,"\begin{itemize}
    \item Maximum cost for a $\text{union}$ operation (number of elements transferring groups): $\frac{5}{6}n$
    \item Merge $\frac{5}{6}$ with $\frac{1}{6} n \Rightarrow$ costs $\frac{5}{6}n$
    \item Justification:
    \item Cost of $n$ $\text{union}$ operations (order of magnitude): $O(n \log n)$
    \item Justification:
    \item For each element, each transfer increases its group by a factor of at least $\frac{6}{5}$, so the number of transfers is at most $\log_{1.2} n$
\end{itemize}",Union-Find
Multiple choice with explanations,3,"Alice ran the Perfect Hash algorithm learned in class (Two Level Hashing) on \( n > 100,000 \) keys. During the execution of the first part of the algorithm (creating the main table), Alice chose a function \( h_1 \) and discovered that the last \( 0.9n \) cells in the table are empty (we do not know what happened with the first cells). Answer True\False and justify: According to the algorithm, Alice can proceed to the second stage of the algorithm (creation of secondary level tables).",False,"\noindent No. Explanation: The sum of the squares is at least approximately $10n$. Definitely greater than $n$. According to the algorithm, return to step 1. $\text{col} \mid \geq \left(\frac{n}{10}\right) \left(\frac{10 \times 9}{2}\right) = \frac{9n}{2} > n \rightarrow \text{algorithm will repeat stage 1} \mid$.",Hash tables
Multiple choice with explanations,2,"Alice ran the Perfect Hash algorithm learned in class (Two Level Hashing) on 𝑛>100,000 keys. In running the first part of the algorithm (creating the main table), Alice chose a function ℎ1 and found that all 0.9𝑛 of the last slots in the table are empty (we do not know what happened in the first slots). Answer true\false and explain: In light of the results of the first stage, if Alice continues directly to the second stage, then the expected number of attempts to obtain good functions in the second stage will be greater than 10.",False,\textit{No. Explanation: The expectation is bounded by 2.},Hash tables
Closed question with short explanations,3,"\textit{An NAVL tree is a binary search tree.}

\textit{It satisfies the following properties:}

\begin{enumerate}
    \item \textit{Each leaf has rank 0.}
    \item \textit{Each external leaf has rank -1.}
    \item \textit{Each internal node has children with rank differences of 1-1, 1-2, 1-3, …, 1-n.}
\end{enumerate}

\textit{What is the minimum number of internal nodes (non-external) of an NAVL tree whose root rank is \( k \)? What is the search complexity of an element in a binary search tree of type NAVL (in the worst case)?}",na,"\begin{itemize}
    \item Number: k
    \item Brief explanation: The tree can be a linked list. When the children of each node at degree $i$ are an external leaf, and a node at degree $i-1$. The degree differences of such a node are $i+1, 1$.
    \item Complexity: $O(n)$
    \item Brief explanation: A tree as described in section A is a linked list. In such trees, searching is in $O(n)$ like in a linked list.
\end{itemize}","Balanced BST (AVL,   B)"
Multiple choice with explanations,6,"\begin{itemize}
    \item In class, we learned about a Lazy Binomial Heap where Successive Linking is performed during each \texttt{deletemin} operation (This refers to an implementation where linking is done until completion and not just one-pass).
    \item Let \texttt{fresh} be the number of ""fresh"" nodes (trees of size 1 that have not yet undergone Successive Linking) in the binomial heap.
    \item Let \texttt{old} be the number of the other nodes in the binomial heap. In this question, we deal with an Impatient Binomial Heap-Lazy where there is no \texttt{deletemin} and therefore Successive Linking is performed during \texttt{insert} operations. The execution of Successive Linking occurs only when an \texttt{insert} operation causes a ""large"" increase in the number of ""fresh"" nodes in the structure.
    \item Assume that Successive Linking is performed with each \texttt{insert} operation that causes \(\texttt{fresh} > 100\). What is the complexity of \texttt{insert}? Given that \(n\) is the number of nodes in the structure at the moment of operation execution.
    \begin{enumerate}
        \item Worst Case = \(O(\log n)\). Amortized = \(O(\log n)\)
        \item Worst Case = \(O(n)\). Amortized = \(O(\log n)\)
        \item Worst Case = \(O(\log n)\). Amortized = \(O(1)\)
        \item Worst Case = \(O(n)\). Amortized = \(O(1)\)
    \end{enumerate}
\end{itemize}",c,na,"Amortized analysis,   Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,7,"In class, we learned about \textbf{select} using a random \textbf{partition}. In a specific run of this algorithm on an array $A$ of length $n$, the function \textbf{select} (including recursive calls to itself) was called $0.75n$ times in total (where $n$ is divisible by $4$). Assume that the runtime of \textbf{partition} is linear in the length of its input.
The runtime complexity was (choose the tightest answer):
\begin{enumerate}
    \item \(O(n)\)
    \item \(O(n \log n)\)
    \item \(O(n^2)\)
    \item Any answer between \(O(n)\) and \(O(n^2)\) is possible and depends on the values of the array.
    \item It is impossible to bound the running time since only the expected running time can be calculated.
\end{enumerate}",3,مקרה הגרוע ביותר הוא בין 1 + 2 + 3 + ⋯ + 0.75n = \Theta(n^2) ל-0.25n + 0.25n + 1 + 0.25n + 2 + ... + n = \Theta(n^2),Selection and median-of-median algorithm
Multiple choice with explanations,5,"In class, we learned about select which uses a random partition. In a specific run performed by this algorithm on an array \( A \) of length \( n \), the function select (including calls to itself) was called 0.75\( n \) times (where \( n \) is divisible by 4) in total. Assume the running time of partition is linear in the length of its input. We are interested in the array \( A \) being sorted (from smallest to largest).
After the above run, the number of elements in the array \( A \) that are not in the correct place is (provide the tightest answer):
A. At most 0.25\( n \)
B. At most 0.75\( n \)
C. At least 0.75\( n \)
D. At most \(\log n\)
E. All elements are in place. The array is sorted.
F. Since partition is random - it is not possible to know, thus the other answers are incorrect.",1,"יש 0.75n צירים. כולם מגיעים למקומם. כל השאר אנחנו לא יודעים.

\textit{There are } 0.75n \textit{ pivots. They all land in their place. All the rest we don't know.}",Selection and median-of-median algorithm
Multiple choice with explanations,5,"\documentclass{article}
\usepackage[utf8]{inputenc}
\begin{document}

In class, we learned about lazy binomial heaps. We will be interested in an implementation where: 
1. insert is performed by adding a tree of size 1 to the end of the list of trees. 
2. In deletemin, after removing the minimum, we insert its children at the end of the list of trees and perform successive linking by scanning the trees (starting from the head of the list and ending at its tail). 

Alice (A) performed insert of the elements \(1, 2, \ldots, 2^k, 2^k + 1, 2^k + 2\) in ascending order and then executed two deletemin operations. 
Bob (B) performed insert of the same elements in descending order and then executed two deletemin operations. 
Let \(\#\text{links}(A), \#\text{links}(B)\) denote the number of link operations that will be performed in the second deletemin operation in the structure of A and in the structure of B, respectively. 
For \(1 < k\):

a. \(0 \sim |\#\text{links}(A) - \#\text{links}(B)|\)

b. \(|\#\text{links}(A) - \#\text{links}(B)| \sim \log k\)

c. \(|\#\text{links}(A) - \#\text{links}(B)| \sim k\)

d. \(|\#\text{links}(A) - \#\text{links}(B)| \sim 2^k\)

e. The answer depends on \(k\)

\end{document}",3,"\textit{Short explanation: (To illustrate the explanation, we can use $k=3$, but the reasoning is valid for general $k>1$). In one data structure, the minimum is in a tree of size 1, whereas in the other data structure, the minimum is in a tree of size $2k$ (with $k$ children).}","Binomial heaps,   Fibonacci heaps"
Multiple choice with explanations,1.6,"\begin{quote}
Bob implemented the perfect hash algorithm studied in class (two-level hashing). In running the first part of the algorithm (creating the main table), Bob chose the function \( h_1 \) and discovered that for the last \(1 + n - \sqrt{n} \) keys that he inserted into the main table, each one entered a ""private"" cell (i.e., without collision). Based on this information, Bob decided not to check the other keys and not to try additional functions in the first stage, and move on to the second stage. Answer true\false and justify. Within no more than 2 attempts in the second stage (for each sub-table), a perfect hash will be found.
\end{quote}",False,\textit{Not true. The second stage produces square tables and therefore works on average in two attempts. The data from the question did not affect this.},Hash tables
Multiple choice with explanations,1.6,"Bob improved the perfect hash algorithm learned in class (two-level hashing). During the execution of the first part of the algorithm (creating the main table), Bob chose a function h1 and found that the last \(1 + n - \sqrt{n} \) keys he inserted into the main table each went into a ""private"" cell (i.e., without collision). Based on this information, Bob decided not to check the other keys and not to try additional functions in the first stage and to move to execute the second stage. Answer true/false and explain. In expectation, within 2 attempts in the second stage (for each subtable), a perfect hash will be found.",1,"\textit{Correct. The second stage produces square tables, thus works on average in two attempts. The details of the question did not affect this.}",Hash tables
Multiple choice with explanations,1.6,"Bob ran the perfect hash algorithm learned in class (two-level hashing). In executing the first part of the algorithm (creating the main table), Bob chose a function $h_1$ and discovered that the last $\1+ n - sqrt{n}$ keys he inserted into the main table each went into a “private” cell (i.e., without collisions). Based on this information, Bob decided not to check the other keys, nor to try additional functions in the first stage, and moved on to execute the second stage. Answer True/False and justify:
The function $h_1$ does not guarantee Bob that the overall size of the structure will be linear in $n$, and therefore, he will not achieve the performance of the two-level hashing (perfect hash) learned in class.",False,"\textit{It does promise, by calculating the sum of the squares of the cells (the worst case that all the square root of N remaining elements fall in the same place is still linear).}",Hash tables
Multiple choice with explanations,1.6,"Bob implemented the perfect hash algorithm that was taught in class (two level hashing). When running the first part of the algorithm (creating the main table), Bob chose a function \( h_1 \) and discovered that the last \(  n + 1 - \sqrt{n} \) keys he inserted into the main table each went into a ""private"" cell (i.e., without collision). Based on this information, Bob decided not to check the other keys and not to try additional functions in the first stage, and moved on to perform the second stage. Answer true\false and explain. Answer 6 is incorrect. However, within 4 attempts on average in the second stage (for each sub-table) a perfect hash will be found.",False,\textit{Not correct. The second stage produces square tables and therefore works on average in two attempts. The question data did not affect this.},Hash tables
Multiple choice with explanations,1.6,"Bob ran the perfect hash algorithm learned in class (two-level hashing). When running the first part of the algorithm (creating the main table), Bob chose a function $h_1$ and found that the last $n + 1 - \sqrt{n}$ keys he inserted into the main table each went into a ""private"" cell (i.e., without collision). Based on this information, Bob decided not to check the other keys or try additional functions in the first step and proceeded to perform the second step. Answer True/False and explain. Within $O(n^2)$ time, it is guaranteed that a perfect hash will be found in the second step.",False,"\textit{Not true. Theoretically, there could be infinite attempts.}",Hash tables
Multiple choice with explanations,6,"The regular expression

\(ab+a\)

matches any string that starts with the letter a, continues with a series of repetitions of the letter b, and ends with the letter a. For example:

\(abba, aba, abbba\)

match the expression, whereas:

\(abb, bba, bb, bbaa, ab, baa\)

do not match.

Propose an efficient algorithm to check if a string of length \(n\) contains 2 substrings that match the above regular expression and are equal to each other.
What is the runtime of the most efficient algorithm? Circle the best answer.
a. Worst case time - \(O(n)\)
b. Worst case time - \(O(n^2)\)
c. Amortized time - \(O(n)\) and worst case time - \(O(n^2)\)
d. Amortized time - \(O(n \log n)\) and worst case time - \(O(n \log n)\)

Describe the algorithm and justify its runtime.",1,"\textbf{Algorithm Description:}  
We build a suffix tree for a string of length \( n \). We traverse the tree as follows – from the root, we look for the letters \( ab \). If we do not find a path starting with \( ab \), we return false.  
After that, we traverse the tree and look for the letters \( b \) and \( a \). If we find the letter \( a \), we check if the continuation of its edge leads to an internal node. If yes, we return true. If not, or if we do not find the letter \( a \), we continue on the path that the letter \( b \) leads to.  

\textbf{Runtime Analysis:}  
Tree construction – \( O(n) \)  
Traversing the path in the tree starting with \( a \) and then including only the series of letters \( b \) – path length in the tree is at most \( O(n) \), and each comparison in it is at most \( O(1) \) – total \( O(n) \).  
Checking at each stage in the path if there is a split for the letter \( a \), and if it leads to an internal node – \( O(1) \times n = O(n) \).","Complexity analysis,   Amortized analysis"
Multiple choice with explanations,4,\textit{Define an easy binary search tree as a binary tree where the following rule holds: each node x is greater than its left child (if it exists) and smaller than its right child (if it exists). Prove or disprove: any sequence of n different numbers can be inserted into an easy binary search tree in O(n) time.},1,"Yes.  
A short explanation:  
You can simply insert the elements in the order in which they are stored and construct a tree in the form of a thread. Each time a new element is inserted, check if it is smaller or larger than the previous element, and accordingly decide if it is the left/right child of the previous element.",Binary search trees
Multiple choice with explanations,2,"A sloppy minimum heap is a heap where instead of returning the minimum, it returns one of the k smallest elements in the heap. Additionally, in the delete operation, it deletes the same element that it returned as the approximate minimum.
Is the sequence [7,8,5,6,3,4,1,2,...N,N-1] a possible output of a sloppy heap for k=1?",False,"\textit{No, because when $K = 1$, the messy heap is a regular heap and therefore the output should be sorted.}",Binary heaps
Multiple choice with explanations,4,"A lazy minimum heap is a heap that instead of returning the minimum returns one of the k smallest elements in the heap. Additionally, in the delete operation, it deletes the same element that was returned as the approximate minimum. Prove or disprove the following claim:
For \( k=2 \), it is possible to implement a lazy heap such that all operations take \( O(1) \) amortized time.",False,"\textit{Not possible. If it were possible, we would reach a contradiction to the sorting lower bound. We will perform a reduction from the sorting problem - we will insert the \( n \) elements we want to sort into a sloppy heap, and now find the approximate minimum and remove it. We will do this \( n \) times. We will take the sequence we obtained and note that it has at most \( n \) disorder violations, so now it is possible to perform insertion sort in linear time and obtain the sorted sequence.}","Amortized analysis,   Binary heaps"
Closed question with short explanations,2,"\begin{quote}
In the following section, the initialization time and space complexity is O(1), and the space complexity at any given moment is O of the number of elements in the data structure. Describe how a vector of length m can be represented to support insertion at position i, reading from position i, and deletion at position i in O(\log m) worst-case time for each operation.
\end{quote}",na,"\textbf{Central Data Structure: AVL Tree. Short Description:} The key of an element will be the index \( i \), and the value will be the content of the vector at index \( i \).","Complexity analysis,   Balanced BST (AVL,   B)"
Closed question with short explanations,3,"\textbf{In the following section, the time and space complexity of initialization is \(O(1)\), and the space complexity at any given moment is \(O\) of the number of elements in the data structure. Describe how a matrix of size \(m \times m\) can be represented, supporting the operations of insertion at position \((i,j)\), reading from position \((i,j)\), and deletion from position \((i,j)\), in \(O(\log m)\) worst-case time for each operation.}",na,"\textbf{Central Data Structure:} AVL Tree. \textbf{Brief Description:}
The key will be the pair (i,j) where the comparison between keys will be lexicographic (compare i, and if there is a tie, compare j).
The value will be the content of the matrix at position (i,j).","Complexity analysis,   Balanced BST (AVL,   B)"
Closed question with short explanations,2,"\text{Describe an implementation of a data structure with the following operations (x is a key):}

\text{Insert(x) - in amortized time \(O(\log n)\).}

\text{Search(x) - in amortized time \(O(\log n)\).}

\text{Delete(x) - in amortized time \(O(\log n)\).}",na,"\textbf{Central Data Structure:} AVL Tree. \textbf{Brief Description:} We will use the standard operations of an AVL tree. \textbf{Complexity Explanation:} All operations are in worst-case in \(O(\log n)\), and therefore, in particular, amortized directly from the definition of amortized.","Amortized analysis,   OTHER"
Closed question with short explanations,2,"\begin{quote}
Describe an implementation of a data structure with the following operations (x is a key):
\begin{itemize}
    \item Insert(x) - in amortized time $O(\log n)$.
    \item Search(x) - in amortized time $O(\log n)$.
    \item Delete(x) - in amortized time $O(1)$.
\end{itemize}
\end{quote}",na,"\textbf{Central Data Structure:} AVL Tree. \textbf{Brief Description:} We will use the standard operations of an AVL tree. \textbf{Explanation for Complexity:} In the banker's method - at the time of insertion \((x(\text{insert})\), we insert the element, and additionally leave \(\log n\) coins for its deletion.","Amortized analysis,   OTHER"
Closed question with short explanations,7,"\textit{Describe an implementation of a data structure with the following operations (x is a key):} \\
Insert(x) - \textit{in amortized time} $O(\log n)$. \\
Search(x) - \textit{in amortized time} $O(\log n)$. \\
Delete(x) - \textit{in amortized time} $O(1)$ \textit{and also in worst-case time} $O(1)$.",na,"\textbf{Central Data Structure:} AVL Tree. \textbf{Secondary Data Structure:} Linked List. The worst-case complexity of insert is \(O(n \log n)\). The worst-case complexity of search is \(O(n \log n)\). \textbf{Solution:}  
During initialization, we will initialize an AVL tree and a linked list.  
During deletion, we will insert \(x\) at the beginning of the linked list.  
During search/insert, before performing the actual operation, we will traverse the entire linked list and delete element by element.  
After emptying the list, we will perform the actual operation (search/insert).  
Amortized analysis using the banker's method - at the time of insertion \((x\text{, insert})\), we will insert the element and also leave it with \(\log n\) coins for its deletion.","Complexity analysis,   Amortized analysis,   OTHER"
Closed question with short explanations,6.25,Two AVL trees of size \( n \) each are given. Can they be merged in \( O(n) \) time? Prove using an algorithm or disprove.,na,"\textbf{Proof.} Perform an in-order walk on each of the trees and obtain two sorted arrays of their elements. Perform a merge sort to obtain a single sorted array. Construct a balanced tree from it as follows: the median will be the root, the 1/4 and 3/4 order statistics will be the left and right child of the root respectively, and so on.","Balanced BST (AVL,   B)"
