# Articles
## Articles from proposal
- [Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning (Xu et al 2024)](https://aclanthology.org/2024.emnlp-main.313.pdf)
- [Predicting the Difficulty of Multiple Choice Questions in a High-stakes Medical Exam](https://aclanthology.org/W19-4402.pdf)

## Potentially Relevant Articles
- [How Hard can this Question be? An Exploratory Analysis of Features Assessing Question Difficulty using LLMs](https://educationaldatamining.org/edm2024/proceedings/2024.EDM-posters.90/2024.EDM-posters.90.pdf) - They calculate the difficulty of a question by assessing the performance of a model that is prompted to answer. Because the model is going to formulate an answer differently than their given annotated answer, they calculate the cross entropy loss for the annotated correct answer, given the prompt for answering a question based on the context. They say that a higher cross-entropy loss signifies a greater uncertainty in generating the correct response, thus reflecting a more challanging question for the model. They also have [code that shows exactly how they do this](https://github.com/readerbench/EDM-Question-Difficulty)
- [UnibucLLM: Harnessing LLMs for Automated Prediction of Item Difficulty and Response Time for Multiple-Choice Questions](https://arxiv.org/pdf/2404.13343v1) - They use a two phase process. First they use 3 different models to answer a multiple choice question, and then use those answers as well as the given "gold" answer to create a feature set. Then they use this feature set to train another model (they used BERT and GPT-2) to predict question difficulty from 0 to 1. 
- [A quantitative study of NLP approaches to question difficulty estimation](https://arxiv.org/pdf/2305.10236) - This article compares different approaches taken in the literature to question difficulty estimation, I think this article is mostly useful in so far as it can point us to different articles that actually perform the question difficulty estimation. Its findings are that Transformer based models are the best across different educational domains, that models based on linguistic features perform well on reading comprehension question, while frequency based features (TF-IDF, Term Frequency-Inverse Document Frequency) and word embeddings (word2vec) perform better in domain knowledge assessment. 
- [R2DE: a NLP approach to estimating IRT parameters of newly generated questions](https://arxiv.org/pdf/2001.07569) - They introduce a model (R2DE - Regressor for Difficulty and Discrimination Estimation) that's capable of assessing multiple choice question by only looking at the text of the questions and the text of the possible choices. Then they train a regression model that takes the question and estimated question difficulty from student data to predict question difficulty. *This seems an awful lot like what we're trying to do*.
- [Introducing a framework to assess newly created questions with Natural Language Processing](https://arxiv.org/pdf/2004.13530) - This is by the same author as the R2DE paper, and it looks like it does pretty much the same thing as that article.
- [Text-based Question Difficulty Prediction: A Systematic Review of Automatic Approaches](https://livrepository.liverpool.ac.uk/3172298/) - This article compares different approaches of Question Difficulty Prediction, again, I think this article is useful in so far as it can point us to different articles and show us different approaches.

## Datasets
- [GPQA: A Graduate-Level Google-Proof Q&A Benchmark](https://arxiv.org/abs/2311.12022)

## Models
- [UnifiedQA](https://github.com/allenai/unifiedqa?tab=readme-ov-file)
