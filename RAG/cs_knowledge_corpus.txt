Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.
Auxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term software development is used for this larger overall process – with the terms programming, implementation, and coding reserved for the writing and editing of code per se. Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process. (Full article...)
Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks.[1][2] It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.
Auxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term software development is used for this larger overall process – with the terms programming, implementation, and coding reserved for the writing and editing of code per se. Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.
Code-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.[7]
The first computer program is generally dated to 1843 when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.[8] However, Charles Babbage himself had written a program for the AE in 1837.[9][10]
These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier[16] by allowing programmers to specify calculations by entering a formula using infix notation.
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.
Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.[22] Some of these factors include:
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.
The academic field and the engineering practice of computer programming are concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into orders using Big O notation, which expresses resource use—such as execution time or memory consumption—in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.
Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic programming languages.
It is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language,[23] the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers[24] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).
Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.
Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in "high-level" languages than in "low-level" ones.
Programming languages are essential for software development. They are the building blocks for all software, from the simplest applications to the most sophisticated ones.
Learning to program has a long history related to professional standards and practices, academic initiatives and curriculum, and commercial books and materials for students, self-taught learners, hobbyists, and others who desire to create or customize software for personal use. Since the 1960s, learning to program has taken on the characteristics of a popular movement, with the rise of academic disciplines, inspirational leaders, collective identities, and strategies to grow the movement and make institutionalize change.[26] Through these social ideals and educational agendas, learning to code has become important not just for scientists and engineers, but for millions of citizens who have come to believe that creating software is beneficial to society and its members.
In 1957, there were approximately 15,000 computer programmers employed in the U.S., a figure that accounts for 80% of the world's active developers. In 2014, there were approximately 18.5 million professional programmers in the world, of which 11 million can be considered professional and 7.5 million student or hobbyists.[27] Before the rise of the commercial Internet in the mid-1990s, most programmers learned about software construction through books, magazines, user groups, and informal instruction methods, with academic coursework and corporate training playing important roles for professional workers.[28]
When high-level languages arrived, they were introduced by numerous books and materials that explained language keywords, managing program flow, working with data, and other concepts. These languages included FLOW-MATIC, COBOL, FORTRAN, ALGOL, Pascal, BASIC, and C. An example of an early programming primer from these years is Marshal H. Wrubel's A Primer of Programming for Digital Computers (1959), which included step-by-step instructions for filling out coding sheets, creating punched cards, and using the keywords in IBM's early FORTRAN system.[29] Daniel McCracken's A Guide to FORTRAN Programming (1961) presented FORTRAN to a larger audience, including students and office workers.
In 1961, Alan Perlis suggested that all university freshmen at Carnegie Technical Institute take a course in computer programming.[30] His advice was published in the popular technical journal Computers and Automation, which became a regular source of information for professional programmers.
Early and influential programmer's guides included John G. Kemeny and Thomas E. Kurtz's BASIC Programming (1967), Kathleen Jensen and Niklaus Wirth's The Pascal User Manual and Report (1971), and Brian Kernighan and Dennis Ritchie's The C Programming Language (1978). Similar books for popular audiences (but with a much lighter tone) included Bob Albrecht's My Computer Loves Me When I Speak BASIC (1972), Al Kelley and Ira Pohl's A Book on C (1984), and Dan Gookin's C for Dummies (1994).
Beyond language-specific primers, there were numerous books and academic journals that introduced professional programming practices. Many were designed for university courses in computer science, software engineering, or related disciplines. Donald Knuth's The Art of Computer Programming (1968 and later), presented hundreds of computational algorithms and their analysis. The Elements of Programming Style (1974), by Brian W. Kernighan and P. J. Plauger, concerned itself with programming style, the idea that programs should be written not only to satisfy the compiler but human readers. Jon Bentley's Programming Pearls (1986) offered practical advice about the art and craft of programming in professional and academic contexts. Texts specifically designed for students included Doug Cooper and Michael Clancy's Oh Pascal! (1982), Alfred Aho's Data Structures and Algorithms (1983), and Daniel Watt's Learning with Logo (1983).
As personal computers became mass-market products, thousands of trade books and magazines sought to teach professional, hobbyist, and casual users to write computer programs. A sample of these learning resources includes BASIC Computer Games, Microcomputer Edition (1978), by David Ahl; Programming the Z80 (1979), by Rodnay Zaks; Programmer's CP/M Handbook (1983), by Andy Johnson-Laird; C Primer Plus (1984), by Mitchell Waite and The Waite Group; The Peter Norton Programmer's Guide to the IBM PC (1985), by Peter Norton; Advanced MS-DOS (1986), by Ray Duncan; Learn BASIC Now (1989), by Michael Halvorson and David Rygymr; Programming Windows (1992 and later), by Charles Petzold; Code Complete: A Practical Handbook for Software Construction (1993), by Steve McConnell; and Tricks of the Game-Programming Gurus (1994), by André LaMothe.
The PC software industry spurred the creation of numerous book publishers that offered programming primers and tutorials, as well as books for advanced software developers.[31] These publishers included Addison-Wesley, IDG, Macmillan Inc., McGraw-Hill, Microsoft Press, O'Reilly Media, Prentice Hall, Sybex, Ventana Press, Waite Group Press, Wiley, Wrox Press, and Ziff-Davis.
Computer magazines and journals also provided learning content for professional and hobbyist programmers. A partial list of these resources includes Amiga World, Byte (magazine), Communications of the ACM, Computer (magazine), Compute!, Computer Language (magazine), Computers and Electronics, Dr. Dobb's Journal, IEEE Software, Macworld, PC Magazine, PC/Computing, and UnixWorld.
Between 2000 and 2010, computer book and magazine publishers declined significantly as providers of programming instruction, as programmers moved to Internet resources to expand their access to information. This shift brought forward new digital products and mechanisms to learn programming skills. During the transition, digital books from publishers transferred information that had traditionally been delivered in print to new and expanding audiences.[32]
Important Internet resources for learning to code included blogs, wikis, videos, online databases, subscription sites, and custom websites focused on coding skills. New commercial resources included YouTube videos, Lynda.com tutorials (later LinkedIn Learning), Khan Academy, Codecademy, GitHub, W3Schools, and numerous coding bootcamps.
Most software development systems and game engines included rich online help resources, including integrated development environments (IDEs), context-sensitive help, APIs, and other digital resources. Commercial software development kits (SDKs) also provided a collection of software development tools and documentation in one installable package.
Commercial and non-profit organizations published learning websites for developers, created blogs, and established newsfeeds and social media resources about programming. Corporations like Apple, Microsoft, Oracle, Google, and Amazon built corporate websites providing support for programmers, including resources like the Microsoft Developer Network (MSDN). Contemporary movements like Hour of Code (Code.org) show how learning to program has become associated with digital learning strategies, education agendas, and corporate philanthropy.
Although programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language.[33][34]
In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ ⓘ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation.[1] Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning).
In contrast, a heuristic is an approach to solving problems that do not have well-defined correct or optimal results.[2] For example, although social media recommender systems are commonly called "algorithms", they actually rely on heuristics as there is no truly "correct" recommendation.
As an effective method, an algorithm can be expressed within a finite amount of space and time[3] and in a well-defined formal language[4] for calculating a function.[5] Starting from an initial state and initial input (perhaps empty),[6] the instructions describe a computation that, when executed, proceeds through a finite[7] number of well-defined successive states, eventually producing "output"[8] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[9]
Around 825 AD, Persian scientist and polymath Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī ("Book of Indian computation") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī ("Addition and subtraction in Indian arithmetic").[1] In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared, for example Liber Alghoarismi de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath.[10] Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi, or "Thus spoke Al-Khwarizmi".[2] Around 1230, the English word algorism is attested and then by Chaucer in 1391, English adopted the French term.[3][4][clarification needed] In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, "number"; cf. "arithmetic"), the Latin word was altered to algorithmus.[citation needed]
One informal definition is "a set of rules that precisely defines a sequence of operations",[11][need quotation to verify] which would include all computer programs (including programs that do not perform numeric calculations), and any prescribed bureaucratic procedure[12]
or cook-book recipe.[13] In general, a program is an algorithm only if it stops eventually[14]—even though infinite loops may sometimes prove desirable. Boolos, Jeffrey & 1974, 1999 define an algorithm to be an explicit set of instructions for determining an output, that can be followed by a computing machine or a human who could only carry out specific elementary operations on symbols.[15]
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device.
The earliest evidence of algorithms is found in ancient Mesopotamian mathematics. A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC describes the earliest division algorithm.[16] During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas.[23] Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.[24]
Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC.[16] Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,[25][20]: Ch 9.2  and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).[20]: Ch 9.1 Examples of ancient Indian mathematics included the Shulba Sutras, the Kerala School, and the Brāhmasphuṭasiddhānta.[17]
The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.[22]
Bolter credits the invention of the weight-driven clock as "the key invention [of Europe in the Middle Ages]," specifically the verge escapement mechanism[26] producing the tick and tock of a mechanical clock. "The accurate automatic machine"[27] led immediately to "mechanical automata" in the 13th century and "computational machines"—the difference and analytical engines of Charles Babbage and Ada Lovelace in the mid-19th century.[28] Lovelace designed the first algorithm intended for processing on a computer, Babbage's analytical engine, which is the first device considered a real Turing-complete computer instead of just a calculator. Although the full implementation of Babbage's second device was not realized for decades after her lifetime, Lovelace has been called "history's first programmer".
In 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define "effective calculability"[32] or "effective method".[33] Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form but are also used to define or document algorithms.
There are many possible representations and Turing machine programs can be expressed as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), as a form of rudimentary machine code or assembly code called "sets of quadruples", and more. Algorithm representations can also be classified into three accepted levels of Turing machine description: high-level description, implementation description, and formal description.[34] A high-level description describes the qualities of the algorithm itself, ignoring how it is implemented on the Turing machine.[34] An implementation description describes the general manner in which the machine moves its head and stores data to carry out the algorithm, but does not give exact states.[34] In the most detail, a formal description gives the exact state table and list of transitions of the Turing machine.[34]
The graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). It has four primary symbols: arrows showing program flow, rectangles (SEQUENCE, GOTO), diamonds (IF-THEN-ELSE), and dots (OR-tie). Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure.
It is often important to know how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of ⁠



O
(
n
)


{\displaystyle O(n)}

⁠, using big O notation. The algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. If the space required to store the input numbers is not counted, it has a space requirement of ⁠



O
(
1
)


{\displaystyle O(1)}

⁠, otherwise ⁠



O
(
n
)


{\displaystyle O(n)}

⁠ is required.
Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost ⁠



O
(
log
⁡
n
)


{\displaystyle O(\log n)}

⁠) outperforms a sequential search (cost ⁠



O
(
n
)


{\displaystyle O(n)}

⁠ ) when used for table lookups on sorted lists or arrays.
The analysis, and study of algorithms is a discipline of computer science. Algorithms are often studied abstractly, without referencing any specific programming language or implementation. Algorithm analysis resembles other mathematical disciplines as it focuses on the algorithm's properties, not implementation. Pseudocode is typical for analysis as it is a simple and general representation. Most algorithms are implemented on particular hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many "one-off" problems but it may be critical for algorithms designed for fast interactive, commercial, or long-life scientific usage. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.
Empirical testing is useful for uncovering unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Empirical tests cannot replace formal analysis, though, and are non-trivial to perform fairly.[35]
To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[36] In general, speed improvements depend on special properties of the problem, which are very common in practical applications.[37] Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.
Algorithm design is a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns,[38] with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases.[39]
Per the Church–Turing thesis, any algorithm can be computed by any Turing complete model. Turing completeness only requires four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. However, Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language".[40] Tausworthe augments the three Böhm-Jacopini canonical structures:[41] SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE.[42] An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.[43]
By themselves, algorithms are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial,[44] and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).
Another way of classifying algorithms is by their design methodology or paradigm. Some common paradigms are:
For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:
One of the simplest algorithms finds the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be described in plain English as:
(Quasi-)formal description:
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:
A programming language is a system of notation for writing computer programs.[1]
Programming languages are described in terms of their syntax (form) and semantics (meaning), usually defined by a formal language. Languages usually provide features such as a type system, variables, and mechanisms for error handling. An implementation of a programming language is required in order to execute programs, namely an interpreter or a compiler. An interpreter directly executes the source code, while a compiler produces an executable program.
Computer architecture has strongly influenced the design of programming languages, with the most common type (imperative languages—which implement operations in a specified order) developed to perform well on the popular von Neumann architecture. While early programming languages were closely tied to the hardware, over time they have developed more abstraction to hide implementation details for greater simplicity.
Thousands of programming languages—often classified as imperative, functional, logic, or object-oriented—have been developed for a wide variety of uses. Many aspects of programming language design involve tradeoffs—for example, exception handling simplifies error handling, but at a performance cost. Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.
Programming languages differ from natural languages in that natural languages are used for interaction between people, while programming languages are designed to allow humans to communicate instructions to machines.[citation needed]
The term computer language is sometimes used interchangeably with "programming language".[2] However, usage of these terms varies among authors.
In one usage, programming languages are described as a subset of computer languages.[3] Similarly, the term "computer language" may be used in contrast to the term "programming language" to describe languages used in computing but not considered programming languages[citation needed].  Most practical programming languages are Turing complete,[4] and as such are equivalent in what programs they can compute.
Another usage regards programming languages as theoretical constructs for programming abstract machines and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources.[5] John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.[6]
The first programmable computers were invented at the end of the 1940s, and with them, the first programming languages.[7] The earliest computers were programmed in first-generation programming languages (1GLs), machine language (simple instructions that could be directly executed by the processor). This code was very difficult to debug and was not portable between different computer systems.[8] In order to improve the ease of programming, assembly languages (or second-generation programming languages—2GLs) were invented, diverging from the machine language to make programs easier to understand for humans, although they did not increase portability.[9]
Initially, hardware resources were scarce and expensive, while human resources were cheaper. Therefore, cumbersome languages that were time-consuming to use, but were closer to the hardware for higher efficiency were favored.[10] The introduction of high-level programming languages (third-generation programming languages—3GLs)—revolutionized programming. These languages abstracted away the details of the hardware, instead being designed to express algorithms that could be understood more easily by humans. For example, arithmetic expressions could now be written in symbolic notation and later translated into machine code that the hardware could execute.[9] In 1957, Fortran (FORmula TRANslation) was invented. Often considered the first compiled high-level programming language,[9][11] Fortran has remained in use into the twenty-first century.[12]
Around 1960, the first mainframes—general purpose computers—were developed, although they could only be operated by professionals and the cost was extreme. The data and instructions were input by punch cards, meaning that no input could be added while the program was running. The languages developed at this time therefore are designed for minimal interaction.[14] After the invention of the microprocessor, computers in the 1970s became dramatically cheaper.[15] New computers also allowed more user interaction, which was supported by newer programming languages.[16]
Lisp, implemented in 1958, was the first functional programming language.[17] Unlike Fortran, it supported recursion and conditional expressions,[18] and it also introduced dynamic memory management on a heap and automatic garbage collection.[19] For the next decades, Lisp dominated artificial intelligence applications.[20] In 1978, another functional language, ML, introduced inferred types and polymorphic parameters.[16][21]
After ALGOL (ALGOrithmic Language) was released in 1958 and 1960,[22] it became the standard in computing literature for describing algorithms. Although its commercial success was limited, most popular imperative languages—including C, Pascal, Ada, C++, Java, and C#—are directly or indirectly descended from ALGOL 60.[23][12] Among its innovations adopted by later programming languages included greater portability and the first use of context-free, BNF grammar.[24] Simula, the first language to support object-oriented programming (including subtypes, dynamic dispatch, and inheritance), also descends from ALGOL and achieved commercial success.[25] C, another ALGOL descendant, has sustained popularity into the twenty-first century. C allows access to lower-level machine operations more than other contemporary languages. Its power and efficiency, generated in part with flexible pointer operations, comes at the cost of making it more difficult to write correct code.[16]
Prolog, designed in 1972, was the first logic programming language, communicating with a computer using formal logic notation.[26][27] With logic programming, the programmer specifies a desired result and allows the interpreter to decide how to achieve it.[28][27]
During the 1980s, the invention of the personal computer transformed the roles for which programming languages were used.[29] New languages introduced in the 1980s included C++, a superset of C that can compile C programs but also supports classes and inheritance.[30] Ada and other new languages introduced support for concurrency.[31] The Japanese government invested heavily into the so-called fifth-generation languages that added support for concurrency to logic programming constructs, but these languages were outperformed by other concurrency-supporting languages.[32][33]
Due to the rapid growth of the Internet and the World Wide Web in the 1990s, new programming languages were introduced to support Web pages and networking.[34] Java, based on C++ and designed for increased portability across systems and security, enjoyed large-scale success because these features are essential for many Internet applications.[35][36] Another development was that of dynamically typed scripting languages—Python, JavaScript, PHP, and Ruby—designed to quickly produce small programs that coordinate existing applications. Due to their integration with HTML, they have also been used for building web pages hosted on servers.[37][38]
During the 2000s, there was a slowdown in the development of new programming languages that achieved widespread popularity.[39] One innovation was service-oriented programming, designed to exploit distributed systems whose components are connected by a network. Services are similar to objects in object-oriented programming, but run on a separate process.[40] C# and F# cross-pollinated ideas between imperative and functional programming.[41] After 2010, several new languages—Rust, Go, Swift, Zig and Carbon —competed for the performance-critical software for which C had historically been used.[42] Most of the new programming languages use static typing while a few numbers of new languages use dynamic typing like Ring and Julia.[43][44]
Some of the new programming languages are classified as visual programming languages like Scratch, LabVIEW and PWCT. Also, some of these languages mix between textual and visual programming usage like Ballerina.[45][46][47][48] Also, this trend lead to developing projects that help in developing new VPLs like Blockly by Google.[49] Many game engines like Unreal and Unity added support for visual scripting too.[50][51]
Every programming language includes fundamental elements for describing data and the operations or transformations applied to them, such as adding two numbers or selecting an item from a collection. These elements are governed by syntactic and semantic rules that define their structure and meaning, respectively.
A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, some programming languages are graphical, using visual relationships between symbols to specify a program.
The programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars.[52] Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution.[53] In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.[54]
Static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms.[1][failed verification] For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct.[55] Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Programming languages such as Java and C# have definite assignment analysis, a form of data flow analysis, as part of their respective static semantics.[56]
Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research goes into formal semantics of programming languages, which allows execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.[56]
A data type is a set of allowable values and operations that can be performed on these values.[57] Each programming language's type system defines which data types exist, the type of an expression, and how type equivalence and type compatibility function in the language.[58]
Early programming languages often supported only built-in, numeric types such as the integer (signed and unsigned) and floating point (to support operations on real numbers that are not integers). Most programming languages support multiple sizes of floats (often called float and double) and integers depending on the size and precision required by the programmer. Storing an integer in a type that is too small to represent it leads to integer overflow. The most common way of representing negative numbers with signed types is twos complement, although ones complement is also used.[62] Other common types include Boolean—which is either true or false—and character—traditionally one byte, sufficient to represent all ASCII characters.[63]
Arrays are a data type whose elements, in many languages, must consist of a single type of fixed length. Other languages define arrays as references to data stored elsewhere and support elements of varying types.[64] Depending on the programming language, sequences of multiple characters, called strings, may be supported as arrays of characters or their own primitive type.[65] Strings may be of fixed or variable length, which enables greater flexibility at the cost of increased storage space and more complexity.[66] Other data types that may be supported include lists,[67] associative (unordered) arrays accessed via keys,[68] records in which data is mapped to names in an ordered structure,[69] and tuples—similar to records but without names for data fields.[70] Pointers store memory addresses, typically referencing locations on the heap where other data is stored.[71]
The simplest user-defined type is an ordinal type, often called an enumeration, whose values can be mapped onto the set of positive integers.[72] Since the mid-1980s, most programming languages also support abstract data types, in which the representation of the data and operations are hidden from the user, who can only access an interface.[73] The benefits of data abstraction can include increased reliability, reduced complexity, less potential for name collision, and allowing the underlying data structure to be changed without the client needing to alter its code.[74]
In static typing, all expressions have their types determined before a program executes, typically at compile-time.[59] Most widely used, statically typed programming languages require the types of variables to be specified explicitly. In some languages, types are implicit; one form of this is when the compiler can infer types based on context. The downside of implicit typing is the potential for errors to go undetected.[75] Complete type inference has traditionally been associated with functional languages such as Haskell and ML.[76]
With dynamic typing, the type is not attached to the variable but only the value encoded in it. A single variable can be reused for a value of a different type. Although this provides more flexibility to the programmer, it is at the cost of lower reliability and less ability for the programming language to check for errors.[77] Some languages allow variables of a union type to which any type of value can be assigned, in an exception to their usual static typing rules.[78]
In computing, multiple instructions can be executed simultaneously. Many programming languages support instruction-level and subprogram-level concurrency.[79] By the twenty-first century, additional processing power on computers was increasingly coming from the use of additional processors, which requires programmers to design software that makes use of multiple processors simultaneously to achieve improved performance.[80] Interpreted languages such as Python and Ruby do not support the concurrent use of multiple processors.[81] Other programming languages do support managing data shared between different threads by controlling the order of execution of key instructions via the use of semaphores, controlling access to shared data via monitor, or enabling message passing between threads.[82]
Many programming languages include exception handlers, a section of code triggered by runtime errors that can deal with them in two main ways:[83]
Some programming languages support dedicating a block of code to run regardless of whether an exception occurs before the code is reached; this is called finalization.[84]
One of the most important influences on programming language design has been computer architecture. Imperative languages, the most commonly used type, were designed to perform well on von Neumann architecture, the most common computer architecture.[89] In von Neumann architecture, the memory stores both data and instructions, while the CPU that performs instructions on data is separate, and data must be piped back and forth to the CPU. The central elements in these languages are variables, assignment, and iteration, which is more efficient than recursion on these machines.[90]
Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse.[citation needed] The birth of programming languages in the 1950s was stimulated by the desire to make a universal programming language suitable for all machines and uses, avoiding the need to write code for different computers.[91] By the early 1960s, the idea of a universal language was rejected due to the differing requirements of the variety of purposes for which code was written.[92]
Desirable qualities of programming languages include readability, writability, and reliability.[93] These features can reduce the cost of training programmers in a language, the amount of time needed to write and maintain programs in the language, the cost of compiling the code, and increase runtime performance.[94]
Programming language design often involves tradeoffs.[104] For example, features to improve reliability typically come at the cost of performance.[105] Increased expressivity due to a large number of operators makes writing code easier but comes at the cost of readability.[105]
Natural-language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs.[106] Alan Perlis was similarly dismissive of the idea.[107]
The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.
An implementation of a programming language is the conversion of a program into machine code that can be executed by the hardware. The machine code then can be executed with the help of the operating system.[111] The most common form of interpretation in production code is by a compiler, which translates the source code via an intermediate-level language into machine code, known as an executable. Once the program is compiled, it will run more quickly than with other implementation methods.[112] Some compilers are able to provide further optimization to reduce memory or computation usage when the executable runs, but increasing compilation time.[113]
Another implementation method is to run the program with an interpreter, which translates each line of software into machine code just before it executes. Although it can make debugging easier, the downside of interpretation is that it runs 10 to 100 times slower than a compiled executable.[114] Hybrid interpretation methods provide some of the benefits of compilation and some of the benefits of interpretation via partial compilation. One form this takes is just-in-time compilation, in which the software is compiled ahead of time into an intermediate language, and then into machine code immediately before execution.[115]
Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain-specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.[citation needed]
Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language,[116] and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.[117]
Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language. Some languages may make the transition from closed to open; for example, Erlang was originally Ericsson's internal programming language.[118]
Open source programming languages are particularly helpful for open science applications, enhancing the capacity for replication and code sharing.[119]
Thousands of different programming languages have been created, mainly in the computing field.[120]
Individual software projects commonly use five programming languages or more.[121]
Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers "do exactly what they are told to do", and cannot "understand" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.
A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives).[122] Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.
Determining which is the most widely used programming language is difficult since the definition of usage varies by context. One language may occupy the greater number of programmer hours, a different one has more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes;[124][125] Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time, and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.
Combining and averaging information from various internet sites, stackify.com reported the ten most popular programming languages (in descending order by overall popularity): Java, C, C++, Python, C#, JavaScript, VB .NET, R, PHP, and MATLAB.[129]
As of June 2024, the top five programming languages as measured by TIOBE index are Python, C++, C, Java and C#. TIOBE provide a list of top 100 programming languages according to popularity and update this list every month.[130]
A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate, or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC language has many dialects.
Programming languages are often placed into four main categories: imperative, functional, logic, and object oriented.[131]
Although markup languages are not programming languages, some have extensions that support limited programming. Additionally, there are special-purpose languages that are not easily compared to other programming languages.[135]
A high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language. The amount of abstraction provided defines how "high-level" a programming language is.[1]
In the 1960s, a high-level programming language using a compiler was commonly called an autocode.[2]
Examples of autocodes are COBOL and Fortran.[3]
The first high-level programming language designed for computers was Plankalkül, created by Konrad Zuse.[4] However, it was not implemented in his time, and his original contributions were largely isolated from other developments due to World War II, aside from the language's influence on the "Superplan" language by Heinz Rutishauser and also to some degree ALGOL. The first significantly widespread high-level language was Fortran, a machine-independent development of IBM's earlier Autocode systems. The ALGOL family, with ALGOL 58 defined in 1958 and ALGOL 60 defined in 1960 by committees of European and American computer scientists, introduced recursion as well as nested functions under lexical scope. ALGOL 60 was also the first language with a clear distinction between value and name-parameters and their corresponding semantics.[5] ALGOL also introduced several structured programming concepts, such as the while-do and if-then-else constructs and its syntax was the first to be described in formal notation – Backus–Naur form (BNF). During roughly the same period, COBOL introduced records (also called structs) and Lisp introduced a fully general lambda abstraction in a programming language for the first time.
"High-level language" refers to the higher level of abstraction from machine language. Rather than dealing with registers, memory addresses, and call stacks, high-level languages deal with variables, arrays, objects, complex arithmetic or Boolean expressions, subroutines and functions, loops, threads, locks, and other abstract computer science concepts, with a focus on usability over optimal program efficiency. Unlike low-level assembly languages, high-level languages have few, if any, language elements that translate directly into a machine's native opcodes. Other features, such as string handling routines, object-oriented language features, and file input/output, may also be present. One thing to note about high-level programming languages is that these languages allow the programmer to be detached and separated from the machine. That is, unlike low-level languages like assembly or machine language, high-level programming can amplify the programmer's instructions and trigger a lot of data movements in the background without their knowledge. The responsibility and power of executing instructions have been handed over to the machine from the programmer.
High-level languages intend to provide features that standardize common tasks, permit rich debugging, and maintain architectural agnosticism; while low-level languages often produce more efficient code through optimization for a specific system architecture. Abstraction penalty is the cost that high-level programming techniques pay for being unable to optimize performance or use certain hardware because they don't take advantage of certain low-level architectural resources. High-level programming exhibits features like more generic data structures and operations, run-time interpretation, and intermediate code files; which often result in execution of far more operations than necessary, higher memory consumption, and larger binary program size.[6][7][8] For this reason, code which needs to run particularly quickly and efficiently may require the use of a lower-level language, even if a higher-level language would make the coding easier. In many cases, critical portions of a program mostly in a high-level language can be hand-coded in assembly language, leading to a much faster, more efficient, or simply reliably functioning optimised program.
However, with the growing complexity of modern microprocessor architectures, well-designed compilers for high-level languages frequently produce code comparable in efficiency to what most low-level programmers can produce by hand, and the higher abstraction may allow for more powerful techniques providing better overall results than their low-level counterparts in particular settings.[9]
High-level languages are designed independent of a specific computing system architecture. This facilitates executing a program written in such a language on any computing system with compatible support for the Interpreted or JIT program. High-level languages can be improved as their designers develop improvements. In other cases, new high-level languages evolve from one or more others with the goal of aggregating the most popular constructs with new or improved features. An example of this is Scala which maintains backward compatibility with Java, meaning that programs and libraries written in Java will continue to be usable even if a programming shop switches to Scala; this makes the transition easier and the lifespan of such high-level coding indefinite. In contrast, low-level programs rarely survive beyond the system architecture which they were written for without major revision. This is the engineering 'trade-off' for the 'Abstraction Penalty'.
Examples of high-level programming languages in active use today include Python, JavaScript, Visual Basic, Delphi, Perl, PHP, ECMAScript, Ruby, C#, Java and many others.
The terms high-level and low-level are inherently relative. Some decades ago,[timeframe?] the C language, and similar languages, were most often considered "high-level", as it supported concepts such as expression evaluation, parameterised recursive functions, and data types and structures, while assembly language was considered "low-level". Today, many programmers might refer to C as low-level, as it lacks a large runtime-system (no garbage collection, etc.), basically supports only scalar operations, and provides direct memory addressing; it therefore, readily blends with assembly language and the machine level of CPUs and microcontrollers. Also, in the introduction chapter of The C Programming Language (second edition) by Brian Kernighan and Dennis Ritchie, C is described as "not a very high level" language.[10]
Assembly language may itself be regarded as a higher level (but often still one-to-one if used without macros) representation of machine code, as it supports concepts such as constants and (limited) expressions, sometimes even variables, procedures, and data structures. Machine code, in turn, is inherently at a slightly higher level than the microcode or micro-operations used internally in many processors.[11]
In software engineering, domain is the targeted subject area of a computer program. Formally it represents the target subject of a specific programming project, whether narrowly or broadly defined.[1] For example, for a particular programming project that has as a goal of the creation of a program for a particular hospital, that hospital would be the domain. Or, the project can be expanded in scope to include all hospitals as its domain.[1]: 352  In a computer programming design, you define a domain by delineating a set of common requirements, terminology, and functionality for any software program constructed to solve a problem in the area of computer programming, known as domain engineering. The word "domain" is also taken as a synonym of application domain.[1]
Domain in the realm of software engineering commonly refers to the subject area on which the application is intended to apply. In other words, during application development, the domain is the "sphere of knowledge and activity around which the application logic revolves." —Andrew Powell-Morse[2]
Domain: A sphere of knowledge, influence, or activity. The subject area to which the user applies a program is the domain of the software. —Eric Evans[3]
Software testing can provide objective, independent information about the quality of software and the risk of its failure to a user or sponsor.[1]
Software testing can determine the correctness of software for specific scenarios but cannot determine correctness for all scenarios.[2][3] It cannot find all bugs.
Based on the criteria for measuring correctness from an oracle, software testing employs principles and mechanisms that might recognize a problem. Examples of oracles include specifications, contracts,[4] comparable products, past versions of the same product, inferences about intended or expected purpose, user or customer expectations, relevant standards, and applicable laws.
Software testing is often dynamic in nature; running the software to verify actual output matches expected. It can also be static in nature; reviewing code and its associated documentation.
Software testing is often used to answer the question: Does the software do what it is supposed to do and what it needs to do?
Information learned from software testing may be used to improve the process by which software is developed.[5]: 41–43
Software testing should follow a "pyramid" approach wherein most of your tests should be unit tests, followed by integration tests and finally end-to-end (e2e) tests should have the lowest proportion.[6][7][8]
A study conducted by NIST in 2002 reported that software bugs cost the U.S. economy $59.5 billion annually. More than a third of this cost could be avoided if better software testing was performed.[9][dubious – discuss]
Outsourcing software testing because of costs is very common, with China, the Philippines, and India being preferred destinations.[citation needed]
Glenford J. Myers initially introduced the separation of debugging from testing in 1979.[10] Although his attention was on breakage testing ("A successful test case is one that detects an as-yet undiscovered error."[10]: 16 ), it illustrated the desire of the software engineering community to separate fundamental development activities, such as debugging, from that of verification.
Software testing typically includes handling software bugs – a defect in the code that causes an undesirable result.[11]: 31  Bugs generally slow testing progress and involve programmer assistance to debug and fix.
A defect that does not cause failure at one point in time may later occur due to environmental changes. Examples of environment change include running on new computer hardware, changes in data, and interacting with different software.[12]
Software testing may involve a Requirements gap – omission from the design for a requirement.[5]: 426  Requirement gaps can often be non-functional requirements such as testability, scalability, maintainability, performance, and security.
A fundamental limitation of software testing is that testing under all combinations of inputs and preconditions (initial state) is not feasible, even with a simple product.[3]: 17–18 [13] 
Defects that manifest in unusual conditions are difficult to find in testing. Also, non-functional dimensions of quality (how it is supposed to be versus what it is supposed to do) – usability, scalability, performance, compatibility, and reliability – can be subjective; something that constitutes sufficient value to one person may not to another.
Software testing can be categorized into levels based on how much of the software system is the focus of a test.[18][19][20][21]
There are many approaches to software testing. Reviews, walkthroughs, or inspections are referred to as static testing, whereas executing programmed code with a given set of test cases is referred to as dynamic testing.[23][24]
Static testing is often implicit, like proofreading, plus when programming tools/text editors check source code structure or compilers (pre-compilers) check syntax and data flow as static program analysis. Dynamic testing takes place when the program itself is run. Dynamic testing may begin before the program is 100% complete in order to test particular sections of code and are applied to discrete functions or modules.[23][24] Typical techniques for these are either using stubs/drivers or execution from a debugger environment.[24]
Passive testing means verifying the system's behavior without any interaction with the software product. Contrary to active testing, testers do not provide any test data but look at system logs and traces. They mine for patterns and specific behavior in order to make some kind of decisions.[25] This is related to offline runtime verification and log analysis.
Software testing can often be divided into white-box and black-box. These two approaches are used to describe the point of view that the tester takes when designing test cases. A hybrid approach called grey-box that includes aspects of both boxes may also be applied to software testing methodology.[31][32]
White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) verifies the internal structures or workings of a program, as opposed to the functionality exposed to the end-user. In white-box testing, an internal perspective of the system (the source code), as well as programming skills are used to design test cases. The tester chooses inputs to exercise paths through the code and determines the appropriate outputs.[31][32] This is analogous to testing nodes in a circuit, e.g., in-circuit testing (ICT).
While white-box testing can be applied at the unit, integration, and system levels of the software testing process, it is usually done at the unit level.[33] It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it might not detect unimplemented parts of the specification or missing requirements.
Code coverage tools can evaluate the completeness of a test suite that was created with any method, including black-box testing. This allows the software team to examine parts of a system that are rarely tested and ensures that the most important function points have been tested.[35] Code coverage as a software metric can be reported as a percentage for:[31][35][36]
Black-box testing (also known as functional testing) describes designing test cases without knowledge of the implementation, without reading the source code. The testers are only aware of what the software is supposed to do, not how it does it.[38] Black-box testing methods include: equivalence partitioning, boundary value analysis, all-pairs testing, state transition tables, decision table testing, fuzz testing, model-based testing, use case testing, exploratory testing, and specification-based testing.[31][32][36]
Specification-based testing aims to test the functionality of software according to the applicable requirements.[39] This level of testing usually requires thorough test cases to be provided to the tester, who then can simply verify that for a given input, the output value (or behavior), either "is" or "is not" the same as the expected value specified in the test case. Test cases are built around specifications and requirements, i.e., what the application is supposed to do. It uses external descriptions of the software, including specifications, requirements, and designs, to derive test cases. These tests can be functional or non-functional, though usually functional. Specification-based testing may be necessary to assure correct functionality, but it is insufficient to guard against complex or high-risk situations.[40]
The aim of visual testing is to provide developers with the ability to examine what was happening at the point of software failure by presenting the data in such a way that the developer can easily find the information he or she requires, and the information is expressed clearly.[44][45]
Ad hoc testing and exploratory testing are important methodologies for checking software integrity because they require less preparation time to implement, while the important bugs can be found quickly.[46] In ad hoc testing, where testing takes place in an improvised impromptu way, the ability of the tester(s) to base testing off documented methods and then improvise variations of those tests can result in a more rigorous examination of defect fixes.[46] However, unless strict documentation of the procedures is maintained, one of the limits of ad hoc testing is lack of repeatability.[46]
Grey-box testing (American spelling: gray-box testing) involves using knowledge of internal data structures and algorithms for purposes of designing tests while executing those tests at the user, or black-box level. The tester will often have access to both "the source code and the executable binary."[47] Grey-box testing may also include reverse engineering (using dynamic code analysis) to determine, for instance, boundary values or error messages.[47] Manipulating input data and formatting output do not qualify as grey-box, as the input and output are clearly outside of the "black box" that we are calling the system under test. This distinction is particularly important when conducting integration testing between two modules of code written by two different developers, where only the interfaces are exposed for the test.
By knowing the underlying concepts of how the software works, the tester makes better-informed testing choices while testing the software from outside. Typically, a grey-box tester will be permitted to set up an isolated testing environment with activities, such as seeding a database. The tester can observe the state of the product being tested after performing certain actions such as executing SQL statements against the database and then executing queries to ensure that the expected changes have been reflected. Grey-box testing implements intelligent test scenarios based on limited information. This will particularly apply to data type handling, exception handling, and so on.[48]
Most software systems have installation procedures that are needed before they can be used for their main purpose. Testing these procedures to achieve an installed software system that may be used is known as installation testing.[49]: 139  These procedures may involve full or partial upgrades, and install/uninstall processes.
A common cause of software failure (real or perceived) is a lack of its compatibility with other application software, operating systems (or operating system versions, old or new), or target environments that differ greatly from the original (such as a terminal or GUI application intended to be run on the desktop now being required to become a Web application, which must render in a Web browser). For example, in the case of a lack of backward compatibility, this can occur because the programmers develop and test software only on the latest version of the target environment, which not all users may be running. This results in the unintended consequence that the latest work may not function on earlier versions of the target environment, or on older hardware that earlier versions of the target environment were capable of using. Sometimes such issues can be fixed by proactively abstracting operating system functionality into a separate program module or library.
Smoke testing consists of minimal attempts to operate the software, designed to determine whether there are any basic problems that will prevent it from working at all. Such tests can be used as build verification test.
Regression testing focuses on finding defects after a major code change has occurred. Specifically, it seeks to uncover software regressions, as degraded or lost features, including old bugs that have come back. Such regressions occur whenever software functionality that was previously working correctly, stops working as intended. Typically, regressions occur as an unintended consequence of program changes, when the newly developed part of the software collides with the previously existing code. Regression testing is typically the largest test effort in commercial software development,[50] due to checking numerous details in prior software features, and even new software can be developed while using some old test cases to test parts of the new design to ensure prior functionality is still supported.
Acceptance testing is system-level testing to ensure the software meets customer expectations.[51][52][53][54]
Tests are frequently grouped into these levels by where they are performed in the software development process, or by the level of specificity of the test.[54]
OAT is used to conduct operational readiness (pre-release) of a product, service or system as part of a quality management system. OAT is a common type of non-functional software testing, used mainly in software development and software maintenance projects. This type of testing focuses on the operational readiness of the system to be supported, or to become part of the production environment. Hence, it is also known as operational readiness testing (ORT) or operations readiness and assurance (OR&A) testing. Functional testing within OAT is limited to those tests that are required to verify the non-functional aspects of the system.
In addition, the software testing should ensure that the portability of the system, as well as working as expected, does not also damage or partially corrupt its operating environment or cause other processes within that environment to become inoperative.[55]
Contractual acceptance testing is performed based on the contract's acceptance criteria defined during the agreement of the contract, while regulatory acceptance testing is performed based on the relevant regulations to the software product. Both of these two tests can be performed by users or independent testers. Regulation acceptance testing sometimes involves the regulatory agencies auditing the test results.[54]
Alpha testing is simulated or actual operational testing by potential users/customers or an independent test team at the developers' site. Alpha testing is often employed for off-the-shelf software as a form of internal acceptance testing before the software goes to beta testing.[56]
Beta testing comes after alpha testing and can be considered a form of external user acceptance testing. Versions of the software, known as beta versions, are released to a limited audience outside of the programming team known as beta testers. The software is released to groups of people so that further testing can ensure the product has few faults or bugs. Beta versions can be made available to the open public to increase the feedback field to a maximal number of future users and to deliver value earlier, for an extended or even indefinite period of time (perpetual beta).[57]
Non-functional testing refers to aspects of the software that may not be related to a specific function or user action, such as scalability or other performance, behavior under certain constraints, or security. Testing will determine the breaking point, the point at which extremes of scalability or performance leads to unstable execution. Non-functional requirements tend to be those that reflect the quality of the product, particularly in the context of the suitability perspective of its users.
Continuous testing is the process of executing automated tests as part of the software delivery pipeline to obtain immediate feedback on the business risks associated with a software release candidate.[58][59] Continuous testing includes the validation of both functional requirements and non-functional requirements; the scope of testing extends from validating bottom-up requirements or user stories to assessing the system requirements associated with overarching business goals.[60][61]
Destructive testing attempts to cause the software or a sub-system to fail. It verifies that the software functions properly even when it receives invalid or unexpected inputs, thereby establishing the robustness of input validation and error-management routines.[citation needed] Software fault injection, in the form of fuzzing, is an example of failure testing. Various commercial non-functional testing tools are linked from the software fault injection page; there are also numerous open-source and free software tools available that perform destructive testing.
Load testing is primarily concerned with testing that the system can continue to operate under a specific load, whether that be large quantities of data or a large number of users. This is generally referred to as software scalability. The related load testing activity of when performed as a non-functional activity is often referred to as endurance testing. Volume testing is a way to test software functions even when certain components (for example a file or database) increase radically in size. Stress testing is a way to test reliability under unexpected or rare workloads. Stability testing (often referred to as load or endurance testing) checks to see if the software can continuously function well in or above an acceptable period.
Real-time software systems have strict timing constraints. To test if timing constraints are met, real-time testing is used.
Accessibility testing is done to ensure that the software is accessible to persons with disabilities. Some of the common web accessibility tests are
Security testing is essential for software that processes confidential data to prevent system intrusion by hackers.
Testing for internationalization and localization validates that the software can be used with different languages and geographic regions. The process of pseudolocalization is used to test the ability of an application to be translated to another language, and make it easier to identify when the localization process may introduce new bugs into the product.
Globalization testing verifies that the software is adapted for a new culture, such as different currencies or time zones.[64]
Development testing is a software development process that involves the synchronized application of a broad spectrum of defect prevention and detection strategies in order to reduce software development risks, time, and costs. It is performed by the software developer or engineer during the construction phase of the software development lifecycle. Development testing aims to eliminate construction errors before code is promoted to other testing; this strategy is intended to increase the quality of the resulting software as well as the efficiency of the overall development process.
Depending on the organization's expectations for software development, development testing might include static code analysis, data flow analysis, metrics analysis, peer code reviews, unit testing, code coverage analysis, traceability, and other software testing practices.
Concurrent or concurrency testing assesses the behaviour and performance of software and systems that use concurrent computing, generally under normal usage conditions. Typical problems this type of testing will expose are deadlocks, race conditions and problems with shared memory/resource handling.
In software testing, conformance testing verifies that a product performs according to its specified standards. Compilers, for instance, are extensively tested to determine whether they meet the recognized standard for that language.
Metamorphic testing (MT) is a property-based software testing technique, which can be an effective approach for addressing the test oracle problem and test case generation problem. The test oracle problem is the difficulty of determining the expected outcomes of selected test cases or to determine whether the actual outputs agree with the expected outcomes.
In an organization, testers may be in a separate team from the rest of the software development team or they may be integrated into one team. Software testing can also be performed by non-dedicated software testers.
Notable software testing roles and titles include:[66] test manager, test lead, test analyst, test designer, tester, automation developer, and test administrator.[67]
Agile software development commonly involves testing while the code is being written and organizing teams with both programmers and testers and with team members performing both programming and testing.
One agile practice, test-driven software development (TDD), is a way of unit testing such that unit-level testing is performed while writing the product code.[70] Test code is updated as new features are added and failure conditions are discovered (bugs fixed). Commonly, the unit test code is maintained with the project code, integrated in the build process, and run on each build and as part of regression testing. Goals of this continuous integration is to support development and reduce defects.[71][70]
Even in organizations that separate teams by programming and testing functions, many often have the programmers perform unit testing.[72]
The terms verification and validation are commonly used interchangeably in the industry; it is also common to see these two terms defined with contradictory definitions. According to the IEEE Standard Glossary of Software Engineering Terminology:[11]: 80–81
In the case of IEEE standards, the specified requirements, mentioned in the definition of validation, are the set of problems, needs and wants of the stakeholders that the software must solve and satisfy. Such requirements are documented in a Software Requirements Specification (SRS). And, the products mentioned in the definition of verification, are the output artifacts of every phase of the software development process. These products are, in fact, specifications such as Architectural Design Specification, Detailed Design Specification, etc. The SRS is also a specification, but it cannot be verified (at least not in the sense used here, more on this subject below).
But, for the ISO 9000, the specified requirements are the set of specifications, as just mentioned above, that must be verified. A specification, as previously explained, is the product of a software development process phase that receives another specification as input. A specification is verified successfully when it correctly implements its input specification. All the specifications can be verified except the SRS because it is the first one (it can be validated, though). Examples: The Design Specification must implement the SRS; and, the Construction phase artifacts must implement the Design Specification.
Both the SRS and the software must be validated. The SRS can be validated statically by consulting with the stakeholders. Nevertheless, running some partial implementation of the software or a prototype of any kind (dynamic testing) and obtaining positive feedback from them, can further increase the certainty that the SRS is correctly formulated. On the other hand, the software, as a final and running product (not its artifacts and documents, including the source code) must be validated dynamically with the stakeholders by executing the software and having them to try it.
In some organizations, software testing is part of a software quality assurance (SQA) process.[3]: 347  In SQA, software process specialists and auditors are concerned with the software development process rather than just the artifacts such as documentation, code and systems. They examine and change the software engineering process itself to reduce the number of faults that end up in the delivered software: the so-called defect rate. What constitutes an acceptable defect rate depends on the nature of the software; a flight simulator video game would have much higher defect tolerance than software for an actual airplane. Although there are close links with SQA, testing departments often exist independently, and there may be no SQA function in some companies.[citation needed]
Software testing is an activity to investigate software under test in order to provide quality-related information to stakeholders. By contrast, QA (quality assurance) is the implementation of policies and procedures intended to prevent defects from reaching customers.
There are a number of frequently used software metrics, or measures, which are used to assist in determining the state of the software or the adequacy of the testing.
A software testing process can produce several artifacts. The actual artifacts produced are a factor of the software development model used, stakeholder and organisational needs.
A test case normally consists of a unique identifier, requirement references from a design specification, preconditions, events, a series of steps (also known as actions) to follow, input, output, expected result, and the actual result. Clinically defined, a test case is an input and an expected result.[75] This can be as terse as "for condition x your derived result is y", although normally test cases describe in more detail the input scenario and what results might be expected. It can occasionally be a series of steps (but often steps are contained in a separate test procedure that can be exercised against multiple test cases, as a matter of economy) but with one expected result or expected outcome. The optional fields are a test case ID, test step, or order of execution number, related requirement(s), depth, test category, author, and check boxes for whether the test is automatable and has been automated. Larger test cases may also contain prerequisite states or steps, and descriptions. A test case should also contain a place for the actual result. These steps can be stored in a word processor document, spreadsheet, database, or other common repositories. In a database system, you may also be able to see past test results, who generated the results, and what system configuration was used to generate those results. These past results would usually be stored in a separate table.
A test script is a procedure or programming code that replicates user actions. Initially, the term was derived from the product of work created by automated regression test tools. A test case will be a baseline to create test scripts using a tool or a program.
The software, tools, samples of data input and output, and configurations are all referred to collectively as a test harness.
Several certification programs exist to support the professional aspirations of software testers and quality assurance specialists. A few practitioners argue that the testing field is not ready for certification, as mentioned in the controversy section.
Boehm doesn't even cite a paper for the TRW data, except when writing for "Making Software" in 2010, and there he cited the original 1976 article. There exists a large study conducted at TRW at the right time for Boehm to cite it, but that paper doesn't contain the sort of data that would support Boehm's claims.[86]
An artifact is one of many kinds of tangible by-products produced during the development of software. Some artifacts (e.g., use cases, class diagrams, requirements and design documents) help describe the function, architecture, and design of software. Other artifacts are concerned with the process of development itself—such as project plans, business cases, and risk assessments.
The term artifact in connection with software development is largely associated with specific development methods or processes e.g., Unified Process. This usage of the term may have originated with those methods.
Build tools often refer to source code compiled for testing as an artifact, because the executable is necessary to carrying out the testing plan.  Without the executable to test, the testing plan artifact is limited to non-execution based testing.  In non-execution based testing, the artifacts are the walkthroughs, inspections and correctness proofs.  On the other hand, execution based testing requires at minimum two artifacts: a test suite and the executable.  Artifact occasionally may refer to the released code (in the case of a code library) or released executable (in the case of a program) produced, but more commonly an artifact is the byproduct of software development rather than the product itself. Open source code libraries often contain a testing harness to allow contributors to ensure their changes do not cause regression bugs in the code library.
In end-user development an artifact is either an application or a complex data object that is created by an end-user without the need to know a general programming language. Artifacts describe automated behavior or control sequences, such as database requests or grammar rules,[1] or user-generated content.
Artifacts vary in their maintainability.  Maintainability is primarily affected by the role the artifact fulfills.  The role can be either practical or symbolic.  In the earliest stages of software development, artifacts may be created by the design team to serve a symbolic role to show the project sponsor how serious the contractor is about meeting the project's needs.  Symbolic artifacts often convey information poorly, but are impressive-looking. Symbolic enhance understanding.  Generally speaking, Illuminated Scrolls are also considered unmaintainable due to the diligence it requires to preserve the symbolic quality.  For this reason, once Illuminated Scrolls are shown to the project sponsor and approved, they are replaced by artifacts which serve a practical role.  Practical artifacts usually need to be maintained throughout the project lifecycle, and, as such, are generally highly maintainable.
Artifacts are significant from a project management perspective as deliverables. The deliverables of a software project are likely to be the same as its artifacts with the addition of the software itself.
Software development is the process of designing and implementing a software solution to satisfy a user. The process is more encompassing than programming, writing code, in that it includes conceiving the goal, evaluating feasibility, analyzing requirements, design, testing and release. The process is part of software engineering which also includes organizational management, project management, configuration management and other aspects.[1]
Software development involves many skills and job specializations including programming, testing, documentation, graphic design, user support, marketing, and fundraising.
Software development involves many tools including: compiler, integrated development environment (IDE), version control, computer-aided software engineering, and word processor.
Another focus in many programming methodologies is the idea of trying to catch issues such as security vulnerabilities and bugs as early as possible (shift-left testing) to reduce the cost of tracking and fixing them.[13]
In 2009, it was estimated that 32 percent of software projects were delivered on time and budget, and with the full functionality. An additional 44 percent were delivered, but missing at least one of these features. The remaining 24 percent were cancelled prior to release.[14]
The sources of ideas for software products are plentiful. These ideas can come from market research including the demographics of potential new customers, existing customers, sales prospects who rejected the product, other internal software development staff, or a creative third party. Ideas for software products are usually first evaluated by marketing personnel for economic feasibility, fit with existing channels of distribution, possible effects on existing product lines, required features, and fit with the company's marketing objectives. In the marketing evaluation phase, the cost and time assumptions become evaluated.[16] The feasibility analysis estimates the project's return on investment, its development cost and timeframe. Based on this analysis, the company can make a business decision to invest in further development.[17] After deciding to develop the software, the company is focused on delivering the product at or below the estimated cost and time, and with a high standard of quality (i.e., lack of bugs) and the desired functionality. Nevertheless, most software projects run late and sometimes compromises are made in features or quality to meet a deadline.[18]
Software analysis begins with a requirements analysis to capture the business needs of the software.[19] Challenges for the identification of needs are that current or potential users may have different and incompatible needs, may not understand their own needs, and change their needs during the process of software development.[20] Ultimately, the result of analysis is a detailed specification for the product that developers can work from. Software analysts often decompose the project into smaller objects, components that can be reused for increased cost-effectiveness, efficiency, and reliability.[19] Decomposing the project may enable a multi-threaded implementation that runs significantly faster on multiprocessor computers.[21]
During the analysis and design phases of software development, structured analysis is often used to break down the customer's requirements into pieces that can be implemented by software programmers.[22] The underlying logic of the program may be represented in data-flow diagrams, data dictionaries, pseudocode, state transition diagrams, and/or entity relationship diagrams.[23] If the project incorporates a piece of legacy software that has not been modeled, this software may be modeled to help ensure it is correctly incorporated with the newer software.[24]
Design involves choices about the implementation of the software, such as which programming languages and database software to use, or how the hardware and network communications will be organized. Design may be iterative with users consulted about their needs in a process of trial and error. Design often involves people expert in aspect such as database design, screen architecture, and the performance of servers and other hardware.[19] Designers often attempt to find patterns in the software's functionality to spin off distinct modules that can be reused with object-oriented programming. An example of this is the model–view–controller, an interface between a graphical user interface and the backend.[25]
The central feature of software development is creating and understanding the software that implements the desired functionality.[26] There are various strategies for writing the code. Cohesive software has various components that are independent from each other.[19] Coupling is the interrelation of different software components, which is viewed as undesirable because it increases the difficulty of maintenance.[27] Often, software programmers do not follow industry best practices, resulting in code that is inefficient, difficult to understand, or lacking documentation on its functionality.[28] These standards are especially likely to break down in the presence of deadlines.[29] As a result, testing, debugging, and revising the code becomes much more difficult. Code refactoring, for example adding more comments to the code, is a solution to improve the understandability of code.[30]
Testing is the process of ensuring that the code executes correctly and without errors. Debugging is performed by each software developer on their own code to confirm that the code does what it is intended to. In particular, it is crucial that the software executes on all inputs, even if the result is incorrect.[31] Code reviews by other developers are often used to scrutinize new code added to the project, and according to some estimates dramatically reduce the number of bugs persisting after testing is complete.[32] Once the code has been submitted, quality assurance—a separate department of non-programmers for most large companies—test the accuracy of the entire software product. Acceptance tests derived from the original software requirements are a popular tool for this.[31] Quality testing also often includes stress and load checking (whether the software is robust to heavy levels of input or usage), integration testing (to ensure that the software is adequately integrated with other software), and compatibility testing (measuring the software's performance across different operating systems or browsers).[31] When tests are written before the code, this is called test-driven development.[33]
Production is the phase in which software is deployed to the end user.[34] During production, the developer may create technical support resources for users[35][34] or a process for fixing bugs and errors that were not caught earlier. There might also be a return to earlier development phases if user needs changed or were misunderstood.[34]
Software development is performed by software developers, usually working on a team. Efficient communications between team members is essential to success. This is more easily achieved if the team is small, used to working together, and located near each other.[36] Communications also help identify problems at an earlier state of development and avoid duplicated effort. Many development projects avoid the risk of losing essential knowledge held by only one employee by ensuring that multiple workers are familiar with each component.[37] Software development involves professionals from various fields, not just software programmers but also individuals specialized in testing, documentation writing, graphic design, user support, marketing, and fundraising. Although workers for proprietary software are paid, most contributors to open-source software are volunteers.[38] Alternately, they may be paid by companies whose business model does not involve selling the software, but something else—such as services and modifications to open source software.[39]
Computer-aided software engineering (CASE) is tools for the partial automation of software development.[40] CASE enables designers to sketch out the logic of a program, whether one to be written, or an already existing one to help integrate it with new code or reverse engineer it (for example, to change the programming language).[41]
Documentation comes in two forms that are usually kept separate—that intended for software developers, and that made available to the end user to help them use the software.[42][43] Most developer documentation is in the form of code comments for each file, class, and method that cover the application programming interface (API)—how the piece of software can be accessed by another—and often implementation details.[44] This documentation is helpful for new developers to understand the project when they begin working on it.[45] In agile development, the documentation is often written at the same time as the code.[46] User documentation is more frequently written by technical writers.[47]
Accurate estimation is crucial at the feasibility stage and in delivering the product on time and within budget. The process of generating estimations is often delegated by the project manager.[48] Because the effort estimation is directly related to the size of the complete application, it is strongly influenced by addition of features in the requirements—the more requirements, the higher the development cost. Aspects not related to functionality, such as the experience of the software developers and code reusability, are also essential to consider in estimation.[49] As of 2019[update], most of the tools for estimating the amount of time and resources for software development were designed for conventional applications and are not applicable to web applications or mobile applications.[50]
An integrated development environment (IDE) supports software development with enhanced features compared to a simple text editor.[51] IDEs often include automated compiling, syntax highlighting of errors,[52] debugging assistance,[53] integration with version control, and semi-automation of tests.[51]
Version control is a popular way of managing changes made to the software. Whenever a new version is checked in, the software saves a backup of all modified files. If multiple programmers are working on the software simultaneously, it manages the merging of their code changes. The software highlights cases where there is a conflict between two sets of changes and allows programmers to fix the conflict.[54]
A view model is a framework that provides the viewpoints on the system and its environment, to be used in the software development process. It is a graphical representation of the underlying semantics of a view.
Intellectual property can be an issue when developers integrate open-source code or libraries into a proprietary product, because most open-source licenses used for software require that modifications be released under the same license. As an alternative, developers may choose a proprietary alternative or write their own software module.[57]
Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications. It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs.[1][2][3][4]
The terms programmer and coder overlap software engineer, but they imply only the construction aspect of a typical software engineer workload.[5]
A software engineer applies a software development process,[1][6] which involves defining, implementing, testing, managing, and maintaining software systems, as well as developing the software development process itself.
The development of software engineering was seen as a struggle. Problems included software that was over budget, exceeded deadlines, required extensive debugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed.
In 1968, NATO held the first software engineering conference where issues related to software were addressed. Guidelines and best practices for the development of software were established.[7]
The origins of the term software engineering have been attributed to various sources. The term appeared in a list of services offered by companies in the June 1965 issue of "Computers and Automation"[8] and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) in "President's Letter to the ACM Membership" by Anthony A. Oettinger.[9][10][11] It is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer.[12] Margaret Hamilton described the discipline of "software engineering" during the Apollo missions to give what they were doing legitimacy.[13] At the time there was perceived to be a "software crisis".[14][15][16] The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of "Software Engineering" with the Plenary Sessions' keynotes of Frederick Brooks[17] and Margaret Hamilton.[18]
In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States.[19]
Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.[19] The Process Maturity Levels introduced became the Capability Maturity Model Integration for Development (CMMI-DEV), which defined how the US Government evaluates the abilities of a software development team.
Modern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK).[6] Software engineering is considered one of the major computing disciplines.[20]
Margaret Hamilton promoted the term "software engineering" during her work on the Apollo program. The term "engineering" was used to acknowledge that the work should be taken just as seriously as other contributions toward the advancement of technology. Hamilton details her use of the term:
When I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new "term" per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.[30]
Individual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering.[31][32] Steve McConnell has said that it is not, but that it should be.[33] Donald Knuth has said that programming is an art and a science.[34] Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused in the United States.[35]
Requirements engineering is about elicitation, analysis, specification, and validation of requirements for software. Software requirements can be functional, non-functional or domain.
Functional requirements describe expected behaviors (i.e. outputs). Non-functional requirements specify issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects.[36]
Software design is the process of making high-level plans for the software. Design is sometimes divided into levels:
Software construction typically involves programming (a.k.a. coding), unit testing, integration testing, and debugging so as to implement the design.[1][6]“Software testing is related to, but different from, ... debugging”.[6]
Testing during this phase is generally performed by the programmer and with the purpose to verify that the code behaves as designed and to know when the code is ready for the next level of testing.[citation needed]
Software testing is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the software under test.[1][6]
When described separately from construction, testing typically is performed by test engineers or quality assurance instead of the programmers who wrote it.  It is performed at the system level and is considered an aspect of software quality.
Software maintenance refers to supporting the software after release. It may include but is not limited to: error correction, optimization, deletion of unused and discarded features, and enhancement of existing features.[1][6]
Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004, the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.[39]
Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014.[20] A number of universities have Software Engineering degree programs; as of 2010[update], there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.
In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.
Half of all practitioners today have degrees in computer science, information systems, or information technology.[citation needed] A small but growing number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering bachelor's degree in the world; in the following year, the University of Sheffield established a similar program.[40] In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States; however, it did not obtain ABET accreditation until 2003, the same year as Rice University, Clarkson University, Milwaukee School of Engineering, and Mississippi State University.[41] In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering.[citation needed]
Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society. As of 2004[update], about 50 universities in the U.S. offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering master's degree was established at Seattle University in 1979. Since then, graduate software engineering degrees have been made available from many more universities. Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.
In 1998, the US Naval Postgraduate School (NPS) established the first doctorate program in Software Engineering in the world.[citation needed] Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers.[42] ETS (École de technologie supérieure) University and UQAM (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer.[6]
Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario,[43] and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society.
In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, thereby allowing Software Engineers to be licensed and recognized.[44] NCEES ended the exam after April 2019 due to lack of participation.[45] Mandatory licensing is currently still largely debated, and perceived as controversial.[46][47]
The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current version is SWEBOK v4.[6] The IEEE also promulgates a "Software Engineering Code of Ethics".[48]
There are an estimated 26.9 million professional software engineers in the world as of 2022, up from 21 million in 2016.[49][50]
Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns, often university or college students during a summer break, or externships. Specializations include analysts, architects, developers, testers, technical support, middleware analysts, project managers, software product managers, educators, and researchers.
Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008.[51] Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, Thrombosis, Obesity, and hand and wrist problems such as carpal tunnel syndrome.[52]
The U. S. Bureau of Labor Statistics (BLS) counted 1,365,500 software developers holding jobs in the U.S. in 2018.[53] Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees.[54] The BLS estimates from 2023 to 2033 that computer software engineering would increase by 17%.[55] This is down from the 2022 to 2032 BLS estimate of 25% for software engineering.[55][56] And, is further down from their 30% 2010 to 2020 BLS estimate.[57] Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries.[58][51] In addition, the BLS Job Outlook for Computer Programmers, the U.S. Bureau of Labor Statistics (BLS) Occupational Outlook predicts a decline of -7 percent from 2016 to 2026, a further decline of -9 percent from 2019 to 2029, a decline of -10 percent from 2021 to 2031.[58] and then a decline of -11 percent from 2022 to 2032.[58] Since computer programming can be done from anywhere in the world, companies sometimes hire programmers in countries where wages are lower.[58][59][60] Furthermore, the ratio of women in many software fields has also been declining over the years as compared to other engineering fields.[61] Then there is the additional concern that recent advances in Artificial Intelligence might impact the demand for future generations of Software Engineers.[62][63][64][65][66][67][68] However, this trend may change or slow in the future as many current software engineers in the U.S. market flee the profession or age out of the market in the next few decades.[58]
The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture.[69] IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies.[70] These certification programs are tailored to the institutions that would employ people who use these technologies.
Broader certification of general software engineering skills is available through various professional societies. As of 2006[update], the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP).[71] In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA).[72] The ACM had a professional certification program in the early 1980s,[citation needed] which was discontinued due to lack of interest. The ACM and the IEEE Computer Society together examined the possibility of licensing of software engineers as Professional Engineers in the 1990s,
but eventually decided that such licensing was inappropriate for the professional industrial practice of software engineering.[46] John C. Knight and Nancy G. Leveson presented a more balanced analysis of the licensing issue in 2002.[47]
In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the British Computer Society or Institution of Engineering and Technology and so qualify to be considered for Chartered Engineer status through either of those institutions. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP).[73] In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng.[74] The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.
The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers.[75] Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected.[76] Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations.[77] When North Americans leave work, Asians are just arriving to work. When Asians are leaving work, Europeans arrive to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.
While global outsourcing has several advantages, global – and generally distributed – development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations).[78] Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.
Some claim that the concept of software engineering is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.[83]
Some claim that a core issue with software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a "theoretical environment."[83]
Edsger Dijkstra, a founder of many of the concepts in software development today, rejected the idea of "software engineering" up until his death in 2002, arguing that those terms were poor analogies for what he called the "radical novelty" of computer science:
A number of these phenomena have been bundled under the name "Software Engineering". As economics is known as "The Miserable Science", software engineering should be known as "The Doomed Discipline", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter "How to program if you cannot."[84]
In computer software, a general-purpose programming language (GPL) is a programming language for building software in a wide variety of application domains. Conversely, a domain-specific programming language (DSL) is used within a specific area. For example, Python is a GPL, while SQL is a DSL for querying relational databases.
Early programming languages were designed for scientific computing (numerical calculations) or commercial data processing, as was computer hardware. Scientific languages such as Fortran and Algol supported floating-point calculations and multidimensional arrays, while business languages such as COBOL supported fixed-field file formats and data records. Much less widely used were specialized languages such as IPL-V and LISP for symbolic list processing; COMIT for string manipulation; APT for numerically controlled machines. Systems programming requiring pointer manipulation was typically done in assembly language, though JOVIAL was used for some military applications.[1]
IBM's System/360, announced in 1964, was designed as a unified hardware architecture supporting both scientific and commercial applications, and IBM developed PL/I for it as a single, general-purpose language that supported scientific, commercial, and systems programming. Indeed, a subset of PL/I was used as the standard systems programming language for the Multics operating system.
Since PL/I, the distinction between scientific and commercial programming languages has diminished, with most languages supporting the basic features required by both, and much of the special file format handling delegated to specialized database management systems.
Many specialized languages were also developed starting in the 1960s: GPSS and Simula for discrete event simulation; MAD, BASIC, Logo, and Pascal for teaching programming; C for systems programming; JOSS and APL\360 for interactive programming.[1]
The distinction between general-purpose programming languages and domain-specific programming languages is not always clear.[2] A programming language may be created for a specific task, but used beyond that original domain and thus be considered a general purpose programming language. For example, COBOL, Fortran, and Lisp were created as DSLs (for business processing, numeric computation, and symbolic processing), but became GPL's over time.[dubious – discuss] Inversely, a language may be designed for general use but only applied in a specific area in practice.[3] A programming language that is well suited for a problem, whether it be general-purpose language or DSL, should minimize the level of detail required while still being expressive enough in the problem domain.[4] As the name suggests, general-purpose language is "general" in that it cannot provide support for domain-specific notation while DSLs can be designed in diverse problem domains to handle this problem.[4] General-purpose languages are preferred to DSLs when an application domain is not well understood enough to warrant its own language. In this case, a general-purpose language with an appropriate library of data types and functions for the domain may be used instead.[5] While DSLs are usually smaller than GPL in that they offer a smaller range of notations of abstractions, some DSLs actually contain an entire GPL as a sublanguage. In these instances, the DSLs are able to offer domain-specific expressive power along with the expressive power of GPL.[6]
General Purpose programming languages are all Turing complete, meaning that they can theoretically solve any computational problem. Domain-specific languages are often similarly Turing complete but are not exclusively so.
General-purpose programming languages are more commonly used by programmers. According to a study, C, Python, and Java were the most commonly used programming languages in 2021.[7] One argument in favor of using general-purpose programming languages over domain-specific languages is that more people will be familiar with these languages, overcoming the need to learn a new language.
Additionally, for many tasks (e.g., statistical analysis, machine learning, etc.) there are libraries that are extensively tested and optimized. Theoretically, the presence of these libraries should bridge the gap between general-purpose and domain-specific languages.[4]
The predecessor to C, B, was developed largely for a specific purpose: systems programming.[8] By contrast, C has found use in a variety of computational domains, such as operating systems, device drivers, application software, and embedded systems.
C is suitable for use in a variety of areas because of its generality. It provides economy of expression, flow control, data structures, and a rich set of operators, but does not constrain its users to use it in any one context.[9] As a result, though it was first used by its creators to rewrite the kernel of the Unix operating system,[10] it was easily adapted for use in application development, embedded systems (e.g., microprocessor programming), video games (e.g., Doom), and so on. Today, C remains one of the most popular and widely-used programming languages.[11]
Conceived as an extension to C, C++ introduced object-oriented features, as well as other conveniences like references, operator overloading, and default arguments. Like C, C++'s generality allowed it to be used in a wide range of areas. While its C++'s core area of application is in systems programming (because of C++'s ability to grant access to low-level architecture),[12] it has been used extensively to build desktop applications, video games, databases, financial systems, and much more.[13] Major software and finance companies, such as Microsoft, Apple, Bloomberg, and Morgan Stanley, still widely use C++ in their internal and external applications.[13]
Python was conceived as a language that emphasized code readability and extensibility.[14][15] The former allowed non-software engineers to easily learn and write computer programs, while the latter allowed domain specialists to easily create libraries suited to their own use cases. For these reasons, Python has been used across a wide range of domains.
Erlang (/ˈɜːrlæŋ/ UR-lang) is a general-purpose, concurrent, functional high-level programming language, and a garbage-collected runtime system. The term Erlang is used interchangeably with Erlang/OTP, or Open Telecom Platform (OTP), which consists of the Erlang runtime system, several ready-to-use components (OTP) mainly written in Erlang, and a set of design principles for Erlang programs.[5]
The Erlang programming language has immutable data, pattern matching, and functional programming.[7] The sequential subset of the Erlang language supports eager evaluation, single assignment, and dynamic typing.
It was originally proprietary software within Ericsson, developed by Joe Armstrong, Robert Virding, and Mike Williams in 1986,[8] but was released as free and open-source software in 1998.[9][10] Erlang/OTP is supported and maintained by the Open Telecom Platform (OTP) product unit at Ericsson.
The name Erlang, attributed to Bjarne Däcker, has been presumed by those working on the telephony switches (for whom the language was designed) to be a reference to Danish mathematician and engineer Agner Krarup Erlang and a syllabic abbreviation of "Ericsson Language".[8][11][12] Erlang was designed with the aim of improving the development of telephony applications.[13] The initial version of Erlang was implemented in Prolog and was influenced by the programming language PLEX used in earlier Ericsson exchanges. By 1988 Erlang had proven that it was suitable for prototyping telephone exchanges, but the Prolog interpreter was far too slow. One group within Ericsson estimated that it would need to be 40 times faster to be suitable for production use. In 1992, work began on the BEAM virtual machine (VM), which compiles Erlang to C using a mix of natively compiled code and threaded code to strike a balance between performance and disk space.[14] According to co-inventor Joe Armstrong, the language went from laboratory product to real applications following the collapse of the next-generation AXE telephone exchange named AXE-N in 1995. As a result, Erlang was chosen for the next Asynchronous Transfer Mode (ATM) exchange AXD.[8]
Erlang applications are built of very lightweight Erlang processes in the Erlang runtime system. Erlang processes can be seen as "living" objects (object-oriented programming), with data encapsulation and message passing, but capable of changing behavior during runtime. The Erlang runtime system provides strict process isolation between Erlang processes (this includes data and garbage collection, separated individually by each Erlang process) and transparent communication between processes (see Location transparency) on different Erlang nodes (on different hosts).
Erlang is designed with a mechanism that makes it easy for external processes to monitor for crashes (or hardware failures), rather than an in-process mechanism like exception handling used in many other programming languages. Crashes are reported like other messages, which is the only way processes can communicate with each other,[28] and subprocesses can be spawned cheaply (see below). The "let it crash" philosophy prefers that a process be completely restarted rather than trying to recover from a serious failure.[29] Though it still requires handling of errors, this philosophy results in less code devoted to defensive programming where error-handling code is highly contextual and specific.[28]
Erlang's main strength is support for concurrency. It has a small but powerful set of primitives to create processes and communicate among them. Erlang is conceptually similar to the language occam, though it recasts the ideas of communicating sequential processes (CSP) in a functional framework and uses asynchronous message passing.[30] Processes are the primary means to structure an Erlang application. They are neither operating system processes nor threads, but lightweight processes that are scheduled by BEAM. Like operating system processes (but unlike operating system threads), they share no state with each other. The estimated minimal overhead for each is 300 words.[31] Thus, many processes can be created without degrading performance. In 2005, a benchmark with 20 million processes was successfully performed with 64-bit Erlang on a machine with 16 GB random-access memory (RAM; total 800 bytes/process).[32] Erlang has supported symmetric multiprocessing since release R11B of May 2006.
While threads need external library support in most languages, Erlang provides language-level features to create and manage processes with the goal of simplifying concurrent programming. Though all concurrency is explicit in Erlang, processes communicate using message passing instead of shared variables, which removes the need for explicit locks (a locking scheme is still used internally by the VM).[33]
Erlang supports language-level Dynamic Software Updating. To implement this, code is loaded and managed as "module" units; the module is a compilation unit. The system can keep two versions of a module in memory at the same time, and processes can concurrently run code from each. The versions are referred to as the "new" and the "old" version. A process will not move into the new version until it makes an external call to its module.
In 1998, Ericsson released Erlang as free and open-source software to ensure its independence from a single vendor and to increase awareness of the language. Erlang, together with libraries and the real-time distributed database Mnesia, forms the OTP collection of libraries. Ericsson and a few other companies support Erlang commercially.
In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.
In functional programming, functions are treated as first-class citizens, meaning that they can be bound to names (including local identifiers), passed as arguments, and returned from other functions, just as any other data type can. This allows programs to be written in a declarative and composable style, where small functions are combined in a modular manner.
Functional programming is sometimes treated as synonymous with purely functional programming, a subset of functional programming that treats all functions as deterministic mathematical functions, or pure functions. When a pure function is called with some given arguments, it will always return the same result, and cannot be affected by any mutable state or other side effects. This is in contrast with impure procedures, common in imperative programming, which can have side effects (such as modifying the program's state or taking input from a user). Proponents of purely functional programming claim that by restricting side effects, programs can have fewer bugs, be easier to debug and test, and be more suited to formal verification.[1][2]
Functional programming has its roots in academia, evolving from the lambda calculus, a formal system of computation based only on functions. Functional programming has historically been less popular than imperative programming, but many functional languages are seeing use today in industry and education, including Common Lisp, Scheme,[3][4][5][6] Clojure, Wolfram Language,[7][8] Racket,[9] Erlang,[10][11][12] Elixir,[13] OCaml,[14][15] Haskell,[16][17] and F#.[18][19] Lean is a functional programming language commonly used for verifying mathematical theorems.[20] Functional programming is also key to some languages that have found success in specific domains, like JavaScript in the Web,[21] R in statistics,[22][23] J, K and Q in financial analysis, and XQuery/XSLT for XML.[24][25] Domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, such as not allowing mutable values.[26] In addition, many other programming languages support programming in a functional style or have implemented features from functional programming, such as C++11, C#,[27] Kotlin,[28] Perl,[29] PHP,[30] Python,[31] Go,[32] Rust,[33] Raku,[34] Scala,[35] and Java (since Java 8).[36]
The lambda calculus, developed in the 1930s by Alonzo Church, is a formal system of computation built from function application. In 1937 Alan Turing proved that the lambda calculus and Turing machines are equivalent models of computation,[37] showing that the lambda calculus is Turing complete. Lambda calculus forms the basis of all functional programming languages. An equivalent theoretical formulation, combinatory logic, was developed by Moses Schönfinkel and Haskell Curry in the 1920s and 1930s.[38]
Church later developed a weaker system, the simply typed lambda calculus, which extended the lambda calculus by assigning a data type to all terms.[39] This forms the basis for statically typed functional programming.
The first high-level functional programming language, Lisp, was developed in the late 1950s for the IBM 700/7000 series of scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT).[40] Lisp functions were defined using Church's lambda notation, extended with a label construct to allow recursive functions.[41] Lisp first introduced many paradigmatic features of functional programming, though early Lisps were multi-paradigm languages, and incorporated support for numerous programming styles as new paradigms evolved. Later dialects, such as Scheme and Clojure, and offshoots such as Dylan and Julia, sought to simplify and rationalise Lisp around a cleanly functional core, while Common Lisp was designed to preserve and update the paradigmatic features of the numerous older dialects it replaced.[42]
Information Processing Language (IPL), 1956, is sometimes cited as the first computer-based functional programming language.[43] It is an assembly-style language for manipulating lists of symbols. It does have a notion of generator, which amounts to a function that accepts a function as an argument, and, since it is an assembly-level language, code can be data, so IPL can be regarded as having higher-order functions. However, it relies heavily on the mutating list structure and similar imperative features.
Kenneth E. Iverson developed APL in the early 1960s, described in his 1962 book A Programming Language (ISBN 9780471430148). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.
In the mid-1960s, Peter Landin invented SECD machine,[44] the first abstract machine for a functional programming language,[45] described a correspondence between ALGOL 60 and the lambda calculus,[46][47] and proposed the ISWIM programming language.[48]
John Backus presented FP in his 1977 Turing Award lecture "Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs".[49] He defines functional programs as being built up in a hierarchical way by means of "combining forms" that allow an "algebra of programs"; in modern language, this means that functional programs follow the principle of compositionality.[citation needed] Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style now associated with functional programming.
In the 1970s, Guy L. Steele and Gerald Jay Sussman developed Scheme, as described in the Lambda Papers and the 1985 textbook Structure and Interpretation of Computer Programs. Scheme was the first dialect of lisp to use lexical scoping and to require tail-call optimization, features that encourage functional programming.
In the 1980s, Per Martin-Löf developed intuitionistic type theory (also called constructive type theory), which associated functional programs with constructive proofs expressed as dependent types. This led to new approaches to interactive theorem proving and has influenced the development of subsequent functional programming languages.[citation needed]
The lazy functional language, Miranda, developed by David Turner, initially appeared in 1985 and had a strong influence on Haskell. With Miranda being proprietary, Haskell began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing as of 1990.
More recently it has found use in niches such as parametric CAD in the OpenSCAD language built on the CGAL framework, although its restriction on reassigning values (all values are treated as constants) has led to confusion among users who are unfamiliar with functional programming as a concept.[53]
A number of concepts[57] and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages often cater to several programming paradigms, so programmers using "mostly imperative" languages may have utilized some of these concepts.[58]
Higher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: "higher-order" describes a mathematical concept of functions that operate on other functions, while "first-class" is a computer science term for programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).
While most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also lets functions be designated pure.[59] C++11 added constexpr keyword with similar semantics.
Most general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language's type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.[64]
Especially since the development of Hindley–Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, rejecting all invalid programs at compilation time and risking false positive errors, as opposed to the untyped lambda calculus, that accepts all valid programs at compilation time and risks false negative errors, used in Lisp and its variants (such as Scheme), as they reject all invalid programs at runtime when the information is enough to not reject valid programs. The use of algebraic data types makes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like test-driven development, while type inference frees the programmer from the need to manually declare types to the compiler in most cases.
A limited form of dependent types called generalized algebraic data types (GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience.[73] GADT's are available in the Glasgow Haskell Compiler, in OCaml[74] and in Scala,[75] and have been proposed as additions to other languages including Java and C#.[76]
Purely functional data structures are often represented in a different way to their imperative counterparts.[78] For example, the array with constant access and update times is a basic component of most imperative languages, and many imperative data-structures, such as the hash table and binary heap,  are based on arrays. Arrays can be replaced by maps or random access lists, which admit purely functional implementation, but have logarithmic access and update times. Purely functional data structures have persistence, a property of keeping previous versions of the data structure unmodified. In Clojure, persistent data structures are used as functional alternatives to their imperative counterparts. Persistent vectors, for example, use trees for partial updating. Calling the insert method will result in some but not all nodes being created.[79]
Functional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.
Higher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order "map" function that takes a function and a list, generating and returning a new list by applying the function to each list item.
Sometimes the abstractions offered by functional programming might lead to development of more robust code that avoids certain issues that might arise when building upon large amount of complex, imperative code, such as off-by-one errors (see Greenspun's tenth rule).
There are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.
The pure functional programming language Haskell implements them using monads, derived from category theory.[80] Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).[81]
Functional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal.[84]  This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware. Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer chasing), or handled with SIMD instructions.  It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree).[85] However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C according to The Computer Language Benchmarks Game.[86] For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.
Immutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.[87] Even if the involved copying that may seem implicit when dealing with persistent immutable data structures might seem computationally costly, some functional programming languages, like Clojure solve this issue by implementing mechanisms for safe memory sharing between formally immutable data.[88] Rust distinguishes itself by its approach to data immutability which involves immutable references[89] and a concept called lifetimes.[90]
Immutable data with separation of identity and state and shared-nothing schemes can also potentially be more well-suited for concurrent and parallel programming by the virtue of reducing or eliminating the risk of certain concurrency hazards, since concurrent operations are usually atomic and this allows eliminating the need for locks. This is how for example java.util.concurrent classes are implemented, where some of them are immutable variants of the corresponding classes that are not suitable for concurrent use.[91] Functional programming languages often have a concurrency model that instead of shared state and synchronization, leverages message passing mechanisms (such as the actor model, where each actor is a container for state, behavior, child actors and a message queue).[92][93] This approach is common in Erlang/Elixir or Akka.
Some functional programming languages might not optimize abstractions such as higher order functions like "map" or "filter" as efficiently as the underlying imperative operations. Consider, as an example, the following two ways to check if 5 is an even number in Clojure:
has the mean execution time of 4.76 ms, while the second one, in which .equals is a direct invocation of the underlying Java method, has a mean execution time of 2.8 μs – roughly 1700 times faster. Part of that can be attributed to the type checking and exception handling involved in the implementation of even?. For instance the lo library for Go, which implements various higher-order functions common in functional programming languages using generics. In a benchmark provided by the library's author, calling map is 4% slower than an equivalent for loop and has the same allocation profile,[95] which can be attributed to various compiler optimizations, such as inlining.[96]
It is possible to use a functional style of programming in languages that are not traditionally considered functional languages.[98] For example, both D[99] and Fortran 95[59] explicitly support pure functions.
In Perl, lambda, map, reduce, filter, and closures are fully supported and frequently used.  The book Higher-Order Perl, released in 2005, was written to provide an expansive guide on using Perl for functional programming.
In PHP, anonymous classes, closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style.
In C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.
Many object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.
Similarly, the idea of immutable data from functional programming is often included in imperative programming languages,[108] for example the tuple in Python, which is an immutable array, and Object.freeze() in JavaScript.[109]
Logic programming can be viewed as a generalisation of functional programming, in which functions are a special case of relations.[110]
For example, the function, mother(X) = Y, (every X has only one mother Y) can be represented by the relation mother(X, Y). Whereas functions have a strict input-output pattern of arguments, relations can be queried with any pattern of inputs and outputs. Consider the following logic program:
However, the difference between the two representations is simply syntactic. In Ciao Prolog, relations can be nested, like functions in functional programming:[111]
Emacs, a highly extensible text editor family uses its own Lisp dialect for writing plugins. The original author of the most popular Emacs implementation, GNU Emacs and Emacs Lisp, Richard Stallman considers Lisp one of his favorite programming languages.[112]
Helix, since version 24.03 supports previewing AST as S-expressions, which are also the core feature of the Lisp programming language family.[113]
Spreadsheets can be considered a form of pure, zeroth-order, strict-evaluation functional programming system.[114] However, spreadsheets generally lack higher-order functions as well as code reuse, and in some implementations, also lack recursion. Several extensions have been developed for spreadsheet programs to enable higher-order and reusable functions, but so far remain primarily academic in nature.[115]
Due to their composability, functional programming paradigms can be suitable for microservices-based architectures.[116]
Functional programming is an active area of research in the field of programming language theory. There are several peer-reviewed publication venues focusing on functional programming, including the International Conference on Functional Programming, the Journal of Functional Programming, and the Symposium on Trends in Functional Programming.
Functional programming has been employed in a wide range of industrial applications. For example, Erlang, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems,[11] but has since become popular for building a range of applications at companies such as Nortel, Facebook, Électricité de France and WhatsApp.[10][12][117][118][119] Scheme, a dialect of Lisp, was used as the basis for several applications on early Apple Macintosh computers[3][4] and has been applied to problems such as training-simulation software[5] and telescope control.[6] OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis,[14] driver verification, industrial robot programming and static analysis of embedded software.[15] Haskell, though initially intended as a research language,[17] has also been applied in areas such as aerospace systems, hardware design and web programming.[16][17]
Other functional programming languages that have seen use in industry include Scala,[120] F#,[18][19] Wolfram Language,[7] Lisp,[121] Standard ML[122][123] and Clojure.[124] Scala has been widely used in Data science,[125] while ClojureScript,[126] Elm[127] or PureScript[128] are some of the functional frontend programming languages used in production. Elixir's Phoenix framework is also used by some relatively popular commercial projects, such as Font Awesome or Allegro (one of the biggest e-commerce platforms in Poland)[129]'s classified ads platform Allegro Lokalnie.[130]
Functional "platforms" have been popular in finance for risk analytics (particularly with large investment banks). Risk factors are coded as functions that form interdependent graphs (categories) to measure correlations in market shifts, similar in manner to Gröbner basis optimizations but also for regulatory frameworks such as Comprehensive Capital Analysis and Review. Given the use of OCaml and Caml variations in finance, these systems are sometimes considered related to a categorical abstract machine. Functional programming is heavily influenced by category theory.[citation needed]
Many universities teach functional programming.[131][132][133][134] Some treat it as an introductory programming concept[134] while others first teach imperative programming methods.[133][135]
Outside of computer science, functional programming is used to teach problem-solving, algebraic and geometric concepts.[136] It has also been used to teach classical mechanics, as in the book Structure and Interpretation of Classical Mechanics.
In particular, Scheme has been a relatively popular choice for teaching programming for years.[137][138]
There are two slightly differing meanings of the term hot swapping. It may refer only to the ability to add or remove hardware without powering down the system, while the system software may have to be notified by the user of the event in order to cope with it. Examples include PCMCIA, RS-232 and lower-end SCSI devices.
Although most contemporary video game systems can interchange games and multimedia (e.g. Blu-rays) without powering down the system, older generations of systems varied in their support of hot-swapping capabilities. For example, whereas the Sony PlayStation and PlayStation 2 could eject a game disc with the system powered on, the Nintendo Game Boy Advance and the Nintendo 64 would freeze up and could potentially become corrupt if the game cartridge was removed with the power on. Manufacturers specifically warned against such practices in the owner's manual or on the game cartridge.[4] It was supposedly for this reason that Stop 'N' Swop was taken out of the Banjo-Kazooie series. With the Sega Genesis/Mega Drive system, it was sometimes possible to apply cheats (such as a player having infinite lives) and other temporary software alterations to games by hot swapping cartridges, even though the cartridges were not designed to be hot swappable.[5]
Hot swapping can also refer to the ability to alter the running code of a program without needing to interrupt its execution. Interactive programming is a programming paradigm that makes extensive use of hot swapping, so the programming activity becomes part of the program flow itself.
Only a few programming languages support hot swapping natively, including Pike, Lisp, Erlang, Smalltalk, Visual Basic 6 (not VB.NET), Java and most recently Elm[8] and Elixir. Microsoft Visual Studio supports a kind of hot swapping called Edit and Continue, which is supported by C#, VB.NET and C/C++ when running under a debugger.[9]
Hot swapping is the central method in live coding, where programming is an integral part of the runtime process. In general, all programming languages used in live coding, such as SuperCollider, TidalCycles, or Extempore support hot swapping.
Some web-based frameworks, such as Django, support detecting module changes and reloading them on the fly.  However, although the same as hotswapping for most intents and purposes, this is technically just a cache purge, triggered by a new file. This does not apply to markup and programming languages such as HTML and PHP respectively, in the general case, as these files are normally reinterpreted on each use by default.  There are a few CMSes and other PHP-based frameworks (such as Drupal) that employ caching, however.  In these cases, similar abilities and exceptions apply.
Hot swapping also facilitates developing systems where large amounts of data are being processed, as in entire genomes in bioinformatics algorithms.[10]
C (pronounced /ˈsiː/ – like the letter c)[6] is a general-purpose programming language. It was created in the 1970s by Dennis Ritchie and remains very widely used and influential. By design, C's features cleanly reflect the capabilities of the targeted CPUs. It has found lasting use in operating systems code (especially in kernels[7]), device drivers, and protocol stacks, but its use in application software has been decreasing.[8] C is commonly used on computer architectures that range from the largest supercomputers to the smallest microcontrollers and embedded systems.
A successor to the programming language B, C was originally developed at Bell Labs by Ritchie between 1972 and 1973 to construct utilities running on Unix. It was applied to re-implementing the kernel of the Unix operating system.[9] During the 1980s, C gradually gained popularity. It has become one of the most widely used programming languages,[10][11] with C compilers available for practically all modern computer architectures and operating systems. The book The C Programming Language, co-authored by the original language designer, served for many years as the de facto standard for the language.[12][1] C has been standardized since 1989 by the American National Standards Institute (ANSI) and, subsequently, jointly by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC).
C is an imperative procedural language, supporting structured programming, lexical variable scope, and recursion, with a static type system. It was designed to be compiled to provide low-level access to memory and language constructs that map efficiently to machine instructions, all with minimal runtime support. Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant C program written with portability in mind can be compiled for a wide variety of computer platforms and operating systems with few changes to its source code.
Since 2000, C has consistently ranked among the top four languages in the TIOBE index, a measure of the popularity of programming languages.[13]
C is an imperative, procedural language in the ALGOL tradition.  It has a static type system. In C, all executable code is contained within subroutines (also called "functions", though not in the sense of functional programming). Function parameters are passed by value, although arrays are passed as pointers, i.e. the address of the first item in the array. Pass-by-reference is simulated in C by explicitly passing pointers to the thing being referenced.
The origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Dennis Ritchie and Ken Thompson, incorporating several ideas from colleagues.  Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was also developed in assembly language.[9]
Thompson wanted a programming language for developing utilities for the new platform. He first tried writing a Fortran compiler, but he soon gave up the idea and instead created a cut-down version of the recently developed systems programming language called BCPL. The official description of BCPL was not available at the time,[14] and Thompson modified the syntax to be less 'wordy' and similar to a simplified ALGOL known as SMALGOL.[15] He called the result B,[9] describing it as "BCPL semantics with a lot of SMALGOL syntax".[15] Like BCPL, B had a bootstrapping compiler to facilitate porting to new machines.[15] Ultimately, few utilities were written in B because it was too slow and could not take advantage of PDP-11 features such as byte addressability.
Unix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system (which was written in PL/I) and Master Control Program (MCP) for the Burroughs B5000 (which was written in ALGOL) in 1961. In around  1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system.  Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.[16]
In 1978 Brian Kernighan and Dennis Ritchie published the first edition of The C Programming Language.[18] Known as K&R from the initials of its authors, the book served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as "K&R C". As this was released in 1978, it is now also referred to as C78.[19] The second edition of the book[20] covers the later ANSI C standard, described below.
In 1983 the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 "Programming Language C".  This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.
In 1990 the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms "C89" and "C90" refer to the same programming language.
C17 is an informal name for ISO/IEC 9899:2018, a standard for the C programming language published in June 2018. It introduces no new language features, only technical corrections, and clarifications to defects in C11. The standard macro __STDC_VERSION__ is defined as 201710L to indicate that C17 support is available.
Historically, embedded C programming requires non-standard extensions to the C language to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.
As an imperative language, C uses statements to specify actions. The most common statement is an expression statement, consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by if ... [else] conditional execution and by do ... while, while, and for iterative execution (looping). The for statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. break and continue can be used within the loop. Break is used to leave the innermost enclosing loop statement and continue is used to skip to its reinitialisation. There is also a non-structured goto statement which branches directly to the designated label within the function. switch selects a case to be executed based on the value of an integer expression. Different from many other languages, control-flow will fall through to the next case unless terminated by a break.
Expressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next "sequence point"; sequence points include the end of each expression statement, and the entry to and return from each function call.  Sequence points also occur during evaluation of expressions containing certain operators (&&, ||, ?: and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.
Kernighan and Ritchie say in the Introduction of The C Programming Language: "C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better."[31] The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.
Most of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations.  Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. Some of those words were added as keywords with their conventional spelling in C23 and the corresponding macros were removed.
Prior to C89, entry was reserved as a keyword. In the second edition of their book The C Programming Language, which describes what became known as C89, Kernighan and Ritchie wrote, "The ... [keyword] entry, formerly reserved but never used, is no longer reserved." and "The stillborn entry keyword is withdrawn."[33]
The "hello, world" example that appeared in the first edition of K&R has become the model for an introductory program in most programming textbooks. The program prints "hello, world" to the standard output, which is usually a terminal or screen display.
C is often used in low-level systems programming where escapes from the type system may be necessary.  The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a type cast to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.
Careless use of pointers is potentially dangerous.  Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects.  Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer.  In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.
C does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing.  The index values of the resulting "multi-dimensional array" can be thought of as increasing in row-major order. Multi-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, in early versions of C the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this was to allocate the array with an additional "row vector" of pointers to the columns.) C99 introduced "variable-length arrays" which address this issue.
One of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three principal ways to allocate memory for objects:[37]
Where possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary.[37]  Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on C dynamic memory allocation for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated.  (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)
The C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single "archive" file.  Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. For a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., -lm, shorthand for "link the math library").[37]
File input and output (I/O) is not part of the C language itself but instead is handled by libraries (such as the C standard library) and their associated header files (e.g. stdio.h). File handling is generally implemented through high-level I/O which works through streams. A stream is from this perspective a data flow that is independent of devices, while a file is a concrete device. The high-level I/O is done through the association of a stream to a file. In the C standard library, a buffer (a memory area or queue) is temporarily used to store data before it is sent to the final destination. This reduces the time spent waiting for slower devices, for example a hard drive or solid-state drive. Low-level I/O functions are not part of the standard C library[clarification needed] but are generally part of "bare metal" programming (programming that is independent of any operating system such as most embedded programming). With few exceptions, implementations include low-level I/O.
There are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.
C is widely used for systems programming in implementing operating systems and embedded system applications.[46] This is for several reasons:
C enables programmers to create efficient implementations of algorithms and data structures, because the layer of abstraction from hardware is thin, and its overhead is low, an important criterion for computationally intensive programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C.  Many languages support calling library functions in C, for example, the Python-based framework NumPy uses C for the high-performance and hardware-interacting aspects.
A consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C.[48] For example, the reference implementations of Python,[49] Perl,[50] Ruby,[51] and PHP[52] are written in C.
The two most popular web servers, Apache HTTP Server and Nginx, are both written in C.  These web servers interact with the operating system, listen on TCP ports for HTTP requests, and then serve up static web content, or cause the execution of other languages handling to 'render' content such as PHP, which is itself primarily written in C.  C's close-to-the-metal approach allows for the construction of these high-performance software systems.
For some purposes, restricted styles of C have been adopted, e.g. MISRA C or CERT C, in an attempt to reduce the opportunity for bugs.  Databases such as CWE attempt to count the ways C etc. has vulnerabilities, along with recommendations for mitigation.
When object-oriented programming languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.[66]
The C++ programming language (originally named "C with Classes") was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax.[67] C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now[when?] supports most of C, with a few exceptions.
Objective-C was originally a very "thin" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.
Object-oriented programming (OOP) is a programming paradigm based on the concept of objects.[1] Objects can contain data (called fields, attributes or properties) and have actions they can perform (called procedures or methods and implemented in code). In OOP, computer programs are designed by making them out of objects that interact with one another.[2][3]
Many of the most widely used programming languages (such as C++, Java,[4] and Python) support object-oriented programming to a greater or lesser degree, typically as part of multiple paradigms in combination with others such as imperative programming and declarative programming.
The idea of "objects" in programming started with the artificial intelligence group at MIT in the late 1950s and early 1960s. Here, "object" referred to LISP atoms with identified properties (attributes).[5][6]
Another early example was Sketchpad created by Ivan Sutherland at MIT in 1960–1961. In the glossary of his technical report, Sutherland defined terms like "object" and "instance" (with the class concept covered by "master" or "definition"), albeit specialized to graphical interaction.[7] Later, in 1968, AED-0, MIT's version of the ALGOL programming language,  connected data structures ("plexes") and procedures, prefiguring what were later termed "messages", "methods", and "member functions".[8][9]
Topics such as data abstraction and modular programming were common points of discussion at this time.
I thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages (so messaging came at the very beginning – it took a while to see how to do messaging in a programming language efficiently enough to be useful).
Influenced by both MIT and Simula, Alan Kay began developing his own ideas in November 1966. He would go on to create Smalltalk, an influential object-oriented programming language. By 1967, Kay was already using the term "object-oriented programming" in conversation.[1] Although sometimes called the "father" of object-oriented programming,[12] Kay has said his ideas differ from how object-oriented programming is commonly understood, and has implied that the computer science establishment did not adopt his notion.[1]
A 1976 MIT memo co-authored by Barbara Liskov lists Simula 67, CLU, and Alphard as object-oriented languages, but does not mention Smalltalk.[13]
In the 1970s, the first version of the Smalltalk programming language was developed at Xerox PARC by Alan Kay, Dan Ingalls and Adele Goldberg. Smalltalk-72 was notable for use of objects at the language level and its graphical development environment.[14] Smalltalk was a fully dynamic system, allowing users to create and modify classes as they worked.[15] Much of the theory of OOP was developed in the context of Smalltalk, for example multiple inheritance.[16]
In the late 1970s and 1980s, object-oriented programming rose to prominence. The Flavors object-oriented Lisp was developed starting 1979, introducing multiple inheritance and mixins.[17] In August 1981, Byte Magazine highlighted Smalltalk and OOP, introducing these ideas to a wide audience.[18] LOOPS, the object system for Interlisp-D, was influenced by Smalltalk and Flavors, and a paper about it was published in 1982.[19] In 1986, the first Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA) was attended by 1,000 people. This conference marked the beginning of efforts to consolidate Lisp object systems, eventually resulting in the Common Lisp Object System. In the 1980s, there were a few attempts to design processor architectures that included hardware support for objects in memory, but these were not successful. Examples include the Intel iAPX 432 and the Linn Smart Rekursiv.
In the mid-1980s, new object-oriented languages like Objective-C, C++, and Eiffel emerged. Objective-C was developed by Brad Cox, who had used Smalltalk at ITT Inc.. Bjarne Stroustrup created C++ based on his experience using Simula for his PhD thesis.[14] Bertrand Meyer produced the first design of the Eiffel language in 1985, which focused on software quality using a design by contract approach.[20]
In the 1990s, object-oriented programming became the main way of programming, especially as more languages supported it. These included Visual FoxPro 3.0,[21][22] C++,[23] and Delphi[citation needed]. OOP became even more popular with the rise of graphical user interfaces, which used objects for buttons, menus and other elements. One well-known example is Apple's Cocoa framework, used on Mac OS X and written in Objective-C. OOP toolkits also enhanced the popularity of event-driven programming.[citation needed]
Many programming languages that existed before OOP have added object-oriented features, including Ada, BASIC, Fortran, Pascal, and COBOL. This sometimes caused compatibility and maintainability issues, as these languages were not originally designed with OOP in mind.
Object-oriented programming focuses on working with objects, but not all OOP languages have every feature linked to OOP. Below are some common features of languages that are considered strong in OOP or support it along with other programming styles. Important exceptions are also noted.[24][25][26][27] Christopher J. Date pointed out that comparing OOP with other styles, like relational programming, is difficult because there isn't a clear, agreed-upon definition of OOP.[28]
Features from imperative and structured programming are present in OOP languages and are also found in non-OOP languages.
Support for modular programming lets programmers organize related procedures into files and modules. This makes programs easier to manage. Each modules has its own namespace, so items in one module will not conflict with items in another.
Object-oriented programming (OOP) was created to make code easier to reuse and maintain.[29] However, it was not designed to clearly show the flow of a program's instructions—that was left to the compiler. As computers began using more parallel processing and multiple threads, it became more important to understand and control how instructions flow. This is difficult to do with OOP.[30][31][32][33]
An object is a type of data structure that has two main parts: fields and methods. Fields may also be known as members, attributes, or properties, and hold information in the form of state variables. Methods are actions, subroutines, or procedures, defining the object's behavior in code. Objects are usually stored in memory, and in many programming languages, they work like pointers that link directly to a contiguous block containing the object instances's data.
Some believe that OOP places too much focus on using objects rather than on algorithms and data structures.[34][35] For example, programmer Rob Pike pointed out that OOP can make programmers think more about type hierarchy than composition.[36] He has called object-oriented programming "the Roman numerals of computing".[37] Rich Hickey, creator of Clojure, described OOP as overly simplistic, especially when it comes to representing real-world things that change over time.[35] Alexander Stepanov said that OOP tries to fit everything into a single type, which can be limiting. He argued that sometimes we need multisorted algebras—families of interfaces that span multiple types, such as in generic programming. Stepanov also said that calling everything an "object" doesn't add much understanding.[34]
However, more often, objects represent abstract entities, like an open file or a unit converter. Not everyone agrees that OOP makes it easy to copy the real world exactly or that doing so is even necessary. Bob Martin suggests that because classes are software, their relationships don't match the real-world relationships they represent.[40] Bertrand Meyer argues in Object-Oriented Software Construction, that a program is not a model of the world but a model of some part of the world; "Reality is a cousin twice removed".[41] Steve Yegge noted that natural languages lack the OOP approach of strictly prioritizing things (objects/nouns) before actions (methods/verbs), as opposed to functional programming which does the reverse.[42] This can sometimes make OOP solutions more complicated than those written in procedural programming.[43]
In class-based programming, the most common type of OOP, every object is an instance of a specific class. The class defines the data format, like variables (e.g., name, age) and methods (actions the object can take). Every instance of the class has the same set of variables and methods. Objects are created using a special method in the class known as a constructor.
In prototype-based programming, there aren't any classes. Instead, each object is linked to another object, called its prototype or parent. In Self, an object may have multiple or no parents,[45] but in the most popular prototype-based language, Javascript, every object has exactly one prototype link, up to the base Object type whose prototype is null.
However, this is often not true, especially in programming languages that allow mutable objects, objects that change after they are created. In fact, subtype polymorphism as enforced by the type checker in OOP languages cannot guarantee behavioral subtyping in most if not all contexts. For example, the circle-ellipse problem is notoriously difficult to handle using OOP's concept of inheritance. Behavioral subtyping is undecidable in general, so it cannot be easily implemented by a compiler. Because of this, programmers must carefully design class hierarchies to avoid mistakes that the programming language itself cannot catch.
Data abstraction is a way of organizing code so that only certain parts of the data are visible to related functions (data hiding). This helps prevent mistakes and makes the program easier to manage. Because data abstraction works well, many programming styles, like object-oriented programming and functional programming, use it as a key principle. Encapsulation is another important idea in programming. It means keeping the internal details of an object hidden from the outside code. This makes it easier to change how an object works on the inside without affecting other parts of the program, such as in code refactoring. Encapsulation also helps keep related code together (decoupling), making it easier for programmers to understand.
In object-oriented programming, objects act as a barrier between their internal workings and external code. Outside code can only interact with an object by calling specific public methods or variables. If a class only allows access to its data through methods and not directly, this is called information hiding. When designing a program, it's often recommended to keep data as hidden as possible. This means using local variables inside functions when possible, then private variables (which only the object can use), and finally public variables (which can be accessed by any part of the program) if necessary. Keeping data hidden helps prevent problems when changing the code later.[49] Some programming languages, like Java, control information hiding by marking variables as private (hidden) or public (accessible).[50] Other languages, like Python, rely on naming conventions, such as starting a private method's name with an underscore. Intermediate levels of access also exist, such as Java's protected keyword, (which allows access from the same class and its subclasses, but not objects of a different class), and the internal keyword in C#, Swift, and Kotlin, which restricts access to files within the same module.[51]
Abstraction and information hiding are important concepts in programming, especially in object-oriented languages.[52] Programs often create many copies of objects, and each one works independently. Supporters of this approach say it makes code easier to reuse and intuitively represents real-world situations.[53] However, others argue that object-oriented programming does not enhance readability or modularity.[54][55] Eric S. Raymond has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency.[56] Raymond compares this unfavourably to the approach taken with Unix and the C programming language.[56]
One programming principle, called the "open/closed principle", says that classes and functions should be "open for extension, but closed for modification". Luca Cardelli has stated that OOP languages have "extremely poor modularity properties with respect to class extension and modification", and tend to be extremely complex.[54] The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:[55]
Leo Brodie says that information hiding can lead to copying the same code in multiple places (duplicating code),[57] which goes against the don't repeat yourself rule of software development.[58]
Polymorphism is the use of one symbol to represent multiple different types.[59] In object-oriented programming, polymorphism more specifically refers to subtyping or subtype polymorphism, where a function can work with a specific interface and thus manipulate entities of different classes in a uniform manner.[60]
For example, imagine a program has two shapes: a circle and a square. Both come from a common class called "Shape." Each shape has its own way of drawing itself. With subtype polymorphism, the program doesn't need to know the type of each shape, and can simply call the "Draw" method for each shape. The programming language runtime will ensure the correct version of the "Draw" method runs for each shape. Because the details of each shape are handled inside their own classes, this makes the code simpler and more organized, enabling strong separation of concerns.
In object-oriented programming, objects have methods that can change or use the object's data. Many programming languages use a special word, like this or self, to refer to the current object. In languages that support open recursion, a method in an object can call other methods in the same object, including itself, using this special word. This allows a method in one class to call another method defined later in a subclass, a feature known as late binding.
Many popular programming languages, like C++, Java, and Python, use object-oriented programming. In the past, OOP was widely accepted,[62] but recently, some programmers have criticized it and prefer functional programming instead.[63] A study by Potok et al. found no major difference in productivity between OOP and other methods.[64]
Paul Graham, a well-known computer scientist, believes big companies like OOP because it helps manage large teams of average programmers. He argues that OOP adds structure, making it harder for one person to make serious mistakes, but at the same time restrains smart programmers.[65] Eric S. Raymond, a Unix programmer and open-source software advocate, argues that OOP is not the best way to write programs.[56]
Richard Feldman says that, while OOP features helped some languages stay organized, their popularity comes from other reasons.[66] Lawrence Krubner argues that OOP doesn't offer special advantages compared to other styles, like functional programming, and can make coding more complicated.[67] Luca Cardelli says that OOP is slower and takes longer to compile than procedural programming.[54]
In recent years, object-oriented programming (OOP) has become very popular in dynamic programming languages. Some languages, like Python, PowerShell, Ruby and Groovy, were designed with OOP in mind. Others, like Perl, PHP, and ColdFusion, started as non-OOP languages but added OOP features later (starting with Perl 5, PHP 4, and ColdFusion version 6).
The first version of DDM defined distributed file services. Later, it was expanded to support databases through the Distributed Relational Database Architecture (DRDA).
Design patterns are common solutions to problems in software design. Some design patterns are especially useful for object-oriented programming, and design patterns are typically introduced in an OOP context.
Design Patterns: Elements of Reusable Object-Oriented Software is a famous book published in 1994 by four authors: Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. People often call them the "Gang of Four". The book talks about the strengths and weaknesses of object-oriented programming and explains 23 common ways to solve programming problems.
Both object-oriented programming and relational database management systems (RDBMSs) are widely used in software today. However, relational databases don't store objects directly, which creates a challenge when using them together. This issue is called object-relational impedance mismatch.
To solve this problem, developers use different methods, but none of them are perfect.[69] One of the most common solutions is object-relational mapping (ORM), which helps connect object-oriented programs to relational databases. Examples of ORM tools include Visual FoxPro, Java Data Objects, and Ruby on Rails ActiveRecord.
Some databases, called object databases, are designed to work with object-oriented programming. However, they have not been as popular or successful as relational databases.
GRASP (General Responsibility Assignment Software Patterns) is another set of software design rules, created by Craig Larman, that helps developers assign responsibilities to different parts of a program.
In object-oriented programming, objects are things that exist while a program is running. An object can represent anything, like a person, a place, a bank account, or a table of data. Many researchers have tried to formally define how OOP works. Records are the basis for understanding objects. They can represent fields, and also methods, if function literals can be stored. However, inheritance presents difficulties, particularly with the interactions between open recursion and encapsulated state. Researchers have used recursive types and co-algebraic data types to incorporate essential features of OOP.[72] Abadi and Cardelli defined several extensions of System F<: that deal with mutable objects, allowing both subtype polymorphism and parametric polymorphism (generics), and were able to formally model many OOP concepts and constructs.[73] Although far from trivial, static analysis of object-oriented programming languages such as Java is a mature field,[74] with several commercial tools.[75]
Generic programming is a style of computer programming in which algorithms are written in terms of data types to-be-specified-later that are then instantiated when needed for specific types provided as parameters. This approach, pioneered in the programming language ML in 1973,[1][2] permits writing common functions or data types that differ only in the set of types on which they operate when used, thus reducing duplicate code.
Generic programming was introduced to the mainstream with Ada in 1977. With templates in C++, generic programming became part of the repertoire of professional library design. The techniques were further improved and parameterized types were introduced in the influential 1994 book Design Patterns.[3]
New techniques were introduced by Andrei Alexandrescu in his 2001 book Modern C++ Design: Generic Programming and Design Patterns Applied. Subsequently, D implemented the same ideas.
Such software entities are known as generics in Ada, C#, Delphi, Eiffel, F#, Java, Nim, Python, Go, Rust, Swift, TypeScript, and Visual Basic (.NET). They are known as parametric polymorphism in ML, Scala, Julia, and Haskell. (Haskell terminology also uses the term generic for a related but somewhat different concept.)
The term generic programming was originally coined by David Musser and Alexander Stepanov[4] in a more specific sense than the above, to describe a programming paradigm in which fundamental requirements on data types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, with generic functions implemented in terms of these concepts, typically using language genericity mechanisms as described above.
Generic programming centers around the idea of abstracting from concrete, efficient algorithms to obtain generic algorithms that can be combined with different data representations to produce a wide variety of useful software.
The "generic programming" paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra.[6] Early examples of this programming approach were implemented in Scheme and Ada,[7] although the best known example is the Standard Template Library (STL),[8][9] which developed a theory of iterators that is used to decouple sequence data structures and the algorithms operating on them.
For example, given N sequence data structures, e.g. singly linked list, vector etc., and M algorithms to operate on them, e.g. find, sort etc., a direct approach would implement each algorithm specifically for each data structure, giving N × M combinations to implement. However, in the generic programming approach, each data structure returns a model of an iterator concept (a simple value type that can be dereferenced to retrieve the current value, or changed to point to another value in the sequence) and each algorithm is instead written generically with arguments of such iterators, e.g. a pair of iterators pointing to the beginning and end of the subsequence or range to process. Thus, only N + M data structure-algorithm combinations need be implemented. Several iterator concepts are specified in the STL, each a refinement of more restrictive concepts e.g. forward iterators only provide movement to the next value in a sequence (e.g. suitable for a singly linked list or a stream of input data), whereas a random-access iterator also provides direct constant-time access to any element of the sequence (e.g. suitable for a vector). An important point is that a data structure will return a model of the most general concept that can be implemented efficiently—computational complexity requirements are explicitly part of the concept definition. This limits the data structures a given algorithm can be applied to and such complexity requirements are a major determinant of data structure choice. Generic programming similarly has been applied in other domains, e.g. graph algorithms.[10]
Although this approach often uses language features of compile-time genericity and templates, it is independent of particular language-technical details. Generic programming pioneer Alexander Stepanov wrote,
Generic programming is about abstracting and classifying algorithms and data structures. It gets its inspiration from Knuth and not from type theory. Its goal is the incremental construction of systematic catalogs of useful, efficient and abstract algorithms and data structures. Such an undertaking is still a dream.
I believe that iterator theories are as central to Computer Science as theories of rings or Banach spaces are central to Mathematics.
Following Stepanov, we can define generic programming without mentioning language features: Lift algorithms and data structures from concrete examples to their most general and abstract form.
Other programming paradigms that have been described as generic programming include Datatype generic programming as described in  "Generic Programming – an Introduction".[14] The  Scrap your boilerplate approach is a lightweight generic programming approach for Haskell.[15]
In this article we distinguish the high-level programming paradigms of generic programming, above, from the lower-level programming language genericity mechanisms used to implement them (see Programming language support for genericity). For further discussion and comparison of generic programming paradigms, see.[16]
Genericity is implemented and supported differently in various programming languages; the term "generic" has also been used differently in various programming contexts. For example, in Forth the compiler can execute code while compiling and one can create new compiler keywords and new implementations for those words on the fly. It has few words that expose the compiler behaviour and therefore naturally offers genericity capacities that, however, are not referred to as such in most Forth texts. Similarly, dynamically typed languages, especially interpreted ones, usually offer genericity by default as both passing values to functions and value assignment are type-indifferent and such behavior is often used for abstraction or code terseness, however this is not typically labeled genericity as it's a direct consequence of the dynamic typing system employed by the language.[citation needed] The term has been used in functional programming, specifically in Haskell-like languages, which use a structural type system where types are always parametric and the actual code on those types is generic. These uses still serve a similar purpose of code-saving and rendering an abstraction.
Arrays and structs can be viewed as predefined generic types. Every usage of an array or struct type instantiates a new concrete type, or reuses a previous instantiated type. Array element types and struct element types are parameterized types, which are used to instantiate the corresponding generic type. All this is usually built-in in the compiler and the syntax differs from other generic constructs. Some extensible programming languages try to unify built-in and user defined generic types.
A broad survey of genericity mechanisms in programming languages follows. For a specific survey comparing suitability of mechanisms for generic programming, see.[17]
Above, T is a placeholder for whatever type is specified when the list is created. These "containers-of-type-T", commonly called templates, allow a class to be reused with different datatypes as long as certain contracts such as subtypes and signature are kept. This genericity mechanism should not be confused with inclusion polymorphism, which is the algorithmic usage of exchangeable sub-classes: for instance, a list of objects of type Moving_Object containing objects of type Animal and Car. Templates can also be used for type-independent functions as in the Swap example below:
The C++ template construct used above is widely cited[citation needed] as the genericity construct that popularized the notion among programmers and language designers and supports many generic programming idioms. The D language also offers fully generic-capable templates based on the C++ precedent but with a simplified syntax. The Java language has provided genericity facilities syntactically based on C++'s since the introduction of Java Platform, Standard Edition (J2SE) 5.0.
C++ uses templates to enable generic programming techniques. The C++ Standard Library includes the Standard Template Library or STL that provides a framework of templates for common data structures and algorithms. Templates in C++ may also be used for template metaprogramming, which is a way of pre-evaluating some of the code at compile-time rather than run-time. Using template specialization, C++ Templates are Turing complete.
This works whether the arguments x and y are integers, strings, or any other type for which the expression x < y is sensible, or more specifically, for any type for which operator< is defined. Common inheritance is not needed for the set of types that can be used, and so it is very similar to duck typing. A program defining a custom data type can use operator overloading to define the meaning of < for that type, thus allowing its use with the max() function template. While this may seem a minor benefit in this isolated example, in the context of a comprehensive library like the STL it allows the programmer to get extensive functionality for a new data type, just by defining a few operators for it. Merely defining < allows a type to be used with the standard sort(), stable_sort(), and binary_search() algorithms or to be put inside data structures such as sets, heaps, and associative arrays.
Conventionally, D combines the above features to provide compile-time polymorphism using trait-based generic programming.
For example, an input range is defined as any type that satisfies the checks performed by isInputRange, which is defined as follows:
In addition to template metaprogramming, D also provides several features to enable compile-time code generation:
Support for the generics, or "containers-of-type-T" was added to the Java programming language in 2004 as part of J2SE 5.0. In Java, generics are only checked at compile time for type correctness. The generic type information is then removed via a process called type erasure, to maintain compatibility with old JVM implementations, making it unavailable at runtime.[24] For example, a List<String> is converted to the raw type List. The compiler inserts type casts to convert the elements to the String type when they are retrieved from the list, reducing performance compared to other implementations such as C++ templates.
The type class mechanism of Haskell supports generic programming. Six of the predefined type classes in Haskell (including Eq, the types that can be compared for equality, and Show, the types whose values can be rendered as strings) have the special property of supporting derived instances. This means that a programmer defining a new type can state that this type is to be an instance of one of these special type classes, without providing implementations of the class methods as is usually necessary when declaring class instances. All the necessary methods will be "derived" – that is, constructed automatically – based on the structure of the type. For example, the following declaration of a type of binary trees states that it is to be an instance of the classes Eq and Show:
The support for derived instances of Eq and Show makes their methods == and show generic in a qualitatively different way from parametrically polymorphic functions: these "functions" (more accurately, type-indexed families of functions) can be applied to values of various types, and although they behave differently for every argument type, little work is needed to add support for a new type. Ralf Hinze (2004) has shown that a similar effect can be achieved for user-defined type classes by certain programming techniques. Other researchers have proposed approaches to this and other kinds of genericity in the context of Haskell and extensions to Haskell (discussed below).
PolyP was the first generic programming language extension to Haskell. In PolyP, generic functions are called polytypic. The language introduces a special construct in which such polytypic functions can be defined via structural induction over the structure of the pattern functor of a regular datatype. Regular datatypes in PolyP are a subset of Haskell datatypes. A regular datatype t must be of kind * → *, and if a is the formal type argument in the definition, then all recursive calls to t must have the form t a. These restrictions rule out higher-kinded datatypes and nested datatypes, where the recursive calls are of a different form.
The flatten function in PolyP is here provided as an example:
Clean offers generic programming based PolyP and the Generic Haskell as supported by the GHC ≥ 6.0. It parametrizes by kind as those but offers overloading.
Languages in the ML family support generic programming through parametric polymorphism and generic modules called functors. Both Standard ML and OCaml provide functors, which are similar to class templates and to Ada's generic packages. Scheme syntactic abstractions also have a connection to genericity – these are in fact a superset of C++ templates.
A low-level programming language is a programming language that provides little or no abstraction from a computer's instruction set architecture, memory or underlying physical hardware; commands or functions in the language are structurally similar to a processor's instructions. These languages provide the programmer with full control over program memory and the underlying machine code instructions. Because of the low level of abstraction (hence the term "low-level") between the language and machine language, low-level languages are sometimes described as being "close to the hardware". Programs written in low-level languages tend to be relatively non-portable, due to being optimized for a certain type of system architecture.[1][2][3][4]
Low-level languages are directly converted to machine code with or without a compiler or interpreter—second-generation programming languages[5][6] depending on programming language. A program written in a low-level language can be made to run very quickly, with a small memory footprint. Such programs may be architecture dependent or operating system dependent, due to using low level APIs.[1]
Machine code is usually stored in memory as binary data. Programmers almost never write programs directly in machine code; instead, they write code in assembly language or higher-level programming languages.[1]
During the late 1960s and 1970s, high-level languages that included some degree of access to low-level programming functions, such as PL/S, BLISS, BCPL, extended ALGOL and NEWP (for Burroughs large systems/Unisys Clearpath MCP systems), and C, were introduced. One method for this is inline assembly, in which assembly code is embedded in a high-level language that supports this feature. Some of these languages also allow architecture-dependent compiler optimization directives to adjust the way a compiler uses the target processor architecture.
The Free Software Foundation (FSF) is a 501(c)(3) non-profit organization founded by Richard Stallman[6] on October 4, 1985. The organisation supports the free software movement, with the organization's preference for software being distributed under copyleft ("share alike") terms,[7] such as with its own GNU General Public License.[8] The FSF was incorporated in Boston[9] where it is also based.[10]
From its founding until the mid-1990s, FSF's funds were mostly used to employ software developers to write free software for the GNU Project[11] and its employees and volunteers have mostly worked on legal and structural issues for the free software movement and the free software community[which?].
The Free Software Foundation was founded in 1985 as a non-profit corporation supporting free software development. It continued existing GNU projects such as the sale of manuals and tapes, and employed developers of the free software system.[13] Since then, it has continued these activities, as well as advocating for the free software movement. The FSF is also the steward of several free software licenses, meaning it publishes them and has the ability to make revisions as needed.[14]
From 1991 until 2001, GPL enforcement was done informally, usually by Stallman himself, often with assistance from FSF's lawyer, Eben Moglen.[citation needed] Typically, GPL violations during this time were cleared up by short email exchanges between Stallman and the violator.[citation needed] In the interest of promoting copyleft assertiveness by software companies to the level that the FSF was already doing, in 2004 Harald Welte launched gpl-violations.org.
In March 2003, SCO filed suit against IBM alleging that IBM's contributions to various free software, including FSF's GNU, violated SCO's rights. While FSF was never a party to the lawsuit, FSF was subpoenaed on November 5, 2003.[20] During 2003 and 2004, FSF put substantial advocacy effort into responding to the lawsuit and quelling its negative impact on the adoption and promotion of free software.[21][22]
In December 2008, FSF filed a lawsuit against Cisco for using GPL-licensed components shipped with Linksys products. Cisco was notified of the licensing issue in 2003 but Cisco repeatedly disregarded its obligations under the GPL.[28] In May 2009, Cisco and FSF reached settlement under which Cisco agreed to make a monetary donation to the FSF and appoint a Free Software Director to conduct continuous reviews of the company's license compliance practices.[29]
The original purpose of the FSF was to promote the ideals of free software. The organization envisaged the GNU operating system as an example of this.
The GNU General Public License (GPL) is a widely used license for free software projects. The current version (version 3) was released in June 2007. The FSF has also published the GNU Lesser General Public License (LGPL), the GNU Free Documentation License (GFDL), and the GNU Affero General Public License (AGPL).
The FSF's publishing department, responsible for "publishing affordable books on computer science using freely distributable licenses."[35][36]
This is a list of software packages that have been verified as free software. Each package entry contains up to 47 pieces of information such as the project's homepage, developers, programming language, etc. The goals are to provide a search engine for free software, and to provide a cross-reference for users to check if a package has been verified as being free software. The FSF has received a small amount of funding[quantify] from UNESCO for this project.
An abbreviation for "Hardware-Node", the h-node website lists hardware and device drivers that have been verified as compatible with free software. It is user-edited and volunteer supported with hardware entries tested by users before publication.[37][38][39]
FSF sponsors a number of campaigns against what it perceives as dangers to software freedom, including software patents, digital rights management (which the FSF and others[40] have re-termed "digital restrictions management", as part of its effort to highlight technologies that are "designed to take away and limit your rights",[41]) and user interface copyright. Since 2012, Defective by Design is an FSF-initiated campaign against DRM.[42] It also has a campaign to promote Ogg+Vorbis, a free alternative to proprietary formats like AAC and MQA. FSF also sponsors free software projects it deems "high-priority".
"Outstanding new Free Software contributor", "Award for the Advancement of Free Software" and "Free Software Award for Projects of Social Benefit"
The LibrePlanet wiki organizes FSF members into regional groups in order to promote free software activism against digital restrictions management and other issues promoted by the FSF.
The FSF maintains a list of "high-priority projects" to which the Foundation claims that "there is a vital need to draw the free software community's attention".[43] The FSF considers these projects "important because computer users are continually being seduced into using non-free software, because there is no adequate free replacement."[43]
As of 2021, high-priority tasks include reverse engineering proprietary firmware, reversible debugging in GNU Debugger; developing automatic transcription and video editing software, Coreboot, drivers for network routers, a free smartphone operating system and creating replacements for Skype and Siri.[43]
The FSF maintains a list of approved Linux operating systems that maintain free software by default:[47]
Since 2012, the FSF maintains a "Respects Your Freedom" (RYF) hardware certification program. To be granted certification, a product must use 100% Free Software, allow user installation of modified software, be free of backdoors and conform with several other requirements.[48][49]
Eben Moglen and Dan Ravicher previously served individually as pro bono legal counsel to the FSF. After forming the Software Freedom Law Center, Eben Moglen continued to serve as the FSF's general counsel until 2016.[71]
Most of the FSF funding comes from patrons and members.[72] Revenue streams also come from free-software-related compliance labs, job postings, published works, and a web store. FSF offers speakers and seminars for pay, and all FSF projects accept donations.
Revenues fund free-software programs and campaigns, while cash is invested conservatively in socially responsible investing. The financial strategy is designed to maintain the Foundation's long-term future through economic stability.
Linus Torvalds has criticized FSF for using GPLv3 as a weapon in the fight against DRM. Torvalds argues that the issue of DRM and that of a software license should be treated as two separate issues.[79]
On June 16, 2010, Joe Brockmeier, a journalist at Linux Magazine, criticized the Defective by Design campaign by the FSF as "negative" and "juvenile" and not being adequate for providing users with "credible alternatives" to proprietary software.[80] FSF responded to this criticism by saying "that there is a fundamental difference between speaking out against policies or actions and smear campaigns", and "that if one is taking an ethical position, it is justified, and often necessary, to not only speak about the benefits of freedom but against acts of dispossession and disenfranchisement."[81]
In 2009, a license update of LibDWG/LibreDWG to version 3 of the GNU GPL made it impossible for the free software projects LibreCAD and FreeCAD to use LibreDWG legally.[82] Many projects voiced their unhappiness about the GPLv3 license selection for LibreDWG, such as FreeCAD, LibreCAD, Assimp, and Blender.[83] Some suggested the selection of a license with a broader license compatibility, for instance the MIT, BSD, or LGPL 2.1.[83] A request went to the FSF to relicense GNU LibreDWG as GPLv2, which was rejected in 2012.[84]
Stallman resigned from the board in 2019 after making controversial comments about one of the victims of Jeffrey Epstein, but rejoined the board 18 months later.[86] Several prominent organizations and individuals who develop free software objected to the decision, citing past writings on Stallman's blog which they considered antithetical to promoting a diverse community.[87][88] As a result of Stallman's reinstatement, prominent members of the Free Software Foundation quit in protest and Red Hat announced that it would stop funding and supporting the Free Software Foundation.[87][89]
Systems programming, or system programming, is the activity of programming[1] computer system software. The primary distinguishing characteristic of systems programming when compared to application programming is that application programming aims to produce software which provides services to the user directly (e.g. word processor), whereas systems programming aims to produce software and software platforms which provide services to other software, are performance constrained, or both (e.g. operating systems, computational science applications, game engines, industrial automation, and software as a service applications).[1]
Systems programming requires a great degree of hardware awareness. Its goal is to achieve efficient use of available resources, either because the software itself is performance-critical or because even small efficiency improvements directly transform into significant savings of time or money.
In systems programming, often limited programming facilities are available. The use of automatic garbage collection is not common and debugging is sometimes hard to do. The runtime library, if available at all, is usually far less powerful, and does less error checking. Because of those limitations, monitoring and logging are often used; operating systems may have extremely elaborate logging subsystems.
Implementing certain parts in operating systems and networking requires systems programming, for example implementing paging (virtual memory) or a device driver for an operating system.
Originally systems programmers invariably wrote in assembly language. Experiments with hardware support in high level languages in the late 1960s led to such languages as PL/S, BLISS, BCPL, and extended ALGOL for Burroughs large systems. Forth also has applications as a systems language. 
In the 1970s, C became widespread, aided by the growth of Unix.
More recently a subset of C++ called Embedded C++ has seen some use, for instance it is used in the I/O Kit drivers of macOS.[2]
Engineers working at Google created Go in 2007 to address developer productivity in large distributed systems, with developer-focused features such as Concurrency, Garbage Collection, and faster program compilation than C and C++.[3]
In 2015 Rust came out, a general-purpose programming language often used in systems programming. Rust was designed with memory safety in mind and to be as performant as C and C++.
For historical reasons, some organizations use the term systems programmer to describe a job function which would be more accurately termed systems administrator.  This is particularly true in organizations whose computer resources have historically been dominated by mainframes, although the term is even used to describe job functions which do not involve mainframes. This usage arose because administration of IBM mainframes often involved the writing of custom assembler code (IBM's Basic Assembly Language  (BAL)), which integrated with the operating system such as OS/MVS, DOS/VSE or VM/CMS.  Indeed, some IBM software products had substantial code contributions from customer programming staff. This type of programming is progressively less common, and increasingly done in C rather than Assembly, but the term systems programmer is still used as the de-facto job title for staff administering IBM mainframes even in cases where they do not regularly engage in systems programming activities.
Embedded software is computer software, written to control machines or devices that are not typically thought of as computers, commonly known as embedded systems. It is typically specialized for the particular hardware that it runs on and has time and memory constraints.[1] This term is sometimes used interchangeably with firmware.[2]
A precise and stable characteristic feature is that no or not all functions of embedded software are initiated/controlled via a human interface, but through machine-interfaces instead.[3]
Manufacturers build embedded software into the electronics of cars, telephones, modems, robots, appliances, toys, security systems, pacemakers, televisions and set-top boxes, and digital watches, for example.[4] This software can be very simple, such as lighting controls running on an 8-bit microcontroller with a few kilobytes of memory with the suitable level of processing complexity determined with a Probably Approximately Correct Computation framework[5] (a methodology based on randomized algorithms). However, embedded software can become very sophisticated in applications such as routers, optical network elements, airplanes, missiles, and process control systems.[6]
Unlike standard computers that generally use an operating systems such as macOS, Windows or Linux, embedded software may use no operating system. When they do use one, a wide variety of operating systems can be chosen from, typically a real-time operating system. Code for embedded software is typically written in C or C++, but various high-level programming languages, such as Java, Python and JavaScript, are now also in common use to target microcontrollers and embedded systems.[7] Assembly languages are often used too, especially in booting and interrupt handling. Ada is used in some military and aviation projects.
Most consumers are familiar with application software that provide functionality on a computer. However embedded software is often less visible, but no less complicated. Unlike application software, embedded software has fixed hardware requirements and capabilities, and addition of third-party hardware or software is strictly controlled.
Embedded software needs to include all needed device drivers at manufacturing time, and the device drivers are written for the various hardware devices. These device drivers, called BSP (Board support package), form the layer of software containing hardware-specific drivers and other routines that allow a particular operating system (traditionally a real-time operating system, or RTOS) to function in a particular hardware environment (a computer or CPU card), integrated with the RTOS itself. The software is highly dependent on the CPU and specific chips chosen. Most embedded software engineers have at least a passing knowledge of reading schematics, and reading data sheets for components to determine usage of registers and communication system. Conversion between decimal, hexadecimal and binary is useful as well as using bit manipulation.[8]
Web applications are often used for managing hardware, although XML files and other output may be passed to a computer for display. File systems with folders are typically used, however SQL databases are often absent.
Software development requires use of a cross compiler, which runs on a computer but produces executable code for the target device. Debugging requires use of an in-circuit emulator, and debugging hardware such as JTAG or SWD debuggers. Software developers often have access to the complete kernel (OS) source code.
Computer software performance, particularly software application response time, is an aspect of software quality that is important in human–computer interactions.
Most consumers pick a computer architecture (normally Intel IA-32 architecture) to be able to run a large base of pre-existing, pre-compiled software.  Being relatively uninformed on computer benchmarks, some of them pick a particular CPU based on operating frequency (see megahertz myth).
Computers run sets of instructions called a process. In operating systems, the execution of the process can be postponed if other processes are also executing. In addition, the operating system can schedule when to perform the action that the process is commanding. For example, suppose a process commands that a computer card's voltage output be set high-low-high-low and so on at a rate of 1000 Hz. The operating system may choose to adjust the scheduling of each transition (high-low or low-high) based on an internal clock. The latency is the delay between the process instruction commanding the transition and the hardware actually transitioning the voltage from high to low or low to high.
In software engineering, performance testing is, in general, conducted to determine how a system performs in terms of responsiveness and stability under a particular workload. It can also serve to investigate, measure, validate, or verify other quality attributes of the system, such as scalability, reliability, and resource usage.
Performance testing is a subset of performance engineering, an emerging computer science practice which strives to build performance into the implementation, design, and architecture of a system.
In software engineering, profiling ("program profiling", "software profiling") is a form of dynamic program analysis that measures, for example, the space (memory) or time complexity of a program, the usage of particular instructions, or frequency and duration of function calls.  The most common use of profiling information is to aid program optimization.
Perceived performance, in computer engineering, refers to how quickly a software feature appears to perform its task. The concept applies mainly to user acceptance aspects.
Application software is any computer program that is intended for end-user use – not operating, administering or programming the computer. An application (app, application program, software application) is any program that can be categorized as application software.[1][2] Common types of applications include word processor, media player and accounting software.
The term application software refers to all applications collectively[3] and can be used to differentiate from system and utility software.
Applications may be bundled with the computer and its system software or published separately. Applications may be proprietary or open-source.[4]
The delineation between system software such as operating systems and application software is not exact, however, and is occasionally the object of controversy.[6] For example, one of the key questions in the United States v. Microsoft Corp. antitrust trial was whether Microsoft's Internet Explorer web browser was part of its Windows operating system or a separate piece of application software.  As another example, the GNU/Linux naming controversy is, in part, due to disagreement about the relationship between the Linux kernel and the operating systems built over this kernel. In some types of embedded systems, the application software and the operating system software may be indistinguishable from the user, as in the case of software used to control a VCR, DVD player, or microwave oven. The above definitions may exclude some applications that may exist on some computers in large organizations. For an alternative definition of an app:  see Application Portfolio Management.
When used as an adjective, application is not restricted to mean: of or on application software.[6] For example, concepts such as application programming interface (API), application server, application virtualization, application lifecycle management and portable application apply to all computer programs alike, not just application software.
Sometimes a new and popular application arises that only runs on one platform that results in increasing the desirability of that platform. This is called a killer application or killer app, coined in the late 1980s.[7][8] For example, VisiCalc was the first modern spreadsheet software for the Apple II and helped sell the then-new personal computers into offices. For the BlackBerry, it was its email software.
From the legal point of view, application software is mainly classified with a black-box approach, about the rights of its end-users or subscribers (with eventual intermediate and tiered subscription levels).
Software applications are also classified with respect to the programming language in which the source code is written or executed, and concerning their purpose and outputs.
Application software is usually distinguished into two main classes: closed source vs open source software applications, and free or proprietary software applications.
Proprietary software is placed under the exclusive copyright, and a software license grants limited usage rights. The open-closed principle states that software may be "open only for extension, but not for modification". Such applications can only get add-ons from third parties.
Free and open-source software (FOSS) shall be run, distributed, sold, or extended for any purpose, and -being open- shall be modified or reversed in the same way.
FOSS software applications released under a free license may be perpetual and also royalty-free. Perhaps, the owner, the holder or third-party enforcer of any right (copyright, trademark, patent, or ius in re aliena) are entitled to add exceptions, limitations, time decays or expiring dates to the license terms of use.
Public-domain software is a type of FOSS which is royalty-free and - openly or reservedly- can be run, distributed, modified, reversed, republished, or created in derivative works without any copyright attribution and therefore revocation. It can even be sold, but without transferring the public domain property to other single subjects. Public-domain SW can be released under a (un)licensing legal statement, which enforces those terms and conditions for an indefinite duration (for a lifetime, or forever).
Application software can also be seen as being either horizontal or vertical.[12][13]  Horizontal applications are more popular and widespread, because they are general purpose, for example word processors or databases. Vertical applications are niche products, designed for a particular type of industry or business, or department within an organization. Integrated suites of software will try to handle every specific aspect possible of, for example, manufacturing or banking worker, accounting, or customer service.
Applications can also be classified by computing platforms such as a desktop application for a particular operating system,[16] delivery network such as in cloud computing and Web 2.0 applications, or delivery devices such as mobile apps for mobile devices.
The operating system itself can be considered application software when performing simple calculating, measuring, rendering, and word processing tasks not used to control hardware via a command-line interface or graphical user interface. This does not include application software bundled within operating systems such as a software calculator or text editor.
In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term "database" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.
Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.
Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.
Formally, a "database" refers to a set of related data accessed through the use of a "database management system" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.
Because of the close relationship between them, the term "database" is often used casually to refer to both a database and the DBMS used to manipulate it.
Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.[1]
Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:
Both a database and its DBMS conform to the principles of a particular database model.[5] "Database system" refers collectively to the database model, database management system, and database.[6]
Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.[citation needed]
Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.
The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational,[8] SQL/relational, and post-relational.
The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018[update] they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS.[9] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.[citation needed]
Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch, which led to the coining of the term "post-relational" and also the development of hybrid object–relational databases.
The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases. A competing "next generation" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.
The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term "data-base" in a specific technical sense.[10]
As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market.
Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications.
IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014[update].[11]
Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that were primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a "search" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[12]
In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of "tables", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each "fact" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated.
The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit.
For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.
As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic.
Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a "language" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.
In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.
Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata).
IBM started working on a prototype system loosely based on Codd's concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large "chunk". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL[citation needed] – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2).
Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979.[18]
Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).
Another data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.[citation needed]
The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation."[19] dBASE was one of the top selling software titles in the 1980s and early 1990s.
The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields.[20] The term "object–relational impedance mismatch" described the inconvenience of translating between programmed objects and database tables. Object databases and object–relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object–relational mappings (ORMs) attempt to solve the same problem.
XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.
NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.
In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.
NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.
Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).
Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.
One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.
Connolly and Begg define database management system (DBMS) as a "software system that enables users to define, create, maintain and control access to the database."[24] Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access.
The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object–relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems.
It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities.[26] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.
Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.
Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.[28]
A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.
External interaction with the database will be via an application program that interfaces with the DBMS.[29] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information.
A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.
Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:
Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, "data about the data", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database.[33] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. "storage engine". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).
Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.
Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.
Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.
Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.
With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.[34]
Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).
Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.
This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called "subschemas". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.
Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause.[35]
Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).
The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.
A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs.
After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).
When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.
After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.
Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.
Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[36] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc.
Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as "DevOps for database".[37]
The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity–relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like "can a customer also be a supplier?", or "if a product is sold with two different forms of packaging, are those the same product or different products?", or "if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.
Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.
Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).
The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary "fact" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.
The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.
Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.
A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.
While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database.
The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model.[39] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.
The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.
Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more.
The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).
An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.
Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, peripherals, and other resources.
For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer – from cellular phones and video game consoles to web servers and supercomputers.
As of September 2024[update], Android is the most popular operating system with a 46% market share, followed by Microsoft Windows at 26%, iOS and iPadOS at 18%, macOS at 5%, and Linux at 1%. Android, iOS, and iPadOS are mobile operating systems, while Windows, macOS, and Linux are desktop operating systems.[3] Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems),[4][5] such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements.
Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. a LiveUSB from a USB stick).
An operating system is difficult to define,[6] but has been called "the layer of software that manages a computer's resources for its users and their applications".[7] Operating systems include the software that is always running, called a kernel—but can include other software as well.[6][8] The two other types of programs that can run on a computer are system programs—which are associated with the operating system, but may not be part of the kernel—and applications—all other software.[8]
With multiprocessors multiple CPUs share memory. A multicomputer or cluster computer has multiple CPUs, each of which has its own memory. Multicomputers were developed because large multiprocessors are difficult to engineer and prohibitively expensive;[17] they are universal in cloud computing because of the size of the machine needed.[18] The different CPUs often need to send and receive messages to each other;[19] to ensure good performance, the operating systems for these machines need to minimize this copying of packets.[20] Newer systems are often multiqueue—separating groups of users into separate queues—to reduce the need for packet copying and support more concurrent users.[21] Another technique is remote direct memory access, which enables each CPU to access memory belonging to other CPUs.[19] Multicomputer operating systems often support remote procedure calls where a CPU can call a procedure on another CPU,[22] or distributed shared memory, in which the operating system uses virtualization to generate shared memory that does not physically exist.[23]
A distributed system is a group of distinct, networked computers—each of which might have their own operating system and file system. Unlike multicomputers, they may be dispersed anywhere in the world.[24] Middleware, an additional software layer between the operating system and applications, is often used to improve consistency. Although it functions similarly to an operating system, it is not a true operating system.[25]
Embedded operating systems are designed to be used in embedded computer systems, whether they are internet of things objects or not connected to a network. Embedded systems include many household appliances. The distinguishing factor is that they do not load user-installed software. Consequently, they do not need protection between different applications, enabling simpler designs. Very small operating systems might run in less than 10 kilobytes,[26]  and the smallest are for smart cards.[27]  Examples include Embedded Linux, QNX, VxWorks, and the extra-small systems RIOT and TinyOS.[28]
A real-time operating system is an operating system that guarantees to process events or data by or at a specific moment in time. Hard real-time systems require exact timing and are common in manufacturing, avionics, military, and other similar uses.[28] With soft real-time systems, the occasional missed event is acceptable; this category often includes audio or multimedia systems, as well as smartphones.[28] In order for hard real-time systems be sufficiently exact in their timing, often they are just a library with no protection between applications, such as eCos.[28]
A hypervisor is an operating system that runs a virtual machine. The virtual machine is unaware that it is an application and operates as if it had its own hardware.[14][29] Virtual machines can be paused, saved, and resumed, making them useful for operating systems research, development,[30] and debugging.[31] They also enhance portability by enabling applications to be run on a computer even if they are not compatible with the base operating system.[14]
A library operating system (libOS) is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries and composed with a single application and configuration code to construct a unikernel:
[32] a specialized (only the absolute necessary pieces of code are extracted from libraries and bound together
[33]), single address space, machine image that can be deployed to cloud or embedded environments.
The operating system code and application code are not executed in separated protection domains (there is only a single application running, at least conceptually, so there is no need to prevent interference between applications) and OS services are accessed via simple library calls (potentially inlining them based on compiler thresholds), without the usual overhead of context switches,
[34] in a way similarly to embedded and real-time OSes. Note that this overhead is not negligible: to the direct cost of mode switching it's necessary to add the indirect pollution of important processor structures (like CPU caches, the instruction pipeline, and so on) which affects both user-mode and kernel-mode performance.
[35]
The first computers in the late 1940s and 1950s were directly programmed either with plugboards or with machine code inputted on media such as punch cards, without programming languages or operating systems.[36] After the introduction of the transistor in the mid-1950s, mainframes began to be built. These still needed professional operators[36] who manually do what a modern operating system would do, such as scheduling programs to run,[37] but mainframes still had rudimentary operating systems such as Fortran Monitor System (FMS) and IBSYS.[38] In the 1960s, IBM introduced the first series of intercompatible computers (System/360). All of them ran the same operating system—OS/360—which consisted of millions of lines of assembly language that had thousands of bugs. The OS/360 also was the first popular operating system to support multiprogramming, such that the CPU could be put to use on one job while another was waiting on input/output (I/O). Holding multiple jobs in memory necessitated memory partitioning and safeguards against one job accessing the memory allocated to a different one.[39]
Around the same time, teleprinters began to be used as terminals so multiple users could access the computer simultaneously. The operating system MULTICS was intended to allow hundreds of users to access a large computer. Despite its limited adoption, it can be considered the precursor to cloud computing. The UNIX operating system originated as a development of MULTICS for a single user.[40] Because UNIX's source code was available, it became the basis of other, incompatible operating systems, of which the most successful were AT&T's System V and the University of California's Berkeley Software Distribution (BSD).[41] To increase compatibility, the IEEE released the POSIX standard for operating system application programming interfaces (APIs), which is supported by most UNIX systems. MINIX was a stripped-down version of UNIX, developed in 1987 for educational uses, that inspired the commercially available, free software Linux. Since 2008, MINIX is used in controllers of most Intel microchips, while Linux is widespread in data centers and Android smartphones.[42]
The invention of large scale integration enabled the production of personal computers (initially called microcomputers) from around 1980.[43] For around five years, the CP/M (Control Program for Microcomputers) was the most popular operating system for microcomputers.[44] Later, IBM bought the DOS (Disk Operating System) from Microsoft. After modifications requested by IBM, the resulting system was called MS-DOS (MicroSoft Disk Operating System) and was widely used on IBM microcomputers. Later versions increased their sophistication, in part by borrowing features from UNIX.[44]
Apple's Macintosh was the first popular computer to use a graphical user interface (GUI). The GUI proved much more user friendly than the text-only command-line interface earlier operating systems had used. Following the success of Macintosh, MS-DOS was updated with a GUI overlay called Windows. Windows later was rewritten as a stand-alone operating system, borrowing so many features from another (VAX VMS) that a large legal settlement was paid.[45] In the twenty-first century, Windows continues to be popular on personal computers but has less market share of servers. UNIX operating systems, especially Linux, are the most popular on enterprise systems and servers but are also used on mobile devices and many other computer systems.[46]
On mobile devices, Symbian OS was dominant at first, being usurped by BlackBerry OS (introduced 2002) and iOS for iPhones (from 2007). Later on, the open-source Android operating system (introduced 2008), with a Linux kernel and a C library (Bionic) partially based on BSD code, became most popular.[47]
The components of an operating system are designed to ensure that various parts of a computer function cohesively. With the de facto obsoletion of DOS, all user software must interact with the operating system to access hardware.
The kernel is the part of the operating system that provides protection between different applications and users. This protection is key to improving reliability by keeping errors isolated to one program, as well as security by limiting the power of malicious software and protecting private data, and ensuring that one program cannot monopolize the computer's resources.[48] Most operating systems have two modes of operation:[49]  in user mode, the hardware checks that the software is only executing legal instructions, whereas the kernel has unrestricted powers and is not subject to these checks.[50] The kernel also manages memory for other processes and controls access to input/output devices.[51]
The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program typically involves the creation of a process by the operating system kernel, which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program, which then interacts with the user and with hardware devices. However, in some systems an application can request that the operating system execute another application within the same process, either as a subroutine or in a separate thread, e.g., the LINK and ATTACH facilities of OS/360 and successors.
An interrupt (also known as an abort, exception, fault, signal,[52] or trap)[53] provides an efficient way for most operating systems to react to the environment. Interrupts cause the central processing unit (CPU) to have a control flow change away from the currently running program to an interrupt handler, also known as an interrupt service routine (ISR).[54][55] An interrupt service routine may cause the central processing unit (CPU) to have a context switch.[56][a] The details of how a computer processes an interrupt vary from architecture to architecture, and the details of how interrupt service routines behave vary from operating system to operating system.[57] However, several interrupt functions are common.[57] The architecture and operating system must:[57]
A software interrupt is a message to a process that an event has occurred.[52] This contrasts with a hardware interrupt — which is a message to the central processing unit (CPU) that an event has occurred.[58] Software interrupts are similar to hardware interrupts — there is a change away from the currently running process.[59] Similarly, both hardware and software interrupts execute an interrupt service routine.
Software interrupts may be normally occurring events. It is expected that a time slice will occur, so the kernel will have to perform a context switch.[60] A computer program may set a timer to go off after a few seconds in case too much data causes an algorithm to take too long.[61]
Software interrupts may be error conditions, such as a malformed machine instruction.[61] However, the most common error conditions are division by zero and accessing an invalid memory address.[61]
To generate software interrupts for x86 CPUs, the INT assembly language instruction is available.[62] The syntax is INT X, where X is the offset number (in hexadecimal format) to the interrupt vector table.
To generate software interrupts in Unix-like operating systems, the kill(pid,signum) system call will send a signal to another process.[63] pid is the process identifier of the receiving process. signum is the signal number (in mnemonic format)[b] to be sent. (The abrasive name of kill was chosen because early implementations only terminated the process.)[64]
In Unix-like operating systems, signals inform processes of the occurrence of asynchronous events.[63] To communicate asynchronously, interrupts are required.[65] One reason a process needs to asynchronously communicate to another process solves a variation of the classic reader/writer problem.[66] The writer receives a pipe from the shell for its output to be sent to the reader's input stream.[67] The command-line syntax is alpha | bravo. alpha will write to the pipe when its computation is ready and then sleep in the wait queue.[68] bravo will then be moved to the ready queue and soon will read from its input stream.[69] The kernel will generate software interrupts to coordinate the piping.[69]
While the writing takes place, the operating system will context switch to other processes as normal. When the device finishes writing, the device will interrupt the currently running process by asserting an interrupt request. The device will also place an integer onto the data bus.[78] Upon accepting the interrupt request, the operating system will:
Among other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by the programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.
Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen anymore, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.
The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.
In modern operating systems, memory which is accessed less frequently can be temporarily stored on a disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand.
Concurrency refers to the operating system's ability to carry out multiple tasks simultaneously.[81] Virtually all modern operating systems support concurrency.[82]
Threads enable splitting a process' work into multiple parts that can run simultaneously.[83] The number of threads is not limited by the number of processors available. If there are more threads than processors, the operating system kernel schedules, suspends, and resumes threads, controlling when each thread runs and how much CPU time it receives.[84]  During a context switch a running thread is suspended, its state is saved into the thread control block and stack, and the state of the new thread is loaded in.[85] Historically, on many systems a thread could run until it relinquished control (cooperative multitasking). Because this model can allow a single thread to monopolize the processor, most operating systems now can interrupt a thread (preemptive multitasking).[86]
File systems are an abstraction used by the operating system to simplify access to permanent storage. They provide human-readable filenames and other metadata, increase performance via amortization of accesses, prevent multiple threads from accessing the same section of memory, and include checksums to identify corruption.[96] File systems are composed of files (named collections of data, of an arbitrary size) and directories (also called folders) that list human-readable filenames and other directories.[97] An absolute file path begins at the root directory and lists subdirectories divided by punctuation, while a relative path defines the location of a file from a directory.[98][99]
System calls (which are sometimes wrapped by libraries) enable applications to create, delete, open, and close files, as well as link, read, and write to them. All these operations are carried out by the operating system on behalf of the application.[100] The operating system's efforts to reduce latency include storing recently requested blocks of memory in a cache and prefetching data that the application has not asked for, but might need next.[101] Device drivers are software specific to each input/output (I/O) device that enables the operating system to work without modification over different hardware.[102][103]
Another component of file systems is a dictionary that maps a file's name and metadata to the data block where its contents are stored.[104] Most file systems use directories to convert file names to file numbers. To find the block number, the operating system uses an index (often implemented as a tree).[105] Separately, there is a free space map to track free blocks, commonly implemented as a bitmap.[105] Although any free block can be used to store a new file, many operating systems try to group together files in the same directory to maximize performance, or periodically reorganize files to reduce fragmentation.[106]
Security means protecting users from other users of the same computer, as well as from those who seeking remote access to it over a network.[111]  Operating systems security rests on achieving the CIA triad: confidentiality (unauthorized users cannot access data), integrity (unauthorized users cannot modify data), and availability (ensuring that the system remains available to authorized users, even in the event of a denial of service attack).[112] As with other computer systems, isolating security domains—in the case of operating systems, the kernel, processes, and virtual machines—is key to achieving security.[113] Other ways to increase security include simplicity to minimize the attack surface, locking access to resources by default, checking all requests for authorization, principle of least authority (granting the minimum privilege essential for performing a task), privilege separation, and reducing shared data.[114]
Some operating system designs are more secure than others. Those with no isolation between the kernel and applications are least secure, while those with a monolithic kernel like most general-purpose operating systems are still vulnerable if any part of the kernel is compromised. A more secure design features microkernels that separate the kernel's privileges into many separate security domains and reduce the consequences of a single kernel breach.[115] Unikernels are another approach that improves security by minimizing the kernel and separating out other operating systems functionality by application.[115]
Most operating systems are written in C or C++, which create potential vulnerabilities for exploitation. Despite attempts to protect against them, vulnerabilities are caused by buffer overflow attacks, which are enabled by the lack of bounds checking.[116]  Hardware vulnerabilities, some of them caused by CPU optimizations, can also be used to compromise the operating system.[117] There are known instances of operating system programmers deliberately implanting vulnerabilities, such as back doors.[118]
Operating systems security is hampered by their increasing complexity and the resulting inevitability of bugs.[119] Because formal verification of operating systems may not be feasible, developers use operating system hardening to reduce vulnerabilities,[120] e.g. address space layout randomization, control-flow integrity,[121] access restrictions,[122] and other techniques.[123] There are no restrictions on who can contribute code to open source operating systems; such operating systems have transparent change histories and distributed governance structures.[124] Open source developers strive to work collaboratively to find and eliminate security vulnerabilities, using code review and type checking to expunge malicious code.[125][126] Andrew S. Tanenbaum advises releasing the source code of all operating systems, arguing that it prevents developers from placing trust in secrecy and thus relying on the unreliable practice of security by obscurity.[127]
For personal computers, including smartphones and tablet computers, and for workstations, user input is typically from a combination of keyboard, mouse, and trackpad or touchscreen, all of which are connected to the operating system with specialized software.[128] Personal computer users who are not software developers or coders often prefer GUIs for both input and output; GUIs are supported by most personal computers.[129] The software to support GUIs is more complex than a command line for input and plain text output. Plain text output is often preferred by programmers, and is easy to support.[130]
A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.[131]
In some cases, hobby development is in support of a "homebrew" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is her/his own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.
If an application is written for use on a specific operating system, and is ported to another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.
This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.
Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.
As of September 2024[update], Android is the most popular operating system with a 46% market share, followed by Microsoft Windows at 26%, iOS and iPadOS at 18%, macOS at 5%, and Linux at 1%. Android, iOS, and iPadOS are mobile operating systems, while Windows, macOS, and Linux are desktop operating systems.[3]
Linux is a free software distributed under the GNU General Public License (GPL), which means that all of its derivatives are legally required to release their source code.[132] Linux was designed by programmers for their own use, thus emphasizing simplicity and consistency, with a small number of basic elements that can be combined in nearly unlimited ways, and avoiding redundancy.[133]
Its design is similar to other UNIX systems not using a microkernel.[134] It is written in C[135] and uses UNIX System V syntax, but also supports BSD syntax. Linux supports standard UNIX networking features, as well as the full suite of UNIX tools, while supporting multiple users and employing preemptive multitasking. Initially of a minimalist design, Linux is a flexible system that can work in under 16 MB of RAM, but still is used on large multiprocessor systems.[134] Similar to other UNIX systems, Linux distributions are composed of a kernel, system libraries, and system utilities.[136] Linux has a graphical user interface (GUI) with a desktop, folder and file icons, as well as the option to access the operating system via a command line.[137]
Android is a partially open-source operating system closely based on Linux and has become the most widely used operating system by users, due to its popularity on smartphones and, to a lesser extent, embedded systems needing a GUI, such as "smart watches, automotive dashboards, airplane seatbacks, medical devices, and home appliances".[138] Unlike Linux, much of Android is written in Java and uses object-oriented design.[139]
Windows is a proprietary operating system that is widely used on desktop computers, laptops, tablets, phones, workstations, enterprise servers, and Xbox consoles.[141] The operating system was designed for "security, reliability, compatibility, high performance, extensibility, portability, and international support"—later on, energy efficiency and support for dynamic devices also became priorities.[142]
Windows Executive works via kernel-mode objects for important data structures like processes, threads, and sections (memory objects, for example files).[143] The operating system supports demand paging of virtual memory, which speeds up I/O for many applications. I/O device drivers use the Windows Driver Model.[143] The NTFS file system has a master table and each file is represented as a record with metadata.[144] The scheduling includes preemptive multitasking.[145] Windows has many security features;[146] especially important are the use of access-control lists and integrity levels. Every process has an authentication token and each object is given a security descriptor. Later releases have added even more security features.[144]
In computer programming, a variable is an abstract storage location paired with an associated symbolic name, which contains some known or unknown quantity of data or object referred to as a value; or in simpler terms, a variable is a named container for a particular set of bits or type of data (like integer, float, string, etc...).[1] A variable can eventually be associated with or identified by a memory address. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may thus change during the course of program execution.[2][3][4][5]
Variables in programming may not directly correspond to the concept of variables in mathematics. The latter is abstract, having no reference to a physical object such as storage location. The value of a computing variable is not necessarily part of an equation or formula as in mathematics. Variables in computer programming are frequently given long names to make them relatively descriptive of their use, whereas variables in mathematics often have terse, one- or two-character names for brevity in transcription and manipulation.
In imperative programming languages, values can generally be accessed or changed at any time. In pure functional and logic languages, variables are bound to expressions and keep a single value during their entire lifetime due to the requirements of referential transparency. In imperative languages, the same behavior is exhibited by (named) constants (symbolic constants), which are typically contrasted with (normal) variables.
Depending on the type system of a programming language, variables may only be able to store a specified data type (e.g. integer or string). Alternatively, a datatype may be associated only with the current value, allowing a single variable to store anything supported by the programming language. Variables are the containers for storing the values.
Unless the programming language features garbage collection, a variable whose extent permanently outlasts its scope can result in a memory leak, whereby the memory allocated for the variable can never be freed since the variable which would be used to reference it for deallocation purposes is no longer accessible. However, it can be permissible for a variable binding to extend beyond its scope, as occurs in Lisp closures and C static local variables; when execution passes back into the variable's scope, the variable may once again be used. A variable whose scope begins before its extent does is said to be uninitialized and often has an undefined, arbitrary value if accessed (see wild pointer), since it has yet to be explicitly given a particular value. A variable whose extent ends before its scope may become a dangling pointer and deemed uninitialized once more since its value has been destroyed. Variables described by the previous two cases may be said to be out of extent or unbound. In many languages, it is an error to try to use the value of a variable when it is out of extent. In other languages, doing so may yield unpredictable results. Such a variable may, however, be assigned a new value, which gives it a new extent.
It is considered good programming practice to make the scope of variables as narrow as feasible so that different parts of a program do not accidentally interact with each other by modifying each other's variables. Doing so also prevents action at a distance. Common techniques for doing so are to have different sections of a program use different name spaces, or to make individual variables "private" through either dynamic variable scoping or lexical variable scoping.
Many programming languages employ a reserved value (often named null or nil) to indicate an invalid or uninitialized variable.
Variables often store simple data, like integers and literal strings, but some programming languages allow a variable to store values of other datatypes as well. Such languages may also enable functions to be parametric polymorphic. These functions operate like variables to represent data of multiple types. For example, a function named length may determine the length of a list. Such a length function may be parametric polymorphic by including a type variable in its type signature, since the number of elements in the list is independent of the elements' types.
The specifics of variable allocation and the representation of their values vary widely, both among programming languages and among implementations of a given language. Many language implementations allocate space for local variables, whose extent lasts for a single function call on the call stack, and whose memory is automatically reclaimed when the function returns. More generally, in name binding, the name of a variable is bound to the address of some particular block (contiguous sequence) of bytes in memory, and operations on the variable manipulate that block. Referencing is more common for variables whose values have large or unknown sizes when the code is compiled. Such variables reference the location of the value instead of storing the value itself, which is allocated from a pool of memory called the heap.
When a variable refers to a data structure created dynamically, some of its components may be only indirectly accessed through the variable. In such circumstances, garbage collectors (or analogous program features in languages that lack garbage collectors) must deal with a case where only a portion of the memory reachable from the variable needs to be reclaimed.
Unlike their mathematical counterparts, programming variables and constants commonly take multiple-character names, e.g. COST or total. Single-character names are most commonly used only for auxiliary variables; for instance, i, j, k for array index variables.
Some naming conventions are enforced at the language level as part of the language syntax which involves the format of valid identifiers. In almost all languages, variable names cannot start with a digit (0–9) and cannot contain whitespace characters. Whether or not punctuation marks are permitted in variable names varies from language to language; many languages only permit the underscore ("_") in variable names and forbid all other punctuation. In some programming languages, sigils (symbols or punctuation) are affixed to variable identifiers to indicate the variable's datatype or scope.
In computer science, conditionals (that is, conditional statements, conditional expressions and conditional constructs) are programming language constructs that perform different computations or actions or return different values depending on the value of a Boolean expression, called a condition.
Conditional statements are imperative constructs executed for side-effect, while conditional expressions return values. Many programming languages (such as C) have distinct conditional statements and conditional expressions. Although in pure functional programming, conditional expressions do not have side-effects, many languages with conditional expressions (such as Lisp) support conditional side-effects.
The if–then or if–then–else construction is used in many programming languages. Although the syntax varies from language to language, the basic structure (in pseudocode form) looks like this:
In early programming languages, especially some dialects of BASIC in the 1980s home computers, an if–then statement could only contain GOTO statements (equivalent to a branch instruction). This led to a hard-to-read style of programming known as spaghetti programming, with programs in this style called spaghetti code.  As a result, structured programming, which allows (virtually) arbitrary statements to be put in statement blocks inside an if statement, gained in popularity, until it became the norm even in most BASIC programming circles. Such mechanisms and principles were based on the older but more advanced ALGOL family of languages, and ALGOL-like languages such as Pascal and Modula-2 influenced modern BASIC variants for many years. While it is possible while using only GOTO statements in if–then statements to write programs that are not spaghetti code and are just as well structured and readable as programs written in a structured programming language, structured programming makes this easier and enforces it.  Structured if–then–else statements like the example above are one of the key elements of structured programming, and they are present in most popular high-level programming languages such as C, Java, JavaScript and Visual Basic .
The else keyword is made to target a specific if–then statement preceding it, but for nested if–then statements, classic programming languages such as ALGOL 60 struggled to define which specific statement to target. Without clear boundaries for which statement is which, an else keyword could target any preceding if–then statement in the nest, as parsed.
Pattern matching may be seen as an alternative to both if–then–else, and case statements. It is available in many programming languages with functional programming features, such as Wolfram Language, ML and many others. Here is a simple example written in the OCaml language:
In programming languages that have associative arrays or comparable data structures, such as Python, Perl, PHP or Objective-C, it is idiomatic to use them to implement conditional assignment.[13]
Free software, libre software, libreware[1][2] sometimes known as freedom-respecting software is computer software distributed under terms that allow users to run the software for any purpose as well as to study, change, and distribute it and any adapted versions.[3][4][5][6] Free software is a matter of liberty, not price; all users are legally free to do what they want with their copies of a free software (including profiting from them) regardless of how much is paid to obtain the program.[7][2] Computer programs are deemed "free" if they give end-users (not just the developer) ultimate control over the software and, subsequently, over their devices.[5][8]
The right to study and modify a computer program entails that the source code—the preferred format for making changes—be made available to users of that program. While this is often called "access to source code" or "public availability", the Free Software Foundation (FSF) recommends against thinking in those terms,[9] because it might give the impression that users have an obligation (as opposed to a right) to give non-users a copy of the program.
Although the term "free software" had already been used loosely in the past and other permissive software like the Berkeley Software Distribution released in 1978 existed,[10] Richard Stallman is credited with tying it to the sense under discussion and starting the free software movement in 1983, when he launched the GNU Project: a collaborative effort to create a freedom-respecting operating system, and to revive the spirit of cooperation once prevalent among hackers during the early days of computing.[11][12]
For software under the purview of copyright to be free, it must carry a software license whereby the author grants users the aforementioned rights. Software that is not covered by copyright law, such as software in the public domain, is free as long as the source code is also in the public domain, or otherwise available without restrictions.
Proprietary software uses restrictive software licences or EULAs and usually does not provide users with the source code. Users are thus legally or technically prevented from changing the software, and this results in reliance on the publisher to provide updates, help, and support. (See also vendor lock-in and abandonware). Users often may not reverse engineer, modify, or redistribute proprietary software.[15][16] Beyond copyright law, contracts and a lack of source code, there can exist additional obstacles keeping users from exercising freedom over a piece of software, such as software patents and digital rights management (more specifically, tivoization).[17]
Free software can be a for-profit, commercial activity or not. Some free software is developed by volunteer computer programmers while other is developed by corporations; or even by both.[18][7]
Although both definitions refer to almost equivalent corpora of programs, the Free Software Foundation recommends using the term "free software" rather than "open-source software" (an alternative, yet similar, concept coined in 1998), because the goals and messaging are quite dissimilar. According to the Free Software Foundation, "Open source" and its associated campaign mostly focus on the technicalities of the public development model and marketing free software to businesses, while taking the ethical issue of user rights very lightly or even antagonistically.[19] Stallman has also stated that considering the practical advantages of free software is like considering the practical advantages of not being handcuffed, in that it is not necessary for an individual to consider practical reasons in order to realize that being handcuffed is undesirable in itself.[20]
The FSF also notes that "Open Source" has exactly one specific meaning in common English, namely that "you can look at the source code." It states that while the term "Free Software" can lead to two different interpretations, at least one of them is consistent with the intended meaning unlike the term "Open Source".[a] The loan adjective "libre" is often used to avoid the ambiguity of the word "free" in the English language, and the ambiguity with the older usage of "free software" as public-domain software.[10] (See Gratis versus libre.)
The first formal definition of free software was published by FSF in February 1986.[21] That definition, written by Richard Stallman, is still maintained today and states that software is free software if people who receive a copy of the software have the following four freedoms.[22][23] The numbering begins with zero, not only as a spoof on the common usage of zero-based numbering in programming languages, but also because "Freedom 0" was not initially included in the list, but later added first in the list as it was considered very important.
Freedoms 1 and 3 require source code to be available because studying and modifying software without its source code can range from highly impractical to nearly impossible.
Thus, free software means that computer users have the freedom to cooperate with whom they choose, and to control the software they use. To summarize this into a remark distinguishing libre (freedom) software from gratis (zero price) software, the Free Software Foundation says: "Free software is a matter of liberty, not price. To understand the concept, you should think of 'free' as in 'free speech', not as in 'free beer'".[22] (See Gratis versus libre.)
In the late 1990s, other groups published their own definitions that describe an almost identical set of software. The most notable are Debian Free Software Guidelines published in 1997,[24] and The Open Source Definition, published in 1998.
The BSD-based operating systems, such as FreeBSD, OpenBSD, and NetBSD, do not have their own formal definitions of free software. Users of these systems generally find the same set of software to be acceptable, but sometimes see copyleft as restrictive. They generally advocate permissive free software licenses, which allow others to use the software as they wish, without being legally forced to provide the source code. Their view is that this permissive approach is more free. The Kerberos, X11, and Apache software licenses are substantially similar in intent and implementation.
There are thousands of free applications and many operating systems available on the Internet. Users can easily download and install those applications via a package manager that comes included with most Linux distributions.
The Free Software Directory maintains a large database of free-software packages. Some of the best-known examples include Linux-libre, Linux-based operating systems, the GNU Compiler Collection and C library; the MySQL relational database; the Apache web server; and the Sendmail mail transport agent. Other influential examples include the Emacs text editor; the GIMP raster drawing and image editor; the X Window System graphical-display system; the LibreOffice office suite; and the TeX and LaTeX typesetting systems.
From the 1950s up until the early 1970s, it was normal for computer users to have the software freedoms associated with free software, which was typically public-domain software.[10] Software was commonly shared by individuals who used computers and by hardware manufacturers who welcomed the fact that people were making software that made their hardware useful. Organizations of users and suppliers, for example, SHARE, were formed to facilitate exchange of software. As software was often written in an interpreted language such as BASIC, the source code was distributed to use these programs. Software was also shared and distributed as printed source code (Type-in program) in computer magazines (like Creative Computing, SoftSide, Compute!, Byte, etc.) and books, like the bestseller BASIC Computer Games.[25] By the early 1970s, the picture changed: software costs were dramatically increasing, a growing software industry was competing with the hardware manufacturer's bundled software products (free in that the cost was included in the hardware cost), leased machines required software support while providing no revenue for software, and some customers able to better meet their own needs did not want the costs of "free" software bundled with hardware product costs. In United States vs. IBM, filed January 17, 1969, the government charged that bundled software was anti-competitive.[26] While some software might always be free, there would henceforth be a growing amount of software produced primarily for sale. In the 1970s and early 1980s, the software industry began using technical measures (such as only distributing binary copies of computer programs) to prevent computer users from being able to study or adapt the software applications as they saw fit. In 1980, copyright law was extended to computer programs.
In 1983, Richard Stallman, one of the original authors of the popular Emacs program and a longtime member of the hacker community at the MIT Artificial Intelligence Laboratory, announced the GNU Project, the purpose of which was to produce a completely non-proprietary Unix-compatible operating system, saying that he had become frustrated with the shift in climate surrounding the computer world and its users. In his initial declaration of the project and its purpose, he specifically cited as a motivation his opposition to being asked to agree to non-disclosure agreements and restrictive licenses which prohibited the free sharing of potentially profitable in-development software, a prohibition directly contrary to the traditional hacker ethic. Software development for the GNU operating system began in January 1984, and the Free Software Foundation (FSF) was founded in October 1985. He developed a free software definition and the concept of "copyleft", designed to ensure software freedom for all.
Some non-software industries are beginning to use techniques similar to those used in free software development for their research and development process; scientists, for example, are looking towards more open development processes, and hardware such as microchips are beginning to be developed with specifications released under copyleft licenses (see the OpenCores project, for instance). Creative Commons and the free-culture movement have also been largely influenced by the free software movement.
In 1983, Richard Stallman, longtime member of the hacker community at the MIT Artificial Intelligence Laboratory, announced the GNU Project, saying that he had become frustrated with the effects of the change in culture of the computer industry and its users.[27] Software development for the GNU operating system began in January 1984, and the Free Software Foundation (FSF) was founded in October 1985. An article outlining the project and its goals was published in March 1985 titled the GNU Manifesto. The manifesto included significant explanation of the GNU philosophy, Free Software Definition and "copyleft" ideas.
The Linux kernel, started by Linus Torvalds, was released as freely modifiable source code in 1991. The first licence was a proprietary software licence. However, with version 0.12 in February 1992, he relicensed the project under the GNU General Public License.[28] Much like Unix, Torvalds' kernel attracted the attention of volunteer programmers.
FreeBSD and NetBSD (both derived from 386BSD) were released as free software when the USL v. BSDi lawsuit was settled out of court in 1993. OpenBSD forked from NetBSD in 1995. Also in 1995, The Apache HTTP Server, commonly referred to as Apache, was released under the Apache License 1.0.
All free-software licenses must grant users all the freedoms discussed above. However, unless the applications' licenses are compatible, combining programs by mixing source code or directly linking binaries is problematic, because of license technicalities. Programs indirectly connected together may avoid this problem.
The majority of free software falls under a small set of licenses. The most popular of these licenses are:[30][31]
The Free Software Foundation and the Open Source Initiative both publish lists of licenses that they find to comply with their own definitions of free software and open-source software respectively:
The FSF list is not prescriptive: free-software licenses can exist that the FSF has not heard about, or considered important enough to write about. So it is possible for a license to be free and not in the FSF list. The OSI list only lists licenses that have been submitted, considered and approved. All open-source licenses must meet the Open Source Definition in order to be officially recognized as open source software. Free software, on the other hand, is a more informal classification that does not rely on official recognition. Nevertheless, software licensed under licenses that do not meet the Free Software Definition cannot rightly be considered free software.
Apart from these two organizations, the Debian project is seen by some to provide useful advice on whether particular licenses comply with their Debian Free Software Guidelines. Debian does not publish a list of approved licenses, so its judgments have to be tracked by checking what software they have allowed into their software archives. That is summarized at the Debian web site.[32]
Proponents of permissive and copyleft licenses disagree on whether software freedom should be viewed as a negative or positive liberty. Due to their restrictions on distribution, not everyone considers copyleft licenses to be free.[34] Conversely, a permissive license may provide an incentive to create non-free software by reducing the cost of developing restricted software. Since this is incompatible with the spirit of software freedom, many people consider permissive licenses to be less free than copyleft licenses.[35]
There is debate over the security of free software in comparison to proprietary software, with a major issue being security through obscurity. A popular quantitative test in computer security is to use relative counting of known unpatched security flaws. Generally, users of this method advise avoiding products that lack fixes for known security flaws, at least until a fix is available.
Free software advocates strongly believe that this methodology is biased by counting more vulnerabilities for the free software systems, since their source code is accessible and their community is more forthcoming about what problems exist as a part of full disclosure,[39][40] and proprietary software systems can have undisclosed societal drawbacks, such as disenfranchising less fortunate would-be users of free programs. As users can analyse and trace the source code, many more people with no commercial constraints can inspect the code and find bugs and loopholes than a corporation would find practicable. According to Richard Stallman, user access to the source code makes deploying free software with undesirable hidden spyware functionality far more difficult than for proprietary software.[41]
In 2006, OpenBSD started the first campaign against the use of binary blobs in kernels. Blobs are usually freely distributable device drivers for hardware from vendors that do not reveal driver source code to users or developers. This restricts the users' freedom effectively to modify the software and distribute modified versions. Also, since the blobs are undocumented and may have bugs, they pose a security risk to any operating system whose kernel includes them. The proclaimed aim of the campaign against blobs is to collect hardware documentation that allows developers to write free software drivers for that hardware, ultimately enabling all free operating systems to become or remain blob-free.
The issue of binary blobs in the Linux kernel and other device drivers motivated some developers in Ireland to launch gNewSense, a Linux-based distribution with all the binary blobs removed. The project received support from the Free Software Foundation and stimulated the creation, headed by the Free Software Foundation Latin America, of the Linux-libre kernel.[46] As of October 2012[update], Trisquel is the most popular FSF endorsed Linux distribution ranked by Distrowatch (over 12 months).[47] While Debian is not endorsed by the FSF and does not use Linux-libre, it is also a popular distribution available without kernel blobs by default since 2011.[46]
Selling software under any free-software licence is permissible, as is commercial use. This is true for licenses with or without copyleft.[18][49][50]
Since free software may be freely redistributed, it is generally available at little or no fee. Free software business models are usually based on adding value such as customization, accompanying hardware, support, training, integration, or certification.[18] Exceptions exist however, where the user is charged to obtain a copy of the free application itself.[51]
Fees are usually charged for distribution on compact discs and bootable USB drives, or for services of installing or maintaining the operation of free software. Development of large, commercially used free software is often funded by a combination of user donations, crowdfunding, corporate contributions, and tax money. The SELinux project at the United States National Security Agency is an example of a federally funded free-software project.
Proprietary software, on the other hand, tends to use a different business model, where a customer of the proprietary application pays a fee for a license to legally access and use it. This license may grant the customer the ability to configure some or no parts of the software themselves. Often some level of support is included in the purchase of proprietary software, but additional support services (especially for enterprise applications) are usually available for an additional fee. Some proprietary software vendors will also customize software for a fee.[52]
The Free Software Foundation encourages selling free software. As the Foundation has written, "distributing free software is an opportunity to raise funds for development. Don't waste it!".[7] For example, the FSF's own recommended license (the GNU GPL) states that "[you] may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee."[53]
Microsoft CEO Steve Ballmer stated in 2001 that "open source is not available to commercial companies. The way the license is written, if you use any open-source software, you have to make the rest of your software open source."[54] This misunderstanding is based on a requirement of copyleft licenses (like the GPL) that if one distributes modified versions of software, they must release the source and use the same license. This requirement does not extend to other software from the same developer.[55] The claim of incompatibility between commercial companies and free software is also a misunderstanding. There are several large companies, e.g. Red Hat and IBM (IBM acquired RedHat in 2019),[56] which do substantial commercial business in the development of free software.[citation needed]
Free software played a significant part in the development of the Internet, the World Wide Web and the infrastructure of dot-com companies.[57][58] Free software allows users to cooperate in enhancing and refining the programs they use; free software is a pure public good rather than a private good. Companies that contribute to free software increase commercial innovation.[59]
"We migrated key functions from Windows to Linux because we needed an operating system that was stable and reliable – one that would give us in-house control. So if we needed to patch, adjust, or adapt, we could."
The economic viability of free software has been recognized by large corporations such as IBM, Red Hat, and Sun Microsystems.[62][63][64][65][66] Many companies whose core business is not in the IT sector choose free software for their Internet information and sales sites, due to the lower initial capital investment and ability to freely customize the application packages. Most companies in the software business include free software in their commercial products if the licenses allow that.[18]
Free software is generally available at no cost and can result in permanently lower TCO (total cost of ownership) compared to proprietary software.[67] With free software, businesses can fit software to their specific needs by changing the software themselves or by hiring programmers to modify it for them. Free software often has no warranty, and more importantly, generally does not assign legal liability to anyone. However, warranties are permitted between any two parties upon the condition of the software and its usage. Such an agreement is made separately from the free software license.
A report by Standish Group estimates that adoption of free software has caused a drop in revenue to the proprietary software industry by about $60 billion per year.[68] Eric S. Raymond argued that the term free software is too ambiguous and intimidating for the business community. Raymond promoted the term open-source software as a friendlier alternative for the business and corporate world.[69]
Scala (/ˈskɑːlɑː/ SKAH-lah)[7][8] is a strong statically typed high-level general-purpose programming language that supports both object-oriented programming and functional programming. Designed to be concise,[9] many of Scala's design decisions are intended to address criticisms of Java.[6]
Unlike Java, Scala has many features of functional programming languages (like Scheme, Standard ML, and Haskell), including currying, immutability, lazy evaluation, and pattern matching. It also has an advanced type system supporting algebraic data types, covariance and contravariance, higher-order types (but not higher-rank types), anonymous types, operator overloading, optional parameters, named parameters, raw strings, and an experimental exception-only version of algebraic effects that can be seen as a more powerful version of Java's checked exceptions.[12]
The design of Scala started in 2001 at the École Polytechnique Fédérale de Lausanne (EPFL) (in Lausanne, Switzerland) by Martin Odersky. It followed on from work on Funnel, a programming language combining ideas from functional programming and Petri nets.[14] Odersky formerly worked on Generic Java, and javac, Sun's Java compiler.[14]
The reference Scala software distribution, including compiler and libraries, is released under the Apache license.[23]
While supporting all of the object-oriented features available in Java (and in fact, augmenting them in various ways), Scala also provides a large number of capabilities that are normally found only in functional programming languages. Together, these features allow Scala programs to be written in an almost completely functional style and also allow functional and object-oriented styles to be mixed.
Due to type inference, the type of variables, function return values, and many other expressions can typically be omitted, as the compiler can deduce it. Examples are val x = "foo" (for an immutable constant or immutable object) or var x = 1.5 (for a variable whose value can later be changed). Type inference in Scala is essentially local, in contrast to the more global Hindley-Milner algorithm used in Haskell, ML and other more purely functional languages. This is done to facilitate object-oriented programming. The result is that certain types still need to be declared (most notably, function parameters, and the return types of recursive functions), e.g.
Immutable classes are encouraged by convention however, and the Scala standard library provides a rich set of immutable collection classes.
Scala provides mutable and immutable variants of most collection classes, and the immutable version is always used unless the mutable version is explicitly imported.[34]
The immutable variants are persistent data structures that always return an updated copy of an old object instead of updating the old object destructively in place.
An example of this is immutable linked lists where prepending an element to a list is done by returning a new list node consisting of the element and a reference to the list tail.
Appending an element to a list can only be done by prepending all elements in the old list to a new list with only the new element.
In the same way, inserting an element in the middle of a list will copy the first half of the list, but keep a reference to the second half of the list. This is called structural sharing.
This allows for very easy concurrency — no locks are needed as no shared objects are ever modified.[35]
Functional programming languages commonly provide tail call optimization to allow for extensive use of recursion without stack overflow problems. Limitations in Java bytecode complicate tail call optimization on the JVM. In general, a function that calls itself with a tail call can be optimized, but mutually recursive functions cannot. Trampolines have been suggested as a workaround.[36] Trampoline support has been provided by the Scala library with the object scala.util.control.TailCalls since Scala 2.8.0 (released 14 July 2010). A function may optionally be annotated with @tailrec, in which case it will not compile unless it is tail recursive.[37]
Scala has built-in support for pattern matching, which can be thought of as a more sophisticated, extensible version of a switch statement, where arbitrary data types can be matched (rather than just simple types like integers, Booleans and strings), including arbitrary nesting. A special type of class known as a case class is provided, which includes automatic support for pattern matching and can be used to model the algebraic data types used in many functional programming languages. (From the perspective of Scala, a case class is simply a normal class for which the compiler automatically adds certain behaviors that could also be provided manually, e.g., definitions of methods providing for deep comparisons and hashing, and destructuring a case class on its constructor parameters during pattern matching.)
The idea here is that we partition a list into the elements less than a pivot and the elements not less, recursively sort each part, and paste the results together with the pivot in between. This uses the same divide-and-conquer strategy of mergesort and other fast sorting algorithms.
Scala's standard library includes support for futures and promises, in addition to the standard Java concurrency APIs. Originally, it also included support for the actor model, which is now available as a separate source-available platform Akka[43] licensed by Lightbend Inc. Akka actors may be distributed or combined with software transactional memory (transactors). Alternative communicating sequential processes (CSP) implementations for channel-based message passing are Communicating Scala Objects,[44] or simply via JCSP.
Scala also comes with built-in support for data-parallel programming in the form of Parallel Collections[46] integrated into its Standard Library since version 2.9.0.
Besides futures and promises, actor support, and data parallelism, Scala also supports asynchronous programming with software transactional memory, and event streams.[48]
There are several ways to test code in Scala. ScalaTest supports multiple testing styles and can integrate with Java-based testing frameworks.[49] ScalaCheck is a library similar to Haskell's QuickCheck.[50] specs2 is a library for writing executable software specifications.[51] ScalaMock provides support for testing high-order and curried functions.[52] JUnit and TestNG are popular testing frameworks written in Java.
Scala is often compared with Groovy and Clojure, two other programming languages also using the JVM. Substantial differences between these languages exist in the type system, in the extent to which each language supports object-oriented and functional programming, and in the similarity of their syntax to that of Java.
Scala is statically typed, while both Groovy and Clojure are dynamically typed. This makes the type system more complex and difficult to understand but allows almost all[38] type errors to be caught at compile-time and can result in significantly faster execution. By contrast, dynamic typing requires more testing to ensure program correctness, and thus is generally slower, to allow greater programming flexibility and simplicity. Regarding speed differences, current versions of Groovy and Clojure allow optional type annotations to help programs avoid the overhead of dynamic typing in cases where types are practically static. This overhead is further reduced when using recent versions of the JVM, which has been enhanced with an invoke dynamic instruction for methods that are defined with dynamically typed arguments. These advances reduce the speed gap between static and dynamic typing, although a statically typed language, like Scala, is still the preferred choice when execution efficiency is very important.
Regarding programming paradigms, Scala inherits the object-oriented model of Java and extends it in various ways. Groovy, while also strongly object-oriented, is more focused in reducing verbosity. In Clojure, object-oriented programming is deemphasised with functional programming being the main strength of the language. Scala also has many functional programming facilities, including features found in advanced functional languages like Haskell, and tries to be agnostic between the two paradigms, letting the developer choose between the two paradigms or, more frequently, some combination thereof.
In the 2018 edition of the State of Java survey,[142] which collected data from 5160 developers on various Java-related topics, Scala places third in terms of use of alternative languages on the JVM. Relative to the prior year's edition of the survey, Scala's use among alternative JVM languages fell from 28.4% to 21.5%, overtaken by Kotlin, which rose from 11.4% in 2017 to 28.8% in 2018.
The Popularity of Programming Language Index,[143] which tracks searches for language tutorials, ranked Scala 15th in April 2018 with a small downward trend, and 17th in Jan 2021. This makes Scala the 3rd most popular JVM-based language after Java and Kotlin, ranked 12th.
The RedMonk Programming Language Rankings, which establishes rankings based on the number of GitHub projects and questions asked on Stack Overflow, in January 2021 ranked Scala 14th.[144] Here, Scala was placed inside a second-tier group of languages–ahead of Go, PowerShell, and Haskell, and behind Swift, Objective-C, Typescript, and R.
The TIOBE index[145] of programming language popularity employs internet search engine rankings and similar publication counting to determine language popularity. In September 2021, it showed Scala in 31st place. In this ranking, Scala was ahead of Haskell (38th) and Erlang, but below Go (14th), Swift (15th), and Perl (19th).
Java is a high-level, general-purpose, memory-safe, object-oriented programming language. It is intended to let programmers write once, run anywhere (WORA),[16] meaning that compiled Java code can run on all platforms that support Java without the need to recompile.[17] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture. The syntax of Java is similar to C and C++, but has fewer low-level facilities than either of them. The Java runtime provides dynamic capabilities (such as reflection and runtime code modification) that are typically not available in traditional compiled languages.
Java gained popularity shortly after its release, and has been a popular programming language since then.[18] Java was the third most popular programming language in 2022[update] according to GitHub.[19] Although still widely popular, there has been a gradual decline in use of Java in recent years with other languages using JVM gaining popularity.[20]
Java was designed by James Gosling at Sun Microsystems. It was released in May 1995 as a core component of Sun's Java platform. The original and reference implementation Java compilers, virtual machines, and class libraries were released by Sun under proprietary licenses. As of May 2007, in compliance with the specifications of the Java Community Process, Sun had relicensed most of its Java technologies under the GPL-2.0-only license. Oracle, which bought Sun in 2010, offers its own HotSpot Java Virtual Machine. However, the official reference implementation is the OpenJDK JVM, which is open-source software used by most developers and is the default JVM for almost all Linux distributions.
In 1997, Sun Microsystems approached the ISO/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process.[27][28][29] Java remains a de facto standard, controlled through the Java Community Process.[30] At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.
On November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software (FOSS), under the terms of the GPL-2.0-only license. On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.[31]
Java software runs on most devices from laptops to data centers, game consoles to scientific supercomputers.[36]
One design goal of Java is portability, which means that programs written for the Java platform must run similarly on any combination of hardware and operating system with adequate run time support. This is achieved by compiling the Java language code to an intermediate representation called Java bytecode, instead of directly to architecture-specific machine code. Java bytecode instructions are analogous to machine code, but they are intended to be executed by a virtual machine (VM) written specifically for the host hardware. End-users commonly use a Java Runtime Environment (JRE) installed on their device for standalone Java applications or a web browser for Java applets.
Some platforms offer direct hardware support for Java; there are micro controllers that can run Java bytecode in hardware instead of a software Java virtual machine,[53] and some ARM-based processors could have hardware support for executing Java bytecode through their Jazelle option, though support has mostly been dropped in current implementations of ARM.
Having solved the memory management problem does not relieve the programmer of the burden of handling properly other kinds of resources, like network or database connections, file handles, etc., especially in the presence of exceptions.
The syntax of Java is largely influenced by C++ and C. Unlike C++, which combines the syntax for structured, generic, and object-oriented programming, Java was built almost exclusively as an object-oriented language.[17] All code is written inside classes, and every data item is an object, with the exception of the primitive data types, (i.e. integers, floating-point numbers, boolean values, and characters), which are not objects for performance reasons. Java reuses some popular aspects of C++ (such as the printf method).
JavaFX is a software platform for creating and delivering desktop applications, as well as rich web applications that can run across a wide variety of devices. JavaFX is intended to replace Swing as the standard graphical user interface (GUI) library for Java SE, but since JDK 11 JavaFX has not been in the core JDK and instead in a separate module.[64] JavaFX has support for desktop computers and web browsers on Microsoft Windows, Linux, and macOS. JavaFX does not have support for native OS look and feels.[65]
Criticisms directed at Java include the implementation of generics,[67] speed,[50] the handling of unsigned numbers,[68] the implementation of floating-point arithmetic,[69] and a history of security vulnerabilities in the primary Java VM implementation HotSpot.[70]
Developers have criticized the complexity and verbosity of the Java Persistence API (JPA), a standard part of Java EE. This has led to increased adoption of higher-level abstractions like Spring Data JPA, which aims to simplify database operations and reduce boilerplate code. The growing popularity of such frameworks suggests limitations in the standard JPA implementation's ease-of-use for modern Java development.[71]
The Oracle implementation is packaged into two different distributions: The Java Runtime Environment (JRE) which contains the parts of the Java SE platform required to run Java programs and is intended for end users, and the Java Development Kit (JDK), which is intended for software developers and includes development tools such as the Java compiler, Javadoc, Jar, and a debugger. Oracle has also released GraalVM, a high performance Java dynamic compiler and interpreter.
The Java programming language requires the presence of a software platform in order for compiled programs to be executed.
Oracle supplies the Java platform for use with Java. The Android SDK is an alternative software platform, used primarily for developing Android applications with its own GUI system.
The Java language is a key pillar in Android, an open source mobile operating system. Although Android, built on the Linux kernel, is written largely in C, the Android SDK uses the Java language as the basis for Android applications but does not use any of its standard GUI, SE, ME or other established Java standards.[83] The bytecode language supported by the Android SDK is incompatible with Java bytecode and runs on its own virtual machine, optimized for low-memory devices such as smartphones and tablet computers. Depending on the Android version, the bytecode is either interpreted by the Dalvik virtual machine or compiled into native code by the Android Runtime.
In computer science, the syntax of a computer language is the rules that define the combinations of symbols that are considered to be correctly structured statements or expressions in that language. This applies both to programming languages, where the document represents source code, and to markup languages, where the document represents data.
The syntax of a language defines its surface form.[1] Text-based computer languages are based on sequences of characters, while visual programming languages are based on the spatial layout and connections between symbols (which may be textual or graphical). Documents that are syntactically invalid are said to have a syntax error. When designing the syntax of a language, a designer might start by writing down examples of both legal and illegal strings, before trying to figure out the general rules from these examples.[2]
Second, the parser turns the linear sequence of tokens into a hierarchical syntax tree; this is known as "parsing" narrowly speaking. This ensures that the line of tokens conform to the formal grammars of the programming language. The parsing stage itself can be divided into two parts: the parse tree, or "concrete syntax tree", which is determined by the grammar, but is generally far too detailed for practical use, and the abstract syntax tree (AST), which simplifies this into a usable form. The AST and contextual analysis steps can be considered a form of semantic analysis, as they are adding meaning and interpretation to the syntax, or alternatively as informal, manual implementations of syntactical rules that would be difficult or awkward to describe or implement formally.
Tools have been written that automatically generate a lexer from a lexical specification written in regular expressions and a parser from the phrase grammar written in BNF: this allows one to use declarative programming, rather than need to have procedural or functional programming. A notable example is the lex-yacc pair. These automatically produce a concrete syntax tree; the parser writer must then manually write code describing how this is converted to an abstract syntax tree. Contextual analysis is also generally implemented manually. Despite the existence of these automatic tools, parsing is often implemented manually, for various reasons – perhaps the phrase structure is not context-free, or an alternative implementation improves performance or error-reporting, or allows the grammar to be changed more easily. Parsers are often written in functional languages, such as Haskell, or in scripting languages, such as Python or Perl, or in C or C++.
The syntax of textual programming languages is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (a metalanguage for grammatical structure) to inductively specify syntactic categories (nonterminal) and terminal symbols.[7] Syntactic categories are defined by rules called productions, which specify the values that belong to a particular syntactic category.[1] Terminal symbols are the concrete characters or strings of characters (for example keywords such as define, if, let, or void) from which syntactically valid programs are constructed.
Syntax can be divided into context-free syntax and context-sensitive syntax.[7] Context-free syntax are rules directed by the metalanguage of the programming language. These would not be constrained by the context surrounding or referring that part of the syntax, whereas context-sensitive syntax would.
Below is a simple grammar, defined using the notation of regular expressions and Extended Backus–Naur form. It describes the syntax of S-expressions, a data syntax of the programming language Lisp, which defines productions for the syntactic categories expression, atom, number, symbol, and list:
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The phrase grammar of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars,[8] though the overall syntax is context-sensitive (due to variable declarations and nested scopes), hence Type-1. However, there are exceptions, and for some languages the phrase grammar is Type-0 (Turing-complete).
is syntactically valid, but not semantically defined, as it uses an uninitialized variable. Even though compilers for some programming languages (e.g., Java and C#) would detect uninitialized variable errors of this kind, they should be regarded as semantic errors rather than syntax errors.[6][13]
To quickly compare syntax of various programming languages, take a look at the list of "Hello, World!" program examples:
Agent-oriented programming allows the developer to build, extend and use software agents, which are abstractions of objects that can message other agents.
Array programming (also termed vector or multidimensional) languages generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays.
Aspect-oriented programming enables developers to add new functionality to code, known as "advice", without modifying that code itself; rather, it uses a pointcut to implement the advice into code blocks.
An authoring language is a programming language designed for use by a non-computer expert to easily create tutorials, websites, and other interactive computer programs.
A concatenative programming language is a point-free computer programming language in which all expressions denote functions, and the juxtaposition of expressions denotes function composition.[4] Concatenative programming replaces function application, which is common in other programming styles, with function composition as the default way to build subroutines.
A constraint programming language is a declarative programming language where relationships between variables are expressed as constraints. Execution proceeds by attempting to find values for the variables which satisfy all declared constraints.
Dataflow programming languages rely on a (usually visual) representation of the flow of data to specify the program.  Frequently used for reacting to discrete events or for processing streams of data.  Examples of dataflow languages include:
Declarative languages express the logic of a computation without describing its control flow in detail. Declarative programming stands in contrast to imperative programming via imperative programming languages, where control flow is specified by serial orders (imperatives). (Pure) functional and logic-based programming languages are also declarative, and constitute the major subcategories of the declarative category. This section lists additional examples not in those subcategories.
An esoteric programming language is a programming language designed as a test of the boundaries of computer programming language design, as a proof of concept, or as a joke.
Extension programming languages are languages embedded into another program and used to harness its features in extension scripts.
Fourth-generation programming languages are high-level programming languages built around database systems. They are generally used in commercial environments.
Functional programming languages define programs and subroutines as mathematical functions and treat them as first-class. Many so-called functional languages are "impure", containing imperative features. Many functional languages are tied to mathematical calculation tools. Functional languages include:
Imperative programming languages may be multi-paradigm and appear in other classifications. Here is a list of programming languages that follow the imperative paradigm:
Interpreted languages are programming languages in which programs may be executed from source code form, by an interpreter. Theoretically, any language can be compiled or interpreted, so the term interpreted language generally refers to languages that are usually interpreted rather than compiled.
Some programming languages without the inherent ability to manually manage memory, like Cython,[27] Swift,[c] and Scala[28] (Scala Native only), are able to import or call functions like malloc and free from C through a foreign function interface.
List-based languages are a type of data-structured language that are based on the list data structure.
Metaprogramming is the writing of programs that write or manipulate other programs, including themselves, as their data or that do part of the work that is otherwise done at run time during compile time. In many cases, this allows programmers to get more done in the same amount of time as they would take to write all the code manually.
Multiparadigm languages support more than one programming paradigm. They allow a program to use more than one programming style. The goal is to allow programmers to use the best tool for a job, admitting that no one paradigm solves all problems in the easiest or most efficient way.
Several general-purpose programming languages, such as C and Python, are also used for technical computing, this list focuses on languages almost exclusively used for technical computing.
Class-based object-oriented programming languages support objects defined by their class. Class definitions include member data. Message passing is a key concept, if not the main concept, in object-oriented languages.
Procedural programming languages are based on the concept of the unit and scope (the data viewing range) of an executable code statement. A procedural program is composed of one or more units or modules, either user coded or provided in a code library; each module is composed of one or more procedures, also called a function, routine, subroutine, or method, depending on the language. Examples of procedural languages include:
Reflective programming languages let programs examine and possibly modify their high-level structure at runtime or compile-time. This is most common in high-level virtual machine programming languages like Smalltalk, and less common in lower-level programming languages like C. Languages and platforms supporting reflection:
Stack-based languages are a type of data-structured language that are based on the stack data structure.
Synchronous programming languages are optimized for programming reactive systems, systems that are often interrupted and must respond quickly. Many such systems are also called realtime systems, and are used often in embedded systems.
A shading language is a graphics programming language adapted to programming shader effects. Such language forms usually consist of special data types, like "color" and "normal". Due to the variety of target markets for 3D computer graphics.
They provide both higher hardware abstraction and a more flexible programming model than previous paradigms which hardcoded transformation and shading equations. This gives the programmer greater control over the rendering process and delivers richer content at lower overhead.
The system programming languages are for low-level tasks like memory management or task management. A system programming language usually refers to a programming language used for system programming; such languages are designed for writing system software, which usually requires different development approaches when compared with application software.
System software is computer software designed to operate and control the computer hardware, and to provide a platform for running application software. System software includes software categories such as operating systems, utility software, device drivers, compilers, and linkers. Examples of system languages include:
Visual programming languages let users specify programs in a two-(or more)-dimensional way, instead of as one-dimensional text strings, via graphic layouts of various types. Some dataflow programming languages are also visual languages.
In computer programming, a block or code block or block of code is a lexical structure of source code which is grouped together. Blocks consist of one or more declarations and statements. A programming language that permits the creation of blocks, including blocks nested within other blocks, is called a block-structured programming language. Blocks are fundamental to structured programming, where control structures are formed from blocks.
Blocks have two functions: to group statements so that they can be treated as one statement, and to define scopes for names to distinguish them from the same name used elsewhere. In a block-structured programming language, the objects named in outer blocks are visible inside inner blocks, unless they are masked by an object declared with the same name.
Blocks allow the programmer to treat a group of statements as a unit, and the default values which had to appear in initialization in this style of programming can, with a block structure, be placed closer to the decision:
Scheme is a dialect of the Lisp family of programming languages. Scheme was created during the 1970s at the MIT Computer Science and Artificial Intelligence Laboratory (MIT CSAIL) and released by its developers, Guy L. Steele and Gerald Jay Sussman, via a series of memos now known as the Lambda Papers. It was the first dialect of Lisp to choose lexical scope and the first to require implementations to perform tail-call optimization, giving stronger support for functional programming and associated techniques such as recursive algorithms. It was also one of the first programming languages to support first-class continuations. It had a significant influence on the effort that led to the development of Common Lisp.[2]
The Scheme language is standardized in the official Institute of Electrical and Electronics Engineers (IEEE) standard[3] and a de facto standard called the Revisedn Report on the Algorithmic Language Scheme (RnRS). A widely implemented standard is R5RS (1998).[4] The most recently ratified standard of Scheme is "R7RS-small" (2013).[5] The more expansive and modular R6RS was ratified in 2007.[6] Both trace their descent from R5RS; the timeline below reflects the chronological order of ratification.
Scheme started in the 1970s as an attempt to understand Carl Hewitt's Actor model, for which purpose Steele and Sussman wrote a "tiny Lisp interpreter" using Maclisp and then "added mechanisms for creating actors and sending messages".[7]  Scheme was originally called "Schemer", in the tradition of other Lisp-derived languages such as Planner or Conniver. The current name resulted from the authors' use of the ITS operating system, which limited filenames to two components of at most six characters each. Currently, "Schemer" is commonly used to refer to a Scheme programmer.
The R6RS standard has caused controversy because some see it as a departure from the minimalist philosophy.[12][13] In August 2009, the Scheme Steering Committee, which oversees the standardization process, announced its intention to recommend splitting Scheme into two languages: a large modern programming language for programmers; and a small version, a subset of the large version retaining the minimalism praised by educators and casual implementors.[14] Two working groups were created to work on these two new versions of Scheme. The Scheme Reports Process site has links to the working groups' charters, public discussions and issue tracking system.
Scheme is primarily a functional programming language. It shares many characteristics with other members of the Lisp programming language family. Scheme's very simple syntax is based on s-expressions, parenthesized lists in which a prefix operator is followed by its arguments. Scheme programs thus consist of sequences of nested lists. Lists are also the main data structure in Scheme, leading to a close equivalence between source code and data formats (homoiconicity). Scheme programs can easily create and evaluate pieces of Scheme code dynamically.
The reliance on lists as data structures is shared by all Lisp dialects. Scheme inherits a rich set of list-processing primitives such as cons, car and cdr from its Lisp progenitors. Scheme uses strictly but dynamically typed variables and supports first class procedures. Thus, procedures can be assigned as values to variables or passed as arguments to procedures.
In 1998, Sussman and Steele remarked that the minimalism of Scheme was not a conscious design goal, but rather the unintended outcome of the design process. "We were actually trying to build something complicated and discovered, serendipitously, that we had accidentally designed something that met all our goals but was much simpler than we had intended....we realized that the lambda calculus—a small, simple formalism—could serve as the core of a powerful and expressive programming language."[7]
Like most modern programming languages and unlike earlier Lisps such as Maclisp, Scheme is lexically scoped: all possible variable bindings in a program unit can be analyzed by reading the text of the program unit without consideration of the contexts in which it may be called. This contrasts with dynamic scoping which was characteristic of early Lisp dialects, because of the processing costs associated with the primitive textual substitution methods used to implement lexical scoping algorithms in compilers and interpreters of the day. In those Lisps, it was perfectly possible for a reference to a free variable inside a procedure to refer to quite distinct bindings external to the procedure, depending on the context of the call.
Alonzo Church's mathematical notation, the lambda calculus, has inspired Lisp's use of "lambda" as a keyword for introducing a procedure, as well as influencing the development of functional programming techniques involving the use of higher-order functions in Lisp. But early Lisps were not suitable expressions of the lambda calculus because of their treatment of free variables.[7]
The introduction of lexical scope resolved the problem by making an equivalence between some forms of lambda notation and their practical expression in a working programming language. Sussman and Steele showed that the new language could be used to elegantly derive all the imperative and declarative semantics of other programming languages including ALGOL and Fortran, and the dynamic scope of other Lisps, by using lambda expressions not as simple procedure instantiations but as "control structures and environment modifiers".[22] They introduced continuation-passing style along with their first description of Scheme in the first of the Lambda Papers, and in subsequent papers, they proceeded to demonstrate the raw power of this practical use of lambda calculus.
Scheme has an iteration construct, do, but it is more idiomatic in Scheme to use tail recursion to express iteration. Standard-conforming Scheme implementations are required to optimize tail calls so as to support an unbounded number of active tail calls (R5RS sec. 3.5)[4]—a property the Scheme report describes as proper tail recursion—making it safe for Scheme programmers to write iterative algorithms using recursive structures, which are sometimes more intuitive. Tail recursive procedures and the named let form provide support for iteration using tail recursion.
Continuations can be used to emulate the behavior of return statements in imperative programming languages. The following function find-first, given function func and list lst, returns the first element x in lst such that (func x) returns true.
Implementations of the hygienic macro system, also called syntax-rules, are required to respect the lexical scoping of the rest of the language. This is assured by special naming and scoping rules for macro expansion and avoids common programming errors that can occur in the macro systems of other programming languages. R6RS specifies a more sophisticated transformation system, syntax-case, which has been available as a language extension to R5RS Scheme for some time.
The form of the titles of the standards documents since R3RS, "Revisedn Report on the Algorithmic Language Scheme", is a reference to the title of the ALGOL 60 standard document, "Revised Report on the Algorithmic Language Algol 60," The Summary page of R3RS is closely modeled on the Summary page of the ALGOL 60 Report.[31][32]
Scheme is widely used by several[35] schools; in particular, several introductory computer science courses use Scheme in conjunction with the textbook Structure and Interpretation of Computer Programs (SICP).[36] For the past 12 years, PLT has run the ProgramByDesign (formerly TeachScheme!) project, which has exposed close to 600 high school teachers and thousands of high school students to rudimentary Scheme programming. MIT's old introductory programming class 6.001 was taught in Scheme,[37] Although 6.001 has been replaced by more modern courses, SICP continues to be taught at MIT.[38] Likewise, the introductory class at UC Berkeley, CS 61A, was until 2011 taught entirely in Scheme, save minor diversions into Logo to demonstrate dynamic scope. Today, like MIT, Berkeley has replaced the syllabus with a more modern version that is primarily taught in Python 3, but the current syllabus is still based on the old curriculum, and parts of the class are still taught in Scheme.[39]
The textbook How to Design Programs by Matthias Felleisen, currently at Northeastern University, is used by some institutes of higher education for their introductory computer science courses. Both Northeastern University and Worcester Polytechnic Institute use Scheme exclusively for their introductory courses Fundamentals of Computer Science (CS2500) and Introduction to Program Design (CS1101), respectively.[40][41] Rose-Hulman Institute of Technology uses Scheme in its more advanced Programming Language Concepts course.[42] Brandeis University's core course, Structure and Interpretations of Computer Programs (COSI121b), is also taught exclusively in Scheme by theoretical computer scientist Harry Mairson.[43] Indiana University's introductory class, C211, is taught entirely in Scheme. A self-paced version of the course, CS 61AS, continues to use Scheme.[44] The introductory computer science courses at Yale and Grinnell College are also taught in Scheme.[45] Programming Design Paradigms,[46] a mandatory course for the Computer science Graduate Students at Northeastern University, also extensively uses Scheme.
The former introductory computer science course at the University of Minnesota - Twin Cities, CSCI 1901, also used Scheme as its primary language, followed by a course that introduced students to the Java language;[47] however, following the example of MIT, the department replaced 1901 with the Python-based CSCI 1133,[48] while functional programming is covered in detail in the third-semester course CSCI 2041.[49]
Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.
Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture,[12] and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]
Many of these algorithms are insufficient for solving large reasoning problems because they experience a "combinatorial explosion": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.
Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining "interesting" and actionable inferences from large databases),[21] and other areas.[22]
Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]
There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]
In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as "good".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]
State space search searches through a tree of possible states to try to find a goal state.[69] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[70]
Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[75] through the backpropagation algorithm.
Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[77]
Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[84]
Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.
Bayesian networks[92] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][94] learning (using the expectation–maximization algorithm),[h][96] planning (using decision networks)[97] and perception (using dynamic Bayesian networks).[90]
Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[90]
There are many kinds of classifiers in use.[99] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[100] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[101]
The naive Bayes classifier is reportedly the "most widely used learner"[102] at Google, due in part to its scalability.[103]
Neural networks are also used as classifiers.[104]
Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[105] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[106]
Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[113] and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[114] The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]
In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[126] Specialized programming languages such as Prolog were used in early AI research,[127] but general-purpose programming languages like Python have become predominant.[128]
AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).
For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[134] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[134][135] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[136] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[137] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[138][139]
Large language models, such as GPT-4, Gemini, Claude, LLaMa or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning[149] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[150] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[151] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.[152] The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[153] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.[154]
Generative artificial intelligence (Generative AI, GenAI,[166] or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data.[167][168][169] These models learn the underlying patterns and structures of their training data and use them to produce new data[170][171] based on the input, which often comes in the form of natural language prompts.[172][173]
Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora.[174][175][176][177] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[172][178][179]
Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[188][189][190]
Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for "classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights." For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.
AI has potential benefits and potential risks.[203] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to "solve intelligence, and then use that to solve everything else".[204] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[205] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[206]
Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.
Sensitive user data collected may include online activity records, geolocation data, video, or audio.[207] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[208] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[209]
In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[223] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[224]
Machine learning applications will be biased[k] if they learn from biased data.[240] The developers may not be aware that the bias exists.[241] Bias can be introduced by the way training data is selected and by the way a model is deployed.[242][240] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[243] The field of fairness studies how to prevent harms from algorithmic biases.
Criticism of COMPAS highlighted that machine learning models are designed to make "predictions" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these "recommendations" will likely be racist.[252] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]
It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as "cancerous", because pictures of malignancies typically include a ruler to show the scale.[257] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at "low risk" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[258]
People who have been harmed by an algorithm's decision have a right to an explanation.[259] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[260]
DARPA established the XAI ("Explainable Artificial Intelligence") program in 2014 to try to solve these problems.[261]
Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.
AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[271] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.[272][273]
In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI.[276] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[277] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classified only 9% of U.S. jobs as "high risk".[p][279] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[275] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[280][281]
Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously".[282] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[283]
From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[284]
Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:[316][317]
The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[322] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[323] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[324][325] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[326] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[326] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[326] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[327] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[328] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.[329] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the "Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[330]
In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[334] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[335][336] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[337][338]
The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as "astonishing":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[341]
Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[348] In 1965 Herbert Simon predicted, "machines will be capable, within twenty years, of doing any work a man can do".[349] In 1967 Marvin Minsky agreed, writing that "within a generation ... the problem of creating 'artificial intelligence' will substantially be solved".[350] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[352] and ongoing pressure from the U.S. Congress to fund more productive projects.[353] Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[354] The "AI winter", a period when obtaining funding for AI projects was difficult, followed.[9]
AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This "narrow" and "formal" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[365] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as "artificial intelligence" (a tendency known as the AI effect).[366]
However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or "AGI"), which had several well-funded institutions by the 2010s.[4]
Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]
For many specific tasks, other methods were abandoned.[y]
Deep learning's success was based on both hardware improvements (faster computers,[368] graphics processing units, cloud computing[369]) and access to large amounts of data[370] (including curated datasets,[369] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[326]
In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[303]
In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[371] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[372] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[373] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about $50 billion annually was invested in "AI" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in "AI".[374] About 800,000 "AI"-related U.S. job openings existed in 2022.[375] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[376]
Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. "Aeronautical engineering texts", they wrote, "do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"[383] AI founder John McCarthy agreed, writing that "Artificial intelligence is not, by definition, simulation of human intelligence".[384]
Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI,[388] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did "not actually use AI in a material way".[389]
No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term "artificial intelligence" to mean "machine learning with neural networks"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.
The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[396][397] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.
Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.
The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that "[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on."[401] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.
Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[404]
A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[400] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an "intelligence explosion" and Vernor Vinge called a "singularity".[416]
Edward Fredkin argues that "artificial intelligence is the next step in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[419]
Arguments for decomputing have been raised by Dan McQuillan (Resisting AI: An Anti-fascist Approach to Artificial Intelligence, 2022), meaning an opposition to the sweeping application and expansion of artificial intelligence. Similar to degrowth the approach criticizes AI as an outgrowth of the systemic issues and capitalist world we live in. Arguing that a different future is possible, in which distance between people is reduced and not increased to AI intermediaries.[420]
Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the "Multivac" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[424] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[425]
Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[426]
Computer science is the study of computation, information, and automation.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software).[4][5][6]
Algorithms and data structures are central to computer science.[7]
The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.
The fundamental concern of computer science is determining what can and cannot be automated.[2][8][3][9][10] The Turing Award is generally recognized as the highest distinction in computer science.[11][12]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[16]
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[17] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[18] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[19] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[20] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[20] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[21] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[22] the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics,[23] and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic.[24][25] In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine,[26] on which commands could be typed and the results printed automatically.[27] In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[28] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[29]
During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors.[30] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world.[31] Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946.[32] Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[33][34] The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962.[35] Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.
Although first proposed in 1956,[36] the term "computer science" appears in a 1959 article in Communications of the ACM,[37]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[38] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[37]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[39] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[40] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[41] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that "computer science is no more about computers than astronomy is about telescopes."[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.
Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science.[33] Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[36]
The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term "software engineering" means, and how computer science is defined.[47] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[48]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Despite the word science in its name, there is debate over whether or not computer science is a discipline of science,[49] mathematics,[50] or engineering.[51] Allen Newell and Herbert A. Simon argued in 1975,
Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[51]
It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[51] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[51] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[51]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods.[51] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[51]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[52] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[33] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[53] identifiable in some branches of artificial intelligence).[54]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[55]
As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.[56][57]
CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[58]—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.[56]
Theoretical computer science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[3] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[62] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier – whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of the unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[63] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[64]
Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[77][78] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[79]
Software consists of computer programs that instruct the execution of a computer.[1] Software also includes design documents and specifications.
The history of software is closely tied to the development of digital computers in the mid-20th century. Early programs were written in the machine language specific to the hardware. The introduction of high-level programming languages in 1958 allowed for more human-readable instructions, making software development easier and more portable across different computer architectures. Software in a programming language is run through a compiler or interpreter to execute on the architecture's hardware. Over time, software has become complex, owing to developments in networking, operating systems, and databases.
The rise of cloud computing has introduced the new software delivery model Software as a Service (SaaS). In SaaS, applications are hosted by a provider and accessed over the Internet.
The process of developing software involves several stages. The stages include software design, programming, testing, release, and maintenance. Software quality assurance and security are critical aspects of software development, as bugs and security vulnerabilities can lead to system failures and security breaches. Additionally, legal issues such as software licenses and intellectual property rights play a significant role in the distribution of software products.
The first use of the word software to describe computer programs is credited to mathematician John Wilder Tukey in 1958.[3]
The first programmable computers, which appeared at the end of the 1940s,[4] were programmed in machine language. Machine language is difficult to debug and not portable across different computers.[5] Initially, hardware resources were more expensive than human resources.[6] As programs became complex, programmer productivity became the bottleneck. The introduction of high-level programming languages in 1958 hid the details of the hardware and expressed the underlying algorithms into the code .[7][8] Early languages include Fortran, Lisp, and COBOL.[8]
Software can also be categorized by how it is deployed. Traditional applications are purchased with a perpetual license for a specific version of the software, downloaded, and run on hardware belonging to the purchaser.[17]  The rise of the Internet and cloud computing enabled a new model, software as a service (SaaS),[18] in which the provider hosts the software (usually built on top of rented infrastructure or platforms)[19] and provides the use of the software to customers, often in exchange for a subscription fee.[17] By 2023, SaaS products—which are usually delivered via a web application—had become the primary method that companies deliver applications.[20]
Software companies aim to deliver a high-quality product on time and under budget. A challenge is that software development effort estimation is often inaccurate.[21] Software development begins by conceiving the project, evaluating its feasibility, analyzing the business requirements, and making a software design.[22][23] Most software projects speed up their development by reusing or incorporating existing software, either in the form of commercial off-the-shelf (COTS) or open-source software.[24][25] Software quality assurance is typically a combination of manual code review by other engineers[26] and automated software testing. Due to time constraints, testing cannot cover all aspects of the software's intended functionality, so developers often focus on the most critical functionality.[27] Formal methods are used in some safety-critical systems to prove the correctness of code,[28] while user acceptance testing helps to ensure that the product meets customer expectations.[29] There are a variety of software development methodologies, which vary from completing all steps in order to concurrent and iterative models.[30] Software development is driven by requirements taken from prospective users, as opposed to maintenance, which is driven by events such as a change request.[31]
Frequently, software is released in an incomplete state when the development team runs out of time or funding.[32] Despite testing and quality assurance, virtually all software contains bugs where the system does not work as intended. Post-release software maintenance is necessary to remediate these bugs when they are found and keep the software working as the environment changes over time.[33] New features are often added after the release. Over time, the level of maintenance becomes increasingly restricted before being cut off entirely when the product is withdrawn from the market.[34] As software ages, it becomes known as legacy software and can remain in use for decades, even if there is no one left who knows how to fix it.[35] Over the lifetime of the product, software maintenance is estimated to comprise 75 percent or more of the total development cost.[36][37]
Completing a software project involves various forms of expertise, not just in software programmers but also testing, documentation writing, project management, graphic design, user experience, user support, marketing, and fundraising.[38][39][23]
Software quality is defined as meeting the stated requirements as well as customer expectations.[40] Quality is an overarching term that can refer to a code's correct and efficient behavior, its reusability and portability, or the ease of modification.[41] It is usually more cost-effective to build quality into the product from the beginning rather than try to add it later in the development process.[42] Higher quality code will reduce lifetime cost to both suppliers and customers as it is more reliable and easier to maintain.[43][44] Software failures in safety-critical systems can be very serious including death.[43] By some estimates, the cost of poor quality software can be as high as 20 to 40 percent of sales.[45]  Despite developers' goal of delivering a product that works entirely as intended, virtually all software contains bugs.[46]
The rise of the Internet also greatly increased the need for computer security as it enabled malicious actors to conduct cyberattacks remotely.[47][48] If a bug creates a security risk, it is called a vulnerability.[49][50] Software patches are often released to fix identified vulnerabilities, but those that remain unknown (zero days) as well as those that have not been patched are still liable for exploitation.[51] Vulnerabilities vary in their ability to be exploited by malicious actors,[49] and the actual risk is dependent on the nature of the vulnerability as well as the value of the surrounding system.[52]  Although some vulnerabilities can only be used for denial of service attacks that compromise a system's availability, others allow the attacker to inject and run their own code (called malware), without the user being aware of it.[49] To thwart cyberattacks, all software in the system must be designed to withstand and recover from external attack.[48] Despite efforts to ensure security, a significant fraction of computers are infected with malware.[53]
Programming languages are the format in which software is written. Since the 1950s, thousands of different programming languages have been invented; some have been in use for decades, while others have fallen into disuse.[54] Some definitions classify machine code—the exact instructions directly implemented by the hardware—and assembly language—a more human-readable alternative to machine code whose statements can be translated one-to-one into machine code—as programming languages.[55] Programs written in the high-level programming languages used to create software share a few main characteristics: knowledge of machine code is not necessary to write them, they can be ported to other computer systems, and they are more concise and human-readable than machine code.[56] They must be both human-readable and capable of being translated into unambiguous instructions for computer hardware.[57]
The invention of high-level programming languages was simultaneous with the compilers needed to translate them automatically into machine code.[58] Most programs do not contain all the resources needed to run them and rely on external libraries. Part of the compiler's function is to link these files in such a way that the program can be executed by the hardware. Once compiled, the program can be saved as an object file and the loader (part of the operating system) can take this saved file and execute it as a process on the computer hardware.[59] Some programming languages use an interpreter instead of a compiler. An interpreter converts the program into machine code at run time, which makes them 10 to 100 times slower than compiled programming languages.[60][61]
Software is often released with the knowledge that it is incomplete or contains bugs. Purchasers knowingly buy it in this state, which has led to a legal regime where liability for software products is significantly curtailed compared to other products.[62]
Since the mid-1970s, software and its source code have been protected by copyright law that vests the owner with the exclusive right to copy the code. The underlying ideas or algorithms are not protected by copyright law, but are sometimes treated as a trade secret and concealed by such methods as non-disclosure agreements.[63] A software copyright is often owned by the person or company that financed or made the software (depending on their contracts with employees or contractors who helped to write it).[64] Some software is in the public domain and has no restrictions on who can use it, copy or share it, or modify it; a notable example is software written by the United States Government.  Free and open-source software also allow free use, sharing, and modification, perhaps with a few specified conditions.[64]  The use of some software is governed by an agreement (software license) written by the copyright holder and imposed on the user. Proprietary software is usually sold under a restrictive license that limits its use and sharing.[65]  Some free software licenses require that modified versions must be released under the same license, which prevents the software from being sold
or distributed under proprietary restrictions.[66]
Patents give an inventor an exclusive, time-limited license for a novel product or process.[67] Ideas about what software could accomplish are not protected by law and concrete implementations are instead covered by copyright law. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid.[68] Software patents have been historically controversial. Before the 1998 case State Street Bank & Trust Co. v. Signature Financial Group, Inc., software patents were generally not recognized in the United States. In that case, the Supreme Court decided that business processes could be patented.[69] Patent applications are complex and costly, and lawsuits involving patents can drive up the cost of products.[70] Unlike copyrights, patents generally only apply in the jurisdiction where they were issued.[71]
Engineer Capers Jones writes that "computers and software are making profound changes to every aspect of human life: education, work, warfare, entertainment, medicine, law, and everything else".[73] It has become ubiquitous in everyday life in developed countries.[74] In many cases, software augments the functionality of existing technologies such as household appliances and elevators.[75] Software also spawned entirely new technologies such as the Internet, video games, mobile phones, and GPS.[75][76] New methods of communication, including email, forums, blogs, microblogging, wikis, and social media, were enabled by the Internet.[77] Massive amounts of knowledge exceeding any paper-based library are now available with a quick web search.[76] Most creative professionals have switched to software-based tools such as computer-aided design, 3D modeling, digital image editing, and computer animation.[72] Almost every complex device is controlled by software.[76]
Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[33]
Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a "batteries included" language due to its comprehensive standard library.[34][35]
Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0.[36] Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.[37]
Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.[38][39][40][41]
Python was conceived in the late 1980s[42] by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC programming language, which was inspired by SETL,[43] capable of exception handling and interfacing with the Amoeba operating system.[12] Its implementation began in December 1989.[44] Van Rossum shouldered sole responsibility for the project, as the lead developer, until 12 July 2018, when he announced his "permanent vacation" from his responsibilities as Python's "benevolent dictator for life" (BDFL), a title the Python community bestowed upon him to reflect his long-term commitment as the project's chief decision-maker[45] (he has since come out of retirement and is self-titled "BDFL-emeritus"). In January 2019, active Python core developers elected a five-member Steering Council to lead the project.[46][47]
Python is a multi-paradigm programming language. Object-oriented programming and structured programming are fully supported, and many of their features support functional programming and aspect-oriented programming (including metaprogramming[71] and metaobjects).[72] Many other paradigms are supported via extensions, including design by contract[73][74] and logic programming.[75] Python is often referred to as a 'glue language'[76] because it can seamlessly integrate components written in other languages.
Its design offers some support for functional programming in the Lisp tradition. It has filter,mapandreduce functions; list comprehensions, dictionaries, sets, and generator expressions.[78] The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.[79]
Python claims to strive for a simpler, less-cluttered syntax and grammar while giving developers a choice in their coding methodology. In contrast to Perl's "there is more than one way to do it" motto, Python embraces a "there should be one—and preferably only one—obvious way to do it." philosophy.[80] In practice, however, Python provides many ways to achieve the same task. There are, for example, at least three ways to format a string literal, with no certainty as to which one a programmer should use.[85] Alex Martelli, a Fellow at the Python Software Foundation and Python book author, wrote: "To describe something as 'clever' is not considered a compliment in the Python culture."[86]
Methods on objects are functions attached to the object's class; the syntax instance.method(argument) is, for normal methods and functions, syntactic sugar for Class.method(instance, argument). Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, Ruby).[111] Python also provides methods, often called dunder methods (due to their names beginning and ending with double-underscores), to allow user-defined classes to modify how they are handled by native operations including length, comparison, in arithmetic operations and type conversion.[112]
Python allows programmers to define their own types using classes, most often used for object-oriented programming. New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection.
Python's large standard library[132] is commonly cited as one of its greatest strengths. For Internet-facing applications, many standard formats and protocols such as MIME and HTTP are supported. It includes modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary-precision decimals,[128] manipulating regular expressions, and unit testing.
As of 13 March 2025,[update] the Python Package Index (PyPI), the official repository for third-party Python software, contains over 614,339[134] packages with a wide range of functionality, including:
All current Python versions (i.e. since 3.7) only support operating systems with multi-threading support.
Performance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy '13.[173] Python's performance compared to other programming languages is also benchmarked by The Computer Language Benchmarks Game.[174]
Since 2003, Python has consistently ranked in the top ten most popular programming languages in the TIOBE Programming Community Index where as of December 2022[update] it was the most popular language (ahead of C, C++, and Java).[40] It was selected as Programming Language of the Year (for "the highest rise in ratings in a year") in 2007, 2010, 2018, and 2020 (the only language to have done so four times as of 2020[update][195]).
Python can serve as a scripting language for web applications, e.g. via mod_wsgi for the Apache webserver.[208] With Web Server Gateway Interface, a standard API has evolved to facilitate these applications. Web frameworks like Django, Pylons, Pyramid, TurboGears, web2py, Tornado, Flask, Bottle, and Zope support developers in the design and maintenance of complex applications. Pyjs and IronPython can be used to develop the client-side of Ajax-based applications. SQLAlchemy can be used as a data mapper to a relational database. Twisted is a framework to program communications between computers, and is used (for example) by Dropbox.
Python is commonly used in artificial intelligence projects and machine learning projects with the help of libraries like TensorFlow, Keras, Pytorch, scikit-learn and the Logic language ProbLog.[213][214][215][216][217] As a scripting language with a modular architecture, simple syntax, and rich text processing tools, Python is often used for natural language processing.[218]
The combination of Python and Prolog has proved to be particularly useful for AI applications, with Prolog providing knowledge representation and reasoning capabilities. The Janus system, in particular, exploits the similarities between these two languages,
in part because of their use of dynamic typing, and the simple recursive nature of their
data structures. Typical applications of this combination include  natural language processing, visual query
answering, geospatial reasoning, and handling of semantic web data.[219][220]
The Natlog system, implemented in Python, uses Definite Clause Grammars (DCGs) as prompt generators for text-to-text generators like GPT3 and text-to-image generators like DALL-E or Stable Diffusion.[221]
Python is embedded in many software products as a scripting language, including in finite element method software such as Abaqus, 3D parametric modelers like FreeCAD, 3D animation packages such as 3ds Max, Blender, Cinema 4D, Lightwave, Houdini, Maya, modo, MotionBuilder, Softimage, the visual effects compositor Nuke, 2D imaging programs like GIMP,[224] Inkscape, Scribus and Paint Shop Pro,[225] and musical notation programs like scorewriter and capella. GNU Debugger uses Python as a pretty printer to show complex structures such as C++ containers. Esri promotes Python as the best choice for writing scripts in ArcGIS.[226] It has also been used in several video games,[227][228] and has been adopted as first of the three available programming languages in Google App Engine, the other two being Java and Go.[229]
Many operating systems include Python as a standard component. It ships with most Linux distributions,[230] AmigaOS 4 (using Python 2.7), FreeBSD (as a package), NetBSD, and OpenBSD (as a package) and can be used from the command line (terminal). Many Linux distributions use installers written in Python: Ubuntu uses the Ubiquity installer, while Red Hat Linux and Fedora Linux use the Anaconda installer. Gentoo Linux uses Python in its package management system, Portage.
Most of the Sugar software for the One Laptop per Child XO, developed at Sugar Labs as of 2008[update], is written in Python.[233] The Raspberry Pi single-board computer project has adopted Python as its main user-programming language.
A programming paradigm is a relatively high-level way to conceptualize and structure the implementation of a computer program. A programming language can be classified as supporting one or more paradigms.[1]
Paradigms are separated along and described by different dimensions of programming. Some paradigms are about implications of the execution model, such as allowing side effects, or whether the sequence of operations is defined by the execution model. Other paradigms are about the way code is organized, such as grouping into units that include both state and behavior. Yet others are about syntax and grammar.
Programming paradigms come from computer science research into existing practices of software development. The findings allow for describing and comparing programming practices and the languages used to code programs. For perspective, other fields of research study 
software engineering processes and describe various methodologies to describe and compare them.
A programming language can be described in terms of paradigms. Some languages support only one paradigm. For example, Smalltalk supports object-oriented and Haskell supports functional. Most languages support multiple paradigms. For example, a program written in C++, Object Pascal, or PHP can be purely procedural, purely object-oriented, or can contain aspects of both paradigms, or others.
In contrast, languages in the declarative paradigm do not state the order in which to execute operations. Instead, they supply a number of available operations in the system, along with the conditions under which each is allowed to execute.[7]  The implementation of the language's execution model tracks which operations are free to execute and chooses the order independently. More at  Comparison of multi-paradigm programming languages.
In object-oriented programming, code is organized into objects that contain state that is owned by and (usually) controlled by the code of the object.  Most object-oriented languages are also imperative languages.
In object-oriented programming, programs are treated as a set of interacting objects. In functional programming, programs are treated as a sequence of stateless function evaluations. When programming computers or systems with many processors, in process-oriented programming, programs are treated as sets of concurrent processes that act on a logical shared data structures.
Many programming paradigms are as well known for the techniques they forbid as for those they support. For instance, pure functional programming disallows side-effects, while structured programming disallows the goto construct. Partly for this reason, new paradigms are often regarded as doctrinaire or overly rigid by those accustomed to older ones.[8] Yet, avoiding certain techniques can make it easier to understand program behavior, and to prove theorems about program correctness.
Programming paradigms can also be compared with programming models, which allows invoking an execution model by using only an API. Programming models can also be classified into paradigms based on features of the execution model.
For parallel computing, using a programming model instead of a language is common.  The reason is that details of the parallel hardware leak into the abstractions used to program the hardware.  This causes the programmer to have to map patterns in the algorithm onto patterns in the execution model (which have been inserted due to leakage of hardware into the abstraction).  As a consequence, no one parallel programming language maps well to all computation problems.  Thus, it is more convenient to use a base sequential language and insert API calls to parallel execution models via a programming model.  Such parallel programming models can be classified according to abstractions that reflect the hardware, such as shared memory, distributed memory with message passing, notions of place visible in the code, and so forth.  These can be considered flavors of programming paradigm that apply to only parallel languages and programming models.
Some programming language researchers criticise the notion of paradigms as a classification of programming languages, e.g. Harper,[9] and Krishnamurthi.[10]  They argue that many programming languages cannot be strictly classified into one paradigm, but rather include features from several paradigms. See Comparison of multi-paradigm programming languages.
Different approaches to programming have developed over time. Classification of each approach was either described at the time the approach was first developed, but often not until some time later, retrospectively. An early approach consciously identified as such is structured programming, advocated since the mid 1960s. The concept of a programming paradigm as such dates at least to 1978, in the Turing Award lecture of Robert W. Floyd, entitled The Paradigms of Programming, which cites the notion of paradigm as used by Thomas Kuhn in his The Structure of Scientific Revolutions (1962).[11] Early programming languages did not have clearly defined programming paradigms and sometimes programs made extensive use of goto statements. Liberal use of which lead to spaghetti code which is difficult to understand and maintain. This led to the development of structured programming paradigms that disallowed the use of goto statements; only allowing the use of more structured programming constructs.[12]
Machine code is the lowest-level of computer programming as it is machine instructions that define behavior at the lowest level of abstract possible for a computer. As it is the most prescriptive way to code it is classified as imperative.
Assembly language introduced mnemonics for machine instructions and memory addresses. Assembly is classified as imperative and is sometimes called the second-generation programming language.
Procedural languages, also called the third-generation programming languages are the first described as high-level languages. They support vocabulary related to the problem being solved. For example,
In attempt to improve on procedural languages, object-oriented programming (OOP) languages were created, such as Simula, Smalltalk, C++, Eiffel, Python, PHP, Java, and C#. In these languages, data and methods to manipulate the data are in the same code unit called an object. This encapsulation ensures that the only way that an object can access data is via methods of the object that contains the data. Thus, an object's inner workings may be changed without affecting code that uses the object.
There is controversy raised by Alexander Stepanov, Richard Stallman[13] and other programmers, concerning the efficacy of the OOP paradigm versus the procedural paradigm. The need for every object to have associative methods leads some skeptics to associate OOP with software bloat; an attempt to resolve this dilemma came through polymorphism.
Although most OOP languages are third-generation, it is possible to create an object-oriented assembler language. High Level Assembly (HLA) is an example of this that fully supports advanced data types and object-oriented assembly language programming –  despite its early origins. Thus, differing programming paradigms can be seen rather like motivational memes of their advocates, rather than necessarily representing progress from one level to the next.[citation needed] Precise comparisons of competing paradigms' efficacy are frequently made more difficult because of new and differing terminology applied to similar entities and processes together with numerous implementation distinctions across languages.
A declarative programming program describes what the problem is, not how to solve it. The program is structured as a set of properties to find in the expected result, not as a procedure to follow. Given a database or a set of rules, the computer tries to find a solution matching all the desired properties. An archetype of a declarative language is the fourth generation language SQL, and the family of functional languages and logic programming.
Functional programming is a subset of declarative programming. Programs written using this paradigm use functions, blocks of code intended to behave like mathematical functions. Functional languages discourage changes in the value of variables through assignment, making a great deal of use of recursion instead.
The logic programming paradigm views computation as automated reasoning over a body of knowledge. Facts about the problem domain are expressed as logic formulas, and programs are executed by applying inference rules over them until an answer to the problem is found, or the set of formulas is proved inconsistent.
Symbolic programming is a paradigm that describes programs able to manipulate formulas and program components as data.[4] Programs can thus effectively modify themselves, and appear to "learn", making them suited for applications such as artificial intelligence, expert systems, natural-language processing and computer games. Languages that support this paradigm include Lisp and Prolog.[14]
Differentiable programming structures programs so that they can be differentiated throughout, usually via automatic differentiation.[15][16]
Literate programming, as a form of imperative programming, structures programs as a human-centered web, as in a hypertext essay: documentation is integral to the program, and the program is structured following the logic of prose exposition, rather than compiler convenience.
Symbolic programming techniques such as reflective programming (reflection), which allow a program to refer to itself, might also be considered as a programming paradigm. However, this is compatible with the major paradigms and thus is not a real paradigm in its own right.
Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making specific disciplined use of the structured control flow constructs of selection (if/then/else) and repetition (while and for), block structures, and subroutines.
It emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages,[1] with the latter including support for block structures. Contributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the discovery of what is now known as the structured program theorem in 1966,[2] and the publication of the influential "Go To Statement Considered Harmful" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term "structured programming".[3]
Structured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed.
It is possible to do structured programming with any programming language enabling code blocks and the three types of control structures, even though a procedural programming language is able to break the structure.[4][5] Some of the languages initially used for structured programming include: ALGOL, Pascal, PL/I, Ada and RPL but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features – notably GOTO – in an effort to make unstructured programming more difficult.
Structured programming (sometimes known as modular programming[4]) enforces a logical structure on the program being written to make it more efficient and easier to understand and modify.
The structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs—sequencing, selection, and iteration—are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore, a processor is always executing a "structured program" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by Böhm and Jacopini, possibly because Dijkstra cited this paper himself.[6] The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.
P. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:
Donald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed with abolishing the GOTO statement, and as of 2018[update] has continued to use it in his programs.[8] In his 1974 paper, "Structured Programming with Goto Statements",[9] he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs.[when defined as?][who?]
Structured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for The New York Times research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.[10]
As late as 1987 it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with an open letter titled "'GOTO Considered Harmful' Considered Harmful".[11] Numerous objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.
By the end of the 20th century, nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.
While goto has now largely been replaced by the structured constructs of selection (if/then/else) and repetition (while and for), few languages are purely structured. The most common deviation, found in many languages, is the use of a return statement for early exit from a subroutine. This results in multiple exit points, instead of the single exit point required by structured programming. There are other constructions to handle cases that are awkward in purely structured programming.
The most common deviation from structured programming is early exit from a function or loop. At the level of functions, this is a return statement. At the level of loops, this is a break statement (terminate the loop) or continue statement (terminate the current iteration, proceed with next iteration). In structured programming, these can be replicated by adding additional branches or tests, but for returns from nested code this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have "labeled breaks", which allow breaking out of more than just the innermost loop. Exceptions also allow early exit, but have further consequences, and thus are treated below.
In his 2004 textbook, David Watt writes that "single-entry multi-exit control flows are often desirable". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as escape sequencers, defined as a "sequencer that terminates execution of a textually enclosing command or procedure", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce "spaghetti code". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.[15]
Based on the coding error from the Ariane 501 disaster, software developer Jim Bonang argues that any exceptions thrown from a function violate the single-exit paradigm, and proposes that all inter-procedural exceptions should be forbidden. Bonang proposes that all single-exit conforming C++ should be written along the lines of:
Peter Ritchie also notes that, in principle, even a single throw right before the return in a function constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions that wrap exceptions for the sake of creating a single-exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages that support exceptions of engaging in cargo cult thinking.[17]
David Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic overflows or input/output failures like file not found) is a kind of error that "is detected in some low-level program unit, but [for which] a handler is more naturally located in a high-level program unit". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where "the application code tends to get cluttered by tests of status flags" and that "the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!" He notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) are not as suitable as a dedicated exception sequencer with the semantics discussed above.[18]
The textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like while loops because the transfer of control "is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred."[19] Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like for, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if init() throws an exception in for (init(); check(); increm()), then the usual exit point after check() is not reached.[20] Citing multiple prior studies by others (1999–2004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they "create hidden control-flow paths that are difficult for programmers to reason about".[21]
The necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like parallel do, do not allow early exits from inside to the outside of the parallel construct; this restriction includes all manner of exits, from break to C++ exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside it.[22]
More rarely, subprograms allow multiple entry. This is most commonly only re-entry into a coroutine (or generator/semicoroutine), where a subprogram yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code execution point of view, yielding from a coroutine is closer to structured programming than returning from a subroutine, as the subprogram has not actually terminated, and will continue when called again – it is not an early exit. However, coroutines mean that multiple subprograms have execution state – rather than a single call stack of subroutines – and thus introduce a different form of complexity.
Procedural programming is a programming paradigm, classified as imperative programming,[1] that involves implementing the behavior of a computer program as procedures (a.k.a. functions, subroutines) that call each other. The resulting program is a series of steps that forms a hierarchy of calls to its constituent procedures.
The first major procedural programming languages appeared c. 1957–1964, including Fortran, ALGOL, COBOL, PL/I and BASIC.[2] Pascal and C were published c. 1970–1972.
Computer processors provide hardware support for procedural programming through a stack register and instructions for calling procedures and returning from them. Hardware support for other types of programming is possible, like Lisp machines or Java processors, but no attempt was commercially successful.[contradictory]
Certain software development practices are often employed with procedural programming in order to enhance quality and lower development and maintenance costs.
Minimizing the scope of variables and procedures can enhance software quality by reducing the cognitive load of procedures and modules.
Since a procedure can specify a well-defined interface and be self-contained it supports code reuse—in particular via the software library.
Procedural programming is classified as an imperative programming, because it involves direct command of execution.
Also classified as imperative, object-oriented programming (OOP) involves dividing a program implementation into objects that expose behavior (methods) and data (members) via a well-defined interface. In contrast, procedural programming is about dividing the program implementation into variables, data structures, and subroutines. An important distinction is that while procedural involves procedures to operate on data structures, OOP bundles the two together. An object is a data structure and the behavior associated with that data structure.[3]
The principles of modularity and code reuse in functional languages are fundamentally the same as in procedural languages, since they both stem from structured programming. For example:
The main difference between the styles is that functional programming languages remove or at least deemphasize the imperative elements of procedural programming.  The feature set of functional languages is therefore designed to support writing programs as much as possible in terms of pure functions:
There do exist a few esoteric functional languages (like Unlambda) that eschew structured programming precepts for the sake of being difficult to program in (and therefore challenging).  These languages are the exception to the common ground between procedural and functional languages.
In logic programming, a program is a set of premises, and computation is performed by attempting to prove candidate theorems. From this point of view, logic programs are declarative, focusing on what the problem is, rather than on how to solve it.
However, the backward reasoning technique, implemented by SLD resolution, used to solve problems in logic programming languages such as Prolog, treats programs as goal-reduction procedures. Thus  clauses of the form:
ABC is an imperative general-purpose programming language and integrated development environment (IDE) developed at Centrum Wiskunde & Informatica (CWI), in  Amsterdam, Netherlands by Leo Geurts, Lambert Meertens, and Steven Pemberton.[2] It is interactive, structured, high-level, and intended to be used instead of BASIC, Pascal, or AWK. It is intended for teaching or prototyping, but not as a systems-programming language.
ABC was originally a monolithic implementation, leading to an inability to adapt to new requirements, such as creating a graphical user interface (GUI). ABC could not directly access the underlying file system and operating system.
The full ABC system includes a programming environment with a structure editor (syntax-directed editor), suggestions, static variables (persistent), and multiple workspaces, and is available as an interpreter–compiler. As of 2020[update], the latest version is 1.05.02, and it is ported to Unix, DOS, Atari, and Apple MacOS.
Logic programming is a programming, database and knowledge representation paradigm based on formal logic. A logic program is a set of sentences in logical form, representing knowledge about some problem domain. Computation is performed by applying logical reasoning to that knowledge, to solve problems in the domain.  Major logic programming language families include Prolog, Answer Set Programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:
Logic programming languages that include negative conditions have the knowledge representation capabilities of a non-monotonic logic.
Much of the research in the field of logic programming has been concerned with trying to develop a logical semantics for negation as failure and with developing other semantics and other implementations for negation. These developments have been important, in turn, for supporting the development of formal methods for logic-based program verification and program transformation.
The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green.[3] This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language that places no constraints on the order in which operations are performed.[4]
Logic programming, with its current syntax of facts and rules, can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in artificial intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.[5]
Although it was based on the proof methods of logic, Planner, developed by Carl Hewitt at MIT, was the first language to emerge within this proceduralist paradigm.[6] Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. Winograd used Micro-Planner to implement the landmark, natural-language understanding program SHRDLU.[7] For the sake of efficiency, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA4,[8] Popler,[9] Conniver,[10] QLISP,[11] and the concurrent language Ether.[12]
Colmerauer, with Philippe Roussel, used the procedural interpretation as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David H. D. Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp.[17] Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.
Logic programming gained international attention during the 1980s, when it was chosen by the Japanese  Ministry of International Trade and Industry to develop the software for the Fifth Generation Computer Systems (FGCS) project. The FGCS project aimed to use logic programming to develop advanced Artificial Intelligence applications on massively parallel computers. Although the project initially explored the use of Prolog, it later adopted the use of concurrent logic programming, because it was closer to the FGCS computer architecture.
However, the committed choice feature of concurrent logic programming interfered with the language's logical semantics[18] and with its suitability for knowledge representation and problem solving applications. Moreover, the parallel computer systems developed in the project failed to compete with advances taking place in the development of more conventional, general-purpose computers. Together these two issues resulted in the FGCS project failing to meet its objectives. Interest in both logic programming and AI fell into world-wide decline.[19]
In the meanwhile, more declarative logic programming approaches, including those based on the use of Prolog, continued to make progress independently of the FGCS project. In particular, although Prolog was developed to combine declarative and procedural representations of knowledge, the purely declarative interpretation of logic programs became the focus for applications in the field of deductive databases. Work in this field became prominent around 1977, when Hervé Gallaire and Jack Minker organized a workshop on logic and databases in Toulouse.[20] The field was eventually renamed as Datalog.
This focus on the logical, declarative reading of logic programs was given further impetus by the development of constraint logic programming in the 1980s and Answer Set Programming in the 1990s. It is also receiving renewed emphasis in recent applications of Prolog[21]
The Association for Logic Programming (ALP) was founded in 1986 to promote Logic Programming. Its official journal until 2000, was The Journal of Logic Programming. Its  founding editor-in-chief was J. Alan Robinson.[22]  In 2001, the journal was renamed The Journal of Logic and Algebraic Programming, and the official journal of ALP became Theory and Practice of Logic Programming, published by Cambridge University Press.
Logic programs enjoy a rich variety of semantics and problem solving methods, as well as a wide range of applications in programming, databases, knowledge representation and problem solving.
The procedural interpretation of logic programs, which uses backward reasoning to reduce goals to subgoals, is a special case of the use of a problem-solving strategy to control the use of a declarative, logical representation of knowledge to obtain the behaviour of an algorithm. More generally, different problem-solving strategies can be applied to the same logical representation to obtain different algorithms. Alternatively, different algorithms can be obtained with a given problem-solving strategy by using different logical representations.[23]
In the more general, non-propositional case, where sub-goals can share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies.[26] Such strategies are used, for example, in concurrent logic programming.
In most cases, backward reasoning from a query or goal is more efficient than forward reasoning. But sometimes with Datalog and Answer Set Programming, there may be no query that is separate from the set of clauses as a whole, and then generating all the facts that can be derived from the clauses is a sensible problem-solving strategy. Here is another example, where forward reasoning beats backward reasoning in a more conventional computation task, where the goal ?- fibonacci(n, Result) is to find the nth fibonacci number:
Logic programming can be viewed as a generalisation of functional programming, in which functions are a special case of relations.[28]
For example, the function, mother(X) = Y, (every X has only one mother Y) can be represented by the relation mother(X, Y). In this respect, logic programs are similar to relational databases, which also represent functions as relations.
The term relational programming has been used to cover a variety of programming languages that treat functions as a special case of relations. Some of these languages, such as miniKanren[28]
and relational linear programming[30]
are logic programming languages in the sense of this article.
However, the relational language RML is an imperative programming language
[31] whose core construct is a
relational expression, which is similar to an expression in first-order predicate logic.
Other relational programming languages are based on the relational calculus[32] or relational algebra.[33]
This property of withdrawing a conclusion when new information is added, is called non-monotonicity, and it makes logic programming a non-monotonic logic.
In the alternative stable model semantics, there may be no intended models or several intended models, all of which are minimal and two-valued. The stable model semantics underpins answer set programming (ASP).
Attempts to understand negation in logic programming have also contributed to the development of abstract argumentation frameworks.[41] In an argumentation interpretation of negation, the initial argument that tom should be punished because he is a thief, is attacked by the argument that he should be rehabilitated because he is a minor. But the fact that tom is violent undermines the argument that tom should be rehabilitated and reinstates the argument that tom should be punished.
Metaprogramming, in which programs are treated as data, was already a feature of early Prolog implementations.[42][43] For example, the Edinburgh DEC10 implementation of Prolog included "an interpreter and a compiler, both written in Prolog itself".[43] The simplest metaprogram is the so-called "vanilla" meta-interpreter:
Metaprogramming is an application of the more general use of a metalogic or metalanguage  to describe and reason about another language, called the object language.
Metalogic programming allows object-level and metalevel representations to be combined, as in natural language. For example, in the following program, the atomic formula attends(Person, Meeting) occurs both as an object-level formula, and as an argument of the metapredicates prohibited and approved.
He states that "unlike logic, rule-based systems can also easily represent strategic information
about what to do" (page 45). For example, "IF you want to go home for the weekend, and you have bus fare, THEN
you can catch a bus". He does not observe that the same strategy of reducing a goal to subgoals can be interpreted, in the manner of logic programming, as applying backward reasoning to a logical conditional:
All of these characteristics of rule-based systems - search, forward and backward reasoning, default reasoning, and goal-reduction - are also defining characteristics of logic programming. This suggests that Thagard's conclusion (page 56) that:
Other arguments showing how logic programming can be used to model aspects of human thinking are presented by Keith Stenning and Michiel van Lambalgen in their book,
Human Reasoning and Cognitive Science.[45] They show how the non-monotonic character of logic programs can be used to explain human performance on a variety of psychological tasks. They also show (page 237) that  "closed–world reasoning in its guise as logic programming has an appealing neural implementation, unlike classical logic."
In The Proper Treatment of Events,[46]
Michiel van Lambalgen and Fritz Hamm investigate the use of constraint logic programming to code "temporal notions in natural language by looking at the way human beings construct time".
The use of logic to represent procedural knowledge and strategic information was one of the main goals contributing to the early development of logic programming. Moreover, it continues to be an important feature of the Prolog family of logic programming languages today. However, many applications of logic programming, including Prolog applications, increasingly focus on the use of logic to represent purely declarative knowledge. These applications include both the representation of general commonsense knowledge and the representation of domain specific expertise.
Logic programming has also proved to be useful for representing domain-specific expertise in expert systems.[48] But human expertise, like general-purpose commonsense, is mostly implicit and tacit, and it is often difficult to represent such implicit knowledge in explicit rules. This difficulty does not arise, however, when logic programs are used to represent the existing, explicit rules of a business organisation or legal authority.
Historically, the representation of a large portion of the British Nationality Act as a logic program in the 1980s[49] was "hugely influential for the development of computational representations of legislation, showing how logic programming enables intuitively appealing representations that can be directly deployed to generate automatic inferences".[50]
Various extensions of logic programming have been developed to provide a logical framework for such destructive change of state.[53][54][55]
Prolog has also contributed to the development of other programming languages, including ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog.
Constraint logic programming (CLP) combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of a clause. Constraint predicates are not defined by the facts and rules in the program, but are predefined by some domain-specific model-theoretic structure or theory.
Procedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are simplified and checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.
The following constraint logic program represents a toy temporal database of john's history as a teacher:
Here ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:
Constraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.
Datalog is a database definition language, which combines a relational view of data, as in relational databases, with a logical view, as in logic programming.
Relational databases use a relational calculus or relational algebra, with relational operations, such as union, intersection, set difference and cartesian product to specify queries, which access  a database. Datalog uses logical connectives, such as or, and and not in the bodies of rules to define relations as part of the database itself.
It was recognized early in the development of relational databases that recursive queries cannot be expressed in either relational algebra or relational calculus, and that this defficiency can be remedied by introducing a least-fixed-point operator.[56][57] In contrast, recursive relations can be defined naturally by rules in logic programs, without the need for any new logical connectives or operators.
Datalog differs from more general logic programming by having only constants and variables as terms. Moreover, all facts are variable-free, and rules are restricted, so that if they are executed bottom-up, then the derived facts are also variable-free.
Like Datalog, Answer Set programming (ASP) is not Turing-complete. Moreover, instead of separating goals (or queries) from the program to be used in solving the goals, ASP  treats the whole program as a goal, and solves the goal by generating a stable model that makes the goal true. For this purpose, it uses the stable model semantics, according to which a logic program can have zero, one or more intended models. For example, the following program represents a degenerate variant of the map colouring problem of colouring two countries red or green:
Confusingly, constraints in ASP are different from constraints in CLP. Constraints in CLP are predicates that qualify answers to queries (and solutions of goals). Constraints in ASP are clauses that eliminate models that would otherwise satisfy goals. Constraints in ASP are like integrity constraints in databases.
This combination of ordinary logic programming clauses and constraint clauses illustrates the generate-and-test methodology of problem solving in ASP: The ordinary clauses define a search space of possible solutions, and the constraints filter out unwanted solutions.[58]
Most implementations of ASP proceed in two steps: First they instantiate the program in all possible ways, reducing it to a propositional logic program (known as grounding). Then they apply a propositional logic problem solver, such as the DPLL algorithm or a Boolean SAT solver. However, some implementations, such as s(CASP)[59] use a goal-directed, top-down, SLD resolution-like procedure without
grounding.
Abductive logic programming[60] (ALP), like CLP, extends normal logic programming by allowing the bodies of clauses to contain literals whose predicates are not defined by clauses. In ALP, these predicates are declared as abducible (or assumable), and are used as in abductive reasoning to explain observations, or more generally to add new facts to the program (as assumptions) to solve goals.
Abductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret negation as failure as a form of abductive reasoning.[62]
Inductive logic programming (ILP) is an approach to machine learning that induces logic programs as hypothetical generalisations of positive and negative examples. Given a logic program representing background knowledge and positive examples together with constraints representing negative examples, an ILP system induces a logic program that generalises the positive examples while excluding the negative examples.
Recent work in ILP, combining logic programming, learning and probability, has given rise to the fields of statistical relational learning and probabilistic inductive logic programming.
Concurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).[67]
However, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, ... , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceeds with the subgoals B1, ..., Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of "don't care nondeterminism", rather than "don't know nondeterminism".
Carl Hewitt has argued[68] that, because of the indeterminacy of concurrent computation, concurrent logic programming cannot implement general concurrency. However, according to the logical semantics, any result of a computation of a concurrent logic program is a logical consequence of the program, even though not all logical consequences can be derived.
Concurrent constraint logic programming[69] combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to use only one.
Several researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog[70] and λProlog.[71]
Basing logic programming within linear logic has resulted in the design of logic programming languages that are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO,[72] Lolli,[73] ACL,[74] and Forum.[75] Forum provides a goal-directed interpretation of all linear logic.
Logtalk[77] extends the Prolog programming language with support for objects, protocols, and other OOP concepts. It supports most standard-compliant Prolog systems as backend compilers.
Transaction logic[53] is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2[78] system. Other prototypes are also available.
In computer science, declarative programming is a programming paradigm—a style of building the structure and elements of computer programs—that expresses the logic of a computation without describing its control flow.[1]
Many languages that apply this style attempt to minimize or eliminate side effects by describing what the program must accomplish in terms of the problem domain, rather than describing how to accomplish it as a sequence of the programming language primitives[2] (the how being left up to the language's implementation). This is in contrast with imperative programming, which implements algorithms in explicit steps.[3][4]
Declarative programming often considers programs as theories of a formal logic, and computations as deductions in that logic space. Declarative programming may greatly simplify writing parallel programs.[5]
Common declarative languages include those of database query languages (e.g., SQL, XQuery), regular expressions, logic programming (e.g. Prolog, Datalog, answer set programming), functional programming, configuration management, and algebraic modeling systems.
Declarative programming is often defined as any style of programming that is not imperative. A number of other common definitions attempt to define it by simply contrasting it with imperative programming. For example:
Declarative programming is a non-imperative style of programming in which programs describe their desired results without explicitly listing commands or steps that must be performed. Functional and logic programming languages are characterized by a declarative programming style. In logic programming, programs consist of sentences expressed in logical form, and computation uses those sentences to solve problems, which are also expressed in logical form.
In a pure functional language, such as Haskell, all functions are without side effects, and state changes are only represented as functions that transform the state, which is explicitly represented as a first-class object in the program. Although pure functional languages are non-imperative, they often provide a facility for describing the effect of a function as a series of steps. Other functional languages, such as Lisp, OCaml and Erlang, support a mixture of procedural and functional programming.
Some logic programming languages, such as Prolog, and database query languages, such as SQL, while declarative in principle, also support a procedural style of programming.
Declarative programming is an umbrella term that includes a number of better-known programming paradigms.
Constraint programming states relations between variables in the form of constraints that specify the properties of the target solution. The set of constraints is solved by giving a value to each variable so that the solution is consistent with the maximum number of constraints. Constraint programming often complements other paradigms: functional, logical, or even imperative programming.
Well-known examples of declarative domain-specific languages (DSLs) include the yacc parser generator input language, QML, the Make build specification language, Puppet's configuration management language, regular expressions, Datalog, answer set programming and a subset of SQL (SELECT queries, for example). DSLs have the advantage of being useful while not necessarily needing to be Turing-complete, which makes it easier for a language to be purely declarative.
As of 2013[update], some software systems[which?] combine traditional user-interface markup languages (such as HTML) with declarative markup that defines what (but not how) the back-end server systems should do to support the declared interface. Such systems, typically using a domain-specific XML namespace, may include abstractions of SQL database syntax or parameterized calls to web services using representational state transfer (REST) and SOAP.[citation needed]
Functional programming languages such as Haskell, Scheme, and ML evaluate expressions via function application. Unlike the related but more imperative paradigm of procedural programming, functional programming places little emphasis on explicit sequencing. Instead, computations are characterised by various kinds of recursive higher-order function application and composition, and as such can be regarded simply as a set of mappings between domains and codomains. Many functional languages, including most of those in the ML and Lisp families, are not purely functional, and thus allow the introduction of stateful effects in programs.
Logic programming languages, such as Prolog, Datalog and answer set programming, compute by proving that a goal is a logical consequence of the program, or by showing that the goal is true in a model defined by the program. Prolog computes by reducing goals to subgoals, top-down using backward reasoning, whereas most Datalog systems compute bottom-up using forward reasoning. Answer set programs typically use SAT solvers to generate a model of the program.
Models, or mathematical representations, of physical systems may be implemented in computer code that is declarative. The code contains a number of equations, not imperative assignments, that describe ("declare") the behavioral relationships. When a model is expressed in this formalism, a computer is able to perform algebraic manipulations to best formulate the solution algorithm. The mathematical causality is typically imposed at the boundaries of the physical system, while the behavioral description of the system itself is declarative or acausal. Declarative modeling languages and environments include Analytica, Modelica and Simile.[8]
Lisp is a family of programming languages loosely inspired by mathematical notation and Alonzo Church's lambda calculus. Some dialects, such as Common Lisp, are primarily imperative but support functional programming. Others, such as Scheme, are designed for functional programming.
Prolog (1972) stands for "PROgramming in LOGic." It was developed for natural language question answering,[12] using SL resolution[13] both to deduce answers to queries and to parse and generate natural language sentences.
Prolog executes programs top-down, using SLD resolution to reason backwards, reducing goals to subgoals. In this example, it uses the last rule of the program to reduce the goal of answering the query eat(X,jerry) to the subgoals of first finding an X such that big(X) holds and then of showing that small(jerry) holds. It repeatedly uses rules to further reduce subgoals to other subgoals, until it eventually succeeds in unifying all subgoals with facts in the program. This backward reasoning, goal-reduction strategy treats rules in logic programs as procedures, and makes Prolog both a declarative and procedural programming language.[14]
The origins of Datalog date back to the beginning of logic programming, but it was identified as a separate area around 1977. Syntactically and semantically, it is a subset of Prolog. But because it does not have compound terms, it is not Turing-complete.
Datalog has been applied to such problems as data integration, information extraction, networking, security, cloud computing and machine learning.[16][17]
Answer set programming (ASP) evolved in the late 1990s, based on the stable model (answer set) semantics of logic programming. Like Datalog, it is a subset of Prolog; and, because it does not have compound terms, it is not Turing-complete.
Most implementations of ASP execute a program by first "grounding" the program, replacing all variables in rules by constants in all possible ways, and then using a propositional SAT solver, such as the DPLL algorithm to generate one or more models of the program.
Lisp (historically LISP, an abbreviation of "list processing") is a family of programming languages with a long history and a distinctive, fully parenthesized prefix notation.[3]
Originally specified in the late 1950s, it is the second-oldest high-level programming language still in common use, after Fortran.[4][5] Lisp has changed since its early days, and many dialects have existed over its history. Today, the best-known general-purpose Lisp dialects are Common Lisp, Scheme, Racket, and Clojure.[6][7][8]
Lisp was originally created as a practical mathematical notation for computer programs, influenced by (though not originally derived from)[9] the notation of Alonzo Church's lambda calculus. It quickly became a favored programming language for artificial intelligence (AI) research.[10] As one of the earliest programming languages, Lisp pioneered many ideas in computer science, including tree data structures, automatic storage management, dynamic typing, conditionals, higher-order functions, recursion, the self-hosting compiler,[11] and the read–eval–print loop.[12]
The name LISP derives from "LISt Processor".[13] Linked lists are one of Lisp's major data structures, and Lisp source code is made of lists. Thus, Lisp programs can manipulate source code as a data structure, giving rise to the macro systems that allow programmers to create new syntax or new domain-specific languages embedded in Lisp.
John McCarthy began developing Lisp in 1958 while he was at the Massachusetts Institute of Technology (MIT). He was motivated by a desire to create an AI programming language that would work on the IBM 704, as he believed that "IBM looked like a good bet to pursue Artificial Intelligence research vigorously."[14] He was inspired by Information Processing Language, which was also based on list processing, but did not use it because it was designed for different hardware and he found an algebraic language more appealing.[14] Due to these factors, he consulted on the design of the Fortran List Processing Language, which was implemented as a Fortran library. However, he was dissatisfied with it because it did not support recursion or a modern if-then-else statement (which was a new concept when lisp was first introduced) [note 1].[14]
McCarthy published Lisp's design in a paper in Communications of the ACM on April 1, 1960, entitled "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I"[19].[20] He showed that with a few simple operators and a notation for anonymous functions borrowed from Church, one can build a Turing-complete language for algorithms.
During the 1980s and 1990s, a great effort was made to unify the work on new Lisp dialects (mostly successors to Maclisp such as ZetaLisp and NIL (New Implementation of Lisp) into a single language. The new language, Common Lisp, was somewhat compatible with the dialects it replaced (the book Common Lisp the Language notes the compatibility of various constructs). In 1994, ANSI published the Common Lisp standard, "ANSI X3.226-1994 Information Technology Programming Language Common Lisp".
Since inception, Lisp was closely connected with the artificial intelligence research community, especially on PDP-10[23] systems. Lisp was used as the implementation of the language Micro Planner, which was used in the famous AI system SHRDLU. In the 1970s, as AI research spawned commercial offshoots, the performance of existing Lisp systems became a growing issue, as programmers needed to be familiar with the performance ramifications of the various techniques and choices involved in the implementation of Lisp.[24]
Over its sixty-year history, Lisp has spawned many variations on the core theme of an S-expression language. Some of these variations have been standardized and implemented by different groups with different priorities (for example, both Common Lisp and Scheme have multiple implementations). However, in other cases a software project defines a lisp without a standard and there is no clear distinction between the dialect and the implementation (for example, Clojure and Emacs Lisp fall into this category).
The Scheme community actively maintains over twenty implementations. Several significant new implementations (Chicken, Gambit, Gauche, Ikarus, Larceny, Ypsilon) have been developed in the 2000s (decade). The Revised5 Report on the Algorithmic Language Scheme[46] standard of Scheme was widely accepted in the Scheme community. The Scheme Requests for Implementation process has created a lot of quasi-standard libraries and extensions for Scheme. User communities of individual Scheme implementations continue to grow. A new language standardization process was started in 2003 and led to the R6RS Scheme standard in 2007. Academic use of Scheme for teaching computer science seems to have declined somewhat. Some universities are no longer using Scheme in their computer science introductory courses;[47][48] MIT now uses Python instead of Scheme for its undergraduate computer science program and MITx massive open online course.[49][50]
Common Lisp is a successor to Maclisp. The primary influences were Lisp Machine Lisp, Maclisp, NIL, S-1 Lisp, Spice Lisp, and Scheme.[51] It has many of the features of Lisp Machine Lisp (a large Lisp dialect used to program Lisp Machines), but was designed to be efficiently implementable on any personal computer or workstation. Common Lisp is a general-purpose programming language and thus has a large language standard including many built-in data types, functions, macros and other language elements, and an object system (Common Lisp Object System). Common Lisp also borrowed certain features from Scheme such as lexical scoping and lexical closures. Common Lisp implementations are available for targeting different platforms such as the LLVM,[52] the Java virtual machine,[53]
x86-64, PowerPC, Alpha, ARM, Motorola 68000, and MIPS,[54] and operating systems such as Windows, macOS, Linux, Solaris, FreeBSD, NetBSD, OpenBSD, Dragonfly BSD, and Heroku.[55]
Scheme is a statically scoped and properly tail-recursive dialect of the Lisp programming language invented by Guy L. Steele, Jr. and Gerald Jay Sussman. It was designed to have exceptionally clear and simple semantics and few different ways to form expressions. Designed about a decade earlier than Common Lisp, Scheme is a more minimalist design. It has a much smaller set of standard features but with certain implementation features (such as tail-call optimization and full continuations) not specified in Common Lisp. A wide variety of programming paradigms, including imperative, functional, and message passing styles, find convenient expression in Scheme. Scheme continues to evolve with a series of standards (Revisedn Report on the Algorithmic Language Scheme) and a series of Scheme Requests for Implementation.
Lisp was the first language where the structure of program code is represented faithfully and directly in a standard data structure—a quality much later dubbed "homoiconicity". Thus, Lisp functions can be manipulated, altered or even created within a Lisp program without lower-level manipulations. This is generally considered one of the main advantages of the language with regard to its expressive power, and makes the language suitable for syntactic macros and meta-circular evaluation.
Lisp deeply influenced Alan Kay, the leader of the research team that developed Smalltalk at Xerox PARC; and in turn Lisp was influenced by Smalltalk, with later dialects adopting object-oriented programming features (inheritance classes, encapsulating instances, message passing, etc.) in the 1970s. The Flavors object system introduced the concept of multiple inheritance and the mixin. The Common Lisp Object System provides multiple inheritance, multimethods with multiple dispatch, and first-class generic functions, yielding a flexible and powerful form of dynamic dispatch. It has served as the template for many subsequent Lisp (including Scheme) object systems, which are often implemented via a metaobject protocol, a reflective meta-circular design in which the object system is defined in terms of itself: Lisp was only the second language after Smalltalk (and is still one of the very few languages) to possess such a metaobject system. Many years later, Alan Kay suggested that as a result of the confluence of these features, only Smalltalk and Lisp could be regarded as properly conceived object-oriented programming systems.[63]
Lisp introduced the concept of automatic garbage collection, in which the system walks the heap looking for unused memory. Progress in modern sophisticated garbage collection algorithms such as generational garbage collection was stimulated by its use in Lisp.[64]
The use of parentheses is Lisp's most immediately obvious difference from other programming language families. As a result, students have long given Lisp nicknames such as Lost In Stupid Parentheses, or Lots of Irritating Superfluous Parentheses.[67] However, the S-expression syntax is also responsible for much of Lisp's power: the syntax is simple and consistent, which facilitates manipulation by computer. However, the syntax of Lisp is not limited to traditional parentheses notation. It can be extended to include alternative notations. For example, XMLisp is a Common Lisp extension that employs the metaobject protocol to integrate S-expressions with the Extensible Markup Language (XML).
The reliance on expressions gives the language great flexibility. Because Lisp functions are written as lists, they can be processed exactly like data. This allows easy writing of programs which manipulate other programs (metaprogramming). Many Lisp dialects exploit this feature using macro systems, which enables extension of the language almost without limit.
As more data types were introduced in later Lisp dialects, and programming styles evolved, the concept of an atom lost importance.[citation needed] Many dialects still retained the predicate atom for legacy compatibility,[citation needed] defining it true for any object which is not a cons.
Of the many data structures that can be built out of cons cells, one of the most basic is called a proper list. A proper list is either the special nil (empty list) symbol, or a cons in which the car points to a datum (which may be another cons structure, such as a list), and the cdr points to another proper list.
Because conses and lists are so universal in Lisp systems, it is a common misconception that they are Lisp's only data structures. In fact, all but the most simplistic Lisps have other data structures, such as vectors (arrays), hash tables, structures, and so forth.
Aficionados of functional programming avoid destructive functions. In the Scheme dialect, which favors the functional style, the names of destructive functions are marked with a cautionary exclamation point, or "bang"—such as set-car! (read set car bang), which replaces the car of a cons. In the Common Lisp dialect, destructive functions are commonplace; the equivalent of set-car! is named rplaca for "replace car". This function is rarely seen, however, as Common Lisp includes a special facility, setf, to make it easier to define and use destructive functions. A frequent style in Common Lisp is to write code functionally (without destructive calls) when prototyping, then to add destructive calls as an optimization where it is safe to do so.
A fundamental distinction between Lisp and other languages is that in Lisp, the textual representation of a program is simply a human-readable description of the same internal data structures (linked lists, symbols, number, characters, etc.) as would be used by the underlying Lisp system.
Lisp uses this to implement a very powerful macro system. Like other macro languages such as the one defined by the C preprocessor (the macro preprocessor for the C, Objective-C and C++ programming languages), a macro returns code that can then be compiled. However, unlike C preprocessor macros, the macros are Lisp functions and so can exploit the full power of Lisp.
Further, because Lisp code has the same structure as lists, macros can be built with any of the list-processing functions in the language. In short, anything that Lisp can do to a data structure, Lisp macros can do to code. In contrast, in most other languages, the parser's output is purely internal to the language implementation and cannot be manipulated by the programmer.
The read function accepts textual S-expressions as input, and parses them into an internal data structure. For instance, if you type the text (+ 1 2) at the prompt, read translates this into a linked list with three elements: the symbol +, the number 1, and the number 2. It so happens that this list is also a valid piece of Lisp code; that is, it can be evaluated. This is because the car of the list names a function—the addition operation.
Programmers in the Scheme dialect often express loops using tail recursion. Scheme's commonality in academic computer science has led some students to believe that tail recursion is the only, or the most common, way to write iterations in Lisp, but this is incorrect. All oft-seen Lisp dialects have imperative-style iteration constructs, from Scheme's do loop to Common Lisp's complex loop expressions. Moreover, the key issue that makes this an objective rather than subjective matter is that Scheme makes specific requirements for the handling of tail calls, and thus the reason that the use of tail recursion is generally encouraged for Scheme is that the practice is expressly supported by the language definition. By contrast, ANSI Common Lisp does not require[73] the optimization commonly termed a tail call elimination. Thus, the fact that tail recursive style as a casual replacement for the use of more traditional iteration constructs (such as do, dolist or loop) is discouraged[74] in Common Lisp is not just a matter of stylistic preference, but potentially one of efficiency (since an apparent tail call in Common Lisp may not compile as a simple jump) and program correctness (since tail recursion may increase stack use in Common Lisp, risking stack overflow).
In contrast to most other major programming languages, Lisp allows implementing control structures using the language. Several control structures are implemented as Lisp macros, and can even be macro-expanded by the programmer who wants to know how they work.
Often, the same algorithm can be expressed in Lisp in either an imperative or a functional style. As noted above, Scheme tends to favor the functional style, using tail recursion and continuations to express control flow. However, imperative style is still quite possible. The style preferred by many Common Lisp programmers may seem more familiar to programmers used to structured languages such as C, while that preferred by Schemers more closely resembles pure-functional languages such as Haskell.
Because of Lisp's early heritage in list processing, it has a wide array of higher-order functions relating to iteration over sequences. In many cases where an explicit loop would be needed in other languages (like a for loop in C) in Lisp the same task can be accomplished with a higher-order function. (The same is true of many functional programming languages.)
Several operating systems, including language-based systems, are based on Lisp (use Lisp features, conventions, methods, data structures, etc.), or are written in Lisp,[77] including:
Genera, renamed Open Genera,[78] by Symbolics; Medley, written in Interlisp, originally a family of graphical operating systems that ran on Xerox's later Star workstations;[79][80] Mezzano;[81] Interim;[82][83] ChrysaLisp,[84] by developers of Tao Systems' TAOS;[85] and also the Guix System for GNU/Linux.
Raku is a member of the Perl family of programming languages.[6] Formerly named Perl 6, it was renamed in October 2019.[7][8] Raku introduces elements of many modern and historical languages. Compatibility with Perl was not a goal, though a compatibility mode is part of the specification. The design process for Raku began in 2000.
The Raku design process was first announced on 19 July 2000, on the fourth day of that year's Perl Conference,[10] by Larry Wall in his State of the Onion 2000 talk.[11] At that time, the primary goals were to remove "historical warts" from the language; "easy things should stay easy, hard things should get easier, and impossible things should get hard"; and a general cleanup of the internal design and application programming interfaces (APIs). The process began with a series of Request for Comments (RFCs). This process was open to all contributors, and left no aspect of the language closed to change.[12]
Once the RFC process was complete, Wall reviewed and classified each of the 361 requests received. He then began the process of writing several "Apocalypses", using the original meaning of the term, "revealing".[13] While the original goal was to write one Apocalypse for each chapter of Programming Perl, it became obvious that, as each Apocalypse was written, previous Apocalypses were being invalidated by later changes. For this reason, a set of Synopses was published, each one relating the contents of an Apocalypse, but with any subsequent changes reflected in updates. Today, the Raku specification is managed through the "roast" testing suite,[14] while the Synopses are kept as a historical reference.[15]
An implication of these goals was that Perl 6 would not have backward compatibility with the existing Perl codebase. This meant that some code which was correctly interpreted by a Perl 5 compiler would not be accepted by a Perl 6 compiler. Since backward compatibility is a common goal when enhancing software, the breaking changes in Perl 6 had to be stated explicitly. The distinction between Perl 5 and Perl 6 became so large that eventually Perl 6 was renamed Raku.
The language's mascot is "Camelia, the Raku bug".[1]
Her name is a nod to the camel mascot associated with Perl, and her form, in the pun-loving tradition of the Perl community, is a play on "software bug". Spiral designs embedded in her butterfly-like wings resemble the characters "P6", the favored nickname for Perl 6, and off-center eye placement is an intentional pun on "Wall-eyed".[19]
Ruby is an interpreted, high-level, general-purpose programming language. It was designed with an emphasis on programming productivity and simplicity. In Ruby, everything is an object, including primitive data types. It was developed in the mid-1990s by Yukihiro "Matz" Matsumoto in Japan.
Ruby is dynamically typed and uses garbage collection and just-in-time compilation. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. According to the creator, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, BASIC, and Lisp.[10][3]
By 2000, Ruby was more popular than Python in Japan.[20] In September 2000, the first English language book Programming Ruby was printed, which was later freely released to the public, further widening the adoption of Ruby amongst English speakers. In early 2002, the English-language ruby-talk mailing list was receiving more messages than the Japanese-language ruby-list, demonstrating Ruby's increasing popularity in the non-Japanese speaking world.
Ruby 3.2 was released on December 25, 2022.[44] It brings support for being run inside of a WebAssembly environment via a WASI interface. Regular expressions also receives some improvements, including a faster, memoized matching algorithm to protect against certain ReDoS attacks, and configurable timeouts for regular expression matching. Additional debugging and syntax features are also included in this release, which include syntax suggestion, as well as error highlighting. The MJIT compiler has been re-implemented as a standard library module, while the YJIT, a Rust-based JIT compiler now supports more architectures on Linux.
Matsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design.[45] At a Google Tech Talk in 2008 he said, "I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language."[46] He stresses that systems design needs to emphasize human, rather than computer, needs:[47]
Often people, especially computer engineers, focus on the machines. They think, "By doing this, the machine will run fast. By doing this, the machine will run more effectively. By doing this, the machine will something something something." They are focusing on machines. But in fact we need to focus on humans, on how humans care about doing programming or operating the application of the machines. We are the masters. They are the slaves.
Everyone has an individual background. Someone may come from Python, someone else may come from Perl, and they may be surprised by different aspects of the language. Then they come up to me and say, 'I was surprised by this feature of the language, so Ruby violates the principle of least surprise.' Wait. Wait. The principle of least surprise is not for you only. The principle of least surprise means principle of least my surprise. And it means the principle of least surprise after you learn Ruby very well. For example, I was a C++ programmer before I started designing Ruby. I programmed in C++ exclusively for two or three years. And after two years of C++ programming, it still surprises me.
Ruby has been described as a multi-paradigm programming language: it allows procedural programming (defining functions/variables outside classes makes them part of the root, 'self' Object), with object orientation (everything is an object) or functional programming (it has anonymous functions, closures, and continuations; statements all have values, and functions return the last evaluation). It has support for introspection, reflective programming, metaprogramming, and interpreter-based threads. Ruby features dynamic typing, and supports parametric polymorphism.
One of the differences from Python and Perl is that Ruby keeps all of its instance variables completely private to the class and only exposes them through accessor methods (attr_writer, attr_reader, etc.). Unlike the "getter" and "setter" methods of other languages like C++ or Java, accessor methods in Ruby can be created with a single line of code via metaprogramming; however, accessor methods can also be created in the traditional fashion of C++ and Java. As invocation of these methods does not require the use of parentheses, it is trivial to change an instance variable into a full function, without modifying a single line of calling code or having to do any refactoring achieving similar functionality to C# and VB.NET property members.
Matsumoto originally developed Ruby on the 4.3BSD-based Sony NEWS-OS 3.x, but later migrated his work to SunOS 4.x, and finally to Linux.[61][62] By 1999, Ruby was known to work across many different operating systems. Modern Ruby versions and implementations are available on all major desktop, mobile and server-based operating systems. Ruby is also supported across a number of cloud hosting platforms like Jelastic, Heroku, Google Cloud Platform and others.
The Ruby Application Archive, which hosted applications, documentation, and libraries for Ruby programming, was maintained until 2013, when its function was transferred to RubyGems.[65]
This is an index to notable programming languages, in current or historical use. Dialects of BASIC, esoteric programming languages, and markup languages are not included. A programming language does not need to be imperative or Turing-complete, but must be executable and so does not include markup languages such as HTML or XML, but does include domain-specific languages such as SQL and its dialects.
Ada is a structured, statically typed, imperative, and object-oriented high-level programming language, inspired by Pascal and other languages. It has built-in language support for design by contract (DbC), extremely strong typing, explicit concurrency, tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international technical standard, jointly defined by the International Organization for Standardization (ISO), and the International Electrotechnical Commission (IEC). As of May 2023[update], the standard, ISO/IEC 8652:2023, is called Ada 2022 informally.[11]
Ada was originally designed by a team led by French computer scientist Jean Ichbiah of Honeywell under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede over 450 programming languages then used by the DoD.[12] Ada was named after Ada Lovelace (1815–1852), who has been credited as the first computer programmer.[13]
Ada was originally designed for embedded and real-time systems. The Ada 95 revision, designed by S. Tucker Taft of Intermetrics between 1992 and 1995, improved support for systems, numerical, financial, and object-oriented programming (OOP).
Features of Ada include: strong typing, modular programming mechanisms (packages), run-time checking, parallel processing (tasks, synchronous message passing, protected objects, and nondeterministic select statements), exception handling, and generics. Ada 95 added support for object-oriented programming, including dynamic dispatch.
Ada is designed for developing very large software systems. Ada packages can be compiled separately. Ada package specifications (the package interface) can also be compiled separately without the implementation to check for consistency. This makes it possible to detect problems early during the design phase, before implementation starts.
A large number of compile-time checks are supported to help avoid bugs that would not be detectable until run-time in some other languages or would require explicit checks to be added to the source code.  For example, the syntax requires explicitly named closing of blocks to prevent errors due to mismatched end tokens. The adherence to strong typing allows detecting many common software errors (wrong parameters, range violations, invalid references, mismatched types, etc.) either during compile-time, or otherwise during run-time. As concurrency is part of the language specification, the compiler can in some cases detect potential deadlocks.[14] Compilers also commonly check for misspelled identifiers, visibility of packages, redundant declarations, etc. and can provide warnings and useful suggestions on how to fix the error.
One notable free software tool that is used by many Ada programmers to aid them in writing Ada source code is the GNAT Programming Studio, and GNAT which is part of the GNU Compiler Collection.
In the 1970s the US Department of Defense (DoD) became concerned by the number of different programming languages being used for its embedded computer system projects, many of which were obsolete or hardware-dependent, and none of which supported safe modular programming. In 1975, a working group, the High Order Language Working Group (HOLWG), was formed with the intent to reduce this number by finding or creating a programming language generally suitable for the department's and the UK Ministry of Defence's requirements. After many iterations beginning with an original straw-man proposal[19] the eventual programming language was named Ada. The total number of high-level programming languages in use for such projects fell from over 450 in 1983 to 37 by 1996.
HOLWG crafted the Steelman language requirements , a series of documents stating the requirements they felt a programming language should satisfy. Many existing languages were formally reviewed, but the team concluded in 1977 that no existing language met the specifications. The requirements were created by the United States Department of Defense in The Department of Defense Common High Order Language program in 1978. The predecessors of this document were called, in order, "Strawman", "Woodenman", "Tinman" and "Ironman".[20] The requirements focused on the needs of embedded computer applications, and emphasised reliability, maintainability, and efficiency. Notably, they included exception handling facilities, run-time checking, and parallel computing.
It was concluded that no existing language met these criteria to a sufficient extent,[21] so a contest was called to create a language that would be closer to fulfilling them. The design that won this contest became the Ada programming language. The resulting language followed the Steelman requirements closely, though not exactly.
Requests for proposals for a new programming language were issued and four contractors were hired to develop their proposals under the names of Red (Intermetrics led by Benjamin Brosgol), Green (Honeywell, led by Jean Ichbiah), Blue (SofTech, led by John Goodenough)[22] and Yellow (SRI International, led by Jay Spitzen). In April 1978, after public scrutiny, the Red and Green proposals passed to the next phase. In May 1979, the Green proposal, designed by Jean Ichbiah at Honeywell, was chosen and given the name Ada—after Augusta Ada King, Countess of Lovelace, usually known as Ada Lovelace. This proposal was influenced by the language LIS that Ichbiah and his group had developed in the 1970s. The preliminary Ada reference manual was published in ACM SIGPLAN Notices in June 1979. The Military Standard reference manual was approved on December 10, 1980 (Ada Lovelace's birthday), and given the number MIL-STD-1815 in honor of Ada Lovelace's birth year. In 1981, Tony Hoare took advantage of his Turing Award speech to criticize Ada for being overly complex and hence unreliable,[23] but subsequently seemed to recant in the foreword he wrote for an Ada textbook.[24]
Ada attracted much attention from the programming community as a whole during its early days. Its backers and others predicted that it might become a dominant language for general purpose programming and not only defense-related work.[25] Ichbiah publicly stated that within ten years, only two programming languages would remain: Ada and Lisp.[26]  Early Ada compilers struggled to implement the large, complex language, and both compile-time and run-time performance tended to be slow and tools primitive.[25]   Compiler vendors expended most of their efforts in passing the massive, language-conformance-testing, government-required Ada Compiler Validation Capability (ACVC) validation suite that was required in another novel feature of the Ada language effort.[26]
In 1991, the US Department of Defense began to require the use of Ada (the Ada mandate) for all software,[30] though exceptions to this rule were often granted.[25]  The Department of Defense Ada mandate was effectively removed in 1997, as the DoD began to embrace commercial off-the-shelf (COTS) technology.[25] Similar requirements existed in other NATO countries: Ada was required for NATO systems involving command and control and other functions, and Ada was the mandated or preferred language for defense-related applications in countries such as Sweden, Germany, and Canada.[31]
Because of Ada's safety-critical support features, it is now used not only for military applications, but also in commercial projects where a software bug can have severe consequences, e.g., avionics and air traffic control, commercial rockets such as the Ariane 4 and 5, satellites and other space systems, railway transport and banking.[16]
For example, the Primary Flight Control System, the fly-by-wire system software in the Boeing 777, was written in Ada, as were the fly-by-wire systems for the aerodynamically unstable Eurofighter Typhoon,[32] Saab Gripen,[33] Lockheed Martin F-22 Raptor and the DFCS replacement flight control system for the Grumman F-14 Tomcat. The Canadian Automated Air Traffic System was written in 1 million lines of Ada (SLOC count). It featured advanced distributed processing, a distributed Ada database, and object-oriented design. Ada is also used in other air traffic systems, e.g., the UK's next-generation Interim Future Area Control Tools Support (iFACTS) air traffic control system is designed and implemented using SPARK Ada.[34]
It is also used in the French TVM in-cab signalling system on the TGV high-speed rail system, and the metro suburban trains in Paris, London, Hong Kong and New York City.[16][35]
The Ada 95 revision of the language went beyond the Steelman requirements, targeting general-purpose systems in addition to embedded ones, and adding features supporting object-oriented programming.[36]
Ada 95, the joint ISO/IEC/ANSI standard ISO/IEC 8652:1995[41][42] was published in February 1995, making it the first ISO standard object-oriented programming language. To help with the standard revision and future acceptance, the US Air Force funded the development of the GNAT Compiler. Presently, the GNAT Compiler is part of the GNU Compiler Collection.
On May 2, 2023, the Ada community saw the formal approval of publication of the Ada 2022 edition of the programming language standard.[11]
Ada is an ALGOL-like programming language featuring control structures with reserved words such as if, then, else, while, for, and so on. However, Ada also has many data structuring facilities and other abstractions which were not included in the original ALGOL 60, such as type definitions, records, pointers, enumerations. Such constructs were in part inherited from or inspired by Pascal.
Ada is a structured programming language, meaning that the flow of control is structured into standard statements. All standard constructs and deep-level early exit are supported, so the use of the also supported "go to" commands is seldom needed.
Programming languages are used for controlling the behavior of a machine (often a computer). Like natural languages, programming languages follow rules for syntax and semantics.
There are thousands of programming languages[1] and new ones are created every year. Few languages ever become sufficiently popular that they are used by more than a few people, but professional programmers may use dozens of languages in a career.
Most programming languages are not standardized by an international (or national) standard, even widely used ones, such as Perl or Standard ML (despite the name). Notable standardized programming languages include ALGOL, C, C++, JavaScript (under the name ECMAScript), Smalltalk, Prolog, Common Lisp, Scheme (IEEE standard), ISLISP, Ada, Fortran, COBOL, SQL, and XQuery.
The following table compares general and technical information for a selection of commonly used programming languages. See the individual languages' articles for further information.
Most programming languages will print an error message or throw an exception if an input/output operation or other system call (e.g., chmod, kill) fails, unless the programmer has explicitly arranged for different handling of these events. Thus, these languages fail safely in this regard.
The literature on programming languages contains an abundance of informal claims about their relative expressive power, but there is no framework for formalizing such statements nor for deriving interesting consequences.[52] This table provides two measures of expressiveness from two different sources. An additional measure of expressiveness, in GZip bytes, can be found on the Computer Language Benchmarks Game.[53]
Benchmarks are designed to mimic a particular type of workload on a component or system. The computer programs used for compiling some of the benchmark data in this section may not have been fully optimized, and the relevance of the data is disputed. The most accurate benchmarks are those that are customized to your particular situation. Other people's benchmark data may have some value to others, but proper interpretation brings many challenges. The Computer Language Benchmarks Game site warns against over-generalizing from benchmark data, but contains a large number of micro-benchmarks of reader-contributed code snippets, with an interface that generates various charts and tables comparing specific programming languages and types of tests.[56]
Margaret Elaine Hamilton (née Heafield; born August 17, 1936) is an American computer scientist. She directed the Software Engineering Division at the MIT Instrumentation Laboratory, where she led the development of the on-board flight software for NASA's Apollo Guidance Computer for the Apollo program. She later founded two software companies, Higher Order Software in 1976 and Hamilton Technologies in 1986, both in Cambridge, Massachusetts.
Hamilton has published more than 130 papers, proceedings, and reports, about sixty projects, and six major programs. She coined the term "software engineering", stating "I began to use the term 'software engineering' to distinguish it from hardware and other kinds of engineering, yet treat each type of engineering as part of the overall systems engineering process."[1][2][3]
On November 22, 2016, Hamilton received the Presidential Medal of Freedom from president Barack Obama for her work leading to the development of on-board flight software for NASA's Apollo Moon missions.[4]
In Boston, Hamilton initially intended to enroll in graduate study in abstract mathematics at Brandeis University.[6] However, in mid-1959, Hamilton began working for Edward Norton Lorenz, in the meteorology department at MIT.[14] She developed software for predicting weather, programming on the LGP-30 and the PDP-1 computers at Marvin Minsky's Project MAC.[15][16][17] Her work contributed to Lorenz's publications on chaos theory. At the time, computer science and software engineering were not yet established disciplines; instead, programmers learned on the job with hands-on experience.[18] She moved on to another project in the summer of 1961, and hired and trained Ellen Fetter as her replacement.[14]
From 1961 to 1963, Hamilton worked on the Semi-Automatic Ground Environment (SAGE) Project at the MIT Lincoln Lab,[16] where she was one of the programmers who wrote software for the prototype AN/FSQ-7 computer (the XD-1), used by the U.S. Air Force to search for possibly unfriendly aircraft.[5] She also wrote software for a satellite tracking project at the Air Force Cambridge Research Laboratories.[16] The SAGE Project was an extension of Project Whirlwind, started by MIT to create a computer system that could predict weather systems and track their movements using simulators. SAGE was soon developed for military use in anti-aircraft air defense. Hamilton said:
What they used to do when you came into this organization as a beginner, was to assign you this program which nobody was able to ever figure out or get to run. When I was the beginner they gave it to me as well. And what had happened was it was tricky programming, and the person who wrote it took delight in the fact that all of his comments were in Greek and Latin. So I was assigned this program and I actually got it to work. It even printed out its answers in Latin and Greek. I was the first one to get it to work.[19]
It was her efforts on this project that made her a candidate for the position at NASA as the lead developer for Apollo flight software.[6]
Hamilton learned of the Apollo project in 1965 and wanted to get involved due to it being "very exciting" as a Moon program.[23] She joined the MIT Instrumentation Laboratory, which developed the Apollo Guidance Computer for the Apollo lunar exploration program. Hamilton was the first programmer hired for the Apollo project at MIT and the first female programmer in the project,[23][24] and later became Director of the Software Engineering Division.[25] She was responsible for the team writing and testing all on-board in-flight software for the Apollo spacecraft's Command and Lunar Module and for the subsequent Skylab space station.[26][27][18] Another part of her team designed and developed the systems software.[28] This included error detection and recovery software such as restarts and the Display Interface Routines (also known as the Priority Displays), which Hamilton designed and developed.[29] She worked to gain hands-on experience during a time when computer science courses were uncommon and software engineering courses did not exist.[18]
Her areas of expertise include: systems design and software development, enterprise and process modeling, development paradigm, formal systems modeling languages, system-oriented objects for systems modeling and development, automated life-cycle environments, methods for maximizing software reliability and reuse, domain analysis, correctness by built-in language properties, open-architecture techniques for robust systems, full life-cycle automation, quality assurance, seamless integration, error detection and recovery techniques, human-machine interface systems, operating systems, end-to-end testing techniques, and life-cycle management techniques.[18][20] These techniques are intended to make code more reliable because they help programmers identify and fix errors sooner in the development process.
In one of the critical moments of the Apollo 11 mission, the Apollo Guidance Computer, together with the on-board flight software, averted an abort of the landing on the Moon. Three minutes before the lunar lander reached the Moon's surface, several computer alarms were triggered. According to software engineer Robert Wills, Buzz Aldrin entered the codes to request that the computer display altitude and other data on the computer’s screen. The system was designed to support seven simultaneous programs running, but Aldrin’s request was the eighth. This action was something he requested many times whilst working in the simulator. The result was a series of unexpected error codes during the live descent. The on-board flight software captured these alarms with the "never supposed to happen displays" interrupting the astronauts with priority alarm displays.[30]
Hamilton had prepared for just this situation years before:
By some accounts, the astronauts had inadvertently left the rendezvous radar switch on, causing these alarms to be triggered (the claim that the radar was left on inadvertently by the astronauts is disputed by Robert Wills with the National Museum of Computing[32]). The computer was overloaded with interrupts caused by incorrectly phased power supplied to the lander's rendezvous radar.[33][34][35] The program alarms indicated "executive overflows", meaning the guidance computer could not complete all of its tasks in real time and had to postpone some of them.[36] The asynchronous executive designed by J. Halcombe Laning[37][33][38] was used by Hamilton's team to develop asynchronous flight software:
Because of the flight software's system-software's error detection and recovery techniques that included its system-wide "kill and recompute" from a "safe place" restart approach to its snapshot and rollback techniques, the Display Interface Routines (AKA the priority displays) together with its man-in-the-loop capabilities were able to be created in order to have the capability to interrupt the astronauts' normal mission displays with priority displays of critical alarms in case of an emergency. This depended on our assigning a unique priority to every process in the software in order to ensure that all of its events would take place in the correct order and at the right time relative to everything else that was going on.[39]
Hamilton's priority alarm displays interrupted the astronauts' normal displays to warn them that there was an emergency "giving the astronauts a go/no go decision (to land or not to land)".[40] Jack Garman, a NASA computer engineer in mission control, recognized the meaning of the errors that were presented to the astronauts by the priority displays and shouted, "Go, go!" and they continued.[41] Paul Curto, a senior technologist who nominated Hamilton for a NASA Space Act Award, called Hamilton's work "the foundation for ultra-reliable software design".[28]
The computer (or rather the software in it) was smart enough to recognize that it was being asked to perform more tasks than it should be performing. It then sent out an alarm, which meant to the astronaut, 'I'm overloaded with more tasks than I should be doing at this time and I'm going to keep only the more important tasks'; i.e., the ones needed for landing ... Actually, the computer was programmed to do more than recognize error conditions. A complete set of recovery programs was incorporated into the software. The software's action, in this case, was to eliminate lower priority tasks and re-establish the more important ones ... If the computer hadn't recognized this problem and taken recovery action, I doubt if Apollo 11 would have been the successful moon landing it was.
In 1976, Hamilton co-founded with Saydean Zeldin a company called Higher Order Software (HOS)[43] to further develop ideas about error prevention and fault tolerance emerging from their experience at MIT working on the Apollo program.[44][45] They created a product called USE.IT, based on the HOS methodology they developed at MIT.[46][47][48] It was successfully used in numerous government programs[49][50] including a project to formalize and implement C-IDEF, an automated version of IDEF, a modeling language developed by the U.S. Air Force in the Integrated Computer-Aided Manufacturing (ICAM) project.[51] In 1980, British-Israeli computer scientist David Harel published a proposal for a structured programming language derived from HOS from the viewpoint of and/or subgoals.[52] Others have used HOS to formalize the semantics of linguistic quantifiers,[53] and to formalize the design of reliable real-time embedded systems.[54]
Hamilton was the CEO of HOS through 1984[44] and left the company in 1985. In March 1986, she founded Hamilton Technologies, Inc. in Cambridge, Massachusetts. The company was developed around the Universal Systems Language (USL) and its associated automated environment, the 001 Tool Suite, based on her paradigm of development before the fact for systems design and software development.[35][55][56]
Hamilton has been credited with naming the discipline of "software engineering".[1][57][58][59] Hamilton details how she came to make up the term "software engineering":
When I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new 'term' per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.[39]
When Hamilton started using the term "software engineering" during the early Apollo missions,[60] software development was not taken seriously compared to other engineering,[61] nor was it regarded as a science. Hamilton was concerned with legitimizing software development as an engineering discipline.[62] Over time the term "software engineering" gained the same respect as any other technical discipline.[57][63] The IEEE Software September/October 2018 issue celebrates the 50th anniversary of software engineering.[64] Hamilton talks about "Errors" and how they influenced her work related to software engineering and how her language, USL, could be used to prevent the majority of "Errors" in a system.[65] With USL, rather than continuing to test for errors, her program was designed to keep most errors out of the system from the beginning.[66] USL was created after her knowledge and experience from the Apollo mission, in which she determined a mathematical theory for systems and software.[67] This method was then, and still is, highly impactful to the field of software engineering. Writing in Wired, Robert McMillan noted: "At MIT she assisted in the creation of the core principles in computer programming as she worked with her colleagues in writing code for the world's first portable computer".[68] Hamilton's innovations go beyond the feats of playing an important role in getting humans to the Moon. According to Wired's Karen Tegan Padir: "She, along with that other early programming pioneer, COBOL inventor Grace Hopper, also deserve tremendous credit for helping to open the door for more women to enter and succeed in STEM fields like software."[69][70]
In 2017, a "Women of NASA" LEGO set went on sale featuring minifigures of Hamilton, Mae Jemison, Sally Ride, and Nancy Grace Roman. The set was initially proposed by Maia Weinstock as a tribute to the women's contributions to NASA history, and Hamilton's section of the set features a recreation of her famous 1969 photo posing with a stack of her software listings.[71][72]
Blender is a free and open-source 3D computer graphics software tool set that runs on Windows, macOS, BSD, Haiku, IRIX and Linux. It is used for creating animated films, visual effects, art, 3D-printed models, motion graphics, interactive 3D applications, and virtual reality. It is also used in creating video games.
Blender was initially developed as an in-house application by the Dutch animation studio NeoGeo (no relation to the video game brand), and was officially launched on January 2, 1994.[13] Version 1.00 was released in January 1995,[14] with the primary author being the company co-owner and software developer Ton Roosendaal. The name Blender was inspired by a song by the Swiss electronic band Yello, from the album Baby, which NeoGeo used in its showreel.[15][16][17] Some design choices and experiences for Blender were carried over from an earlier software application, called Traces, that Roosendaal developed for NeoGeo on the Commodore Amiga platform during the 1987–1991 period.[18]
In May 2002, Roosendaal started the non-profit Blender Foundation, with the first goal to find a way to continue developing and promoting Blender as a community-based open-source project. On July 18, 2002, Roosendaal started the "Free Blender" campaign, a crowdfunding precursor.[20][21] The campaign aimed at open-sourcing Blender for a one-time payment of €100,000 (USD 100,670 at the time), with the money being collected from the community.[22] On September 7, 2002, it was announced that they had collected enough funds and would release the Blender source code. Today, Blender is free and open-source software, largely developed by its community as well as 26 full-time employees and 12 freelancers employed by the Blender Institute.[23]
In February 2002, the fate of the Blender software company, NaN, became evident as it faced imminent closure in March. Nevertheless, one more release was pushed out, Blender 2.25. As a sort of Easter egg and last personal tag, the artists and developers decided to add a 3D model of a chimpanzee head (called a "monkey" in the software). It was created by Willem-Paul van Overbruggen (SLiD3), who named it Suzanne, after the orangutan in the Kevin Smith film Jay and Silent Bob Strike Back.[29]
Cycles supports GPU rendering, which is used to speed up rendering times. There are three GPU rendering modes: CUDA, which is the preferred method for older Nvidia graphics cards; OptiX, which utilizes the hardware ray-tracing capabilities of Nvidia's Turing architecture & Ampere architecture; HIP, which supports rendering on AMD Radeon graphics cards; and oneAPI for Intel and Intel Arc GPUs. The toolkit software associated with these rendering modes does not come within Blender and needs to be separately installed and configured as per their respective source instructions.
The integrator is the core rendering algorithm used for lighting computations. Cycles currently supports a path tracing integrator with direct light sampling. It works well for a variety of lighting setups, but it is not as suitable for caustics and certain other complex lighting situations. Rays are traced from the camera into the scene, bouncing around until they find a light source (a lamp, an object material emitting light, or the world background), or until they are simply terminated based on the number of maximum bounces determined in the light path settings for the renderer. To find lamps and surfaces emitting light, both indirect light sampling (letting the ray follow the surface bidirectional scattering distribution function, or BSDF) and direct light sampling (picking a light source and tracing a ray towards it) are used.[63]
The software supports a variety of 3D file formats for import and export, among them Alembic, 3D Studio (3DS), FBX, DXF, SVG, STL (for 3D printing), UDIM, USD, VRML, WebM, X3D and OBJ.[83]
Historically, Blender has used Phabricator to manage its development but due to the announcement in 2021 that Phabricator would be discontinued,[91] the Blender Institute began work on migrating to another software in early 2022.[92] After extensive debate on what software it should choose[93] it was finally decided to migrate to Gitea.[94] The migration from Phabricator to Gitea is currently a work in progress.[95]
On April 23, 2021, the Blender Foundation announced the Cycles X project, where they improved the Cycles architecture for future development. Key changes included a new kernel, removal of default tiled rendering (replaced by progressive refine), removal of branched path tracing, and the removal of OpenCL support. Volumetric rendering was also replaced with better algorithms.[102][103][104] Cycles X had only been accessible in an experimental branch[105] until September 21, 2021, when it was merged into the Blender 3.0 alpha.[106]
Ubisoft Animation Studio will use Blender to replace its internal content creation software starting in 2020.[142]
The Blender ID is a unified login for Blender software and service users, providing a login for Blender Studio, the Blender Store, the Blender Conference, Blender Network, Blender Development Fund, and the Blender Foundation Certified Trainer Program.[187]
The Blender Open Data is a platform to collect, display, and query benchmark data produced by the Blender community with related Blender Benchmark software.[188]
In mathematics, economics, and computer science, the Gale–Shapley algorithm (also known as the deferred acceptance algorithm,[1] propose-and-reject algorithm,[2] or Boston Pool algorithm[1]) is an algorithm for finding a solution to the stable matching problem. It is named for David Gale and Lloyd Shapley, who published it in 1962, although it had been used for the National Resident Matching Program since the early 1950s. Shapley and Alvin E. Roth (who pointed out its prior application) won the 2012 Nobel Prize in Economics for work including this algorithm.
The stable matching problem seeks to pair up equal numbers of participants of two types, using preferences from each participant. The pairing must be stable: no pair of unmatched participants should mutually prefer each other to their assigned match. In each round of the Gale–Shapley algorithm, unmatched participants of one type propose a match to the next participant on their preference list. Each proposal is accepted if its recipient prefers it to their current match. The resulting procedure is a truthful mechanism from the point of view of the proposing participants, who receive their most-preferred pairing consistent with stability. In contrast, the recipients of proposals receive their least-preferred pairing. The algorithm can be implemented to run in time quadratic in the number of participants, and linear in the size of the input to the algorithm.
The stable matching problem, and the Gale–Shapley algorithm solving it, have widespread real-world applications, including matching American medical students to residencies and French university applicants to schools. For more, see Stable marriage problem § Applications.
In other words, a matching is stable when there is no pair (A, B) where both participants prefer each other to their matched partners. If such a pair exists, the matching is not stable, in the sense that the members of this pair would prefer to leave the system and be matched to each other, possibly leaving other participants unmatched. A stable matching always exists, and the algorithmic problem solved by the Gale–Shapley algorithm is to find one.[3]
The stable matching problem has also been called the stable marriage problem, using a metaphor of marriage between men and women, and many sources describe the Gale–Shapley algorithm in terms of marriage proposals. However, this metaphor has been criticized as both sexist and unrealistic: the steps of the algorithm do not accurately reflect typical or even stereotypical human behavior.[4][5]
In 1962, David Gale and Lloyd Shapley proved that, for any equal number of participants of each type, it is always possible to find a matching in which all pairs are stable. They presented an algorithm to do so.[6][7] In 1984, Alvin E. Roth observed that essentially the same algorithm had already been in practical use since the early 1950s, as the "Boston Pool algorithm" used by the National Resident Matching Program.[1][8]
The Gale–Shapley algorithm involves a number of "rounds" (or "iterations"). In terms of job applicants and employers, it can be expressed as follows:[9]
To implement the algorithm efficiently, each employer needs to be able to find its next applicant quickly, and each applicant needs to be able to compare employers quickly. One way to do this is to number each applicant and each employer from 1 to 



n


{\displaystyle n}

, where 



n


{\displaystyle n}

 is the number of employers and applicants, and to store the following data structures:[10]
Setting up these data structures takes 



O
(

n

2


)


{\displaystyle O(n^{2})}

 time. With these structures it is possible to find an employer with an unfilled position, make an offer from that employer to their next applicant, determine whether the offer is accepted, and update all of the data structures to reflect the results of these steps, in constant time per offer. Once the algorithm terminates, the resulting matching can be read off from the array of employers for each applicant. There can be 



O
(

n

2


)


{\displaystyle O(n^{2})}

 offers before each employer runs out of offers to make, so the total time is 



O
(

n

2


)


{\displaystyle O(n^{2})}

.[10]
There may be many stable matchings for the same system of preferences. This raises the question: which matching is returned by the Gale–Shapley algorithm? Is it the matching better for applicants, for employers, or an intermediate one? As it turns out, the Gale–Shapley algorithm in which employers make offers to applicants always yields the same stable matching (regardless of the order in which job offers are made), and its choice is the stable matching that is the best for all employers and worst for all applicants among all stable matchings.[9]
In a reversed form of the algorithm, each round consists of unemployed applicants writing a single job application to their preferred employer, and the employer either accepting the application (possibly firing an existing employee to do so) or rejecting it. This produces a matching that is best for all applicants and worst for all employers among all stable matchings. These two matchings are the top and bottom elements of the lattice of stable matchings.[12]
In both forms of the algorithm, one group of participants proposes matches, and the other group decides whether to accept or reject each proposal. The matching is always best for the group that makes the propositions, and worst for the group that decides how to handle each proposal.[12]
The Gale–Shapley algorithm is a truthful mechanism from the point of view of the proposing side. This means that no proposer can get a better matching by misrepresenting their preferences. Moreover, the Gale–Shapley algorithm is even group-strategy proof for proposers, i.e., no coalition of proposers can coordinate a misrepresentation of their preferences such that all proposers in the coalition are strictly better-off.[13] However, it is possible for some coalition to misrepresent their preferences such that some proposers are better-off, and the others retain the same partner.[14]
The Gale–Shapley algorithm is non-truthful for the non-proposing participants. Each may be able to misrepresent their preferences and get a better match.[15] A particular form of manipulation is truncation: presenting only the topmost alternatives, implying that the bottom alternatives are not acceptable at all. Under complete information, it is sufficient to consider misrepresentations of the form of truncation strategies. However, successful misrepresentation requires knowledge of the other agents' preferences; without such knowledge, misrepresentation can give an agent a worse assignment. Moreover, even after an agent sees the final matching, they cannot deduce a strategy that would guarantee a better outcome in hindsight. This makes the Gale–Shapley algorithm a regret-free truth-telling mechanism. Moreover, in the Gale–Shapley algorithm, truth-telling is the only strategy that guarantees no regret. The Gale–Shapley algorithm is the only regret-free mechanism in the class of quantile-stable matching mechanisms.[16]
In their original work on the problem, Gale and Shapley considered a more general form of the stable matching problem, suitable for university and college admission. In this problem, each university or college may have its own quota, a target number of students to admit, and the number of students applying for admission may differ from the sum of the quotas, necessarily causing either some students to remain unmatched or some quotas to remain unfilled. Additionally, preference lists may be incomplete: if a university omits a student from their list, it means they would prefer to leave their quota unfilled than to admit that student, and if a student omits a university from their list, it means they would prefer to remain unadmitted than to go to that university. Nevertheless, it is possible to define stable matchings for this more general problem, to prove that stable matchings always exist, and to apply the same algorithm to find one.[6]
A form of the Gale–Shapley algorithm, performed through a real-world protocol rather than calculated on computers, has been used for coordinating higher education admissions in France since 2018, through the Parcoursup system. In this process, over the course of the summer before the start of school, applicants receive offers of admission, and must choose in each round of the process whether to accept any new offer (and if so turn down any previous offer that they accepted). The method is complicated by additional constraints that make the problem it solves not exactly the stable matching problem. It has the advantage that the students do not need to commit to their preferences at the start of the process, but rather can determine their own preferences as the algorithm progresses, on the basis of head-to-head comparisons between offers that they have received. It is important that this process performs a small number of rounds of proposals, so that it terminates before the start date of the schools, but although high numbers of rounds can occur in theory, they tend not to occur in practice.[17] It has been shown theoretically that, if the Gale–Shapley algorithm needs to be terminated early, after a small number of rounds in which every vacant position makes a new offer, it nevertheless produces matchings that have a high ratio of matched participants to unstable pairs.[18]
The history of programming languages spans from documentation of early mechanical computers to modern tools for software development. Early programming languages were highly specialized, relying on mathematical notation and similarly obscure syntax.[1] Throughout the 20th century, research in compiler theory led to the creation of high-level programming languages, which use a more accessible syntax to communicate instructions.
The first high-level programming language was Plankalkül, created by Konrad Zuse between 1942 and 1945.[2] The first high-level language to have an associated compiler was created by Corrado Böhm in 1951, for his PhD thesis.[3] The first commercially available language was FORTRAN (FORmula TRANslation), developed in 1956 (first manual appeared in 1956, but first developed in 1954) by a team led by John Backus at IBM.
In the 1940s, the first recognizably modern electrically powered computers were created. The limited speed and memory capacity forced programmers to write hand-tuned assembly language programs. It was eventually realized that programming in assembly language required a great deal of intellectual effort.[citation needed]
An early proposal for a high-level programming language was Plankalkül, developed by Konrad Zuse for his Z1 computer between 1942 and 1945 but not implemented at the time.[7]
The first functioning programming languages designed to communicate instructions to a computer were written in the early 1950s. John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer.[8] Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be interpreted into machine code every time it ran, making the process much slower than running the equivalent machine code.
In the early 1950s, Alick Glennie developed Autocode, possibly the first compiled programming language, at the University of Manchester. In 1954, a second iteration of the language, known as the "Mark 1 Autocode", was developed for the Mark 1 by R. A. Brooker. Brooker, with the University of Manchester, also developed an autocode for the Ferranti Mercury in the 1950s. The version for the EDSAC 2 was devised by Douglas Hartree of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances and was noted for its object code optimization and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.
Another early programming language was devised by Grace Hopper in the US, named FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English language programming language and implemented a prototype.[14] The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959.[15] Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendant AIMACO were in use at the time.[16]
Other languages still in use today include LISP (1958), invented by John McCarthy and COBOL (1959), created by the Short Range Committee. Another milestone in the late 1950s was the publication, by a committee of American and European computer scientists, of "a new language for algorithms"; the ALGOL 60 Report (the "ALGOrithmic Language"). This report consolidated many ideas circulating at the time and featured three key language innovations:
The period from the late 1960s to the late 1970s brought a major flowering of programming languages. Most of the major language paradigms now in use were invented in this period:[original research?]
The 1960s and 1970s also saw considerable debate over the merits of "structured programming", which essentially meant programming without the use of goto. A significant fraction of programmers believed that, even in languages that provide goto, it is bad programming style to use it except in rare circumstances. This debate was closely related to language design: some languages had no goto, which forced the use of structured programming.
The 1980s were years of relative consolidation in imperative languages. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the prior decade. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called fifth-generation programming languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Research in Miranda, a functional language with lazy evaluation, began to take hold in this decade.
One important new trend in language design was an increased focus on programming for large-scale systems through the use of modules, or large-scale organizational units of code. Modula, Ada, and ML all developed notable module systems in the 1980s. Module systems were often wedded to generic programming constructs: generics being, in essence, parametrized modules[citation needed] (see also Polymorphism (computer science)).
Although major new paradigms for imperative programming languages did not appear, many researchers expanded on the ideas of prior languages and adapted them to new contexts. For example, the languages of the Argus and Emerald systems adapted object-oriented programming to distributed computing systems.
The 1980s also brought advances in programming language implementation. The reduced instruction set computer (RISC) movement in computer architecture postulated that hardware should be designed for compilers rather than for human assembly programmers. Aided by central processing unit (CPU) speed improvements that enabled increasingly aggressive compiling methods, the RISC movement sparked greater interest in compiler technology for high-level languages.
The rapid growth of the Internet in the mid-1990s was the next major historic event in programming languages. By opening up a radically new platform for computer systems, the Internet created an opportunity for new languages to be adopted. In particular, the JavaScript programming language rose to popularity because of its early integration with the Netscape Navigator web browser. Various other scripting languages achieved widespread use in developing customized applications for web servers such as PHP. The 1990s saw no fundamental novelty in imperative languages, but much recombination and maturation of old ideas. This era began the spread of functional languages. A big driving philosophy was programmer productivity. Many rapid application development (RAD) languages emerged, which usually came with an integrated development environment (IDE), garbage collection, and were descendants of older languages. All such languages were object-oriented. These included Object Pascal, Objective Caml (renamed OCaml), Visual Basic, and Java. Java in particular received much attention.
Some programming languages included other languages in their distribution to save the development time. for example both of Python and Ruby included Tcl to support GUI programming through libraries like Tkinter.
Big Tech companies introduced multiple new programming languages that are designed to serve their needs. for example:
Many Big Tech companies continued introducing new programming languages that are designed to serve their needs and provides first-class support for their platforms. for example:
The development of new programming languages continues, and some new languages appears with focus on providing a replacement for current languages. These new languages try to provide the advantages of a known language like C++ (versatile and fast) while adding safety or reducing complexity. Other new languages try to bring ease of use as provided by Python while adding performance as a priority. Also, the growing of Machine Learning and AI tools still plays a big rule behind these languages' development, where some visual languages focus on integrating these AI tools while other textual languages focus on providing more suitable support for developing them. [22][23][24]
Lisp (historically LISP, an abbreviation of "list processing") is a family of programming languages with a long history and a distinctive, fully parenthesized prefix notation.[3]
Originally specified in the late 1950s, it is the second-oldest high-level programming language still in common use, after Fortran.[4][5] Lisp has changed since its early days, and many dialects have existed over its history. Today, the best-known general-purpose Lisp dialects are Common Lisp, Scheme, Racket, and Clojure.[6][7][8]
Lisp was originally created as a practical mathematical notation for computer programs, influenced by (though not originally derived from)[9] the notation of Alonzo Church's lambda calculus. It quickly became a favored programming language for artificial intelligence (AI) research.[10] As one of the earliest programming languages, Lisp pioneered many ideas in computer science, including tree data structures, automatic storage management, dynamic typing, conditionals, higher-order functions, recursion, the self-hosting compiler,[11] and the read–eval–print loop.[12]
The name LISP derives from "LISt Processor".[13] Linked lists are one of Lisp's major data structures, and Lisp source code is made of lists. Thus, Lisp programs can manipulate source code as a data structure, giving rise to the macro systems that allow programmers to create new syntax or new domain-specific languages embedded in Lisp.
John McCarthy began developing Lisp in 1958 while he was at the Massachusetts Institute of Technology (MIT). He was motivated by a desire to create an AI programming language that would work on the IBM 704, as he believed that "IBM looked like a good bet to pursue Artificial Intelligence research vigorously."[14] He was inspired by Information Processing Language, which was also based on list processing, but did not use it because it was designed for different hardware and he found an algebraic language more appealing.[14] Due to these factors, he consulted on the design of the Fortran List Processing Language, which was implemented as a Fortran library. However, he was dissatisfied with it because it did not support recursion or a modern if-then-else statement (which was a new concept when lisp was first introduced) [note 1].[14]
McCarthy published Lisp's design in a paper in Communications of the ACM on April 1, 1960, entitled "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I"[19].[20] He showed that with a few simple operators and a notation for anonymous functions borrowed from Church, one can build a Turing-complete language for algorithms.
During the 1980s and 1990s, a great effort was made to unify the work on new Lisp dialects (mostly successors to Maclisp such as ZetaLisp and NIL (New Implementation of Lisp) into a single language. The new language, Common Lisp, was somewhat compatible with the dialects it replaced (the book Common Lisp the Language notes the compatibility of various constructs). In 1994, ANSI published the Common Lisp standard, "ANSI X3.226-1994 Information Technology Programming Language Common Lisp".
Since inception, Lisp was closely connected with the artificial intelligence research community, especially on PDP-10[23] systems. Lisp was used as the implementation of the language Micro Planner, which was used in the famous AI system SHRDLU. In the 1970s, as AI research spawned commercial offshoots, the performance of existing Lisp systems became a growing issue, as programmers needed to be familiar with the performance ramifications of the various techniques and choices involved in the implementation of Lisp.[24]
Over its sixty-year history, Lisp has spawned many variations on the core theme of an S-expression language. Some of these variations have been standardized and implemented by different groups with different priorities (for example, both Common Lisp and Scheme have multiple implementations). However, in other cases a software project defines a lisp without a standard and there is no clear distinction between the dialect and the implementation (for example, Clojure and Emacs Lisp fall into this category).
The Scheme community actively maintains over twenty implementations. Several significant new implementations (Chicken, Gambit, Gauche, Ikarus, Larceny, Ypsilon) have been developed in the 2000s (decade). The Revised5 Report on the Algorithmic Language Scheme[46] standard of Scheme was widely accepted in the Scheme community. The Scheme Requests for Implementation process has created a lot of quasi-standard libraries and extensions for Scheme. User communities of individual Scheme implementations continue to grow. A new language standardization process was started in 2003 and led to the R6RS Scheme standard in 2007. Academic use of Scheme for teaching computer science seems to have declined somewhat. Some universities are no longer using Scheme in their computer science introductory courses;[47][48] MIT now uses Python instead of Scheme for its undergraduate computer science program and MITx massive open online course.[49][50]
Common Lisp is a successor to Maclisp. The primary influences were Lisp Machine Lisp, Maclisp, NIL, S-1 Lisp, Spice Lisp, and Scheme.[51] It has many of the features of Lisp Machine Lisp (a large Lisp dialect used to program Lisp Machines), but was designed to be efficiently implementable on any personal computer or workstation. Common Lisp is a general-purpose programming language and thus has a large language standard including many built-in data types, functions, macros and other language elements, and an object system (Common Lisp Object System). Common Lisp also borrowed certain features from Scheme such as lexical scoping and lexical closures. Common Lisp implementations are available for targeting different platforms such as the LLVM,[52] the Java virtual machine,[53]
x86-64, PowerPC, Alpha, ARM, Motorola 68000, and MIPS,[54] and operating systems such as Windows, macOS, Linux, Solaris, FreeBSD, NetBSD, OpenBSD, Dragonfly BSD, and Heroku.[55]
Scheme is a statically scoped and properly tail-recursive dialect of the Lisp programming language invented by Guy L. Steele, Jr. and Gerald Jay Sussman. It was designed to have exceptionally clear and simple semantics and few different ways to form expressions. Designed about a decade earlier than Common Lisp, Scheme is a more minimalist design. It has a much smaller set of standard features but with certain implementation features (such as tail-call optimization and full continuations) not specified in Common Lisp. A wide variety of programming paradigms, including imperative, functional, and message passing styles, find convenient expression in Scheme. Scheme continues to evolve with a series of standards (Revisedn Report on the Algorithmic Language Scheme) and a series of Scheme Requests for Implementation.
Lisp was the first language where the structure of program code is represented faithfully and directly in a standard data structure—a quality much later dubbed "homoiconicity". Thus, Lisp functions can be manipulated, altered or even created within a Lisp program without lower-level manipulations. This is generally considered one of the main advantages of the language with regard to its expressive power, and makes the language suitable for syntactic macros and meta-circular evaluation.
Lisp deeply influenced Alan Kay, the leader of the research team that developed Smalltalk at Xerox PARC; and in turn Lisp was influenced by Smalltalk, with later dialects adopting object-oriented programming features (inheritance classes, encapsulating instances, message passing, etc.) in the 1970s. The Flavors object system introduced the concept of multiple inheritance and the mixin. The Common Lisp Object System provides multiple inheritance, multimethods with multiple dispatch, and first-class generic functions, yielding a flexible and powerful form of dynamic dispatch. It has served as the template for many subsequent Lisp (including Scheme) object systems, which are often implemented via a metaobject protocol, a reflective meta-circular design in which the object system is defined in terms of itself: Lisp was only the second language after Smalltalk (and is still one of the very few languages) to possess such a metaobject system. Many years later, Alan Kay suggested that as a result of the confluence of these features, only Smalltalk and Lisp could be regarded as properly conceived object-oriented programming systems.[63]
Lisp introduced the concept of automatic garbage collection, in which the system walks the heap looking for unused memory. Progress in modern sophisticated garbage collection algorithms such as generational garbage collection was stimulated by its use in Lisp.[64]
The use of parentheses is Lisp's most immediately obvious difference from other programming language families. As a result, students have long given Lisp nicknames such as Lost In Stupid Parentheses, or Lots of Irritating Superfluous Parentheses.[67] However, the S-expression syntax is also responsible for much of Lisp's power: the syntax is simple and consistent, which facilitates manipulation by computer. However, the syntax of Lisp is not limited to traditional parentheses notation. It can be extended to include alternative notations. For example, XMLisp is a Common Lisp extension that employs the metaobject protocol to integrate S-expressions with the Extensible Markup Language (XML).
The reliance on expressions gives the language great flexibility. Because Lisp functions are written as lists, they can be processed exactly like data. This allows easy writing of programs which manipulate other programs (metaprogramming). Many Lisp dialects exploit this feature using macro systems, which enables extension of the language almost without limit.
As more data types were introduced in later Lisp dialects, and programming styles evolved, the concept of an atom lost importance.[citation needed] Many dialects still retained the predicate atom for legacy compatibility,[citation needed] defining it true for any object which is not a cons.
Of the many data structures that can be built out of cons cells, one of the most basic is called a proper list. A proper list is either the special nil (empty list) symbol, or a cons in which the car points to a datum (which may be another cons structure, such as a list), and the cdr points to another proper list.
Because conses and lists are so universal in Lisp systems, it is a common misconception that they are Lisp's only data structures. In fact, all but the most simplistic Lisps have other data structures, such as vectors (arrays), hash tables, structures, and so forth.
Aficionados of functional programming avoid destructive functions. In the Scheme dialect, which favors the functional style, the names of destructive functions are marked with a cautionary exclamation point, or "bang"—such as set-car! (read set car bang), which replaces the car of a cons. In the Common Lisp dialect, destructive functions are commonplace; the equivalent of set-car! is named rplaca for "replace car". This function is rarely seen, however, as Common Lisp includes a special facility, setf, to make it easier to define and use destructive functions. A frequent style in Common Lisp is to write code functionally (without destructive calls) when prototyping, then to add destructive calls as an optimization where it is safe to do so.
A fundamental distinction between Lisp and other languages is that in Lisp, the textual representation of a program is simply a human-readable description of the same internal data structures (linked lists, symbols, number, characters, etc.) as would be used by the underlying Lisp system.
Lisp uses this to implement a very powerful macro system. Like other macro languages such as the one defined by the C preprocessor (the macro preprocessor for the C, Objective-C and C++ programming languages), a macro returns code that can then be compiled. However, unlike C preprocessor macros, the macros are Lisp functions and so can exploit the full power of Lisp.
Further, because Lisp code has the same structure as lists, macros can be built with any of the list-processing functions in the language. In short, anything that Lisp can do to a data structure, Lisp macros can do to code. In contrast, in most other languages, the parser's output is purely internal to the language implementation and cannot be manipulated by the programmer.
The read function accepts textual S-expressions as input, and parses them into an internal data structure. For instance, if you type the text (+ 1 2) at the prompt, read translates this into a linked list with three elements: the symbol +, the number 1, and the number 2. It so happens that this list is also a valid piece of Lisp code; that is, it can be evaluated. This is because the car of the list names a function—the addition operation.
Programmers in the Scheme dialect often express loops using tail recursion. Scheme's commonality in academic computer science has led some students to believe that tail recursion is the only, or the most common, way to write iterations in Lisp, but this is incorrect. All oft-seen Lisp dialects have imperative-style iteration constructs, from Scheme's do loop to Common Lisp's complex loop expressions. Moreover, the key issue that makes this an objective rather than subjective matter is that Scheme makes specific requirements for the handling of tail calls, and thus the reason that the use of tail recursion is generally encouraged for Scheme is that the practice is expressly supported by the language definition. By contrast, ANSI Common Lisp does not require[73] the optimization commonly termed a tail call elimination. Thus, the fact that tail recursive style as a casual replacement for the use of more traditional iteration constructs (such as do, dolist or loop) is discouraged[74] in Common Lisp is not just a matter of stylistic preference, but potentially one of efficiency (since an apparent tail call in Common Lisp may not compile as a simple jump) and program correctness (since tail recursion may increase stack use in Common Lisp, risking stack overflow).
In contrast to most other major programming languages, Lisp allows implementing control structures using the language. Several control structures are implemented as Lisp macros, and can even be macro-expanded by the programmer who wants to know how they work.
Often, the same algorithm can be expressed in Lisp in either an imperative or a functional style. As noted above, Scheme tends to favor the functional style, using tail recursion and continuations to express control flow. However, imperative style is still quite possible. The style preferred by many Common Lisp programmers may seem more familiar to programmers used to structured languages such as C, while that preferred by Schemers more closely resembles pure-functional languages such as Haskell.
Because of Lisp's early heritage in list processing, it has a wide array of higher-order functions relating to iteration over sequences. In many cases where an explicit loop would be needed in other languages (like a for loop in C) in Lisp the same task can be accomplished with a higher-order function. (The same is true of many functional programming languages.)
Several operating systems, including language-based systems, are based on Lisp (use Lisp features, conventions, methods, data structures, etc.), or are written in Lisp,[77] including:
Genera, renamed Open Genera,[78] by Symbolics; Medley, written in Interlisp, originally a family of graphical operating systems that ran on Xerox's later Star workstations;[79][80] Mezzano;[81] Interim;[82][83] ChrysaLisp,[84] by developers of Tao Systems' TAOS;[85] and also the Guix System for GNU/Linux.
From the invention of computer programming languages up to the mid-1970s, most computer programmers created, edited and stored their programs line by line on punch cards.
The description below describes an all-IBM shop (a "shop" is programmer jargon for a programming site) but shops using other brands of mainframes (or minicomputers) would have similar equipment although because of cost or availability might have different manufacturer's equipment, e.g. an NCR, ICL, Hewlett-Packard (HP) or Control Data shop would have NCR, ICL, HP, or Control Data computers, printers and so forth, but have IBM 029 keypunches. IBM's huge size and industry footprint often caused many of their conventions to be adopted by other vendors, so the example below is fairly similar to most places, even in non-IBM shops.
Many early programming languages, including FORTRAN, COBOL and the various IBM assembler languages, used only the first 72 columns of a card – a tradition that traces back to the IBM 711 card reader used on the IBM 704/709/7090/7094 series (especially the IBM 704, the first mass-produced computer with floating-point arithmetic hardware), which could only read 72 of the 80 columns in one pass.
Columns 73-80 were ignored by the compilers and could be used for identification or a sequence number so that if the card deck was dropped it could be restored to its proper order using a card sorter.  Depending on the programming language, debugging output statements could be quickly activated and "commented out" by using cards with such statements punched with the comment character (e.g., 'C' in Fortran) in column 80 of the card; turning the card end-for-end would put the 'C' in the leading column, which transformed the now backwards card's contents into a comment while leaving the physical card in place in deck.
In the context of software engineering, software quality refers to two related but distinct notions:[citation needed]
Many aspects of structural quality can be evaluated only statically through the analysis of the software's inner structure, its source code (see Software metrics),[3] at the unit level, and at the system level (sometimes referred to as end-to-end testing[4]), which is in effect how its architecture adheres to sound principles of software architecture outlined in a paper on the topic by Object Management Group (OMG).[5]
Some structural qualities, such as usability, can be assessed only dynamically (users or others acting on their behalf interact with the software or, at least, some prototype or partial implementation; even the interaction with a mock version made in cardboard represents a dynamic test because such version can be considered a prototype). Other aspects, such as reliability, might involve not only the software but also the underlying hardware, therefore, it can be assessed both statically and dynamically (stress test).[citation needed]
Functional quality is typically assessed dynamically but it is also possible to use static tests (such as software reviews).[citation needed]
Historically, the structure, classification, and terminology of attributes and metrics applicable to software quality management have been derived or extracted from the ISO 9126 and the subsequent ISO/IEC 25000 standard.[7] Based on these models (see Models), the Consortium for IT Software Quality (CISQ) has defined five major desirable structural characteristics needed for a piece of software to provide business value:[8] Reliability, Efficiency, Security, Maintainability, and (adequate) Size.[9][10][11]
Software quality measurement quantifies to what extent a software program or system rates along each of these five dimensions. An aggregated measure of software quality can be computed through a qualitative or a quantitative scoring scheme or a mix of both and then a weighting system reflecting the priorities. This view of software quality being positioned on a linear continuum is supplemented by the analysis of "critical programming errors" that under specific circumstances can lead to catastrophic outages or performance degradations that make a given system unsuitable for use regardless of rating based on aggregated measurements. Such programming errors found at the system level represent up to 90 percent of production issues, whilst at the unit-level, even if far more numerous, programming errors account for less than 10 percent of production issues (see also Ninety–ninety rule). As a consequence, code quality without the context of the whole system, as W. Edwards Deming described it, has limited value.[citation needed]
To view, explore, analyze, and communicate software quality measurements, concepts and techniques of information visualization provide visual, interactive means useful, in particular, if several software quality measures have to be related to each other or to components of a software or system. For example, software maps represent a specialized approach that "can express and combine information about software development, software quality, and system dynamics".[12]
Software quality also plays a role in the release phase of a software project. Specifically, the quality and establishment of the release processes (also patch processes),[13][14] configuration management[15] are important parts of an overall software engineering process.[16][17][18]
Software quality is the "capability of a software product to conform to requirements."[36][37] while for others it can be synonymous with customer- or value-creation[38][39] or even defect level.[40] Software quality measurements can be split into three parts: process quality, product quality which includes internal and external properties and lastly, quality in use, which is the effect of the software.[41]
ASQ uses the following definition: Software quality describes the desirable attributes of software products. There are two main approaches exist: defect management and quality attributes.[42]
The Project Management Institute's PMBOK Guide "Software Extension" defines not "Software quality" itself, but Software Quality Assurance (SQA) as "a continuous process that audits other software processes to ensure that those processes are being followed (includes for example a software quality management plan)." whereas Software Quality Control (SCQ) means "taking care of applying methods, tools, techniques to ensure satisfaction of the work products toward quality requirements for a software under development or modification."[44]
Tom DeMarco has proposed that "a product's quality is a function of how much it changes the world for the better."[citation needed] This can be interpreted as meaning that functional quality and user satisfaction are more important than structural quality in determining software quality.
Another definition, coined by Gerald Weinberg in Quality Software Management: Systems Thinking, is "Quality is value to some person."[54][55]
One of the challenges in defining quality is that "everyone feels they understand it"[56] and other definitions of software quality could be based on extending the various descriptions of the concept of quality used in business.
Software quality also often gets mixed-up with Quality Assurance or Problem Resolution Management[57] or Quality Control[58]  or DevOps. It does over-lap with before mentioned areas (see also PMI definitions), but is distinctive as it does not solely focus on testing but also on processes, management, improvements, assessments, etc.[58]
Although the concepts presented in this section are applicable to both structural and functional software quality, measurement of the latter is essentially performed through software testing.[59] Testing is not enough: According to one study, "individual programmers are less than 50% efficient at finding bugs in their own software. And most forms of testing are only 35% efficient. This makes it difficult to determine [software] quality."[60]
Software quality measurement is about quantifying to what extent a system or software possesses desirable characteristics. This can be performed through qualitative or quantitative means or a mix of both. In both cases, for each desirable characteristic, there are a set of measurable attributes the existence of which in a piece of software or system tend to be correlated and associated with this characteristic. For example, an attribute associated with portability is the number of target-dependent statements in a program. More precisely, using the Quality Function Deployment approach, these measurable attributes are the "hows" that need to be enforced to enable the "whats" in the Software Quality definition above.
The structure, classification and terminology of attributes and metrics applicable to software quality management have been derived or extracted from the ISO 9126-3 and the subsequent ISO/IEC 25000:2005 quality model. The main focus is on internal structural quality. Subcategories have been created to handle specific areas like business application architecture and technical characteristics such as data access and manipulation or the notion of transactions.
The dependence tree between software quality characteristics and their measurable attributes is represented in the diagram on the right, where each of the 5 characteristics that matter for the user (right) or owner of the business system depends on measurable attributes (left):
Correlations between programming errors and production defects unveil that basic code errors account for 92 percent of the total errors in the source code. These numerous code-level issues eventually count for only 10 percent of the defects in production. Bad software engineering practices at the architecture levels account for only 8 percent of total defects, but consume over half the effort spent on fixing problems, and lead to 90 percent of the serious reliability, security, and efficiency issues in production.[61][62]
Many of the existing software measures count structural elements of the application that result from parsing the source code for such individual instructions[63] tokens[64] control structures (Complexity), and objects.[65]
Software quality measurement is about quantifying to what extent a system or software rates along these dimensions. The analysis can be performed using a qualitative or quantitative approach or a mix of both to provide an aggregate view [using for example weighted average(s) that reflect relative importance between the factors being measured].
This view of software quality on a linear continuum has to be supplemented by the identification of discrete Critical Programming Errors. These vulnerabilities may not fail a test case, but they are the result of bad practices that under specific circumstances can lead to catastrophic outages, performance degradations, security breaches, corrupted data, and myriad other problems[66] that make a given system de facto unsuitable for use regardless of its rating based on aggregated measurements. A well-known example of vulnerability is the Common Weakness Enumeration,[67] a repository of vulnerabilities in the source code that make applications exposed to security breaches.
Structural quality analysis and measurement is performed through the analysis of the source code, the architecture, software framework, database schema in relationship to principles and standards that together define the conceptual and logical architecture of a system. This is distinct from the basic, local, component-level code analysis typically performed by development tools which are mostly concerned with implementation considerations and are crucial during debugging and testing activities.
Assessing reliability requires checks of at least the following software engineering best practices and technical attributes:
Depending on the application architecture and the third-party components used (such as external libraries or frameworks), custom checks should be defined along the lines drawn by the above list of best practices to ensure a better assessment of the reliability of the delivered software.
As with Reliability, the causes of performance inefficiency are often found in violations of good architectural and coding practice which can be detected by measuring the static quality attributes of an application. These static attributes predict potential operational performance bottlenecks and future scalability problems, especially for applications requiring high execution speed for handling complex algorithms or huge volumes of data.
Assessing performance efficiency requires checking at least the following software engineering best practices and technical attributes:
Software quality includes software security.[70] Many security vulnerabilities result from poor coding and architectural practices such as SQL injection or cross-site scripting.[71][72] These are well documented in lists maintained by CWE,[73] and the SEI/Computer Emergency Center (CERT) at Carnegie Mellon University.[69]
Assessing security requires at least checking the following software engineering best practices and technical attributes:
Maintainability includes concepts of modularity, understandability, changeability, testability, reusability, and transferability from one development team to another. These do not take the form of critical issues at the code level. Rather, poor maintainability is typically the result of thousands of minor violations with best practices in documentation, complexity avoidance strategy, and basic programming practices that make the difference between clean and easy-to-read code vs. unorganized and difficult-to-read code.[79]
Assessing maintainability requires checking the following software engineering best practices and technical attributes:
Measuring software size requires that the whole source code be correctly gathered, including database structure scripts, data manipulation source code, component headers, configuration files etc. There are essentially two types of software sizes to be measured, the technical size (footprint) and the functional size:
The function point analysis sizing standard is supported by the International Function Point Users Group (IFPUG). It can be applied early in the software development life-cycle and it is not dependent on lines of code like the somewhat inaccurate Backfiring method. The method is technology agnostic and can be used for comparative analysis across organizations and across industries.
One common limitation to the Function Point methodology is that it is a manual process and therefore it can be labor-intensive and costly in large scale initiatives such as application development or outsourcing engagements. This negative aspect of applying the methodology may be what motivated industry IT leaders to form the Consortium for IT Software Quality focused on introducing a computable metrics standard for automating the measuring of software size while the IFPUG keep promoting a manual approach as most of its activity rely on FP counters certifications.
CISQ defines Sizing as to estimate the size of software to support cost estimating, progress tracking or other related software project management activities. Two standards are used: Automated Function Points to measure the functional size of software and Automated Enhancement Points to measure the size of both functional and non-functional code in one measure.[83]
Critical Programming Errors are specific architectural and/or coding bad practices that result in the highest, immediate or long term, business disruption risk.[84]
In the 1960s, more emphasis was given to reliability testing on component and system levels. The famous military standard MIL-STD-781 was created at that time. Around this period also the much-used predecessor to military handbook 217 was published by RCA and was used for the prediction of failure rates of electronic components. The emphasis on component reliability and empirical research (e.g. Mil Std 217) alone slowly decreased. More pragmatic approaches, as used in the consumer industries, were being used. In the 1980s, televisions were increasingly made up of solid-state semiconductors. Automobiles rapidly increased their use of semiconductors with a variety of microcomputers under the hood and in the dash. Large air conditioning systems developed electronic controllers, as did microwave ovens and a variety of other appliances. Communications systems began to adopt
electronics to replace older mechanical switching systems. Bellcore issued the first consumer prediction methodology for telecommunications, and SAE developed a similar document SAE870050 for automotive applications. The nature of predictions evolved during the decade, and it became apparent that die complexity wasn't the only factor that determined failure rates for integrated circuits (ICs).
Kam Wong published a paper questioning the bathtub curve[9]—see also reliability-centered maintenance. During this decade, the failure rate of many components dropped by a factor of 10. Software became important to the reliability of systems. By the 1990s, the pace of IC development was picking up. Wider use of stand-alone microcomputers was common, and the PC market helped keep IC densities following Moore's law and doubling about every 18 months. Reliability engineering was now changing as it moved towards understanding the physics of failure. Failure rates for components kept dropping, but system-level issues became more prominent. Systems thinking has become more and more important. For software, the CMM model (Capability Maturity Model) was developed, which gave a more qualitative approach to reliability. ISO 9000 added reliability measures as part of the design and development portion of certification. The expansion of the World Wide Web created new challenges of security and trust. The older problem of too little reliable information available had now been replaced by too much information of questionable value. Consumer reliability problems could now be discussed online in real-time using data. New technologies such as micro-electromechanical systems (MEMS), handheld GPS, and hand-held devices that combine cell phones and computers all represent challenges to maintaining reliability. Product development time continued to shorten through this
decade and what had been done in three years was being done in 18 months. This meant that reliability tools and tasks had to be more closely tied to the development process itself. In many ways, reliability has become part of everyday life and consumer expectations.
Implementing a reliability program is not simply a software purchase; it is not just a checklist of items that must be completed that ensure one has reliable products and processes. A reliability program is a complex learning and knowledge-based system unique to one's products and processes. It is supported by leadership, built on the skills that one develops within a team, integrated into business processes, and executed by following proven standard work practices.[15]
One of the most important design techniques is redundancy. This means that if one part of the system fails, there is an alternate success path, such as a backup system. The reason why this is the ultimate design choice is related to the fact that high-confidence reliability evidence for new parts or systems is often not available, or is extremely expensive to obtain. By combining redundancy, together with a high level of failure monitoring, and the avoidance of common cause failures; even a system with relatively poor single-channel (part) reliability, can be made highly reliable at a system level (up to mission critical reliability). No testing of reliability has to be required for this. In conjunction with redundancy, the use of dissimilar designs or manufacturing processes (e.g. via different suppliers of similar parts) for single independent channels, can provide less sensitivity to quality issues (e.g. early childhood failures at a single supplier), allowing very-high levels of reliability to be achieved at all moments of the development cycle (from early life to long-term). Redundancy can also be applied in systems engineering by double checking requirements, data, designs, calculations, software, and tests to overcome systematic failures.
For electronic assemblies, there has been an increasing shift towards a different approach called physics of failure. This technique relies on understanding the physical static and dynamic failure mechanisms. It accounts for variation in load, strength, and stress that lead to failure with a high level of detail, made possible with the use of modern finite element method (FEM) software programs that can handle complex geometries and mechanisms such as creep, stress relaxation, fatigue, and probabilistic design (Monte Carlo Methods/DOE). The material or component can be re-designed to reduce the probability of failure and to make it more robust against such variations. Another common design technique is component derating: i.e. selecting components whose specifications significantly exceed the expected stress levels, such as using heavier gauge electrical wire than might normally be specified for the expected electric current.
Reliability verification is also called reliability testing, which refers to the use of modeling, statistics, and other methods to evaluate the reliability of the product based on the product's life span and expected performance.[26] Most product on the market requires reliability testing, such as automotive, integrated circuit, heavy machinery used to mine nature resources, Aircraft auto software.[27][28]
Reliability test requirements can follow from any analysis for which the first estimate of failure probability, failure mode or effect needs to be justified. Evidence can be generated with some level of confidence by testing. With software-based systems, the probability is a mix of software and hardware-based failures. Testing reliability requirements is problematic for several reasons. A single test is in most cases insufficient to generate enough statistical data. Multiple tests or long-duration tests are usually very expensive. Some tests are simply impractical, and environmental conditions can be hard to predict over a systems life-cycle.
Software reliability is a special aspect of reliability engineering. It focuses on foundations and techniques to make software more reliable, i.e., resilient to faults. System reliability, by definition, includes all parts of the system, including hardware, software, supporting infrastructure (including critical external interfaces), operators and procedures. Traditionally, reliability engineering focuses on critical hardware parts of the system. Since the widespread use of digital integrated circuit technology, software has become an increasingly critical part of most electronics and, hence, nearly all present day systems. Therefore, software reliability has gained prominence within the field of system reliability.
There are significant differences, however, in how software and hardware behave. 
Most hardware unreliability is the result of a component or material failure that results in the system not performing its intended function. Repairing or replacing the hardware component restores the system to its original operating state. 
However, software does not fail in the same sense that hardware fails. Instead, software unreliability is the result of unanticipated results of software operations. Even relatively small software programs can have astronomically large combinations of inputs and states that are infeasible to exhaustively test. Restoring software to its original state only works until the same combination of inputs and states results in the same unintended result. Software reliability engineering must take this into account.
Despite this difference in the source of failure between software and hardware, several software reliability models based on statistics have been proposed to quantify what we experience with software: the longer software is run, the higher the probability that it will eventually be used in an untested manner and exhibit a latent defect that results in a failure (Shooman 1987), (Musa 2005), (Denney 2005).
As with hardware, software reliability depends on good requirements, design and implementation. Software reliability engineering relies heavily on a disciplined software engineering process to anticipate and design against unintended consequences. There is more overlap between software quality engineering and software reliability engineering than between hardware quality and reliability. A good software development plan is a key aspect of the software reliability program. The software development plan describes the design and coding standards, peer reviews, unit tests, configuration management, software metrics and software models to be used during software development.
A common reliability metric is the number of software faults per line of code (FLOC), usually expressed as faults per thousand lines of code. This metric, along with software execution time, is key to most software reliability models and estimates. The theory is that the software reliability increases as the number of faults (or fault density) decreases. Establishing a direct connection between fault density and mean-time-between-failure is difficult, however, because of the way software faults are distributed in the code, their severity, and the probability of the combination of inputs necessary to encounter the fault. Nevertheless, fault density serves as a useful indicator for the reliability engineer. Other software metrics, such as complexity, are also used. This metric remains controversial, since changes in software development and verification practices can have dramatic impact on overall defect rates.
Software testing is an important aspect of software reliability. Even the best software development process results in some software faults that are nearly undetectable until tested. Software is tested at several levels, starting with individual units, through integration and full-up system testing. All phases of testing, software faults are discovered, corrected, and re-tested. Reliability estimates are updated based on the fault density and other metrics. At a system level, mean-time-between-failure data can be collected and used to estimate reliability. Unlike hardware, performing exactly the same test on exactly the same software configuration does not provide increased statistical confidence. Instead, software reliability uses different metrics, such as code coverage.
The Software Engineering Institute's capability maturity model is a common means of assessing the overall software development process for reliability and quality purposes.
Software portability is a design objective for source code to be easily made to run on different platforms. An aid to portability is the generalized abstraction between the application logic and system interfaces. When software with the same functionality is produced for several computing platforms, portability is the key issue for development cost reduction.
When operating systems of the same family are installed on two computers with processors with similar instruction sets it is often possible to transfer the files implementing program files between them.
In the simplest case, the file or files may simply be copied from one machine to the other. However, in many cases, the software is installed on a computer in a way which depends upon its detailed hardware, software, and setup, with device drivers for particular devices, using installed operating system and supporting software components, and using different drives or directories.
In some cases, software, usually described as "portable software", is specifically designed to run on different computers with compatible operating systems and processors, without any machine-dependent installation. Porting is no more than transferring specified directories and their contents. Software installed on portable mass storage devices such as USB sticks can be used on any compatible computer on simply plugging the storage device in, and stores all configuration information on the removable device. Hardware- and software-specific information is often stored in configuration files in specified locations such as the registry on Windows).
Software which is not portable in this sense must be modified much more to support the environment on the destination machine.
As of 2011 the majority of desktop and laptop computers used microprocessors compatible with the 32- and 64-bit x86 instruction sets. Smaller portable devices use processors with different and incompatible instruction sets, such as ARM. The difference between larger and smaller devices is such that detailed software operation is different; an application designed to display suitably on a large screen cannot simply be ported to a pocket-sized smartphone with a tiny screen even if the functionality is similar.
Web applications are required to be processor independent, so portability can be achieved by using web programming techniques, writing in JavaScript. Such a program can run in a common web browser. Such web applications must, for security reasons, have limited control over the host computer, especially regarding reading and writing files. Non-web programs, installed upon a computer in the normal manner, can have more control, and yet achieve system portability by linking to portable libraries providing the same interface on different systems.
Software can be compiled and linked from source code for different operating systems and processors if written in a programming language supporting compilation for the platforms. This is usually a task for the program developers; typical users have neither access to the source code nor the required skills.
In open-source environments such as Linux the source code is available to all. In earlier days source code was often distributed in a standardised format, and could be built into executable code with a standard Make tool for any particular system by moderately knowledgeable users if no errors occurred during the build. Some Linux distributions distribute software to users in source form. In these cases there is usually no need for detailed adaptation of the software for the system; it is distributed in a way which modifies the compilation process to match the system.
Sometimes the effort consists of recompiling the source code,  but sometimes it is necessary to rewrite major parts of the software. Many language specifications describe implementation defined behaviour (e.g. right shifting a signed integer in C can do a logical or an arithmetic shift). Operating system functions or third party libraries might not be available on the target system. Some functions can be available on a target system, but exhibit slightly different behavior such as utime() fails under Windows with EACCES, when it is called for a directory). The program code can contain unportable things, like the paths of include files, drive letters, or the backslash. Implementation defined things like byte order and the size of an int can also raise the porting effort. In practice the claim of languages, like C and C++, to have the WOCA (write once, compile anywhere) is arguable.
A software bug is a design defect (bug) in computer software. A computer program with many or serious bugs may be described as buggy.
The effects of a software bug range from minor (such as a misspelled word in the user interface) to severe (such as frequent crashing).
In 2002, a study commissioned by the US Department of Commerce's National Institute of Standards and Technology concluded that "software bugs, or errors, are so prevalent and so detrimental that they cost the US economy an estimated $59 billion annually, or about 0.6 percent of the gross domestic product".[1]
Since the 1950s, some computer systems have been designed to detect or auto-correct various software errors during operations.
Mistake metamorphism (from Greek meta = "change", morph = "form") refers to the evolution of a defect in the final stage of software deployment. Transformation of a mistake committed by an analyst in the early stages of the software development lifecycle, which leads to a defect in the final stage of the cycle has been called mistake metamorphism.[2]
Sometimes the use of bug to describe the behavior of software is contentious due to perception. Some suggest that the term should be abandoned; contending that bug implies that the defect arose on its own and push to use defect instead since it more clearly indicates they are caused by a human.[8]
Preventing bugs as early as possible in the software development process is a target of investment and innovation.[11][12]
Newer programming languages tend to be designed to prevent common bugs based on vulnerabilities of existing languages. Lessons learned from older languages such as BASIC and C are used to inform the design of later languages such as C# and Rust.
A compiled language allows for detecting some typos (such as a misspelled identifier) before runtime which is earlier in the software development process than for an interpreted language.
Languages may include features such as a static type system, restricted namespaces and modular programming. For example, for a typed, compiled language (like C):
Programming techniques such as programming style and defensive programming are intended to prevent typos.
Agile software development may involve frequent software releases with relatively small changes. Defects are revealed by user feedback.
Tools for static code analysis help developers by inspecting the program text beyond the compiler's capabilities to spot potential problems. Although in general the problem of finding all programming errors given a specification is not solvable (see halting problem), these tools exploit the fact that human programmers tend to make certain kinds of simple mistakes often when writing software.
Tools to monitor the performance of the software as it is running, either specifically to find problems such as bottlenecks or to give assurance as to correct working, may be embedded in the code explicitly (perhaps as simple as a statement saying PRINT "I AM HERE"), or provided as tools. It is often a surprise to find where most of the time is taken by a piece of code, and this removal of assumptions might cause the code to be rewritten.
Open source development allows anyone to examine source code. A school of thought popularized by Eric S. Raymond as Linus's law says that popular open-source software has more chance of having few or no bugs than other software, because "given enough eyeballs, all bugs are shallow".[13] This assertion has been disputed, however: computer security specialist Elias Levy wrote that "it is easy to hide vulnerabilities in complex, little understood and undocumented source code," because, "even if people are reviewing the code, that doesn't mean they're qualified to do so."[14] An example of an open-source software bug was the 2008 OpenSSL vulnerability in Debian.
Debugging can be a significant part of the software development lifecycle. Maurice Wilkes, an early computing pioneer, described his realization in the late 1940s that
“a good part of the remainder of my life was going to be spent in finding errors in my own programs”.[15]
In an embedded system, the software is often modified to work around a hardware bug since it's cheaper than modifying the hardware.
Tools are often used to track bugs and other issues with software. Typically, different tools
are used by the software development team to track their workload than by customer service to track user feedback.[17]
A tracked item is often called bug, defect, ticket, issue, feature, or for agile software development, story or epic. Items are often categorized by aspects such as severity, priority and version number.
A software release that emphasizes bug fixes may be called a maintenance release – to differentiate it from a release that emphasizes new features or other changes.
It is common practice to release software with known, low-priority bugs or other issues.  Possible reasons include but are not limited to:
The amount and type of damage a software bug may cause affects decision-making, processes and policy regarding software quality. In applications such as human spaceflight, aviation, nuclear power, health care, public transport or automotive safety, since software flaws have the potential to cause human injury or even death, such software will have far more scrutiny and quality control than, for example, an online shopping website. In applications such as banking, where software flaws have the potential to cause serious financial damage to a bank or its customers, quality control is also more important than, say, a photo editing application.
Another study in 1990 reported that exceptionally good software development processes can achieve deployment failure rates as low as 0.1 per 1000 SLOC.[29] This figure is iterated in literature such as Code Complete by Steve McConnell,[30] and the NASA study on Flight Software Complexity.[31]
Some projects even attained zero defects: the firmware in the IBM Wheelwriter typewriter which consists of 63,000 SLOC, and the Space Shuttle software with 500,000 SLOC.[29]
Numerical operations can result in unexpected output, slow processing, or crashing.[34]
Such a bug can be from a lack of awareness of the qualities of the data storage such as a loss of precision due to rounding, numerically unstable algorithms, arithmetic overflow and underflow, or from lack of awareness of how calculations are handled by different software coding languages such as division by zero which in some languages may throw an exception, and in others may return a special value such as NaN or infinity.
The Open Technology Institute, run by the group, New America,[39] released a report "Bugs in the System" in August 2016 stating that U.S. policymakers should make reforms to help researchers identify and address software bugs. The report "highlights the need for reform in the field of software vulnerability discovery and disclosure."[40] One of the report's authors said that Congress has not done enough to address cyber software vulnerability, even though Congress has passed a number of bills to combat the larger issue of cyber security.[40]
Government researchers, companies, and cyber security experts are the people who typically discover software flaws. The report calls for reforming computer crime and copyright laws.[40]
In computer science, algorithmic efficiency is a property of an algorithm which relates to the amount of computational resources used by the algorithm. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.
For maximum efficiency it is desirable to minimize resource usage. However, different resources such as time and space complexity cannot be compared directly, so which of two algorithms is considered to be more efficient often depends on which measure of efficiency is considered most important.
For example, bubble sort and timsort are both algorithms to sort a list of items from smallest to largest. Bubble sort organizes the list in time proportional to the number of elements squared (



O
(

n

2


)


{\textstyle O(n^{2})}

, see Big O notation), but only requires a small amount of extra memory which is constant with respect to the length of the list (



O
(
1
)


{\textstyle O(1)}

). Timsort sorts the list in time linearithmic (proportional to a quantity times its logarithm) in the list's length (



O
(
n
log
⁡
n
)


{\textstyle O(n\log n)}

), but has a space requirement linear in the length of the list (



O
(
n
)


{\textstyle O(n)}

). If large lists must be sorted at high speed for a given application, timsort is a better choice; however, if minimizing the memory footprint of the sorting is more important, bubble sort is a better choice.
Early electronic computers had both limited speed and limited random access memory. Therefore, a space–time trade-off occurred. A task could use a fast algorithm using a lot of memory, or it could use a slow algorithm using little memory. The engineering trade-off was therefore to use the fastest algorithm that could fit in the available memory.
"In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal and I believe the same viewpoint should prevail in software engineering"[2]
An algorithm is considered efficient if its resource consumption, also known as computational cost, is at or below some acceptable level. Roughly speaking, 'acceptable' means:  it will run in a reasonable amount of time or space on an available computer, typically as a function of the size of the input. Since the 1950s computers have seen dramatic increases in both the available computational power and in the available amount of memory, so current acceptable levels would have been unacceptable even 10 years ago. In fact, thanks to the approximate doubling of computer power every 2 years, tasks that are acceptably efficient on modern smartphones and embedded systems may have been unacceptably inefficient for industrial servers 10 years ago.
Computer manufacturers frequently bring out new models, often with higher performance. Software costs can be quite high, so in some cases the simplest and cheapest way of getting higher performance might be to just buy a faster computer, provided it is compatible with an existing computer.
There are many ways in which the resources used by an algorithm can be measured: the two most common measures are speed and memory usage; other measures could include transmission speed, temporary disk usage, long-term disk usage, power consumption, total cost of ownership, response time to external stimuli, etc. Many of these measures depend on the size of the input to the algorithm, i.e. the amount of data to be processed. They might also depend on the way in which the data is arranged; for example, some sorting algorithms perform poorly on data which is already sorted, or which is sorted in reverse order.
In practice, there are other factors which can affect the efficiency of an algorithm, such as requirements for accuracy and/or reliability. As detailed below, the way in which an algorithm is implemented can also have a significant effect on actual efficiency, though many aspects of this relate to optimization issues.
In the theoretical analysis of algorithms, the normal practice is to estimate their complexity in the asymptotic sense. The most commonly used notation to describe resource consumption or "complexity" is Donald Knuth's Big O notation, representing the complexity of an algorithm as a function of the size of the input 



n


{\textstyle n}

. Big O notation is an asymptotic measure of function complexity, where 



f
(
n
)
=
O


(


g
(
n
)


)




{\textstyle f(n)=O{\bigl (}g(n){\bigr )}}

 roughly means the time requirement for an algorithm is proportional to 



g
(
n
)


{\displaystyle g(n)}

, omitting lower-order terms that contribute less than 



g
(
n
)


{\displaystyle g(n)}

 to the growth of the function as 



n


{\textstyle n}

 grows arbitrarily large. This estimate may be misleading when 



n


{\textstyle n}

 is small, but is generally sufficiently accurate when 



n


{\textstyle n}

 is large as the notation is asymptotic. For example, bubble sort may be faster than merge sort when only a few items are to be sorted; however either implementation is likely to meet performance requirements for a small list. Typically, programmers are interested in algorithms that scale efficiently to large input sizes, and merge sort is preferred over bubble sort for lists of length encountered in most data-intensive programs.
For new versions of software or to provide comparisons with competitive systems, benchmarks are sometimes used, which assist with gauging an algorithms relative performance. If a new sort algorithm is produced, for example, it can be compared with its predecessors to ensure that at least it is efficient as before with known data, taking into consideration any functional improvements. Benchmarks can be used by customers when comparing various products from alternative suppliers to estimate which product will best suit their specific requirements in terms of functionality and performance. For example, in the mainframe world certain proprietary sort products from independent software companies such as Syncsort compete with products from the major suppliers such as IBM for speed.
Some benchmarks provide opportunities for producing an analysis comparing the relative speed of various compiled and interpreted languages for example[3][4]
and The Computer Language Benchmarks Game compares the performance of implementations of typical programming problems in several programming languages.
Even creating "do it yourself" benchmarks can demonstrate the relative performance of different programming languages, using a variety of user specified criteria. This is quite simple, as a "Nine language performance roundup" by Christopher W. Cowell-Shah demonstrates by example.[5]
Implementation issues can also have an effect on efficiency, such as the choice of programming language, or the way in which the algorithm is actually coded,[6] or the choice of a compiler for a particular language, or the compilation options used, or even the operating system being used. In many cases a language implemented by an interpreter may be much slower than a language implemented by a compiler.[3] See the articles on just-in-time compilation and interpreted languages.
There are other factors which may affect time or space issues, but which may be outside of a programmer's control; these include data alignment, data granularity, cache locality, cache coherency, garbage collection, instruction-level parallelism, multi-threading (at either a hardware or software level), simultaneous multitasking, and subroutine calls.[7]
Some processors have capabilities for vector processing, which allow a single instruction to operate on multiple operands; it may or may not be easy for a programmer or compiler to use these capabilities. Algorithms designed for sequential processing may need to be completely redesigned to make use of parallel processing, or they could be easily reconfigured. As parallel and distributed computing grow in importance in the late 2010s, more investments are being made into efficient high-level APIs for parallel and distributed computing systems such as CUDA, TensorFlow, Hadoop, OpenMP and MPI.
Another problem which can arise in programming is that processors compatible with the same instruction set (such as x86-64 or ARM) may implement an instruction in different ways, so that instructions which are relatively fast on some models may be relatively slow on other models. This often presents challenges to optimizing compilers, which must have extensive knowledge of the specific CPU and other hardware available on the compilation target to best optimize a program for performance. In the extreme case, a compiler may be forced to emulate instructions not supported on a compilation target platform, forcing it to generate code or link an external library call to produce a result that is otherwise incomputable on that platform, even if it is natively supported and more efficient in hardware on other platforms. This is often the case in embedded systems with respect to floating-point arithmetic, where small and low-power microcontrollers often lack hardware support for floating-point arithmetic and thus require computationally expensive software routines to produce floating point calculations.
Analysis of algorithms, typically using concepts like time complexity, can be used to get an estimate of the running time as a function of the size of the input data. The result is normally expressed using Big O notation. This is useful for comparing algorithms, especially when a large amount of data is to be processed. More detailed estimates are needed to compare algorithm performance when the amount of data is small, although this is likely to be of less importance. Parallel algorithms may be more difficult to analyze.
A benchmark can be used to assess the performance of an algorithm in practice. Many programming languages have an available function which provides CPU time usage. For long-running algorithms the elapsed time could also be of interest. Results should generally be averaged over several tests.
Run-based profiling can be very sensitive to hardware configuration and the possibility of other programs or tasks running at the same time in a multi-processing and multi-programming environment.
This sort of test also depends heavily on the selection of a particular programming language, compiler, and compiler options, so algorithms being compared must all be implemented under the same conditions.
This section is concerned with use of memory resources (registers, cache, RAM, virtual memory, secondary memory) while the algorithm is being executed. As for time analysis above, analyze the algorithm, typically using space complexity analysis to get an estimate of the run-time memory needed as a function as the size of the input data. The result is normally expressed using Big O notation.
Modern computers can have relatively large amounts of memory (possibly gigabytes), so having to squeeze an algorithm into a confined amount of memory is not the kind of problem it used to be. However, the different types of memory and their relative access speeds can be significant:
An algorithm whose memory needs will fit in cache memory will be much faster than an algorithm which fits in main memory, which in turn will be very much faster than an algorithm which has to resort to paging. Because of this, cache replacement policies are extremely important to high-performance computing, as are cache-aware programming and data alignment. To further complicate the issue, some systems have up to three levels of cache memory, with varying effective speeds. Different systems will have different amounts of these various types of memory, so the effect of algorithm memory needs can vary greatly from one system to another.
In the early days of electronic computing, if an algorithm and its data would not fit in main memory then the algorithm could not be used. Nowadays the use of virtual memory appears to provide much more memory, but at the cost of performance. Much higher speed can be obtained if an algorithm and its data fit in cache memory; in this case minimizing space will also help minimize time. This is called the principle of locality, and can be subdivided into locality of reference, spatial locality, and temporal locality. An algorithm which will not fit completely in cache memory but which exhibits locality of reference may perform reasonably well.
In software engineering, a bottleneck occurs when the capacity of an application or a computer system is limited by a single component, like the neck of a bottle slowing down the overall water flow. The bottleneck has the lowest throughput of all parts of the transaction path.[1]
System designers try to avoid bottlenecks through direct effort towards locating and tuning existing bottlenecks in a software application. Some examples of engineering bottlenecks that appear include the following: a processor, a communication link, and disk IO.[2] A system or application will hit a bottleneck if the work arrives at a comparatively faster pace relative to other processing components.[3] According to the theory of constraints, improving on the occurrences of hot-spot point of the bottleneck constraint improves the overall processing speed of the software.[4] A thought-provoking stipulation of the theory reveals that improving the efficiency of a particular process stage rather than the constraint can generate even more delay and decrease overall processing capabilities of a software.
The process of tracking down bottlenecks (also referred as "hot spots" - sections of the code that execute most frequently - i.e. have the highest execution count) is called performance analysis. Reduction is achieved with the utilization of specialized tools such as performance analyzers or profilers, the objective being to make particular sections of code perform as effectively as possible to improve overall algorithmic efficiency.
Programming style, also known as coding style, refers to the conventions and patterns used in writing source code, resulting in a consistent and readable codebase. These conventions often encompass aspects such as indentation, naming conventions, capitalization, and comments. Consistent programming style is generally considered beneficial for code readability and maintainability, particularly in collaborative environments.
Maintaining a consistent style across a codebase can improve readability and ease of software maintenance. It allows developers to quickly understand code written by others and reduces the likelihood of errors during modifications. Adhering to standardized coding guidelines ensures that teams follow a uniform approach, making the codebase easier to manage and scale. Many organizations and open-source projects adopt specific coding standards to facilitate collaboration and reduce cognitive load.
Style guidelines can be formalized in documents known as coding conventions, which dictate specific formatting and naming rules. These conventions may be prescribed by official standards for a programming language or developed internally within a team or project. For example, Python's PEP 8 is a widely recognized style guide that outlines best practices for writing Python code. In contrast, languages like C or Java may have industry standards that are either formally documented or adhered to by convention.
Indentation style can assist a reader in various way including: identifying control flow and blocks of code. In some programming languages, indentation is used to delimit blocks of code and therefore is not matter of style. In languages that ignore whitespace, indentation can affect readability.
Haskell encourages the use of literate programming, where extended text explains the genesis of the code. In literate Haskell scripts (named with the lhs extension), everything is a comment except blocks marked as code. The program can be written in LaTeX, in such case the code environment marks what is code. Also, each active code paragraph can be marked by preceding and ending it with an empty line, and starting each line of code with a greater than sign and a space. Here an example using LaTeX markup:
In computer programming, a comment is text embedded in source code that a translator (compiler or interpreter) ignores. Generally, a comment is an annotation intended to make the code easier for a programmer to understand – often explaining an aspect that is not readily apparent in the program (non-comment) code.[1] For this article, comment refers to the same concept in a programming language, markup language, configuration file and any similar context.[2] Some development tools, other than a source code translator, do parse comments to provide capabilities such as API document generation, static analysis, and version control integration. The syntax of comments varies by programming language yet there are repeating patterns in the syntax among languages as well as similar aspects related to comment content.
The flexibility supported by comments allows for a wide degree of content style variability. To promote uniformity, style conventions are commonly part of a programming style guide. But, best practices are disputed and contradictory.[3][4]
Support for code comments is defined by each programming language. The features differ by language, but there are several common attributes that apply throughout.
Comments can also be classified as either prologue or inline based on their position and content relative to program code. A prologue comment is a comment (or group of related comments) located near the top of an associated programming topic, such as before a symbol declaration or at the top of a file. An inline comment is a comment that is located on the same line as and to the right of program code to which is refers.[8] Both prologue and inline comments can be represented as either line or block comments. For example:
Comments can describe an algorithm as pseudocode. This could be done before writing the code as a first draft. If left in the code, it can simplify code review by allowing comparison of the resulting code with the intended logic. For example:
Comments can store metadata about the code. Common metadata includes the name of the original author and subsequent maintainers, dates when first written and modified, link to development and user documentation, and legal information such as copyright and software license.
Some programming tools write metadata into the code as comments.[12] For example, a version control tool might write metadata such as author, date and version number into each file when it's committed to the repository.[13]
Sometimes, comments describe development processes related to the code. For example, comments might describe how to build the code or how to submit changes to the software maintainer.
For example, the following Java comment would be suitable in an introductory text designed to teach beginning programming:
Factors such as personal preference, flexibility of programming tools can influence the commenting style used. For example, the first might be preferred by programmers who use a source code editor that does not automatically format a comment as shown in the second example.
Software consultant and technology commentator Allen Holub[35] advocates aligning the left edges of comments:[36]
If a language supports both line and block comments, programming teams may decide upon a convention of when to use which. For example, line comments only for minor comments, and block comments to for higher-level abstractions.
Syntax for comments varies by programming language. There are common patterns used by multiple languages while also a wide range of syntax among the languages in general. To limit the length of this section, some examples are grouped by languages with the same or very similar syntax. Others are for particular languages that have less common syntax.
Instead of a regular block commenting construct, Perl uses literate programming plain old documentation (POD) markup.[40] For example:[41]
Haskell also provides a literate programming method of commenting known as "Bird Style".[45] Lines starting with > are interpreted as code and everything else is considered a comment. One additional requirement is a blank line before and after the code block:
In MATLAB's programming language, the '%' character indicates a single-line comment.  Multi line comments are also available via %{ and %} brackets and can be nested, e.g.
In computer programming, a naming convention is a set of rules for choosing the character sequence to be used for identifiers which denote variables, types, functions, and other entities in source code and documentation.
As most programming languages do not allow whitespace in identifiers, a method of delimiting each word is needed (to make it easier for subsequent readers to interpret which characters belong to which word). Historically some early languages, notably FORTRAN (1955) and ALGOL (1958), allowed spaces within identifiers, determining the end of identifiers by context. This was abandoned in later languages due to the difficulty of tokenization. It is possible to write names by simply concatenating words, and this is sometimes used, as in mypackage for Java package names,[4] though legibility suffers for longer terms, so usually some form of separation is used.
An alternative is to use underscores; this is common in the C family (including Python), with lowercase words, being found for example in The C Programming Language (1978), and has come to be known as snake case or snail case. Underscores with uppercase, as in UPPER_CASE, are commonly used for C preprocessor macros, hence known as MACRO_CASE, and for environment variables in Unix, such as BASH_VERSION in bash. Sometimes this is humorously referred to as SCREAMING_SNAKE_CASE (alternatively SCREAMING_SNAIL_CASE).
Some naming conventions represent rules or requirements that go beyond the requirements
of a specific project or problem domain, and instead reflect a greater
overarching set of principles defined by the software architecture, underlying programming language or other kind of cross-project methodology.
In C and C++, keywords and standard library identifiers are mostly lowercase. In the C standard library, abbreviated names are the most common (e.g. isalnum for a function testing whether a character is alphanumeric), while the C++ standard library often uses an underscore as a word separator (e.g. out_of_range). Identifiers representing macros are, by convention, written using only uppercase letters and underscores (this is related to the convention in many programming languages of using all-upper-case identifiers for constants). Names containing double underscore or beginning with an underscore and a capital letter are reserved for implementation (compiler, standard library) and should not be used (e.g. __reserved or  _Reserved).[19][20] This is superficially similar to stropping, but the semantics differ: the underscores are part of the value of the identifier, rather than being quoting characters (as is stropping): the value of __foo is __foo (which is reserved), not foo (but in a different namespace).
In computing, a visual programming language (visual programming system, VPL, or, VPS), also known as diagrammatic programming,[1][2] graphical programming or block coding, is a programming language that lets users create programs by manipulating program elements graphically rather than by specifying them textually.[3] A VPL allows programming with visual expressions, spatial arrangements of text and graphic symbols, used either as elements of syntax or secondary notation. For example, many VPLs are based on the idea of "boxes and arrows", where boxes or other screen objects are treated as entities, connected by arrows, lines or arcs which represent relations. VPLs are generally the basis of low-code development platforms.
VPLs may be further classified, according to the type and extent of visual expression used, into icon-based languages, form-based languages, and diagram languages. Visual programming environments provide graphical or iconic elements which can be manipulated by users in an interactive way according to some specific spatial grammar for program construction.
The general goal of VPLs is to make programming more accessible to novices and to support programmers at three different levels[4]
As of 2005, current developments try to integrate the visual programming approach with dataflow programming languages to either have immediate access to the program state, resulting in online debugging, or automatic program generation and documentation. Dataflow languages also allow automatic parallelization, which is likely to become one of the greatest programming challenges of the future.[5]
The Visual Basic, Visual C#, Visual J# etc. languages of the Microsoft Visual Studio integrated development environment (IDE) are not visual programming languages: the representation of algorithms etc. is textual even though the IDE embellishes the editing and debugging activities with a rich user interface. A similar consideration applies to most other rapid application development environments which typically support a form designer and sometimes also have graphical tools to illustrate (but not define) control flow and data dependencies.
The following list is not mutually exclusive, as some visual programming environments may incorporate elements from multiple paradigms. The choice of visual programming paradigm often depends on the specific requirements of the application or the preferences of the users or the developers.
Most of the VPLs are designed for education or domain-specific usage where the target users are novice programmers. But there are some research projects try to provide a general-purpose visual programming language that can be used by mainstream programmers in any software project instead of using textual programming languages (like C, C++, Java, etc.).
For example, research projects such as Envision [8][9] and PWCT[10] are designed to achieve this goal. 
It's common for a VPL to be developed using a textual programming language. Developing general-purpose VPLs allows the other way around. For example, a new textual programming language Compiler and Virtual Machine have been developed and maintained using visual programming for many years.[11] This approach increased the abstraction level, thereby hiding unnecessary details. [12] This increased level of abstraction enables developers to concentrate on the program's overall structure and functionality instead of being entangled in the details of code syntax. [13][14]
Many modern video games make use of behavior trees, which are in principle a family of simple programming languages designed to model behaviors for non-player characters. The behaviors are modeled as trees, and are often edited in graphical editors.
Agile software development is an umbrella term for approaches to developing software that reflect the values and principles agreed upon by The Agile Alliance, a group of 17 software practitioners, in 2001.[1] As documented in their Manifesto for Agile Software Development the practitioners value:[2]
The practitioners cite inspiration from new practices at the time including extreme programming, scrum, dynamic systems development method, adaptive software development and being sympathetic to the need for an alternative to documentation driven, heavyweight software development processes.[3]
Many software development practices emerged from the agile mindset. These agile-based practices, sometimes called Agile (with a capital A)[4] include requirements, discovery and solutions improvement through the collaborative effort of self-organizing and cross-functional teams with their customer(s)/end user(s).[5][6]
While there is much anecdotal evidence that the agile mindset and agile-based practices improve the software development process, the empirical evidence is limited and less than conclusive.[7][8][9]
Iterative and incremental software development methods can be traced back as early as 1957,[10] with evolutionary project management[11][12] and adaptive software development[13] emerging in the early 1970s.[14]
During the 1990s, a number of lightweight software development methods evolved in reaction to the prevailing heavyweight methods (often referred to collectively as waterfall) that critics described as overly regulated, planned, and micromanaged.[15] These lightweight methods included: rapid application development (RAD), from 1991;[16][17] the unified process (UP) and dynamic systems development method (DSDM), both from 1994; Scrum, from 1995; Crystal Clear and extreme programming (XP), both from 1996; and feature-driven development (FDD), from 1997. Although these all originated before the publication of the Agile Manifesto, they are now collectively referred to as agile software development methods.[3]
In 2001, seventeen software developers met at a resort in Snowbird, Utah to discuss lightweight development methods. They were: Kent Beck (Extreme Programming), Ward Cunningham (Extreme Programming), Dave Thomas (Pragmatic Programming, Ruby), Jeff Sutherland (Scrum), Ken Schwaber (Scrum), Jim Highsmith (Adaptive Software Development), Alistair Cockburn (Crystal), Robert C. Martin (SOLID), Mike Beedle (Scrum), Arie van Bennekum, Martin Fowler (OOAD and UML), James Grenning, Andrew Hunt (Pragmatic Programming, Ruby), Ron Jeffries (Extreme Programming), Jon Kern, Brian Marick (Ruby, Test-driven development), and Steve Mellor (OOA). The group, The Agile Alliance, published the Manifesto for Agile Software Development.[2]
In 2005, a group headed by Cockburn and Highsmith wrote an addendum of project management principles, the PM Declaration of Interdependence,[21] to guide software project management according to agile software development methods.
In 2009, a group working with Martin wrote an extension of software development principles, the Software Craftsmanship Manifesto, to guide agile software development according to professional conduct and mastery.
We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:
Most agile development methods break product development work into small increments that minimize the amount of up-front planning and design. Iterations, or sprints, are short time frames (timeboxes)[26] that typically last from one to four weeks.[27]: 20  Each iteration involves a cross-functional team working in all functions: planning, analysis, design, coding, unit testing, and acceptance testing. At the end of the iteration a working product is demonstrated to stakeholders. This minimizes overall risk and allows the product to adapt to changes quickly.[28][29] An iteration might not add enough functionality to warrant a market release, but the goal is to have an available release (with minimal bugs) at the end of each iteration.[30] Through incremental development, products have room to "fail often and early" throughout each iterative phase instead of drastically on a final release date.[31] Multiple iterations might be required to release a product or new features. Working software is the primary measure of progress.[25]
The 6th principle of the agile manifesto for software development states "The most efficient and effective method of conveying information to and within a development team is face-to-face conversation". The manifesto, written in 2001 when video conferencing was not widely used, states this in relation to the communication of information, not necessarily that a team should be co-located.
In agile software development, an information radiator is a (normally large) physical display, board with sticky notes or similar, located prominently near the development team, where passers-by can see it.[36] It presents an up-to-date summary of the product development status.[37] A build light indicator may also be used to inform a team about the current status of their product development.
A common characteristic in agile software development is the daily stand-up (known as daily scrum in the Scrum framework). In a brief session (e.g., 15 minutes), team members review collectively how they are progressing toward their goal and agree whether they need to adapt their approach. To keep to the agreed time limit, teams often use simple coded questions (such as what they completed the previous day, what they aim to complete that day, and whether there are any impediments or risks to progress), and delay detailed discussions and problem resolution until after the stand-up.[38]
Specific tools and techniques, such as continuous integration, automated unit testing, pair programming, test-driven development, design patterns, behavior-driven development, domain-driven design, code refactoring and other techniques are often used to improve quality and enhance product development agility.[39] This is predicated on designing and building quality in from the beginning and being able to demonstrate software for customers at any point, or at least at the end of every iteration.[40]
Compared to traditional software engineering, agile software development mainly targets complex systems and product development with dynamic, indeterministic and non-linear  properties. Accurate estimates, stable plans, and predictions are often hard to get in early stages, and confidence in them is likely to be low. Agile practitioners use their free will to reduce the "leap of faith" that is needed before any evidence of value can be obtained.[41] Requirements and design are held to be emergent. Big up-front specifications would probably cause a lot of waste in such cases, i.e., are not economically sound. These basic arguments and previous industry experiences, learned from years of successes and failures, have helped shape agile development's favor of adaptive, iterative and evolutionary development.[42]
Development methods exist on a continuum from adaptive to predictive.[43] Agile software development methods lie on the adaptive side of this continuum. One key of adaptive development methods is a rolling wave approach to schedule planning, which identifies milestones but leaves flexibility in the path to reach them, and also allows for the milestones themselves to change.[44]
One of the differences between agile software development methods and waterfall is the approach to quality and testing. In the waterfall model, work moves through software development life cycle (SDLC) phases—with one phase being completed before another can start—hence the testing phase is separate and follows a build phase. In agile software development, however, testing is completed in the same iteration as programming.
Because testing is done in every iteration—which develops a small piece of the software—users can frequently use those new pieces of software and validate the value. After the users know the real value of the updated piece of software, they can make better decisions about the software's future. Having a value retrospective and software re-planning session in each iteration—Scrum typically has iterations of just two weeks—helps the team continuously adapt its plans so as to maximize the value it delivers. This follows a pattern similar to the plan-do-check-act (PDCA) cycle, as the work is planned, done, checked (in the review and retrospective), and any changes agreed are acted upon.
This iterative approach supports a product rather than a project mindset. This provides greater flexibility throughout the development process; whereas on projects the requirements are defined and locked down from the very beginning, making it difficult to change them later. Iterative product development allows the software to evolve in response to changes in business environment or market requirements.
In a letter to IEEE Computer, Steven Rakitin expressed cynicism about agile software development, calling it "yet another attempt to undermine the discipline of software engineering" and translating "working software over comprehensive documentation" as "we want to spend all our time coding. Remember, real programmers don't write documentation."[47]
This is disputed by proponents of agile software development, who state that developers should write documentation if that is the best way to achieve the relevant goals, but that there are often better ways to achieve those goals than writing static documentation.[48]
Scott Ambler states that documentation should be "just barely good enough" (JBGE),[49] that too much or comprehensive documentation would usually cause waste, and developers rarely trust detailed documentation because it's usually out of sync with code,[48] while too little documentation may also cause problems for maintenance, communication, learning and knowledge sharing. Alistair Cockburn wrote of the Crystal Clear method:
Agile software development methods support a broad range of the software development life cycle.[51] Some methods focus on the practices (e.g., XP, pragmatic programming, agile modeling), while some focus on managing the flow of work (e.g., Scrum, Kanban). Some support activities for requirements specification and development (e.g., FDD), while some seek to cover the full development life cycle (e.g., DSDM, RUP).
Agile software development is supported by a number of concrete practices, covering areas like requirements, design, modeling, coding, testing, planning, risk management, process, quality, etc. Some notable agile software development practices include:[52]
Situation-appropriateness should be considered as a distinguishing characteristic between agile methods and more plan-driven software development methods, with agile methods allowing product development teams to adapt working practices according to the needs of individual products.[70][69] Potentially, most agile methods could be suitable for method tailoring,[51] such as DSDM tailored in a CMM context.[71] and XP tailored with the Rule Description Practices (RDP) technique.[72] Not all agile proponents agree, however, with Schwaber noting "that is how we got into trouble in the first place, thinking that the problem was not having a perfect methodology. Efforts [should] center on the changes [needed] in the enterprise".[73] Bas Vodde reinforced this viewpoint, suggesting that unlike traditional, large methodologies that require you to pick and choose elements, Scrum provides the basics on top of which you add additional elements to localize and contextualize its use.[74] Practitioners seldom use system development methods, or agile methods specifically, by the book, often choosing to omit or tailor some of the practices of a method in order to create an in-house method.[75]
In practice, methods can be tailored using various tools. Generic process modeling languages such as Unified Modeling Language can be used to tailor software development methods. However, dedicated tools for method engineering such as the Essence Theory of Software Engineering of SEMAT also exist.[76]
Agile software development has been widely seen as highly suited to certain types of environments, including small teams of experts working on greenfield projects,[46][77] and the challenges and limitations encountered in the adoption of agile software development methods in a large organization with legacy infrastructure are well-documented and understood.[78]
When agile software development is applied in a distributed setting (with teams dispersed across multiple business locations), it is commonly referred to as distributed agile software development. The goal is to leverage the unique benefits offered by each approach. Distributed development allows organizations to build software by strategically setting up teams in different parts of the globe, virtually building software round-the-clock (more commonly referred to as follow-the-sun model). On the other hand, agile development provides increased transparency, continuous feedback, and more flexibility when responding to changes.
Agile software development methods were initially seen as best suitable for non-critical product developments, thereby excluded from use in regulated domains such as medical devices, pharmaceutical, financial, nuclear systems, automotive, and avionics sectors, etc. However, in the last several years, there have been several initiatives for the adaptation of agile methods for these domains.[84][85][86][87][88]
Although agile software development methods can be used with any programming paradigm or language in practice, they were originally closely associated with object-oriented environments such as Smalltalk, Lisp and later Java, C#. The initial adopters of agile methods were usually small to medium-sized teams working on unprecedented systems with requirements that were difficult to finalize and likely to change as the system was being developed. This section describes common problems that organizations encounter when they try to adopt agile software development methods as well as various techniques to measure the quality and performance of agile teams.[90]
The Agility measurement index, amongst others, rates developments against five dimensions of product development (duration, risk, novelty, effort, and interaction).[91] Other techniques are based on measurable goals[92] and one study suggests that velocity can be used as a metric of agility. There are also agile self-assessments to determine whether a team is using agile software development practices (Nokia test,[93] Karlskrona test,[94] 42 points test).[95]
One of the early studies reporting gains in quality, productivity, and business satisfaction by using agile software developments methods was a survey conducted by Shine Technologies from November 2002 to January 2003.[96]
A similar survey, the State of Agile, is conducted every year starting in 2006 with thousands of participants from around the software development community. This tracks trends on the perceived benefits of agility, lessons learned, and good practices. Each survey has reported increasing numbers saying that agile software development helps them deliver software faster; improves their ability to manage changing customer priorities; and increases their productivity.[97] Surveys have also consistently shown better results with agile product development methods compared to classical project management.[98][99] In balance, there are reports that some feel that agile development methods are still too young to enable extensive academic research of their success.[100]
Organizations and teams implementing agile software development often face difficulties transitioning from more traditional methods such as waterfall development, such as teams having an agile process forced on them.[101] These are often termed agile anti-patterns or more commonly agile smells. Below are some common examples:
A goal of agile software development is to focus more on producing working software and less on documentation. This is in contrast to waterfall models where the process is often highly controlled and minor changes to the system require significant revision of supporting documentation. However, this does not justify completely doing without any analysis or design at all. Failure to pay attention to design can cause a team to proceed rapidly at first, but then to require significant rework as they attempt to scale up the system. One of the key features of agile software development is that it is iterative. When done correctly, agile software development allows the design to emerge as the system is developed and helps the team discover commonalities  and opportunities for re-use.[102]
In agile software development, stories (similar to use case descriptions) are typically used to define requirements and an iteration is a short period of time during which the team commits to specific goals.[103] Adding stories to an iteration in progress is detrimental to a good flow of work. These should be added to the product backlog and prioritized for a subsequent iteration or in rare cases the iteration could be cancelled.[104]
Agile software development is often implemented as a grassroots effort in organizations by software development teams trying to optimize their development processes and ensure consistency in the software development life cycle. By not having sponsor support, teams may face difficulties and resistance from business partners, other development teams and management. Additionally, they may suffer without appropriate funding and resources.[105] This increases the likelihood of failure.[106]
A survey performed by VersionOne found respondents cited insufficient training as the most significant cause for failed agile implementations[107] Teams have fallen into the trap of assuming the reduced processes of agile software development compared to other approaches such as waterfall means that there are no actual rules for agile software development. [citation needed]
Agile software development requires teams to meet product commitments, which means they should focus on work for only that product. However, team members who appear to have spare capacity are often expected to take on other work, which makes it difficult for them to help complete the work to which their team had committed.[110]
Teams may fall into the trap of spending too much time preparing or planning. This is a common trap for teams less familiar with agile software development where the teams feel obliged to have a complete understanding and specification of all stories. Teams should be prepared to move forward with only those stories in which they have confidence, then during the iteration continue to discover and prepare work for subsequent iterations (often referred to as backlog refinement or grooming).
One of the intended benefits of agile software development is to empower the team to make choices, as they are closest to the problem. Additionally, they should make choices as close to implementation as possible, to use more timely information in the decision. If team members are assigned tasks by others or too early in the process, the benefits of localized and timely decision making can be lost.[112]
Being assigned work also constrains team members into certain roles (for example, team member A must always do the database work), which limits opportunities for cross-training.[112] Team members themselves can choose to take on tasks that stretch their abilities and provide cross-training opportunities.
Test automation also supports continued refactoring required by iterative software development. Allowing a developer to quickly run tests to confirm refactoring has not modified the functionality of the application may reduce the workload and increase confidence that cleanup efforts have not introduced new defects.
A common misconception is that agile software development allows continuous change, however an iteration backlog is an agreement of what work can be completed during an iteration.[118] Having too much work-in-progress (WIP) results in inefficiencies such as context-switching and queueing.[119] The team must avoid feeling pressured into taking on additional work.[120]
Agile software development fixes time (iteration duration), quality, and ideally resources in advance (though maintaining fixed resources may be difficult if developers are often pulled away from tasks to handle production incidents), while the scope remains variable. The customer or product owner often pushes for a fixed scope for an iteration. However, teams should be reluctant to commit to the locked time, resources and scope (commonly known as the project management triangle). Efforts to add scope to the fixed time and resources of agile software development may result in decreased quality.[121]
Agile project management is an iterative development process, where feedback is continuously gathered from users and stakeholders to create the right user experience.  Different methods can be used to perform an agile process, these include scrum, extreme programming, lean and kanban.[123]
The term agile management is applied to an iterative, incremental method of managing the design and build activities of engineering, information technology and other business areas that aim to provide new product or service development in a highly flexible and interactive manner, based on the principles expressed in the Manifesto for Agile Software Development.[124]
Agile project management metrics help reduce confusion, identify weak points, and measure team's performance throughout the development cycle. Supply chain agility is the ability of a supply chain to cope with uncertainty and variability on offer and demand. An agile supply chain can increase and reduce its capacity rapidly, so it can adapt to a fast-changing customer demand. Finally, strategic agility is the ability of an organisation to change its course of action as its environment is evolving. The key for strategic agility is to recognize external changes early enough and to allocate resources to adapt to these changing environments.[123]
Agile management also offers a simple framework promoting communication and reflection on past work amongst team members.[127] Teams who were using traditional waterfall planning and adopted the agile way of development typically go through a transformation phase and often take help from agile coaches who help guide the teams through a smoother transformation. There are typically two styles of agile coaching: push-based and pull-based agile coaching. Here a "push-system" can refer to an upfront estimation of what tasks can be fitted into a sprint (pushing work) e.g. typical with scrum; whereas a "pull system" can refer to an environment where tasks are only performed when capacity is available.[128] Agile management approaches have also been employed and adapted to the business and government sectors. For example, within the federal government of the United States, the United States Agency for International Development (USAID) is employing a collaborative project management approach that focuses on incorporating collaborating, learning and adapting (CLA) strategies to iterate and adapt programming.[129]
According to Jean-Loup Richet (research fellow at ESSEC Institute for Strategic Innovation & Services) "this approach can be leveraged effectively for non-software products and for project management in general, especially in areas of innovation and uncertainty." The result is a product or project that best meets current customer needs and is delivered with minimal costs, waste, and time, enabling companies to achieve bottom line gains earlier than via traditional approaches.[131]
Agile software development methods have been extensively used for development of software products and some of them use certain characteristics of software, such as object technologies.[132] However, these techniques can be applied to the development of non-software products, such as computers, medical devices, food, clothing, and music.[133] Agile software development methods have been used in non-development IT infrastructure deployments and migrations. Some of the wider principles of agile software development have also found application in general management[134] (e.g., strategy, governance, risk, finance) under the terms business agility or agile business management. Agile software methodologies have also been adopted for use with the learning engineering process, an iterative data-informed process that applies the learning sciences, human-centered design, and data informed decision-making to support learners and their development.[135]
Agile software development paradigms can be used in other areas of life such as raising children. Its success in child development might be founded on some basic management principles; communication, adaptation, and awareness. In a TED Talk, Bruce Feiler shared how he applied basic agile paradigms to household management and raising children.[136]
Agile practices have been cited as potentially inefficient in large organizations and certain types of development.[137] Many organizations believe that agile software development methodologies are too extreme and adopt a hybrid approach[138] that mixes elements of agile software development and plan-driven approaches.[139] Some methods, such as dynamic systems development method (DSDM) attempt this in a disciplined way, without sacrificing fundamental principles.
Alistair Cockburn organized a celebration of the 10th anniversary of the Manifesto for Agile Software Development in Snowbird, Utah on 12 February 2011, gathering some 30+ people who had been involved at the original meeting and since. A list of about 20 elephants in the room ('undiscussable' agile topics/issues) were collected, including aspects: the alliances, failures and limitations of agile software development practices and context (possible causes: commercial interests, decontextualization, no obvious way to make progress based on failure, limited objective evidence, cognitive biases and reasoning fallacies), politics and culture.[141] As Philippe Kruchten wrote:
Pascal is an imperative and procedural programming language, designed by Niklaus Wirth as a small, efficient language intended to encourage good programming practices using structured programming and data structuring. It is named after French mathematician, philosopher and physicist Blaise Pascal.[a]
On top of ALGOL's scalars and arrays, Pascal enables defining complex datatypes and building dynamic and recursive data structures such as lists, trees and graphs. Pascal has strong typing on all objects, which means that one type of data cannot be converted to or interpreted as another without explicit conversions. Unlike C (and also unlike most other languages in the C-family), Pascal allows nested procedure definitions to any level of depth, and also allows most kinds of definitions and declarations inside subroutines (procedures and functions). A program is thus syntactically similar to a single procedure or function. This is similar to the block structure of ALGOL 60, but restricted from arbitrary block statements to just procedures and functions.
Pascal became very successful in the 1970s, notably on the burgeoning minicomputer market. Compilers were also available for many microcomputers as the field emerged in the late 1970s. It was widely used as a teaching language in university-level programming courses in the 1980s, and also used in production settings for writing commercial software during the same period. It was displaced by the C programming language during the late 1980s and early 1990s as UNIX-based systems became popular, and especially with the release of C++.
A derivative named Object Pascal designed for object-oriented programming was developed in 1985. This was used by Apple Computer (for the Lisa and Macintosh machines) and Borland in the late 1980s and later developed into Delphi on the Microsoft Windows platform. Extensions to the Pascal concepts led to the languages Modula-2 and Oberon, both developed by Wirth.
Much of the history of computer language design during the 1960s can be traced to the ALGOL 60 language. ALGOL was developed during the 1950s with the explicit goal of being able to clearly describe algorithms. It included a number of features for structured programming that remain common in languages to this day.
Shortly after its introduction, in 1962 Wirth began working on his dissertation with Helmut Weber on the Euler programming language. Euler was based on ALGOL's syntax and many concepts but was not a derivative. Its primary goal was to add dynamic lists and types, allowing it to be used in roles similar to Lisp. The language was published in 1965.
Pascal was influenced by the ALGOL W efforts, with the explicit goals of teaching programming in a structured fashion and for the development of system software.[5] A generation of students used Pascal as an introductory language in undergraduate courses. Other goals included providing a reliable and efficient tool for writing large programs,[6] and bridging the canyon between scientific and commercial programming, as represented by the then-widespread languages Fortran and COBOL, with a general-purpose language.[7]
One of the early successes for the language was the introduction of UCSD Pascal, a version that ran on a custom operating system that could be ported to different platforms. A key platform was the Apple II, where it saw widespread use as Apple Pascal. This led to Pascal becoming the primary high-level language used for development in the Apple Lisa, and later, the Macintosh. Parts of the original Macintosh operating system were hand-translated into Motorola 68000 assembly language from the Pascal source code.[8]
The typesetting system TeX by Donald Knuth was written in WEB, the original literate programming system, based on DEC PDP-10 Pascal. Successful commercial applications like Adobe Photoshop[9] were written in Macintosh Programmer's Workshop Pascal, while applications like Total Commander, Skype[10] and Macromedia Captivate were written in Delphi (Object Pascal). Apollo Computer used Pascal as the systems programming language for its operating systems beginning in 1980.
During work on the Lisa, Larry Tesler began corresponding with Wirth on the idea of adding object-oriented extensions to the language, to make Pascal a multi-paradigm programming language. This led initially to Clascal, introduced in 1983. As the Lisa program faded and was replaced by the Macintosh, a further version was created and named Object Pascal. This was introduced on the Mac in 1985 as part of the MacApp application framework, and became Apple's main development language into the early 1990s.
The first Pascal compiler was designed in Zürich for the CDC 6000 series mainframe computer family. Niklaus Wirth reports that a first attempt to implement it in FORTRAN 66 in 1969 was unsuccessful due to FORTRAN 66's inadequacy to express complex data structures. The second attempt was implemented in a C-like language (Scallop by Max Engeli) and then translated by hand (by R. Schild) to Pascal itself for boot-strapping.[13] It was operational by mid-1970. Many Pascal compilers since have been similarly self-hosting, that is, the compiler is itself written in Pascal, and the compiler is usually capable of recompiling itself when new features are added to the language, or when the compiler is to be ported to a new environment. The GNU Pascal compiler is one notable exception, being written in C.
The first successful port of the CDC Pascal compiler to another mainframe was completed by Welsh and Quinn at the Queen's University of Belfast (QUB) in 1972. The target was the International Computers Limited (ICL) 1900 series. This compiler, in turn, was the parent of the Pascal compiler for the Information Computer Systems (ICS) Multum minicomputer. The Multum port was developed – with a view to using Pascal as a systems programming language – by Findlay, Cupples, Cavouras and Davis, working at the Department of Computing Science in Glasgow University. It is thought that Multum Pascal, which was completed in the summer of 1973, may have been the first 16-bit implementation.
UCSD Pascal branched off Pascal-P2, where Kenneth Bowles used it to create the interpretive UCSD p-System. It was one of three operating systems available at the launch of the original IBM Personal Computer.[14] UCSD Pascal used an intermediate code based on byte values, and thus was one of the earliest bytecode compilers. Apple Pascal was released in 1979 for the Apple II and Apple III computer systems. It was an implementation of, or largely based on, UCSD Pascal. Pascal-P1 through Pascal-P4 were not, but rather based on the CDC 6600 60-bit word length.
In 1986, Anders ported Turbo Pascal to the Macintosh and incorporated Apple's Object Pascal extensions into Turbo Pascal. These extensions were then added back into the PC version of Turbo Pascal for version 5.5. At the same time Microsoft also implemented the Object Pascal compiler.[16][17] Turbo Pascal 5.5 had a large influence on the Pascal community, which began concentrating mainly on the IBM PC in the late 1980s. Many PC hobbyists in search of a structured replacement for BASIC used this product. It also began to be adopted by professional developers. Around the same time a number of concepts were imported from C to let Pascal programmers use the C-based application programming interface (API) of Microsoft Windows directly. These extensions included null-terminated strings, pointer arithmetic, function pointers, an address-of operator, and unsafe typecasts.
Turbo Pascal and other derivatives with unit or module structures are modular programming languages. However, it does not provide a nested module concept or qualified import and export of specific symbols.
The universities of Wisconsin–Madison, Zürich, Karlsruhe, and Wuppertal developed the Pascal-SC[18][19] and Pascal-XSC[20][21][22] (Extensions for Scientific Computation) compilers, aimed at programming numerical computations.  Development for Pascal-SC started in 1978 supporting ISO 7185 Pascal level 0, but level 2 support was added at a later stage.[23] Pascal-SC originally targeted the Z80 processor, but was later rewritten for DOS (x86) and 68000. Pascal-XSC has at various times been ported to Unix (Linux, SunOS, HP-UX, AIX) and Microsoft/IBM (DOS with EMX, OS/2, Windows) operating systems. It operates by generating intermediate C source code which is then compiled to a native executable. Some of the Pascal-SC language extensions have been adopted by GNU Pascal.
Pascal Sol was designed around 1983 by a French team to implement a Unix-like system named Sol.  It was standard Pascal level-1 (with parameterized array bounds) but the definition allowed alternative keywords and predefined identifiers in French and the language included a few extensions to ease system programming (e.g. an equivalent to lseek).[24] The Sol team later on moved to the ChorusOS project to design a distributed operating system.[25]
IP Pascal is an implementation of the Pascal programming language using Micropolis DOS, but was moved rapidly to CP/M-80 running on the Z80. It was moved to the 80386 machine types in 1994, and exists today as Windows XP and Linux implementations. In 2008, the system was brought up to a new level and the resulting language termed "Pascaline" (after Pascal's calculator). It includes objects, namespace controls, dynamic arrays, and many other extensions, and generally features the same functionality and type protection as C#. It is the only such implementation that is also compatible with the original Pascal implementation, which is standardized as ISO 7185.
A set is a fundamental concept for modern mathematics, and they may be used in many algorithms. Such a feature is useful and may be faster than an equivalent construct in a language that does not support sets. For example, for many Pascal compilers:
Pascal is a structured programming language, meaning that the flow of control is structured into standard statements, usually without 'goto' commands.
The ISO 7185 was stated to be a clarification of Wirth's 1974 language as detailed by the User Manual and Report [Jensen and Wirth], but was also notable for adding "Conformant Array Parameters" as a level 1 to the standard, level 0 being Pascal without conformant arrays. This addition was made at the request of C. A. R. Hoare, and with the approval of Niklaus Wirth. The precipitating cause was that Hoare wanted to create a Pascal version of the (NAG) Numerical Algorithms Library, which had originally been written in FORTRAN, and found that it was not possible to do so without an extension that would allow array parameters of varying size. Similar considerations motivated the inclusion in ISO 7185 of the facility to specify the parameter types of procedural and functional parameters.
In the early 1990s, Alan Burns and Geoff Davies developed Pascal-FC, an extension to Pl/0 (from the Niklaus' book Algorithms + Data Structures = Programs). Several constructs were added to use Pascal-FC as a teaching tool for Concurrent Programming (such as semaphores, monitors, channels, remote-invocation and resources). To be able to demonstrate concurrency, the compiler output (a kind of P-code) could then be executed on a virtual machine. This virtual machine not only simulated a normal – fair – environment, but could also simulate extreme conditions (unfair mode).
Wirth's initial definition of the language was widely criticized. In particular, Nico Habermann commented in his "Critical Comments on the Programming Language Pascal" (1973) that many of its constructs were poorly defined, in particular for data types, ranges, structures, and goto.[45] Later, Brian Kernighan, who popularized the C language, outlined his criticisms of Pascal in 1981 in his article "Why Pascal is Not My Favorite Programming Language".[46] The most serious problem Kernighan described was that array sizes and string lengths were part of the type, so it was not possible to write a function that would accept variable-length arrays or even strings as parameters. This made it unfeasible to write, for example, a sorting library. Kernighan also criticized the unpredictable order of evaluation of Boolean expressions, poor library support, and lack of static variables, and raised a number of smaller issues. Also, he stated that the language did not provide any simple constructs to "escape" (knowingly and forcibly ignore) restrictions and limitations.  More general complaints from other sources[28][47] noted that the scope of declarations was not clearly defined in the original language definition, which sometimes had serious consequences when using forward declarations to define pointer types, or when record declarations led to mutual recursion, or when an identifier may or may not have been used in an enumeration list. Another difficulty was that, like ALGOL 60, the language did not allow procedures or functions passed as parameters to predefine the expected type of their parameters.
In the two decades after 1975, Pascal gained increasing attention and became a major programming language for important platforms (including Apple II, Apple III, Apple Lisa, Commodore systems, Z-80-based machines and IBM PC) due to the availability of UCSD Pascal and Turbo Pascal.[48]
The Art of Computer Programming (TAOCP) is a comprehensive multi-volume monograph written by the computer scientist Donald Knuth presenting programming algorithms and their analysis. At present it consists of published volumes 1,2,3,4A, and 4B, with more expected to be released in the future. The book is intended to represent the central core of computer programming for sequential machines.
In January 1962, when he was a graduate student in the mathematics department at Caltech, Knuth was approached by Addison-Wesley to write a book about compiler design, and he proposed a larger scope. He came up with a list of twelve chapter titles the same day. In the summer of 1962 he worked on a FORTRAN compiler for UNIVAC, considering that he had "sold my soul to the devil" to develop a FORTRAN compiler[7]: 15  after ALGOL developments with Burroughs. He remained as a consultant to Burroughs over the period 1960 to 1968 while writing Volume 1 "Fundamental Algorithms".
The first volume of "The Art of Computer Programming", "Fundamental Algorithms", took five years to complete between 1963 and 1968 while working at both Caltech and Burroughs.
Section 2.5 of "Fundamental Algorithms" is on Dynamic Storage Allocation. Parts of this are used in the Burroughs approach to memory management. Knuth claims credit for “The “boundary-tag” method, introduced in Section 2.5, was designed by the author in 1962 for use in a control program for the B5000 computer.”[9]: 460
All examples in the books use a hypothetical language called "MIX assembly language" (MIXAL), which runs on "a mythical computer called MIX". Currently,[when?] the MIX computer is being replaced by the MMIX computer, which is a RISC version. The conversion from MIX to MMIX was a large ongoing project for which Knuth solicited volunteers for help. Software such as GNU MDK[11] exists to provide emulation of the MIX architecture. Knuth considers the use of assembly language necessary for the speed and memory usage of algorithms to be judged.
Knuth was awarded the 1974 Turing Award "for his major contributions to the analysis of algorithms […], and in particular for his contributions to the 'art of computer programming' through his well-known books in a continuous series by this title."[12] American Scientist has included this work among "100 or so Books that shaped a Century of Science", referring to the twentieth century.[13] Covers of the third edition of Volume 1 quote Bill Gates as saying, "If you think you're a really good programmer… read (Knuth's) Art of Computer Programming… You should definitely send me a résumé if you can read the whole thing."[14] The New York Times referred to it as "the profession's defining treatise".[15]
IEEE Software is a bimonthly peer-reviewed magazine and scientific journal published by the IEEE Computer Society covering all aspects of software engineering, processes, and practices. Its mission is to be the best source of reliable, useful, peer-reviewed information for leading software practitioners—the developers and managers who want to keep up with rapid technology change. It was established in 1983 and is published by the IEEE Computer Society. According to the Journal Citation Reports, the journal has a 2022 impact factor of 3.3.[1]
IEEE Software received the APEX 2016 Award of Excellence in the "Magazines, Journals & Tabloids — Electronic" category. IEEE Software's November/December 2016 issue, "The Role of the Software Architect," won the 2017 Folio Eddies Digital Award in the "Standalone Digital Magazine; Association/Non-Profit (B-to-B) – Standalone Digital Magazine – less than 6 issues" category. IEEE Software also received an honorable mention in the Folio Digital Awards in 2018.
A software development kit (SDK) is a collection of software development tools in one installable package. They facilitate the creation of applications by having a compiler, debugger and sometimes a software framework. They are normally specific to a hardware platform and operating system combination. To create applications with advanced functionalities such as advertisements, push notifications, etc; most application software developers use specific software development kits.
An SDK can take the form of application programming interfaces[1] in the form of on-device libraries of reusable functions used to interface to a particular programming language, or it may be as complex as hardware-specific tools that can communicate with a particular embedded system.[2] Common tools include debugging facilities and other utilities, often presented in an integrated development environment.[3] SDKs may include sample software and/or technical notes along with documentation, and tutorials to help clarify points made by the primary reference material.[4][5]
SDKs often include licenses that make them unsuitable for building software intended to be developed under an incompatible license. For example, a proprietary SDK is generally incompatible with free software development, while a GNU General Public License'd SDK could be incompatible with proprietary software development, for legal reasons.[6][7] However, SDKs built under the GNU Lesser General Public License are typically usable for proprietary development.[8]  In cases where the underlying technology is new, SDKs may include hardware.  For example, AirTag's 2012 near-field communication SDK included both the paying and the reading halves of the necessary hardware stack.[9]
Providers of SDKs for specific systems or subsystems sometimes substitute a more specific term instead of software. For instance, both Microsoft[13] and Citrix[14] provide a driver development kit for developing device drivers.
Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications. It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs.[1][2][3][4]
The terms programmer and coder overlap software engineer, but they imply only the construction aspect of a typical software engineer workload.[5]
A software engineer applies a software development process,[1][6] which involves defining, implementing, testing, managing, and maintaining software systems, as well as developing the software development process itself.
The development of software engineering was seen as a struggle. Problems included software that was over budget, exceeded deadlines, required extensive debugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed.
In 1968, NATO held the first software engineering conference where issues related to software were addressed. Guidelines and best practices for the development of software were established.[7]
The origins of the term software engineering have been attributed to various sources. The term appeared in a list of services offered by companies in the June 1965 issue of "Computers and Automation"[8] and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) in "President's Letter to the ACM Membership" by Anthony A. Oettinger.[9][10][11] It is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer.[12] Margaret Hamilton described the discipline of "software engineering" during the Apollo missions to give what they were doing legitimacy.[13] At the time there was perceived to be a "software crisis".[14][15][16] The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of "Software Engineering" with the Plenary Sessions' keynotes of Frederick Brooks[17] and Margaret Hamilton.[18]
In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States.[19]
Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.[19] The Process Maturity Levels introduced became the Capability Maturity Model Integration for Development (CMMI-DEV), which defined how the US Government evaluates the abilities of a software development team.
Modern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK).[6] Software engineering is considered one of the major computing disciplines.[20]
Margaret Hamilton promoted the term "software engineering" during her work on the Apollo program. The term "engineering" was used to acknowledge that the work should be taken just as seriously as other contributions toward the advancement of technology. Hamilton details her use of the term:
When I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new "term" per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.[30]
Individual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering.[31][32] Steve McConnell has said that it is not, but that it should be.[33] Donald Knuth has said that programming is an art and a science.[34] Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused in the United States.[35]
Requirements engineering is about elicitation, analysis, specification, and validation of requirements for software. Software requirements can be functional, non-functional or domain.
Functional requirements describe expected behaviors (i.e. outputs). Non-functional requirements specify issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects.[36]
Software design is the process of making high-level plans for the software. Design is sometimes divided into levels:
Software construction typically involves programming (a.k.a. coding), unit testing, integration testing, and debugging so as to implement the design.[1][6]“Software testing is related to, but different from, ... debugging”.[6]
Testing during this phase is generally performed by the programmer and with the purpose to verify that the code behaves as designed and to know when the code is ready for the next level of testing.[citation needed]
Software testing is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the software under test.[1][6]
When described separately from construction, testing typically is performed by test engineers or quality assurance instead of the programmers who wrote it.  It is performed at the system level and is considered an aspect of software quality.
Software maintenance refers to supporting the software after release. It may include but is not limited to: error correction, optimization, deletion of unused and discarded features, and enhancement of existing features.[1][6]
Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004, the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.[39]
Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014.[20] A number of universities have Software Engineering degree programs; as of 2010[update], there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.
In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.
Half of all practitioners today have degrees in computer science, information systems, or information technology.[citation needed] A small but growing number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering bachelor's degree in the world; in the following year, the University of Sheffield established a similar program.[40] In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States; however, it did not obtain ABET accreditation until 2003, the same year as Rice University, Clarkson University, Milwaukee School of Engineering, and Mississippi State University.[41] In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering.[citation needed]
Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society. As of 2004[update], about 50 universities in the U.S. offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering master's degree was established at Seattle University in 1979. Since then, graduate software engineering degrees have been made available from many more universities. Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.
In 1998, the US Naval Postgraduate School (NPS) established the first doctorate program in Software Engineering in the world.[citation needed] Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers.[42] ETS (École de technologie supérieure) University and UQAM (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer.[6]
Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario,[43] and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society.
In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, thereby allowing Software Engineers to be licensed and recognized.[44] NCEES ended the exam after April 2019 due to lack of participation.[45] Mandatory licensing is currently still largely debated, and perceived as controversial.[46][47]
The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current version is SWEBOK v4.[6] The IEEE also promulgates a "Software Engineering Code of Ethics".[48]
There are an estimated 26.9 million professional software engineers in the world as of 2022, up from 21 million in 2016.[49][50]
Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns, often university or college students during a summer break, or externships. Specializations include analysts, architects, developers, testers, technical support, middleware analysts, project managers, software product managers, educators, and researchers.
Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008.[51] Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, Thrombosis, Obesity, and hand and wrist problems such as carpal tunnel syndrome.[52]
The U. S. Bureau of Labor Statistics (BLS) counted 1,365,500 software developers holding jobs in the U.S. in 2018.[53] Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees.[54] The BLS estimates from 2023 to 2033 that computer software engineering would increase by 17%.[55] This is down from the 2022 to 2032 BLS estimate of 25% for software engineering.[55][56] And, is further down from their 30% 2010 to 2020 BLS estimate.[57] Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries.[58][51] In addition, the BLS Job Outlook for Computer Programmers, the U.S. Bureau of Labor Statistics (BLS) Occupational Outlook predicts a decline of -7 percent from 2016 to 2026, a further decline of -9 percent from 2019 to 2029, a decline of -10 percent from 2021 to 2031.[58] and then a decline of -11 percent from 2022 to 2032.[58] Since computer programming can be done from anywhere in the world, companies sometimes hire programmers in countries where wages are lower.[58][59][60] Furthermore, the ratio of women in many software fields has also been declining over the years as compared to other engineering fields.[61] Then there is the additional concern that recent advances in Artificial Intelligence might impact the demand for future generations of Software Engineers.[62][63][64][65][66][67][68] However, this trend may change or slow in the future as many current software engineers in the U.S. market flee the profession or age out of the market in the next few decades.[58]
The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture.[69] IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies.[70] These certification programs are tailored to the institutions that would employ people who use these technologies.
Broader certification of general software engineering skills is available through various professional societies. As of 2006[update], the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP).[71] In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA).[72] The ACM had a professional certification program in the early 1980s,[citation needed] which was discontinued due to lack of interest. The ACM and the IEEE Computer Society together examined the possibility of licensing of software engineers as Professional Engineers in the 1990s,
but eventually decided that such licensing was inappropriate for the professional industrial practice of software engineering.[46] John C. Knight and Nancy G. Leveson presented a more balanced analysis of the licensing issue in 2002.[47]
In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the British Computer Society or Institution of Engineering and Technology and so qualify to be considered for Chartered Engineer status through either of those institutions. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP).[73] In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng.[74] The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.
The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers.[75] Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected.[76] Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations.[77] When North Americans leave work, Asians are just arriving to work. When Asians are leaving work, Europeans arrive to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.
While global outsourcing has several advantages, global – and generally distributed – development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations).[78] Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.
Some claim that the concept of software engineering is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.[83]
Some claim that a core issue with software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a "theoretical environment."[83]
Edsger Dijkstra, a founder of many of the concepts in software development today, rejected the idea of "software engineering" up until his death in 2002, arguing that those terms were poor analogies for what he called the "radical novelty" of computer science:
A number of these phenomena have been bundled under the name "Software Engineering". As economics is known as "The Miserable Science", software engineering should be known as "The Doomed Discipline", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter "How to program if you cannot."[84]
Software maintenance is often considered lower skilled and less rewarding than new development. As such, it is a common target for outsourcing or offshoring. Usually, the team developing the software is different from those who will be maintaining it. The developers lack an incentive to write the code to be easily maintained. Software is often delivered incomplete and almost always contains some bugs that the maintenance team must fix. Software maintenance often initially includes the development of new functionality, but as the product nears the end of its lifespan, maintenance is reduced to the bare minimum and then cut off entirely before the product is withdrawn.
Software maintenance is not as well studied as other phases of the software life cycle, despite comprising the majority of costs. Understanding has not changed significantly since the 1980s. Software maintenance can be categorized into several types depending on whether it is preventative or reactive and whether it is seeking to add functionality or preserve existing functionality, the latter typically in the face of a changed environment.
In the early 1970s, companies began to separate out software maintenance with its own team of engineers to free up software development teams from support tasks.[1] In 1972, R. G. Canning published "The Maintenance 'Iceberg'", in which he contended that software maintenance was an extension of software development with an additional input: the existing system.[1] The discipline of software maintenance has changed little since then.[2] One twenty-first century innovation has been companies deliberately releasing incomplete software and planning to finish it post-release. This type of change, and others that expand functionality, is often called software evolution instead of maintenance.[2]
Despite testing and quality assurance, virtually all software contains bugs where the system does not work as intended. Post-release maintenance is necessary to remediate these bugs when they are found.[3] Most software is a combination of pre-existing commercial off-the-shelf (COTS) and open-source software components with custom-written code. COTS and open-source software is typically updated over time, which can reduce the maintenance burden, but the modifications to these software components will need to be adjusted for in the final product.[4] Unlike software development, which is focused on meeting specified requirements, software maintenance is driven by events—such as user requests or detection of a bug.[5] Its main purpose is to preserve the usefulness of the software, usually in the face of changing requirements.[6]
If conceived of as part of the software development life cycle, maintenance is the last and typically the longest phase of the cycle,[7][8] comprising 80 to 90 percent of the lifecycle cost.[9] Other models consider maintenance separate from software development, instead as part of the software maintenance life cycle (SMLC).[8] SMLC models typically include understanding the code, modifying it, and revalidating it.[8]
Frequently, software is delivered in an incomplete state. Developers will test a product until running out of time or funding, because they face fewer consequences for an imperfect product than going over time or budget.[10] The transition from the development to the maintenance team is often inefficient, without lists of known issues or validation tests, which the maintenance team will  likely recreate.[11] After release, members of the development team are likely to be reassigned or otherwise become unavailable. The maintenance team will require additional resources for the first year after release, both for technical support and fixing defects left over from development.[10]
Initially, software may go through a period of enhancements after release. New features are added according to user feedback. At some point, the company may decide that it is no longer profitable to make functional improvements, and restrict support to bug fixing and emergency updates. Changes become increasingly difficult and expensive due to lack of expertise or decaying architecture due to software aging. After a product is no longer maintained, and does not receive even this limited level of updating, some vendors will seek to extract revenue from the software as long as possible, even though the product is likely to become increasingly avoided. Eventually, the software will be withdrawn from the market, although it may remain in use. During this process, the software becomes a legacy system.[12]
The first step in the change cycle is receiving a change request from a customer and analyzing it to confirm the problem and decide whether to implement the change.[13] This may require input from multiple departments; for example, the marketing team can help evaluate whether the change is expected to bring more business.[14] Software development effort estimation is a difficult problem, including for maintenance change requests,[15] but the request is likely to be declined if it is too expensive or infeasible.[16]  If it is decided to implement the request, it can be assigned to a scheduled release and implemented.[16] Although agile methodology does not have a maintenance phase,[17]  the change cycle can be enacted as a scrum sprint.[18]
Understanding existing code is an essential step before modifying it.[2] The rate of understanding depends both on the code base as well as the skill of the programmer.[19] Following coding conventions such as using clear function and variable names that correspond to their purpose makes understanding easier.[20] Use of conditional loop statements only if the code could execute more than once, and eliminating code that will never execute can also increase understandability.[21] Experienced programmers have an easier time understanding what the code does at a high level.[22] Software visualization is sometimes used to speed up this process.[23]
Modification to the code may take place in any way. On the one hand, it is common to haphazardly apply a quick fix without being granted enough time to update the code documentation.[24] On the other hard structured iterative enhancement can begin by changing the top-level requirements document and propagating the change down to lower levels of the system.[25] Modification often includes code refactoring (improving the structure without changing functionality) and restructuring (improving structure and functionality at the same time). [26] Unlike commercial software, free and open source software change cycles are largely restricted to coding and testing, with minimal documentation. Open-source software projects instead rely on mailing lists and a large number of contributors to understand the code base and fix bugs efficiently.[27]
An additional problem with maintenance is that nearly every change to code will introduce new bugs or unexpected ripple effects, which require another round of fixes.[2] Testing can consume the majority of maintenance resource for safety-critical code, due to the need to revalidate the entire software if any changes are made.[28]  Revalidation may include code review, regression testing with a subset of unit tests, integration tests, and system tests.[26] The goal of the testing is to verify that previous functionality is retained, and the new functionality has been added. [29]
The key purpose of software maintenance is ensuring that the product continues to meet usability requirements. At times, this may mean extending the product's capabilities beyond what was initially envisioned.[30]
According to the ISO/IEC 14764 specification, software maintenance can be classified into four types:[31]
According to some estimates, enhancement (the latter two categories) comprises some 80 percent of software maintenance.[35]
Maintainability is the quality of software enabling it to be easily modified without breaking existing functionality.[31] According to the  ISO/IEC 14764 specification, activity to ensure software maintainability prior to release counts as part of software maintenance.[5] Many software development organizations neglect maintainability, even though doing so will increase long-term costs.[36] Technical debt is incurred when programmers, often out of laziness or urgency to meet a deadline, choose quick and dirty solutions rather than build maintainability into their code.[37] A common cause is underestimates in software development effort estimation, leading to insufficient resources allocated to development.[38] One important aspect is having a large amount of automated software tests that can detect if existing functionality is compromised by a change.[31]
A challenge with maintainability is that many software engineering courses do not emphasize it, and give out one-and-done assignments that have clear and unchanging specifications.[39] Software engineering courses do not cover systems as complex as occur in the real world.[40] Development engineers who know that they will not be responsible for maintaining the software do not have an incentive to build in maintainability.[2]
Maintenance is often considered an unrewarding job for software engineers, who, if assigned to maintenance, were more likely to quit.[41][42] It often pays less than a comparable job in software development.[42] The task is often assigned to temporary workers or lesser-skilled staff,[2][43] although maintenance engineers are also typically older than developers, partly because they must be familiar with outdated technologies.[43] In 2008, around 900,000 of the 1.3 million software engineers and programmers working in the United States were doing maintenance.[44]
Companies started separate teams for maintenance, which led to outsourcing this work to a different company, and by the turn of the twenty-first century, sometimes offshoring the work to another country—whether as part of the original company or a separate entity.[45][9] The typical sources of outsourcing are developed countries such as the United States, the United Kingdom, Japan, and Australia, while destinations are usually lower-cost countries such as China, India, Russia, and Ireland.[46] Reasons for offshoring include taking advantage of lower labor costs, enabling around-the-clock support, reducing time pressure on developers, and to move support closer to the market for the product.[47] Downsides of offshoring include communication barriers in the form of such factors as time zone and organizational disjunction and cultural differences.[9]  Despite many employers considering maintenance lower-skilled work and the phase of software development most suited to offshoring,[9][48] it requires close communication with the customer and rapid response, both of which are hampered by these communication difficulties.[9]
In software engineering, the term legacy system does not have a fixed meaning, but often refers to older systems which are large, difficult to modify, and also necessary for current business needs. Often legacy systems are written in obsolete programming languages, lack documentation, have a deteriorating structure after years of changes, and depend on experts to keep it operational.[49] When dealing with these systems, at some point so much technical debt accumulates that maintenance is not practical or economical.[12] Other choices include:
Despite taking up the lion's share of software development resources, maintenance is the least studied phase of software development.[56][57] Much of the literature has focused on how to develop maintainable code from the outset, with less focus on motivating engineers to make maintainability a priority.[58] As of 2020[update], automated solutions for code refactoring to reduce maintenance effort are an active area of research,[59] as is machine-learning enhanced maintainability assessment.[60]
Software architecture is the set of structures needed to reason about a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relations among them, and properties of both elements and relations.[1]
The architecture of a software system is a metaphor, analogous to the architecture of a building.[2] It functions as the blueprints for the system and the development project, which project management can later use to extrapolate the tasks necessary to be executed by the teams and people involved.
Software architecture is about making fundamental structural choices that are costly to change once implemented. Software architecture choices include specific structural options from possibilities in the design of the software. There are two fundamental laws in software architecture:[3][4]
Documenting software architecture facilitates communication between stakeholders, captures early decisions about the high-level design, and allows the reuse of design components between projects.[5]: 29–35
Software architecture design is commonly juxtaposed with software application design. Whilst application design focuses on the design of the processes and data supporting the required functionality (the services offered by the system), software architecture design focuses on designing the infrastructure within which application functionality can be realized and executed such that the functionality is provided in a way which meets the system's non-functional requirements.
Software architectures can be categorized into two main types: monolith and distributed architecture, each has its own subcategories.[4]
Software architecture tends to become more complex over time. Software architects should use "fitness functions" to continuously keep the architecture in check.[4]
There is no sharp distinction between software architecture versus design and requirements engineering (see Related fields below). They are all part of a "chain of intentionality" from high-level intentions to low-level details.[12]: 18
Software Architecture Pattern refers to a reusable, proven solution to a recurring problem at the system level, addressing concerns related to the overall structure, component interactions, and quality attributes of the system. Software architecture patterns operate at a higher level of abstraction than software design patterns, solving broader system-level challenges. While these patterns typically affect system-level concerns, the distinction between architectural patterns and architectural styles can sometimes be blurry. Examples include Circuit Breaker. [13][14][15]
Software Architecture Style refers to a high-level structural organization that defines the overall system organization, specifying how components are organized, how they interact, and the constraints on those interactions. Architecture styles typically include a vocabulary of component and connector types, as well as semantic models for interpreting the system's properties. These styles represent the most coarse-grained level of system organization. Examples include Layered Architecture, Microservices, and Event-Driven Architecture. [13][14][15]
Multitude of stakeholders: software systems have to cater to a variety of stakeholders such as business managers, owners, users, and operators. These stakeholders all have their own concerns with respect to the system. Balancing these concerns and demonstrating that they are addressed is part of designing the system.[5]: 29–31  This implies that architecture involves dealing with a broad variety of concerns and stakeholders, and has a multidisciplinary nature.
Quality-driven: classic software design approaches (e.g. Jackson Structured Programming) were driven by required functionality and the flow of data through the system, but the current insight[5]: 26–28  is that the architecture of a software system is more closely related to its quality attributes such as fault-tolerance, backward compatibility, extensibility, reliability, maintainability, availability, security, usability, and other such –ilities. Stakeholder concerns often translate into requirements on these quality attributes, which are variously called non-functional requirements, extra-functional requirements, behavioral requirements, or quality attribute requirements.
Recurring styles: like building architecture, the software architecture discipline has developed standard ways to address recurring concerns. These "standard ways" are called by various names at various levels of abstraction. Common terms for recurring solutions are architectural style,[12]: 273–277  tactic,[5]: 70–72  reference architecture and architectural pattern.[17][18][5]: 203–205
Conceptual integrity: a term introduced by Fred Brooks in his 1975 book The Mythical Man-Month to denote the idea that the architecture of a software system represents an overall vision of what it should do and how it should do it. This vision should be separated from its implementation. The architect assumes the role of "keeper of the vision", making sure that additions to the system are in line with the architecture, hence preserving conceptual integrity.[19]: 41–50
Software architecture is an "intellectually graspable" abstraction of a complex system.[5]: 5–6  This abstraction provides a number of benefits:
The comparison between software design and (civil) architecture was first drawn in the late 1960s,[23] but the term "software architecture" did not see widespread usage until the 1990s.[24] The field of computer science had encountered problems associated with complexity since its formation.[25] Earlier problems of complexity were solved by developers by choosing the right data structures, developing algorithms, and by applying the concept of separation of concerns. Although the term "software architecture" is relatively new to the industry, the fundamental principles of the field have been applied sporadically by software engineering pioneers since the mid-1980s. Early attempts to capture and explain software architecture of a system were imprecise and disorganized, often characterized by a set of box-and-line diagrams.[26]
Software architecture as a concept has its origins in the research of Edsger Dijkstra in 1968 and David Parnas in the early 1970s. These scientists emphasized that the structure of a software system matters and getting the structure right is critical. During the 1990s there was a concerted effort to define and codify fundamental aspects of the discipline, with research work concentrating on architectural styles (patterns), architecture description languages, architecture documentation, and formal methods.[27]
Research institutions have played a prominent role in furthering software architecture as a discipline. Mary Shaw and David Garlan of Carnegie Mellon wrote a book titled Software Architecture: Perspectives on an Emerging Discipline in 1996, which promoted software architecture concepts such as components, connectors, and styles. The University of California, Irvine's Institute for Software Research's efforts in software architecture research is directed primarily in architectural styles, architecture description languages, and dynamic architectures.
IEEE 1471-2000, "Recommended Practice for Architecture Description of Software-Intensive Systems", was the first formal standard in the area of software architecture. It was adopted in 2007 by ISO as ISO/IEC 42010:2007. In November 2011, IEEE 1471–2000 was superseded by ISO/IEC/IEEE 42010:2011, "Systems and software engineering – Architecture description" (jointly published by IEEE and ISO).[16]
While in IEEE 1471, software architecture was about the architecture of "software-intensive systems", defined as "any system where software contributes essential influences to the design, construction, deployment, and evolution of the system as a whole", the 2011 edition goes a step further by including the ISO/IEC 15288 and ISO/IEC 12207 definitions of a system, which embrace not only hardware and software, but also "humans, processes, procedures, facilities, materials and naturally occurring entities". This reflects the relationship between software architecture, enterprise architecture and solution architecture.
It's software architect's responsibility to match architectural characteristics (aka non-functional requirements) with business requirements. For example: [4]
There are four core activities in software architecture design.[28] These core architecture activities are performed iteratively and at different stages of the initial software development life-cycle, as well as over the evolution of a system.
The outputs of the analysis activity are those requirements that have a measurable impact on a software system's architecture, called architecturally significant requirements.[31]
Architecture evaluation is the process of determining how well the current design or a portion of it satisfies the requirements derived during analysis. An evaluation can occur whenever an architect is considering a design decision, it can occur after some portion of the design has been completed, it can occur after the final design has been completed or it can occur after the system has been constructed. Some of the available software architecture evaluation techniques include Architecture Tradeoff Analysis Method (ATAM) and TARA.[32] Frameworks for comparing the techniques are discussed in frameworks such as SARA Report[21] and Architecture Reviews: Practice and Experience.[33]
Architecture evolution is the process of maintaining and adapting an existing software architecture to meet changes in requirements and environment. As software architecture provides a fundamental structure of a software system, its evolution and maintenance would necessarily impact its fundamental structure. As such, architecture evolution is concerned with adding new functionality as well as maintaining existing functionality and system behavior.
Architecture requires critical supporting activities. These supporting activities take place throughout the core software architecture process. They include knowledge management and communication, design reasoning and decision-making, and documentation.
Software architecture supporting activities are carried out during core software architecture activities. These supporting activities assist a software architect to carry out analysis, synthesis, evaluation, and evolution. For instance, an architect has to gather knowledge, make decisions, and document during the analysis phase.
Software architecture inherently deals with uncertainties, and the size of architectural components can significantly influence a system's outcomes, both positively and negatively. Neal Ford and Mark Richards propose an iterative approach to address the challenge of identifying and right-sizing components. This method emphasizes continuous refinement as teams develop a more nuanced understanding of system behavior and requirements. [4]
There are also concerns that software architecture leads to too much big design up front, especially among proponents of agile software development. A number of methods have been developed to balance the trade-offs of up-front design and agility,[38] including the agile method DSDM which mandates a "Foundations" phase during which "just enough" architectural foundations are laid. IEEE Software devoted a special issue to the interaction between agility and architecture.
Software architecture erosion refers to a gradual gap between the intended and implemented architecture of a software system over time.[39] The phenomenon of software architecture erosion was initially brought to light in 1992 by Perry and Wolf alongside their definition of software architecture.[2]
Software architecture erosion may occur in each stage of the software development life cycle and has varying impacts on the development speed and the cost of maintenance. Software architecture erosion occurs due to various reasons, such as architectural violations, the accumulation of technical debt, and knowledge vaporization.[40] A famous case of architecture erosion is the failure of Mozilla Web browser.[41] Mozilla is an application created by Netscape with a complex codebase that became harder to maintain due to continuous changes.  Due to initial poor design and growing  architecture erosion, Netscape spent two years redeveloping the Mozilla Web browser, demonstrating the importance of proactive architecture management to prevent costly repairs and project delays.
Architecture erosion can decrease software performance, substantially increase evolutionary costs, and degrade software quality. Various approaches and tools have been proposed to detect architecture erosion. These approaches are primarily classified into four categories: consistency-based, evolution-based, defect-based, and decision-based approaches.[39] For instance, automated architecture conformance checks, static code analysis tools, and refactoring techniques help identify and mitigate erosion early.
Software architecture recovery (or reconstruction, or reverse engineering) includes the methods, techniques, and processes to uncover a software system's architecture from available information, including its implementation and documentation. Architecture recovery is often necessary to make informed decisions in the face of obsolete or out-of-date documentation and 
architecture erosion: implementation and maintenance decisions diverging from the envisioned architecture.[42] Practices exist to recover software architecture as static program analysis. This is a part of the subjects covered by the software intelligence practice.
Architecture is design but not all design is architectural.[1] In practice, the architect is the one who draws the line between software architecture (architectural design) and detailed design (non-architectural design). There are no rules or guidelines that fit all cases, although there have been attempts to formalize the distinction. 
According to the Intension/Locality Hypothesis,[43] the distinction between architectural and detailed design is defined by the Locality Criterion,[43] according to which a statement about software design is non-local (architectural) if and only if a program that satisfies it can be expanded into a program that does not. For example, the client–server style is architectural (strategic) because a program that is built on this principle can be expanded into a program that is not client–server—for example, by adding peer-to-peer nodes.
Requirements engineering and software architecture can be seen as complementary approaches: while software architecture targets the 'solution space' or the 'how', requirements engineering addresses the 'problem space' or the 'what'.[44] Requirements engineering entails the elicitation, negotiation, specification, validation, documentation, and management of requirements. Both requirements engineering and software architecture revolve around stakeholder concerns, needs, and wishes.
There is considerable overlap between requirements engineering and software architecture, as evidenced for example by a study into five industrial software architecture methods that concludes that "the inputs (goals, constraints, etc.) are usually ill-defined, and only get discovered or better understood as the architecture starts to emerge" and that while "most architectural concerns are expressed as requirements on the system, they can also include mandated design decisions".[28] In short, required behavior impacts solution architecture, which in turn may introduce new requirements.[45] Approaches such as the Twin Peaks model[46] aim to exploit the synergistic relation between requirements and architecture.
Computer programming – process that leads from an original formulation of a computing problem to executable computer programs. Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness and resources consumption, and implementation (commonly referred to as coding[1][2]) of algorithms in a target programming language. Source code is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem.
Programming language – formal constructed language designed to communicate instructions to a machine, particularly a computer. Programming languages can be used to create programs to control the behavior of a machine or to express algorithms.
Competitive programming or sport programming is a mind sport involving participants trying to program according to provided specifications. The contests are usually held over the Internet or a local network. Competitive programming is recognized and supported by several multinational software and Internet companies, such as Google,[1][2] and Meta.[3]
A programming competition generally involves the host presenting a set of logical or mathematical problems, also known as puzzles or challenges, to the contestants (who can vary in number from tens or even hundreds to several thousand). Contestants are required to write computer programs capable of solving these problems. Judging is based mostly upon number of problems solved and time spent on writing successful solutions, but may also include other factors (quality of output produced, execution time, memory usage, program size, etc.).
One of the oldest contests known is the International Collegiate Programming Contest (ICPC) which originated in the 1970s[4] and has grown to include 88 countries in its 2011 edition.
From 1990 to 1994, Owen Astrachan, Vivek Khera and David Kotz ran one of the first distributed, internet-based programming contests inspired by the ICPC.[5]
Interest in competitive programming has grown extensively since 2000 to tens of thousands of participants (see Notable competitions), and is strongly connected to the growth of the Internet, which facilitates holding international contests online, eliminating geographical problems.
The aim of competitive programming is to write computer programs which are able to solve given problems. A vast majority of problems appearing in programming contests are mathematical or logical in nature. Typical such tasks belong to one of the following categories: combinatorics, number theory, graph theory, algorithmic game theory, computational geometry, string analysis, discrete mathematics and data structures.[6] Problems related to constraint programming and artificial intelligence are also popular in certain competitions.
Irrespective of the problem category, the process of solving a problem can be divided into two broad steps: constructing an efficient algorithm, and implementing the algorithm in a suitable programming language (the set of programming languages allowed varies from contest to contest). These are the two most commonly tested skills in programming competitions.
In most of the above competitions, competitions are usually organized in several rounds. They usually start with online rounds, which conclude in the onsite final round. The top performers at IOI and ICPC receive gold, silver and bronze medals. In the other contests, cash prizes are awarded to the top finishers. The competitions also attract the interest of recruiters from multiple software and Internet companies, which often reach out to competitors with potential job offers.
The programming community around the world has created and maintained several internet-resources dedicated to competitive programming. They offer standalone contests with or without minor prizes. Users will typically be assigned a rating based on their performance on said contests. The archives of past problems are popular resources for training in competitive programming. There are several organizations that host programming competitions on a regular basis. These include:
Participation in programming contests may increase student enthusiasm for computer science studies. The skills acquired in ICPC-like programming contests also improve career prospects, as they help to pass the "technical interviews", which often require candidates to solve complex programming and algorithmic problems on the spot.[19][20]
There has also been criticism of competitive programming, particularly from professional software developers.[21] One critical point is that many fast-paced programming contests teach competitors bad programming habits and code style (like unnecessary use of macros, lack of OOP abstraction and comments, use of short variable names, etc.).[22][21] Also, by offering only small algorithmic puzzles with relatively short solutions, programming contests like ICPC and IOI do not necessarily teach good software engineering skills and practices, as real software projects typically have many thousands of lines of code and are developed by large teams over long periods of time.[21] Peter Norvig stated that based on the available data, being a winner of programming contests correlated negatively with a programmer's performance at their job at Google (even though contest winners had higher chances of getting hired).[23] Norvig later stated that this correlation was observed on a small data set, but that it could not be confirmed after examining a larger data set.[24][unreliable source?]
Coding best practices or programming best practices are a set of informal, sometimes personal, rules (best practices) that many software developers, in computer programming follow to improve software quality.[1] Many computer programs require being robust and reliable for long periods of time,[2] so any rules need to facilitate both initial development and subsequent maintenance of source code by people other than the original authors.
In the ninety–ninety rule, Tom Cargill explains why programming projects often run late: "The first 90% of the code takes the first 90% of the development time. The last 10% takes another 90% of the time."[3] Any guidance which can redress this lack of foresight is worth considering.
As listed below, there are many attributes associated with good software. Some of these can be mutually contradictory (e.g. being very fast versus performing extensive error checking), and different customers and participants may have different priorities. Weinberg provides an example of how different goals can have a dramatic effect on both effort required and efficiency.[5] Furthermore, he notes that programmers will generally aim to achieve any explicit goals which may be set, probably at the expense of any other quality attributes.
Before coding starts, it is important to ensure that all necessary prerequisites have been completed (or have at least progressed far enough to provide a solid foundation for coding). If the various prerequisites are not satisfied, then the software is likely to be unsatisfactory, even if it is completed.
A software development methodology is a framework that is used to structure, plan, and control the life cycle of a software product. Common methodologies include waterfall, prototyping, iterative and incremental development, spiral development, agile software development, rapid application development, and extreme programming.
Since its introduction in 2001, agile software development has grown in popularity, fueled by software developers seeking a more iterative, collaborative approach to software development.[11]
Hoare points out: "there are two ways of constructing a software design: one way is to make it so simple that there are obviously no deficiencies; the other way is to make it so complicated that there are no obvious deficiencies. The first method is far more difficult."[15] (Emphasis as in the original.)
Software architecture is concerned with deciding what has to be done and which program component is going to do it (how something is done is left to the detailed design phase below). This is particularly important when a software system contains more than one program since it effectively defines the interface between these various programs. It should include some consideration of any user interfaces as well, without going into excessive detail.
The software architecture is also of interest to various stakeholders (sponsors, end-users, etc.) since it gives them a chance to check that their requirements can be met.
The primary purpose of design is to fill in the details which have been glossed over in the architectural design. The intention is that the design should be detailed enough to provide a good guide for actual coding, including details of any particular algorithms to be used. For example, at the architectural level, it may have been noted that some data has to be sorted, while at the design level, it is necessary to decide which sorting algorithm is to be used. As a further example, if an object-oriented approach is being used, then the details of the objects must be determined (attributes and methods).
Mayer states: "No programming language is perfect. There is not even a single best language; there are only languages well suited or perhaps poorly suited for particular purposes. Understanding the problem and associated programming requirements is necessary for choosing the language best suited for the solution."[17]
It is possible that different programming languages may be appropriate for different aspects of the problem. If the languages or their compilers permit, it may be feasible to mix routines written in different languages within the same program.
Even if there is no choice as to which programming language is to be used, McConnell provides some advice: "Every programming language has strengths and weaknesses. Be aware of the specific strengths and weaknesses of the language you're using."[19]
This section is also really a prerequisite to coding, as McConnell points out: "Establish programming conventions before you begin programming. It's nearly impossible to change code to match them later."[19]
As listed near the end of coding conventions, there are different conventions for different programming languages, so it may be counterproductive to apply the same conventions across different languages. It is important to note that there is no one particular coding convention for any programming language. Every organization has a custom coding standard for each type of software project. It is, therefore, imperative that the programmer chooses or makes up a particular set of coding guidelines before the software project commences. Some coding conventions are generic, which may not apply for every software project written with a particular programming language.
Program code should not contain "hard-coded" (literal) values referring to environmental parameters, such as absolute file paths, file names, user names, host names, IP addresses, and URLs, UDP/TCP ports. Otherwise, the application will not run on a host that has a different design than anticipated. A careful programmer can parametrize such variables and configure them for the hosting environment outside of the application proper (for example, in property files, on an application server, or even in a database). Compare the mantra of a "single point of definition".[22](SPOD).
Design code with scalability as a design goal because very often in software projects, new features are always added to a project which becomes bigger. Therefore, the facility to add new features to a software code base becomes an invaluable method in writing software.
Re-use is a very important design goal in software development. Re-use cuts development costs and also reduces the time for development if the components or modules which are reused are already tested. Very often, software projects start with an existing baseline that contains the project in its prior version and depending on the project, many of existing software modules and components are reused, which reduces development and testing time, therefore, increasing the probability of delivering a software project on schedule.
Testing is an integral part of software development that needs to be planned. It is also important that testing is done proactively; meaning that test cases are planned before coding starts, and test cases are developed while the application is being designed and coded.
In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms—the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the size of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same size may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.
The term "analysis of algorithms" was coined by Donald Knuth.[1] Algorithm analysis is an important part of a broader computational complexity theory, which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem. These estimates provide an insight into reasonable directions of search for efficient algorithms.
In theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input. Big O notation, Big-omega notation and Big-theta notation are used to this end.[2] For instance, binary search is said to run in a number of steps proportional to the logarithm of the size n of the sorted list being searched, or in O(log n), colloquially "in logarithmic time". Usually asymptotic estimates are used because different implementations of the same algorithm may differ in efficiency. However the efficiencies of any two "reasonable" implementations of a given algorithm are related by a constant multiplicative factor  called a hidden constant.
Exact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called a model of computation. A model of computation may be defined in terms of an abstract computer, e.g. Turing machine, and/or by postulating that certain operations are executed in unit time.
For example, if the sorted list to which we apply binary search has n elements, and we can guarantee that each lookup of an element in the list can be done in unit time, then at most log2(n) + 1 time units are needed to return an answer.
The latter is more cumbersome to use, so it is only employed when necessary, for example in the analysis of arbitrary-precision arithmetic algorithms, like those used in cryptography.
A key point which is often overlooked is that published lower bounds for problems are often given for a model of computation that is more restricted than the set of operations that you could use in practice and therefore there are algorithms that are faster than what would naively be thought possible.[8]
Run-time analysis is a theoretical classification that estimates and anticipates the increase in running time (or run-time or execution time) of an algorithm as its input size (usually denoted as n) increases.  Run-time efficiency is a topic of great interest in computer science:  A program can take seconds, hours, or even years to finish executing, depending on which algorithm it implements. While software profiling techniques can be used to measure an algorithm's run-time in practice, they cannot provide timing data for all infinitely many possible inputs; the latter can only be achieved by the theoretical methods of run-time analysis.
Since algorithms are platform-independent (i.e. a given algorithm can be implemented in an arbitrary programming language on an arbitrary computer running an arbitrary operating system), there are additional significant drawbacks to using an empirical approach to gauge the comparative performance of a given set of algorithms.
Take as an example a program that looks up a specific entry in a sorted list of size n.  Suppose this program were implemented on Computer A, a state-of-the-art machine, using a linear search algorithm, and on Computer B, a much slower machine, using a binary search algorithm.  Benchmark testing on the two computers running their respective programs might look something like the following:
Based on these metrics, it would be easy to jump to the conclusion that Computer A is running an algorithm that is far superior in efficiency to that of Computer B.  However, if the size of the input-list is increased to a sufficient number, that conclusion is dramatically demonstrated to be in error:
Computer A, running the linear search program, exhibits a linear growth rate.  The program's run-time is directly proportional to its input size.  Doubling the input size doubles the run-time, quadrupling the input size quadruples the run-time, and so forth.  On the other hand, Computer B, running the binary search program, exhibits a logarithmic growth rate.  Quadrupling the input size only increases the run-time by a constant amount (in this example, 50,000 ns).  Even though Computer A is ostensibly a faster machine, Computer B will inevitably surpass Computer A in run-time because it is running an algorithm with a much slower growth rate.
Informally, an algorithm can be said to exhibit a growth rate on the order of a mathematical function if beyond a certain input size n, the function f(n) times a positive constant provides an upper bound or limit for the run-time of that algorithm.  In other words, for a given input size n greater than some n0 and a constant c, the run-time of that algorithm will never be larger than c × f(n).  This concept is frequently expressed using Big O notation.  For example, since the run-time of insertion sort grows quadratically as its input size increases, insertion sort can be said to be of order O(n2).
Big O notation is a convenient way to express the worst-case scenario for a given algorithm, although it can also be used to express the average-case — for example, the worst-case scenario for quicksort is O(n2), but the average-case run-time is O(n log n).
Assuming the run-time follows power rule, t ≈ kna, the coefficient a can be found [9] by taking empirical measurements of run-time {t1, t2} at some problem-size points {n1, n2}, and calculating t2/t1 = (n2/n1)a so that a = log(t2/t1)/log(n2/n1). In other words, this measures the slope of the empirical line on the log–log plot of run-time vs. input size, at some size point. If the order of growth indeed follows the power rule (and so the line on the log–log plot is indeed a straight line), the empirical value of  will  stay constant at different ranges, and if not, it will change (and the line is a curved line)—but still could serve for comparison of any two given algorithms as to their empirical local orders of growth behaviour. Applied to the above table:
It is clearly seen that the first algorithm exhibits a linear order of growth indeed following the power rule. The empirical values for the second one are diminishing rapidly, suggesting it follows another rule of growth and in any case has much lower local orders of growth (and improving further still), empirically, than the first one.
The run-time complexity for the worst-case scenario of a given algorithm can sometimes be evaluated by examining the structure of the algorithm and making some simplifying assumptions.  Consider the following pseudocode:
A given computer will take a discrete amount of time to execute each of the instructions involved with carrying out this algorithm.  Say that the actions carried out in step 1 are considered to consume time at most T1, step 2 uses time at most T2, and so forth.
In the algorithm above, steps 1, 2 and 7 will only be run once.  For a worst-case evaluation, it should be assumed that step 3 will be run as well.  Thus the total amount of time to run steps 1–3 and step 7 is:
A more elegant approach to analyzing this algorithm would be to declare that [T1..T7] are all equal to one unit of time, in a system of units chosen so that one unit is greater than or equal to the actual times for these steps.  This would mean that the algorithm's run-time breaks down as follows:[12]
Algorithm analysis is important in practice because the accidental or unintentional use of an inefficient algorithm can significantly impact system performance. In time-sensitive applications, an algorithm taking too long to run can render its results outdated or useless. An inefficient algorithm can also end up requiring an uneconomical amount of computing power or storage in order to run, again rendering it practically useless.
Analysis of algorithms typically focuses on the asymptotic performance, particularly at the elementary level, but in practical applications constant factors are important, and real-world data is in practice always limited in size. The limit is typically the size of addressable memory, so on 32-bit machines 232 = 4 GiB (greater if segmented memory is used) and on 64-bit machines 264 = 16 EiB. Thus given a limited size, an order of growth (time or space) can be replaced by a constant factor, and in this sense all practical algorithms are O(1) for a large enough constant, or for small enough data.
This interpretation is primarily useful for functions that grow extremely slowly: (binary) iterated logarithm (log*) is less than 5 for all practical data (265536 bits); (binary) log-log (log log n) is less than 6 for virtually all practical data (264 bits); and binary log (log n) is less than 64 for virtually all practical data (264 bits). An algorithm with non-constant complexity may nonetheless be more efficient than an algorithm with constant complexity on practical data if the overhead of the constant time algorithm results in a larger constant factor, e.g., one may have 



K
>
k
log
⁡
log
⁡
n


{\displaystyle K>k\log \log n}

 so long as 



K

/

k
>
6


{\displaystyle K/k>6}

 and 



n
<

2


2

6




=

2

64




{\displaystyle n<2^{2^{6}}=2^{64}}

.
For large data linear or quadratic factors cannot be ignored, but for small data an asymptotically inefficient algorithm may be more efficient. This is particularly used in hybrid algorithms, like Timsort, which use an asymptotically efficient algorithm (here merge sort, with time complexity 



n
log
⁡
n


{\displaystyle n\log n}

), but switch to an asymptotically inefficient algorithm (here insertion sort, with time complexity 




n

2




{\displaystyle n^{2}}

) for small data, as the simpler algorithm is faster on small data.
In computer science, conditionals (that is, conditional statements, conditional expressions and conditional constructs) are programming language constructs that perform different computations or actions or return different values depending on the value of a Boolean expression, called a condition.
Conditional statements are imperative constructs executed for side-effect, while conditional expressions return values. Many programming languages (such as C) have distinct conditional statements and conditional expressions. Although in pure functional programming, conditional expressions do not have side-effects, many languages with conditional expressions (such as Lisp) support conditional side-effects.
The if–then or if–then–else construction is used in many programming languages. Although the syntax varies from language to language, the basic structure (in pseudocode form) looks like this:
In early programming languages, especially some dialects of BASIC in the 1980s home computers, an if–then statement could only contain GOTO statements (equivalent to a branch instruction). This led to a hard-to-read style of programming known as spaghetti programming, with programs in this style called spaghetti code.  As a result, structured programming, which allows (virtually) arbitrary statements to be put in statement blocks inside an if statement, gained in popularity, until it became the norm even in most BASIC programming circles. Such mechanisms and principles were based on the older but more advanced ALGOL family of languages, and ALGOL-like languages such as Pascal and Modula-2 influenced modern BASIC variants for many years. While it is possible while using only GOTO statements in if–then statements to write programs that are not spaghetti code and are just as well structured and readable as programs written in a structured programming language, structured programming makes this easier and enforces it.  Structured if–then–else statements like the example above are one of the key elements of structured programming, and they are present in most popular high-level programming languages such as C, Java, JavaScript and Visual Basic .
The else keyword is made to target a specific if–then statement preceding it, but for nested if–then statements, classic programming languages such as ALGOL 60 struggled to define which specific statement to target. Without clear boundaries for which statement is which, an else keyword could target any preceding if–then statement in the nest, as parsed.
Pattern matching may be seen as an alternative to both if–then–else, and case statements. It is available in many programming languages with functional programming features, such as Wolfram Language, ML and many others. Here is a simple example written in the OCaml language:
In programming languages that have associative arrays or comparable data structures, such as Python, Perl, PHP or Objective-C, it is idiomatic to use them to implement conditional assignment.[13]
A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the "average case" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.
There is a distinction between algorithms that use the random input so that they always terminate with the correct answer, but where the expected running time is finite (Las Vegas algorithms, for example Quicksort[1]), and algorithms which have a chance of producing an incorrect result (Monte Carlo algorithms, for example the Monte Carlo algorithm for the MFAS problem[2]) or fail to produce a result either by signaling a failure or failing to terminate. In some cases, probabilistic algorithms are the only practical means of solving a problem.[3]
In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator.
This algorithm succeeds with probability 1. The number of iterations varies and can be arbitrarily large, but the expected number of iterations is
If an ‘a’ is found, the algorithm succeeds, else the algorithm fails. After k iterations, the probability of finding an ‘a’ is:
This algorithm does not guarantee success, but the run time is bounded. The number of iterations is always less than or equal to k.  Taking k to be constant the run time (expected and absolute) is 



Θ
(
1
)


{\displaystyle \Theta (1)}

.
Randomized algorithms are particularly useful when faced with a malicious "adversary" or attacker who deliberately tries to feed a bad input to the algorithm (see worst-case complexity and competitive analysis (online algorithm)) such as in the Prisoner's dilemma. It is for this reason that randomness is ubiquitous in cryptography. In cryptographic applications, pseudo-random numbers cannot be used, since the adversary can predict them, making the algorithm effectively deterministic. Therefore, either a source of truly random numbers or a cryptographically secure pseudo-random number generator is required.  Another area in which randomness is inherent is quantum computing.
In the example above, the Las Vegas algorithm always outputs the correct answer, but its running time is a random variable.  The Monte Carlo algorithm (related to the Monte Carlo method for simulation) is guaranteed to complete in an amount of time that can be bounded by a function the input size and its parameter k, but allows a small probability of error. Observe that any Las Vegas algorithm can be converted into a Monte Carlo algorithm (via Markov's inequality), by having it output an arbitrary, possibly incorrect answer if it fails to complete within a specified time. Conversely, if an efficient verification procedure exists to check whether an answer is correct, then a Monte Carlo algorithm can be converted into a Las Vegas algorithm by running the Monte Carlo algorithm repeatedly till a correct answer is obtained.
Computational complexity theory models randomized algorithms as probabilistic Turing machines. Both  Las Vegas and Monte Carlo algorithms are considered, and several complexity classes are studied. The most basic randomized complexity class is RP, which is the class of decision problems for which there is an efficient (polynomial time) randomized algorithm (or probabilistic Turing machine) which recognizes NO-instances with absolute certainty and recognizes YES-instances with a probability of at least 1/2. The complement class for RP is co-RP. Problem classes having (possibly nonterminating) algorithms with polynomial time average case running time whose output is always correct are said to be in ZPP.
The class of problems for which both YES and NO-instances are allowed to be identified with some error is called BPP. This class acts as the randomized equivalent of P, i.e. BPP represents the class of efficient randomized algorithms.
Quicksort was discovered by Tony Hoare in 1959, and subsequently published in 1961.[4] In the same year, Hoare published the quickselect algorithm,[5] which finds the median element of a list in linear expected time. It remained open until 1973 whether a deterministic linear-time algorithm existed.[6]
In 1917, Henry Cabourn Pocklington introduced a randomized algorithm known as Pocklington's algorithm for efficiently finding square roots modulo prime numbers.[7]
In 1970, Elwyn Berlekamp introduced a randomized algorithm for efficiently computing the roots of a polynomial over a finite field.[8] In 1977, Robert M. Solovay and Volker Strassen discovered a polynomial-time randomized primality test (i.e., determining the primality of a number). Soon afterwards Michael O. Rabin demonstrated that the 1976 Miller's primality test could also be turned into a polynomial-time randomized algorithm. At that time, no provably polynomial-time deterministic algorithms for primality testing were known.
One of the earliest randomized data structures is the hash table, which was introduced in 1953 by Hans Peter Luhn at IBM.[9] Luhn's hash table used chaining to resolve collisions and was also one of the first applications of linked lists.[9] Subsequently, in 1954, Gene Amdahl, Elaine M. McGraw, Nathaniel Rochester, and Arthur Samuel of IBM Research introduced linear probing,[9] although Andrey Ershov independently had the same idea in 1957.[9] In 1962, Donald Knuth performed the first correct analysis of linear probing,[9] although the memorandum containing his analysis was not published until much later.[10] The first published analysis was due to Konheim and Weiss in 1966.[11]
Early work on randomized data structures also extended beyond hash tables. In 1970, Burton Howard Bloom introduced an approximate-membership data structure known as the Bloom filter.[13] In 1989, Raimund Seidel and Cecilia R. Aragon introduced a randomized balanced search tree known as the treap.[14] In the same year,  William Pugh introduced another randomized search tree known as the skip list.[15]
Prior to the popularization of randomized algorithms in computer science, Paul Erdős popularized the use of randomized constructions as a mathematical technique for establishing the existence of mathematical objects. This technique has become known as the probabilistic method.[16] Erdős gave his first application of the probabilistic method in 1947, when he used a simple randomized construction to establish the existence of Ramsey graphs.[17] He famously used a more sophisticated randomized algorithm in 1959 to establish the existence of graphs with high girth and chromatic number.[18][16]
Quicksort is a familiar, commonly used algorithm in which randomness can be useful. Many deterministic versions of this algorithm require O(n2) time to sort n numbers for some well-defined class of degenerate inputs (such as an already sorted array), with the specific class of inputs that generate this behavior defined by the protocol for pivot selection. However, if the algorithm selects pivot elements uniformly at random, it has a provably high probability of finishing in O(n log n) time regardless of the characteristics of the input.
In computational geometry, a standard technique to build a structure like a convex hull or Delaunay triangulation is to randomly permute the input points and then insert them one by one into the existing structure. The randomization ensures that the expected number of changes to the structure caused by an insertion is small, and so the expected running time of the algorithm can be bounded from above. This technique is known as randomized incremental construction.[19]
In each execution of the outer loop, the algorithm repeats the inner loop until only 2 nodes remain, the corresponding cut is obtained. The run time of one execution is 



O
(
n
)


{\displaystyle O(n)}

, and n denotes the number of vertices.
After m times executions of the outer loop, we output the minimum cut among all the results. The figure 2 gives an
example of one execution of the algorithm. After execution, we get a cut of size 3.
If G is not connected, then G can be partitioned into L and R without any edge between them. So the min cut in a disconnected graph is 0. Now, assume G is connected. Let V=L∪R be the partition of V induced by C : C = { {u,v} ∈ E : u ∈ L,v ∈ R} (well-defined since G is connected). Consider an edge {u,v} of C. Initially, u,v are distinct vertices. As long as we pick an edge ⁠



f
≠
e


{\displaystyle f\neq e}

⁠, u and v do not get merged. Thus, at the end of the algorithm, we have two compound nodes covering the entire graph, one consisting of the vertices of L and the other consisting of the vertices of R. As in figure 2, the size of min cut is 1, and C = {(A,B)}. If we don't select (A,B) for contraction, we can get the min cut.
The probability that the algorithm succeeds is 1 − the probability that all attempts fail. By independence, the probability that all attempts fail is





∏

i
=
1


m


Pr
(

C

i


≠
C
)
=

∏

i
=
1


m


(
1
−
Pr
(

C

i


=
C
)
)
.


{\displaystyle \prod _{i=1}^{m}\Pr(C_{i}\neq C)=\prod _{i=1}^{m}(1-\Pr(C_{i}=C)).}
Cancellation gives 



Pr
[

C

i


=
C
]
≥


2

n
(
n
−
1
)





{\displaystyle \Pr[C_{i}=C]\geq {\frac {2}{n(n-1)}}}

. Thus the probability that the algorithm succeeds is at least 



1
−


(

1
−


2

n
(
n
−
1
)




)


m




{\displaystyle 1-\left(1-{\frac {2}{n(n-1)}}\right)^{m}}

. For 



m
=



n
(
n
−
1
)

2


ln
⁡
n


{\displaystyle m={\frac {n(n-1)}{2}}\ln n}

, this is equivalent to 



1
−


1
n




{\displaystyle 1-{\frac {1}{n}}}

. The algorithm finds the min cut with probability 



1
−


1
n




{\displaystyle 1-{\frac {1}{n}}}

, in time 



O
(
m
n
)
=
O
(

n

3


log
⁡
n
)


{\displaystyle O(mn)=O(n^{3}\log n)}

.
Randomness can be viewed as a resource, like space and time. Derandomization is then the process of removing randomness (or using as little of it as possible).[21][22] It is not currently known[as of?] if all algorithms can be derandomized without significantly increasing their running time.[23] For instance, in computational complexity, it is unknown whether P = BPP,[23] i.e., we do not know whether we can take an arbitrary randomized algorithm that runs in polynomial time with a small error probability and derandomize it to run in polynomial time without using randomness.
Algorithm characterizations are attempts to formalize the word algorithm. Algorithm does not have a generally accepted formal definition. Researchers[1] are actively working on this problem. This article will present some of the "characterizations" of the notion of "algorithm" in more detail.
Over the last 200 years, the definition of the algorithm has become more complicated and detailed as researchers have tried to pin down the term. Indeed, there may be more than one type of "algorithm". But most agree that algorithm has something to do with defining generalized processes for the creation of "output" integers from other "input" integers – "input parameters" arbitrary and infinite in extent, or limited in extent but still variable—by the manipulation of distinguishable symbols (counting numbers) with finite collections of rules that a person can perform with paper and pencil.
When we are doing "arithmetic" we are really calculating by the use of "recursive functions" in the shorthand algorithms we learned in grade school, for example, adding and subtracting.
The proofs that every "recursive function" we can calculate by hand we can compute by machine and vice versa—note the usage of the words calculate versus compute—is remarkable. But this equivalence together with the thesis (unproven assertion) that this includes every calculation/computation indicates why so much emphasis has been placed upon the use of Turing-equivalent machines in the definition of specific algorithms, and why the definition of "algorithm" itself often refers back to "the Turing machine". This is discussed in more detail under Stephen Kleene's characterization.
All algorithms need to be specified in a formal language, and the "simplicity notion" arises from the simplicity of the language. The Chomsky (1956) hierarchy is a containment hierarchy of classes of formal grammars that generate formal languages. It is used for classifying of programming languages and abstract machines.
From the Chomsky hierarchy perspective, if the algorithm can be specified on a simpler language (than  unrestricted), it can be  characterized by this kind of language, else it is a typical "unrestricted algorithm".
Examples: a "general purpose" macro language, like M4 is unrestricted (Turing complete), but the C preprocessor macro language is not, so any algorithm expressed in C preprocessor is a "simple algorithm".
The following are desirable features of a well-defined algorithm, as discussed in Scheider and Gersting (1995):
Properties of specific algorithms that may be desirable include space and time efficiency, generality (i.e. being able to handle many inputs), or determinism.
For his part John Venn, a logician contemporary to Jevons, was less than thrilled, opining that "it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines" (italics added, Venn 1881:120). But of historical use to the developing notion of "algorithm" is his explanation for his negative reaction with respect to a machine that "may subserve a really valuable purpose by enabling us to avoid otherwise inevitable labor":
Kleene's first statement of this was under the section title "12. Algorithmic theories". He would later amplify it in his text (1952) as follows:
He admitted that this definition "does not pretend to mathematical precision" (p. 1). His 1954 monograph was his attempt to define algorithm more accurately; he saw his resulting definition—his "normal" algorithm—as "equivalent to the concept of a recursive function" (p. 3). His definition included four major components (Chapter II.3 pp. 63ff):
In his Introduction Markov observed that "the entire significance for mathematics" of efforts to define algorithm more precisely would be "in connection with the problem of a constructive foundation for mathematics" (p. 2). Ian Stewart (cf Encyclopædia Britannica) shares a similar belief: "...constructive analysis is very much in the same algorithmic spirit as computer science...". For more see constructive mathematics and Intuitionism.
Minsky (1967) baldly asserts that "an algorithm is "an effective procedure" and declines to use the word "algorithm" further in his text; in fact his index makes it clear what he feels about "Algorithm, synonym for Effective procedure"(p. 311):
In his 1967 Theory of Recursive Functions and Effective Computability Hartley Rogers' characterizes "algorithm" roughly as "a clerical (i.e., deterministic, bookkeeping) procedure . . . applied to . . . symbolic inputs and which will eventually yield, for each such input, a corresponding symbolic output"(p. 1). He then goes on to describe the notion "in approximate and intuitive terms" as having 10 "features", 5 of which he asserts that "virtually all mathematicians would agree [to]" (p. 2). The remaining 5 he asserts "are less obvious than *1 to *5 and about which we might find less general agreement" (p. 3).
Knuth (1968, 1973) has given a list of five properties that are widely accepted as requirements for an algorithm:
Knuth offers as an example the Euclidean algorithm for determining the greatest common divisor of two natural numbers (cf. Knuth Vol. 1 p. 2).
Knuth admits that, while his description of an algorithm may be intuitively clear, it lacks formal rigor, since it is not exactly clear what "precisely defined" means, or "rigorously and unambiguously specified" means, or "sufficiently basic", and so forth. He makes an effort in this direction in his first volume where he defines in detail what he calls the "machine language" for his "mythical MIX...the world's first polyunsaturated computer" (pp. 120ff). Many of the algorithms in his books are written in the MIX language. He also uses tree diagrams, flow diagrams and state diagrams.
"Goodness" of an algorithm, "best" algorithms: Knuth states that "In practice, we not only want algorithms, we want good algorithms...." He suggests that some criteria of an algorithm's goodness are the number of steps to perform the algorithm, its "adaptability to computers, its simplicity and elegance, etc." Given a number of algorithms to perform the same computation, which one is "best"? He calls this sort of inquiry "algorithmic analysis: given an algorithm, to determine its performance characteristcis" (all quotes this paragraph: Knuth Vol. 1 p. 7)
Stone is noteworthy because of  his detailed discussion of what constitutes an “effective” rule – his robot, or person-acting-as-robot, must have some information and abilities within them, and if not the information and the ability must be provided in "the algorithm":
After providing us with his definition, Stone introduces the Turing machine model and states that the set of five-tuples that are the machine's instructions are “an algorithm ... known as a Turing machine program” (p. 9). Immediately thereafter he goes on say that a “computation of a Turing machine is described by stating:
While a student at Princeton in the mid-1960s, David Berlinski was a student of Alonzo Church (cf p. 160). His year-2000 book The Advent of the Algorithm: The 300-year Journey from an Idea to the Computer contains the following definition of algorithm:
A careful reading of Gurevich 2000 leads one to conclude (infer?) that he believes that "an algorithm" is actually "a Turing machine" or "a pointer machine" doing a computation. An "algorithm" is not just the symbol-table that guides the behavior of the machine, nor is it just one instance of a machine doing a computation given a particular set of input parameters, nor is it a suitably programmed machine with the power off; rather an algorithm is the machine actually doing any computation of which it is capable. Gurevich does not come right out and say this, so as worded above this conclusion (inference?) is certainly open to debate:
But the authors waffle here, saying "[L]et's stick to "algorithm" and "machine", and the reader is left, again, confused. We have to wait until Dershowitz and Gurevich 2007 to get the following footnote comment:
The above phrase computation as an evolution of the state differs markedly from the definition of Knuth and Stone—the "algorithm" as a Turing machine program. Rather, it corresponds to what Turing called the complete configuration (cf Turing's definition in Undecidable, p. 118) -- and includes both the current instruction (state) and the status of the tape. [cf Kleene (1952) p. 375 where he shows an example of a tape with 6 symbols on it—all other squares are blank—and how to Gödelize its combined table-tape status].
Philosopher Daniel Dennett analyses the importance of evolution as an algorithmic process in his 1995 book Darwin's Dangerous Idea. Dennett identifies three key features of an algorithm:
It is on the basis of this analysis that Dennett concludes that "According to Darwin, evolution is an algorithmic process". (p. 60).
However, in the previous page he has gone out on a much-further limb.[according to whom?] In the context of his chapter titled "Processes as Algorithms", he states:
It is unclear from the above whether Dennett is stating that the physical world by itself and without observers is intrinsically algorithmic (computational) or whether a symbol-processing observer is what is adding "meaning" to the observations.
Daniel Dennett is a proponent of strong artificial intelligence: the idea that the logical structure of an algorithm is sufficient to explain mind. John Searle, the creator of the Chinese room thought experiment, claims that "syntax [that is, logical structure] is by itself not sufficient for semantic content [that is, meaning]" (Searle 2002, p. 16). In other words, the "meaning" of symbols is relative to the mind that is using them; an algorithm—a logical construct—by itself is insufficient for a mind.
Searle cautions those who claim that algorithmic (computational) processes are intrinsic to nature (for example, cosmologists, physicists, chemists, etc.):
An example in Boolos-Burgess-Jeffrey (2002) (pp. 31–32) demonstrates the precision required in a complete specification of an algorithm, in this case to add two numbers: m+n. It is similar to the Stone requirements above.
Does Sipser mean that "algorithm" is just "instructions" for a Turing machine, or is the combination of "instructions + a (specific variety of) Turing machine"? For example, he defines the two standard variants (multi-tape and non-deterministic) of his particular variant (not the same as Turing's original) and goes on, in his Problems (pages 160–161), to describe four more variants (write-once, doubly infinite tape (i.e. left- and right-infinite), left reset, and "stay put instead of left). In addition, he imposes some constraints. First, the input must be encoded as a string (p. 157) and says of numeric encodings in the context of complexity theory:
Van Emde Boas comments on a similar problem with respect to the random-access machine (RAM) abstract model of computation sometimes used in place of the Turing machine when doing "analysis of algorithms":
"The absence or presence of multiplicative and parallel bit manipulation operations is of relevance for the correct understanding of some results in the analysis of algorithms.
With regard to a "description language" for algorithms Sipser finishes the job that Stone and Boolos-Burgess-Jeffrey started (boldface added). He offers us three levels of description of Turing machine algorithms (p. 157):
In Yanofsky (2011)[3] an algorithm is defined to be the set of programs that implement that algorithm: the set of all programs is partitioned into equivalence classes. Although the set of programs does not form a category, the set of algorithms form a category with extra structure. The conditions that describe when two programs are equivalent turn out to be coherence relations which give the extra structure to the category of algorithms.
In Seiller (2024)[4] an algorithm is defined as an edge-labelled graph, together with an interpretation of labels as maps in an abstract data structure. This definition is given together with a formal definition of programs (and models of computation), allowing to formally define the notion of implementation, that is when a program implements an algorithm. The notion of algorithm thus obtained avoids some known issues, and is understood as a specification of some kind. In particular, a given program can (and in fact, always do) implement several algorithms. Another important feature of the approach is that it takes into account the fact that a given algorithm can be implemented in different (and possibly unrelated) computational models.
A division algorithm is an algorithm which, given two integers N and D (respectively the numerator and the denominator), computes their quotient and/or remainder, the result of Euclidean division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration.[1] Newton–Raphson and Goldschmidt algorithms fall into this category.
Variants of these algorithms allow using fast multiplication algorithms. It results that, for large integers, the computer time needed for a division is the same, up to a constant factor, as the time needed for a multiplication, whichever multiplication algorithm is used.
The simplest division algorithm, historically incorporated into a greatest common divisor algorithm presented in Euclid's Elements, Book VII, Proposition 1, finds the remainder given two positive integers using only subtractions and comparisons:
The proof that the quotient and remainder exist and are unique (described at Euclidean division) gives rise to a complete division algorithm, applicable to both negative and positive numbers, using additions, subtractions, and comparisons:
This procedure always produces R ≥ 0. Although very simple, it takes Ω(Q) steps, and so is exponentially slower than even slow division algorithms like long division. It is useful if Q is known to be small (being an output-sensitive algorithm), and can serve as an executable specification.
Long division is the standard algorithm used for pen-and-paper division of multi-digit numbers expressed in decimal notation. It shifts gradually from the left to the right end of the dividend, subtracting the largest possible multiple of the divisor (at the digit level) at each stage; the multiples then become the digits of the quotient, and the final difference is then the remainder.
When used with a binary radix, this method forms the basis for the (unsigned) integer division with remainder algorithm below. Short division is an abbreviated form of long division suitable for one-digit divisors. Chunking –  also known as the partial quotients method or the hangman method –  is a less-efficient form of long division which may be easier to understand. By allowing one to subtract more multiples than what one currently has at each stage, a more freeform variant of long division can be developed as well.
The following algorithm, the binary version of the famous long division, will divide N by D, placing the quotient in Q and the remainder in R.  In the following pseudo-code, all values are treated as unsigned integers.
Non-restoring division uses the digit set {−1, 1} for the quotient digits instead of {0, 1}.  The algorithm is more complex, but has the advantage when implemented in hardware that there is only one decision and addition/subtraction per quotient bit; there is no restoring step after the subtraction,[3] which potentially cuts down the numbers of operations by up to half and lets it be executed faster.[4]  The basic algorithm for binary (radix 2) non-restoring division of non-negative numbers is:[verification needed]
Following this algorithm, the quotient is in a non-standard form consisting of digits of −1 and +1. This form needs to be converted to binary to form the final quotient. Example:
Finally, quotients computed by this algorithm are always odd, and the remainder in R is in the range −D ≤ R < D.  For example, 5 / 2 = 3 R −1.  To convert to a positive remainder, do a single restoring step after Q is converted from non-standard form to standard form:
SRT division is a popular method for division in many microprocessor implementations.[5][6] The algorithm is named after D. W. Sweeney of IBM, James E. Robertson of University of Illinois, and K. D. Tocher of Imperial College London. They all developed the algorithm independently at approximately the same time (published in February 1957, September 1958, and January 1958 respectively).[7][8][9]
It is possible to generate a polynomial fit of degree larger than 2, computing the coefficients using the Remez algorithm.  The trade-off is that the initial guess requires more computational cycles but hopefully in exchange for fewer iterations of Newton–Raphson.
The Goldschmidt method is used in AMD Athlon CPUs and later models.[16][17] It is also known as Anderson Earle Goldschmidt Powers (AEGP) algorithm and is implemented by various IBM processors.[18][19] Although it converges at the same rate as a Newton–Raphson implementation, one advantage of the Goldschmidt method is that the multiplications in the numerator and in the denominator can be done in parallel.[19]
Methods designed for hardware implementation generally do not scale to integers with thousands or millions of decimal digits; these frequently occur, for example, in modular reductions in cryptography. For these large integers, more efficient division algorithms transform the problem to use a small number of multiplications, which can then be done using an asymptotically efficient multiplication algorithm such as the Karatsuba algorithm, Toom–Cook multiplication or the Schönhage–Strassen algorithm. The result is that the computational complexity of the division is of the same order (up to a multiplicative constant) as that of the multiplication. Examples include reduction to multiplication by Newton's method as described above,[20] as well as the slightly faster Burnikel-Ziegler division,[21] Barrett reduction and Montgomery reduction algorithms.[22][verification needed] Newton's method is particularly efficient in scenarios where one must divide by the same divisor many times, since after the initial Newton inversion only one (truncated) multiplication is needed for each division.
When a division operation is performed, the exact quotient 



q


{\displaystyle q}

 and remainder 



r


{\displaystyle r}

 are approximated to fit within the computer’s precision limits. The Division Algorithm states:
In mathematics, the Euclidean algorithm,[note 1] or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers, the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (c. 300 BC).
It is an example of an algorithm, a step-by-step procedure for performing a calculation according to well-defined rules,
and is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.
The Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. For example, 21 is the GCD of 252 and 105 (as 252 = 21 × 12 and 105 = 21 × 5), and the same number 21 is also the GCD of 105 and 252 − 105 = 147. Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until the two numbers become equal. When that occurs, that number is the GCD of the original two numbers. By reversing the steps or using the extended Euclidean algorithm, the GCD can be expressed as a linear combination of the two original numbers, that is the sum of the two numbers, each multiplied by an integer (for example, 21 = 5 × 105 + (−2) × 252). The fact that the GCD can always be expressed in this way is known as Bézout's identity.
The version of the Euclidean algorithm described above—which follows Euclid's original presentation—may require many subtraction steps to find the GCD when one of the given numbers is much bigger than the other. A more efficient version of the algorithm shortcuts these steps, instead replacing the larger of the two numbers by its remainder when divided by the smaller of the two (with this version, the algorithm stops when reaching a zero remainder). With this improvement, the algorithm never requires more steps than five times the number of digits (base 10) of the smaller integer. This was proven by Gabriel Lamé in 1844 (Lamé's Theorem),[1][2] and marks the beginning of computational complexity theory. Additional methods for improving the algorithm's efficiency were developed in the 20th century.
The Euclidean algorithm has many theoretical and practical applications. It is used for reducing fractions to their simplest form and for performing division in modular arithmetic. Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications, and in methods for breaking these cryptosystems by factoring large composite numbers. The Euclidean algorithm may be used to solve Diophantine equations, such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem, to construct continued fractions, and to find accurate rational approximations to real numbers. Finally, it can be used as a basic tool for proving theorems in number theory such as Lagrange's four-square theorem and the uniqueness of prime factorizations.
The original algorithm was described only for natural numbers and geometric lengths (real numbers), but the algorithm was generalized in the 19th century to other types of numbers, such as Gaussian integers and polynomials of one variable. This led to modern abstract algebraic notions such as Euclidean domains.
The Euclidean algorithm calculates the greatest common divisor (GCD) of two natural numbers a and b. The greatest common divisor g is the largest natural number that divides both a and b without leaving a remainder. Synonyms for GCD include greatest common factor (GCF), highest common factor (HCF), highest common divisor (HCD), and greatest common measure (GCM). The greatest common divisor is often written as gcd(a, b) or, more simply, as (a, b),[3] although the latter notation is ambiguous, also used for concepts such as an ideal in the ring of integers, which is closely related to GCD.
The greatest common divisor of two numbers a and b is the product of the prime factors shared by the two numbers, where each prime factor can be repeated as many times as it divides both a and b.[8] For example, since 1386 can be factored into 2 × 3 × 3 × 7 × 11, and 3213 can be factored into 3 × 3 × 3 × 7 × 17, the GCD of 1386 and 3213 equals 63 = 3 × 3 × 7, the product of their shared prime factors (with 3 repeated since 3 × 3 divides both). If two numbers have no common prime factors, their GCD is 1 (obtained here as an instance of the empty product); in other words, they are coprime. A key advantage of the Euclidean algorithm is that it can find the GCD efficiently without having to compute the prime factors.[9][10] Factorization of large integers is believed to be a computationally very difficult problem, and the security of many widely used cryptographic protocols is based upon its infeasibility.[11]
Thus, Euclid's algorithm, which computes the GCD of two integers, suffices to calculate the GCD of arbitrarily many integers.
The Euclidean algorithm can be thought of as constructing a sequence of non-negative integers that begins with the two given integers 




r

−
2


=
a


{\displaystyle r_{-2}=a}

 and 




r

−
1


=
b


{\displaystyle r_{-1}=b}

 and will eventually terminate with the integer zero: 



{

r

−
2


=
a
,
 

r

−
1


=
b
,
 

r

0


,
 

r

1


,
 
⋯
,
 

r

n
−
1


,
 

r

n


=
0
}


{\displaystyle \{r_{-2}=a,\ r_{-1}=b,\ r_{0},\ r_{1},\ \cdots ,\ r_{n-1},\ r_{n}=0\}}

 with 




r

k
+
1


<

r

k




{\displaystyle r_{k+1}<r_{k}}

. The integer 




r

n
−
1




{\displaystyle r_{n-1}}

 will then be the GCD and we can state 




gcd

(
a
,
b
)
=

r

n
−
1




{\displaystyle {\text{gcd}}(a,b)=r_{n-1}}

. The algorithm indicates how to construct the intermediate remainders 




r

k




{\displaystyle r_{k}}

 via division-with-remainder on the preceding pair 



(

r

k
−
2


,
 

r

k
−
1


)


{\displaystyle (r_{k-2},\ r_{k-1})}

 by finding an integer quotient 




q

k




{\displaystyle q_{k}}

 so that:
Because the sequence of non-negative integers 



{

r

k


}


{\displaystyle \{r_{k}\}}

 is strictly decreasing, it eventually must terminate. In other words, since 




r

k


≥
0


{\displaystyle r_{k}\geq 0}

 for every 



k


{\displaystyle k}

, and each 




r

k




{\displaystyle r_{k}}

 is an integer that is strictly smaller than the preceding 




r

k
−
1




{\displaystyle r_{k-1}}

, there eventually cannot be a non-negative integer smaller than zero, and hence the algorithm must terminate. In fact, the algorithm will always terminate at the nth step with 




r

n




{\displaystyle r_{n}}

 equal to zero.[15]
We can generalize slightly by dropping any ordering requirement on the initial two values 



a


{\displaystyle a}

 and 



b


{\displaystyle b}

. If 



a
=
b


{\displaystyle a=b}

, the algorithm may continue and trivially find that 




gcd

(
a
,
 
a
)
=
a


{\displaystyle {\text{gcd}}(a,\ a)=a}

 as the sequence of remainders will be 



{
a
,
 
a
,
 
0
}


{\displaystyle \{a,\ a,\ 0\}}

. If 



a
<
b


{\displaystyle a<b}

, then we can also continue since 



a
≡
0
⋅
b
+
a


{\displaystyle a\equiv 0\cdot b+a}

, suggesting the next remainder should be 



a


{\displaystyle a}

 itself, and the sequence is 



{
a
,
 
b
,
 
a
,
 
⋯
}


{\displaystyle \{a,\ b,\ a,\ \cdots \}}

. Normally, this would be invalid because it breaks the requirement 




r

0


<

r

−
1




{\displaystyle r_{0}<r_{-1}}

 but now we have 



a
<
b


{\displaystyle a<b}

 by construction, so the requirement is automatically satisfied and the Euclidean algorithm can continue as normal. Therefore, dropping any ordering between the first two integers does not affect the conclusion that the sequence must eventually terminate because the next remainder will always satisfy 




r

0


<
b


{\displaystyle r_{0}<b}

 and everything continues as above. The only modifications that need to be made are that 




r

k


<

r

k
−
1




{\displaystyle r_{k}<r_{k-1}}

 only for 



k
≥
0


{\displaystyle k\geq 0}

, and that the sub-sequence of non-negative integers 



{

r

k
−
1


}


{\displaystyle \{r_{k-1}\}}

 for 



k
≥
0


{\displaystyle k\geq 0}

 is strictly decreasing, therefore excluding 



a
=

r

−
2




{\displaystyle a=r_{-2}}

 from both statements.
The validity of the Euclidean algorithm can be proven by a two-step argument.[16] In the first step, the final nonzero remainder rN−1 is shown to divide both a and b. Since it is a common divisor, it must be less than or equal to the greatest common divisor g. In the second step, it is shown that any common divisor of a and b, including g, must divide rN−1; therefore, g must be less than or equal to rN−1. These two opposite inequalities imply rN−1 = g.
For illustration, the Euclidean algorithm can be used to find the greatest common divisor of a = 1071 and b = 462. To begin, multiples of 462 are subtracted from 1071 until the remainder is less than 462. Two such multiples can be subtracted (q0 = 2), leaving a remainder of 147:
Since the last remainder is zero, the algorithm ends with 21 as the greatest common divisor of 1071 and 462. This agrees with the gcd(1071, 462) found by prime factorization above. In tabular form, the steps are:
The Euclidean algorithm can be visualized in terms of the tiling analogy given above for the greatest common divisor.[19] Assume that we wish to cover an a×b rectangle with square tiles exactly, where a is the larger of the two numbers. We first attempt to tile the rectangle using b×b square tiles; however, this leaves an r0×b residual rectangle untiled, where r0 < b. We then attempt to tile the residual rectangle with r0×r0 square tiles. This leaves a second residual rectangle r1×r0, which we attempt to tile using r1×r1 square tiles, and so on. The sequence ends when there is no residual rectangle, i.e., when the square tiles cover the previous residual rectangle exactly. The length of the sides of the smallest square tile is the GCD of the dimensions of the original rectangle. For example, the smallest square tile in the adjacent figure is 21×21 (shown in red), and 21 is the GCD of 1071 and 462, the dimensions of the original rectangle (shown in green).
At every step k, the Euclidean algorithm computes a quotient qk and remainder rk from two numbers rk−1 and rk−2
In Euclid's original version of the algorithm, the quotient and remainder are found by repeated subtraction; that is, rk−1 is subtracted from rk−2 repeatedly until the remainder rk is smaller than rk−1. After that rk and rk−1 are exchanged and the process is iterated. Euclidean division reduces all the steps between two exchanges into a single step, which is thus more efficient. Moreover, the quotients are not needed, thus one may replace Euclidean division by the modulo operation, which gives only the remainder. Thus the iteration of the Euclidean algorithm becomes simply
Implementations of the algorithm may be expressed in pseudocode. For example, the division-based version may be programmed as[21]
In another version of Euclid's algorithm, the quotient at each step is increased by one if the resulting negative remainder is smaller in magnitude than the typical positive remainder.[24][25] Previously, the equation
Leopold Kronecker has shown that this version requires the fewest steps of any version of Euclid's algorithm.[24][25] More generally, it has been proven that, for every input numbers a and b, the number of steps is minimal if and only if qk is chosen in order that 




|



r

k
+
1



r

k




|

<


1
φ


∼
0.618
,


{\displaystyle \left|{\frac {r_{k+1}}{r_{k}}}\right|<{\frac {1}{\varphi }}\sim 0.618,}

 where 



φ


{\displaystyle \varphi }

 is the golden ratio.[26]
The Euclidean algorithm is one of the oldest algorithms in common use.[27] It appears in Euclid's Elements (c. 300 BC), specifically in Book 7 (Propositions 1–2) and Book 10 (Propositions 2–3). In Book 7, the algorithm is formulated for integers, whereas in Book 10, it is formulated for lengths of line segments. (In modern usage, one would say it was formulated there for real numbers. But lengths, areas, and volumes, represented as real numbers in modern usage, are not measured in the same units and there is no natural unit of length, area, or volume; the concept of real numbers was unknown at that time.) The latter algorithm is geometrical. The GCD of two lengths a and b corresponds to the greatest length g that measures a and b evenly; in other words, the lengths a and b are both integer multiples of the length g.
The algorithm was probably not discovered by Euclid, who compiled results from earlier mathematicians in his Elements.[28][29] The mathematician and historian B. L. van der Waerden suggests that Book VII derives from a textbook on number theory written by mathematicians in the school of Pythagoras.[30] The algorithm was probably known by Eudoxus of Cnidus (about 375 BC).[27][31] The algorithm may even pre-date Eudoxus,[32][33] judging from the use of the technical term ἀνθυφαίρεσις (anthyphairesis, reciprocal subtraction) in works by Euclid and Aristotle.[34] Claude Brezinski, following remarks by Pappus of Alexandria, credits the algorithm to Theaetetus (c. 417 – c. 369 BC).[35]
Centuries later, Euclid's algorithm was discovered independently both in India and in China,[36] primarily to solve Diophantine equations that arose in astronomy and making accurate calendars. In the late 5th century, the Indian mathematician and astronomer Aryabhata described the algorithm as the "pulverizer",[37] perhaps because of its effectiveness in solving Diophantine equations.[38] Although a special case of the Chinese remainder theorem had already been described in the Chinese book Sunzi Suanjing,[39] the general solution was published by Qin Jiushao in his 1247 book Shushu Jiuzhang (數書九章 Mathematical Treatise in Nine Sections).[40] The Euclidean algorithm was first described numerically and popularized in Europe in the second edition of Bachet's Problèmes plaisants et délectables (Pleasant and enjoyable problems, 1624).[37] In Europe, it was likewise used to solve Diophantine equations and in developing continued fractions. The extended Euclidean algorithm was published by the English mathematician Nicholas Saunderson,[41] who attributed it to Roger Cotes as a method for computing continued fractions efficiently.[42]
In the 19th century, the Euclidean algorithm led to the development of new number systems, such as Gaussian integers and Eisenstein integers. In 1815, Carl Gauss used the Euclidean algorithm to demonstrate unique factorization of Gaussian integers, although his work was first published in 1832.[43] Gauss mentioned the algorithm in his Disquisitiones Arithmeticae (published 1801), but only as a method for continued fractions.[36] Peter Gustav Lejeune Dirichlet seems to have been the first to describe the Euclidean algorithm as the basis for much of number theory.[44] Lejeune Dirichlet noted that many results of number theory, such as unique factorization, would hold true for any other system of numbers to which the Euclidean algorithm could be applied.[45] Lejeune Dirichlet's lectures on number theory were edited and extended by Richard Dedekind, who used Euclid's algorithm to study algebraic integers, a new general type of number. For example, Dedekind was the first to prove Fermat's two-square theorem using the unique factorization of Gaussian integers.[46] Dedekind also defined the concept of a Euclidean domain, a number system in which a generalized version of the Euclidean algorithm can be defined (as described below). In the closing decades of the 19th century, the Euclidean algorithm gradually became eclipsed by Dedekind's more general theory of ideals.[47]
Other applications of Euclid's algorithm were developed in the 19th century. In 1829, Charles Sturm showed that the algorithm was useful in the Sturm chain method for counting the real roots of polynomials in any given interval.[48]
The Euclidean algorithm was the first integer relation algorithm, which is a method for finding integer relations between commensurate real numbers. Several novel integer relation algorithms have been developed, such as the algorithm of Helaman Ferguson and R.W. Forcade (1979)[49] and the LLL algorithm.[50][51]
In 1969, Cole and Davie developed a two-player game based on the Euclidean algorithm, called The Game of Euclid,[52] which has an optimal strategy.[53] The players begin with two piles of a and b stones. The players take turns removing m multiples of the smaller pile from the larger. Thus, if the two piles consist of x and y stones, where x is larger than y, the next player can reduce the larger pile from x stones to x − my stones, as long as the latter is a nonnegative integer. The winner is the first player to reduce one pile to zero stones.[54][55]
The integers s and t can be calculated from the quotients q0, q1, etc. by reversing the order of equations in Euclid's algorithm.[59] Beginning with the next-to-last equation, g can be expressed in terms of the quotient qN−1 and the two preceding remainders, rN−2 and rN−3:
The Euclidean algorithm, and thus Bézout's identity, can be generalized to the context of Euclidean domains.
The integers s and t of Bézout's identity can be computed efficiently using the extended Euclidean algorithm. This extension adds two recursive equations to Euclid's algorithm[61]
Using this recursion, Bézout's integers s and t are given by s = sN and t = tN, where N + 1 is the step on which the algorithm terminates with rN+1 = 0.
The validity of this approach can be shown by induction. Assume that the recursion formula is correct up to step k − 1 of the algorithm; in other words, assume that
The integers s and t can also be found using an equivalent matrix method.[62] The sequence of equations of Euclid's algorithm
the two integers of Bézout's identity are s = (−1)N+1m22 and t = (−1)Nm12. The matrix method is as efficient as the equivalent recursion, with two multiplications and two additions per step of the Euclidean algorithm.
Bézout's identity is essential to many applications of Euclid's algorithm, such as demonstrating the unique factorization of numbers into prime factors.[64] To illustrate this, suppose that a number L can be written as a product of two factors u and v, that is, L = uv. If another number w also divides L but is coprime with u, then w must divide v, by the following argument: If the greatest common divisor of u and w is 1, then integers s and t can be found such that
where s and t can be found by the extended Euclidean algorithm.[69] This provides one solution to the Diophantine equation, x1 = s (c/g) and y1 = t (c/g).
This equation can be solved by the Euclidean algorithm, as described above. Finding multiplicative inverses is an essential step in the RSA algorithm, which is widely used in electronic commerce; specifically, the equation determines the integer used to decrypt the message.[74] Although the RSA algorithm uses rings rather than fields, the Euclidean algorithm can still be used to find a multiplicative inverse where one exists. The Euclidean algorithm also has other applications in error-correcting codes; for example, it can be used as an alternative to the Berlekamp–Massey algorithm for decoding BCH and Reed–Solomon codes, which are based on Galois fields.[75]
Euclid's algorithm can also be used to solve multiple linear Diophantine equations.[76] Such equations arise in the Chinese remainder theorem, which describes a novel method to represent an integer x. Instead of representing an integer by its digits, it may be represented by its remainders xi modulo a set of N coprime numbers mi:[77]
Since these numbers hi are the multiplicative inverses of the Mi, they may be found using Euclid's algorithm as described in the previous subsection.
The Euclidean algorithm can be used to arrange the set of all positive rational numbers into an infinite binary search tree, called the Stern–Brocot tree.
The number 1 (expressed as a fraction 1/1) is placed at the root of the tree, and the location of any other number a/b can be found by computing gcd(a,b) using the original form of the Euclidean algorithm, in which each step replaces the larger of the two given numbers by its difference with the smaller number (not its remainder), stopping when two equal numbers are reached. A step of the Euclidean algorithm that replaces the first of the two numbers corresponds to a step in the tree from a node to its right child, and a step that replaces the second of the two numbers corresponds to a step in the tree from a node to its left child. The sequence of steps constructed in this way does not depend on whether a/b is given in lowest terms, and forms a path from the root to a node containing the number a/b.[78] This fact can be used to prove that each positive rational number appears exactly once in this tree.
The Euclidean algorithm has almost the same relationship to another binary tree on the rational numbers called the Calkin–Wilf tree. The difference is that the path is reversed: instead of producing a path from the root of the tree to a target, it produces a path from the target to the root.
The Euclidean algorithm has a close relationship with continued fractions.[79] The sequence of equations can be written in the form
Calculating a greatest common divisor is an essential step in several integer factorization algorithms,[80] such as Pollard's rho algorithm,[81] Shor's algorithm,[82] Dixon's factorization method[83] and the Lenstra elliptic curve factorization.[84] The Euclidean algorithm may be used to find this GCD efficiently. Continued fraction factorization uses continued fractions, which are determined using Euclid's algorithm.[85]
The computational efficiency of Euclid's algorithm has been studied thoroughly.[86] This efficiency can be described by the number of division steps the algorithm requires, multiplied by the computational expense of each step. The first known analysis of Euclid's algorithm is due to A. A. L. Reynaud in 1811,[87] who showed that the number of division steps on input (u, v) is bounded by v; later he improved this to v/2 + 2. Later, in 1841, P. J. E. Finck showed[88] that the number of division steps is at most 2 log2 v + 1, and hence Euclid's algorithm runs in time polynomial in the size of the input.[89] Émile Léger, in 1837, studied the worst case, which is when the inputs are consecutive Fibonacci numbers.[89] Finck's analysis was refined by Gabriel Lamé in 1844,[90] who showed that the number of steps required for completion is never more than five times the number h of base-10 digits of the smaller number b.[91][92]
In the uniform cost model (suitable for analyzing the complexity of gcd calculation on numbers that fit into a single machine word), each step of the algorithm takes constant time, and Lamé's analysis implies that the total running time is also O(h). However, in a model of computation suitable for computation with larger numbers, the computational expense of a single remainder computation in the algorithm can be as large as O(h2).[93] In this case the total time for all of the steps of the algorithm can be analyzed using a telescoping series, showing that it is also O(h2). Modern algorithmic techniques based on the Schönhage–Strassen algorithm for fast integer multiplication can be used to speed this up, leading to quasilinear algorithms for the GCD.[94][95]
as may be seen by dividing all the steps in the Euclidean algorithm by g.[97] By the same argument, the number of steps remains the same if a and b are multiplied by a common factor w: T(a, b) = T(wa, wb). Therefore, the number of steps T may vary dramatically between neighboring pairs of numbers, such as T(a, b) and T(a, b + 1), depending on the size of the two GCDs.
If the Euclidean algorithm requires N steps for a pair of natural numbers a > b > 0, the smallest values of a and b for which this is true are the Fibonacci numbers FN+2 and FN+1, respectively.[98] More precisely, if the Euclidean algorithm requires N steps for the pair a > b, then one has a ≥ FN+2 and b ≥ FN+1. This can be shown by induction.[99] If N = 1, b divides a with no remainder; the smallest natural numbers for which this is true is b = 1 and a = 2, which are F2 and F3, respectively. Now assume that the result holds for all values of N up to M − 1. The first step of the M-step algorithm is a = q0b + r0, and the Euclidean algorithm requires M − 1 steps for the pair b > r0. By induction hypothesis, one has b ≥ FM+1 and r0 ≥ FM. Therefore, a = q0b + r0 ≥ b + r0 ≥ FM+1 + FM = FM+2,
which is the desired inequality.
This proof, published by Gabriel Lamé in 1844, represents the beginning of computational complexity theory,[100] and also the first practical application of the Fibonacci numbers.[98]
This result suffices to show that the number of steps in Euclid's algorithm can never be more than five times the number of its digits (base 10).[101] For if the algorithm requires N steps, then b is greater than or equal to FN+1 which in turn is greater than or equal to φN−1, where φ is the golden ratio. Since b ≥ φN−1, then N − 1 ≤ logφb. Since log10φ > 1/5, (N − 1)/5 < log10φ logφb = log10b. Thus, N ≤ 5 log10b. Thus, the Euclidean algorithm always needs less than O(h) divisions, where h is the number of digits in the smaller number b.
The average number of steps taken by the Euclidean algorithm has been defined in three different ways. The first definition is the average time T(a) required to calculate the GCD of a given number a and a smaller natural number b chosen with equal probability from the integers 0 to a − 1[96]
In each step k of the Euclidean algorithm, the quotient qk and remainder rk are computed for a given pair of integers rk−2 and rk−1
For comparison, Euclid's original subtraction-based algorithm can be much slower. A single integer division is equivalent to the quotient q number of subtractions. If the ratio of a and b is very large, the quotient is large and many subtractions will be required. On the other hand, it has been shown that the quotients are very likely to be small integers. The probability of a given quotient q is approximately ln |u/(u − 1)| where u = (q + 1)2.[114] For illustration, the probability of a quotient of 1, 2, 3, or 4 is roughly 41.5%, 17.0%, 9.3%, and 5.9%, respectively. Since the operation of subtraction is faster than division, particularly for large numbers,[115] the subtraction-based Euclid's algorithm is competitive with the division-based version.[116] This is exploited in the binary version of Euclid's algorithm.[117]
Combining the estimated number of steps with the estimated computational expense per step shows that the Euclid's algorithm grows quadratically (h2) with the average number of digits h in the initial two numbers a and b. Let h0, h1, ..., hN−1 represent the number of digits in the successive remainders r0, r1, ..., rN−1. Since the number of steps N grows linearly with h, the running time is bounded by
Euclid's algorithm is widely used in practice, especially for small numbers, due to its simplicity.[118] For comparison, the efficiency of alternatives to Euclid's algorithm may be determined.
The binary GCD algorithm is an efficient alternative that substitutes division with faster operations by exploiting the binary representation used by computers.[119][120] However, this alternative also scales like O(h²). It is generally faster than the Euclidean algorithm on real computers, even though it scales in the same way.[94] Additional efficiency can be gleaned by examining only the leading digits of the two numbers a and b.[121][122] The binary algorithm can be extended to other bases (k-ary algorithms),[123] with up to fivefold increases in speed.[124] Lehmer's GCD algorithm uses the same general principle as the binary algorithm to speed up GCD computations in arbitrary bases.
A recursive approach for very large integers (with more than 25,000 digits) leads to quasilinear integer GCD algorithms,[125] such as those of Schönhage,[126][127] and Stehlé and Zimmermann.[128] These algorithms exploit the 2×2 matrix form of the Euclidean algorithm given above. These quasilinear methods generally scale as O(h log h2 log log h).[94][95]
Although the Euclidean algorithm is used to find the greatest common divisor of two natural numbers (positive integers), it may be generalized to the real numbers, and to other mathematical objects, such as polynomials,[129] quadratic integers[130] and Hurwitz quaternions.[131] In the latter cases, the Euclidean algorithm is used to demonstrate the crucial property of unique factorization, i.e., that such numbers can be factored uniquely into irreducible elements, the counterparts of prime numbers. Unique factorization is essential to many proofs of number theory.
Euclid's algorithm can be applied to real numbers, as described by Euclid in Book 10 of his Elements. The goal of the algorithm is to identify a real number g such that two given real numbers, a and b, are integer multiples of it: a = mg and b = ng, where m and n are integers.[28] This identification is equivalent to finding an integer relation among the real numbers a and b; that is, it determines integers s and t such that sa + tb = 0. If such an equation is possible, a and b are called commensurable lengths, otherwise they are incommensurable lengths.[132][133]
The real-number Euclidean algorithm differs from its integer counterpart in two respects. First, the remainders rk are real numbers, although the quotients qk are integers as before. Second, the algorithm is not guaranteed to end in a finite number N of steps. If it does, the fraction a/b is a rational number, i.e., the ratio of two integers
and can be written as a finite continued fraction [q0; q1, q2, ..., qN]. If the algorithm does not stop, the fraction a/b is an irrational number and can be described by an infinite continued fraction [q0; q1, q2, …].[134] Examples of infinite continued fractions are the golden ratio φ = [1; 1, 1, ...] and the square root of two, √2 = [1; 2, 2, ...].[135] The algorithm is unlikely to stop, since almost all ratios a/b of two real numbers are irrational.[136]
Polynomials in a single variable x can be added, multiplied and factored into irreducible polynomials, which are the analogs of the prime numbers for integers. The greatest common divisor polynomial g(x) of two polynomials a(x) and b(x) is defined as the product of their shared irreducible polynomials, which can be identified using the Euclidean algorithm.[129] The basic procedure is similar to that for integers. At each step k, a quotient polynomial qk(x) and a remainder polynomial rk(x) are identified to satisfy the recursive equation
where r−2(x) = a(x) and r−1(x) = b(x). Each quotient polynomial is chosen such that each remainder is either zero or has a degree that is smaller than the degree of its predecessor: deg[rk(x)] < deg[rk−1(x)]. Since the degree is a nonnegative integer, and since it decreases with every step, the Euclidean algorithm concludes in a finite number of steps. The last nonzero remainder is the greatest common divisor of the original two polynomials, a(x) and b(x).[138]
Many of the applications described above for integers carry over to polynomials.[139] The Euclidean algorithm can be used to solve linear Diophantine equations and Chinese remainder problems for polynomials; continued fractions of polynomials can also be defined.
The polynomial Euclidean algorithm has other applications, such as Sturm chains, a method for counting the zeros of a polynomial that lie inside a given real interval.[140] This in turn has applications in several areas, such as the Routh–Hurwitz stability criterion in control theory.[141]
Finally, the coefficients of the polynomials need not be drawn from integers, real numbers or even the complex numbers. For example, the coefficients may be drawn from a general field, such as the finite fields GF(p) described above. The corresponding conclusions about the Euclidean algorithm and its applications hold even for such polynomials.[129]
The Gaussian integers are complex numbers of the form α = u + vi, where u and v are ordinary integers[note 2] and i is the square root of negative one.[142] By defining an analog of the Euclidean algorithm, Gaussian integers can be shown to be uniquely factorizable, by the argument above.[43] This unique factorization is helpful in many applications, such as deriving all Pythagorean triples or proving Fermat's theorem on sums of two squares.[142] In general, the Euclidean algorithm is convenient in such applications, but not essential; for example, the theorems can often be proven by other arguments.
The Euclidean algorithm developed for two Gaussian integers α and β is nearly the same as that for ordinary integers,[143] but differs in two respects. As before, we set r−2 = α and r−1 = β, and the task at each step k is to identify a quotient qk and a remainder rk such that
where every remainder is strictly smaller than its predecessor: |rk| < |rk−1|. The first difference is that the quotients and remainders are themselves Gaussian integers, and thus are complex numbers. The quotients qk are generally found by rounding the real and complex parts of the exact ratio (such as the complex number α/β) to the nearest integers.[143] The second difference lies in the necessity of defining how one complex remainder can be "smaller" than another. To do this, a norm function f(u + vi) = u2 + v2 is defined, which converts every Gaussian integer u + vi into an ordinary integer. After each step k of the Euclidean algorithm, the norm of the remainder f(rk) is smaller than the norm of the preceding remainder, f(rk−1). Since the norm is a nonnegative integer and decreases with every step, the Euclidean algorithm for Gaussian integers ends in a finite number of steps.[144] The final nonzero remainder is gcd(α, β), the Gaussian integer of largest norm that divides both α and β; it is unique up to multiplication by a unit, ±1 or ±i.[145]
Many of the other applications of the Euclidean algorithm carry over to Gaussian integers. For example, it can be used to solve linear Diophantine equations and Chinese remainder problems for Gaussian integers;[146] continued fractions of Gaussian integers can also be defined.[143]
A set of elements under two binary operations, denoted as addition and multiplication, is called a Euclidean domain if it forms a commutative ring R and, roughly speaking, if a generalized Euclidean algorithm can be performed on them.[147][148] The two operations of such a ring need not be the addition and multiplication of ordinary arithmetic; rather, they can be more general, such as the operations of a mathematical group or monoid. Nevertheless, these general operations should respect many of the laws governing ordinary arithmetic, such as commutativity, associativity and distributivity.
The generalized Euclidean algorithm requires a Euclidean function, i.e., a mapping f from R into the set of nonnegative integers such that, for any two nonzero elements a and b in R, there exist q and r in R such that a = qb + r and f(r) < f(b).[149] Examples of such mappings are the absolute value for integers, the degree for univariate polynomials, and the norm for Gaussian integers above.[150][151] The basic principle is that each step of the algorithm reduces f inexorably; hence, if f can be reduced only a finite number of times, the algorithm must stop in a finite number of steps. This principle relies on the well-ordering property of the non-negative integers, which asserts that every non-empty set of non-negative integers has a smallest member.[152]
The fundamental theorem of arithmetic applies to any Euclidean domain: Any number from a Euclidean domain can be factored uniquely into irreducible elements. Any Euclidean domain is a unique factorization domain (UFD), although the converse is not true.[152] The Euclidean domains and the UFD's are subclasses of the GCD domains, domains in which a greatest common divisor of two numbers always exists.[153] In other words, a greatest common divisor may exist (for all pairs of elements in a domain), although it may not be possible to find it using a Euclidean algorithm. A Euclidean domain is always a principal ideal domain (PID), an integral domain in which every ideal is a principal ideal.[154] Again, the converse is not true: not every PID is a Euclidean domain.
The unique factorization of Euclidean domains is useful in many applications. For example, the unique factorization of the Gaussian integers is convenient in deriving formulae for all Pythagorean triples and in proving Fermat's theorem on sums of two squares.[142] Unique factorization was also a key element in an attempted proof of Fermat's Last Theorem published in 1847 by Gabriel Lamé, the same mathematician who analyzed the efficiency of Euclid's algorithm, based on a suggestion of Joseph Liouville.[155] Lamé's approach required the unique factorization of numbers of the form x + ωy, where x and y are integers, and ω = e2iπ/n is an nth root of 1, that is, ωn = 1. Although this approach succeeds for some values of n (such as n = 3, the Eisenstein integers), in general such numbers do not factor uniquely. This failure of unique factorization in some cyclotomic fields led Ernst Kummer to the concept of ideal numbers and, later, Richard Dedekind to ideals.[156]
The Euclidean algorithm may be applied to some noncommutative rings such as the set of Hurwitz quaternions.[131][160] Let α and β represent two elements from such a ring. They have a common right divisor δ if α = ξδ and β = ηδ for some choice of ξ and η in the ring. Similarly, they have a common left divisor if α = dξ and β = dη for some choice of ξ and η in the ring. Since multiplication is not commutative, there are two versions of the Euclidean algorithm, one for right divisors and one for left divisors.[131][160] Choosing the right divisors, the first step in finding the gcd(α, β) by the Euclidean algorithm can be written
With either choice, the process is repeated as above until the greatest common right or left divisor is identified. As in the Euclidean domain, the "size" of the remainder ρ0 (formally, its Euclidean function or "norm") must be strictly smaller than β, and there must be only a finite number of possible sizes for ρ0, so that the algorithm is guaranteed to terminate.[161]
A programming language is a system of notation for writing computer programs.[1]
Programming languages are described in terms of their syntax (form) and semantics (meaning), usually defined by a formal language. Languages usually provide features such as a type system, variables, and mechanisms for error handling. An implementation of a programming language is required in order to execute programs, namely an interpreter or a compiler. An interpreter directly executes the source code, while a compiler produces an executable program.
Computer architecture has strongly influenced the design of programming languages, with the most common type (imperative languages—which implement operations in a specified order) developed to perform well on the popular von Neumann architecture. While early programming languages were closely tied to the hardware, over time they have developed more abstraction to hide implementation details for greater simplicity.
Thousands of programming languages—often classified as imperative, functional, logic, or object-oriented—have been developed for a wide variety of uses. Many aspects of programming language design involve tradeoffs—for example, exception handling simplifies error handling, but at a performance cost. Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.
Programming languages differ from natural languages in that natural languages are used for interaction between people, while programming languages are designed to allow humans to communicate instructions to machines.[citation needed]
The term computer language is sometimes used interchangeably with "programming language".[2] However, usage of these terms varies among authors.
In one usage, programming languages are described as a subset of computer languages.[3] Similarly, the term "computer language" may be used in contrast to the term "programming language" to describe languages used in computing but not considered programming languages[citation needed].  Most practical programming languages are Turing complete,[4] and as such are equivalent in what programs they can compute.
Another usage regards programming languages as theoretical constructs for programming abstract machines and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources.[5] John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.[6]
The first programmable computers were invented at the end of the 1940s, and with them, the first programming languages.[7] The earliest computers were programmed in first-generation programming languages (1GLs), machine language (simple instructions that could be directly executed by the processor). This code was very difficult to debug and was not portable between different computer systems.[8] In order to improve the ease of programming, assembly languages (or second-generation programming languages—2GLs) were invented, diverging from the machine language to make programs easier to understand for humans, although they did not increase portability.[9]
Initially, hardware resources were scarce and expensive, while human resources were cheaper. Therefore, cumbersome languages that were time-consuming to use, but were closer to the hardware for higher efficiency were favored.[10] The introduction of high-level programming languages (third-generation programming languages—3GLs)—revolutionized programming. These languages abstracted away the details of the hardware, instead being designed to express algorithms that could be understood more easily by humans. For example, arithmetic expressions could now be written in symbolic notation and later translated into machine code that the hardware could execute.[9] In 1957, Fortran (FORmula TRANslation) was invented. Often considered the first compiled high-level programming language,[9][11] Fortran has remained in use into the twenty-first century.[12]
Around 1960, the first mainframes—general purpose computers—were developed, although they could only be operated by professionals and the cost was extreme. The data and instructions were input by punch cards, meaning that no input could be added while the program was running. The languages developed at this time therefore are designed for minimal interaction.[14] After the invention of the microprocessor, computers in the 1970s became dramatically cheaper.[15] New computers also allowed more user interaction, which was supported by newer programming languages.[16]
Lisp, implemented in 1958, was the first functional programming language.[17] Unlike Fortran, it supported recursion and conditional expressions,[18] and it also introduced dynamic memory management on a heap and automatic garbage collection.[19] For the next decades, Lisp dominated artificial intelligence applications.[20] In 1978, another functional language, ML, introduced inferred types and polymorphic parameters.[16][21]
After ALGOL (ALGOrithmic Language) was released in 1958 and 1960,[22] it became the standard in computing literature for describing algorithms. Although its commercial success was limited, most popular imperative languages—including C, Pascal, Ada, C++, Java, and C#—are directly or indirectly descended from ALGOL 60.[23][12] Among its innovations adopted by later programming languages included greater portability and the first use of context-free, BNF grammar.[24] Simula, the first language to support object-oriented programming (including subtypes, dynamic dispatch, and inheritance), also descends from ALGOL and achieved commercial success.[25] C, another ALGOL descendant, has sustained popularity into the twenty-first century. C allows access to lower-level machine operations more than other contemporary languages. Its power and efficiency, generated in part with flexible pointer operations, comes at the cost of making it more difficult to write correct code.[16]
Prolog, designed in 1972, was the first logic programming language, communicating with a computer using formal logic notation.[26][27] With logic programming, the programmer specifies a desired result and allows the interpreter to decide how to achieve it.[28][27]
During the 1980s, the invention of the personal computer transformed the roles for which programming languages were used.[29] New languages introduced in the 1980s included C++, a superset of C that can compile C programs but also supports classes and inheritance.[30] Ada and other new languages introduced support for concurrency.[31] The Japanese government invested heavily into the so-called fifth-generation languages that added support for concurrency to logic programming constructs, but these languages were outperformed by other concurrency-supporting languages.[32][33]
Due to the rapid growth of the Internet and the World Wide Web in the 1990s, new programming languages were introduced to support Web pages and networking.[34] Java, based on C++ and designed for increased portability across systems and security, enjoyed large-scale success because these features are essential for many Internet applications.[35][36] Another development was that of dynamically typed scripting languages—Python, JavaScript, PHP, and Ruby—designed to quickly produce small programs that coordinate existing applications. Due to their integration with HTML, they have also been used for building web pages hosted on servers.[37][38]
During the 2000s, there was a slowdown in the development of new programming languages that achieved widespread popularity.[39] One innovation was service-oriented programming, designed to exploit distributed systems whose components are connected by a network. Services are similar to objects in object-oriented programming, but run on a separate process.[40] C# and F# cross-pollinated ideas between imperative and functional programming.[41] After 2010, several new languages—Rust, Go, Swift, Zig and Carbon —competed for the performance-critical software for which C had historically been used.[42] Most of the new programming languages use static typing while a few numbers of new languages use dynamic typing like Ring and Julia.[43][44]
Some of the new programming languages are classified as visual programming languages like Scratch, LabVIEW and PWCT. Also, some of these languages mix between textual and visual programming usage like Ballerina.[45][46][47][48] Also, this trend lead to developing projects that help in developing new VPLs like Blockly by Google.[49] Many game engines like Unreal and Unity added support for visual scripting too.[50][51]
Every programming language includes fundamental elements for describing data and the operations or transformations applied to them, such as adding two numbers or selecting an item from a collection. These elements are governed by syntactic and semantic rules that define their structure and meaning, respectively.
A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, some programming languages are graphical, using visual relationships between symbols to specify a program.
The programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars.[52] Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution.[53] In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.[54]
Static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms.[1][failed verification] For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct.[55] Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Programming languages such as Java and C# have definite assignment analysis, a form of data flow analysis, as part of their respective static semantics.[56]
Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research goes into formal semantics of programming languages, which allows execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.[56]
A data type is a set of allowable values and operations that can be performed on these values.[57] Each programming language's type system defines which data types exist, the type of an expression, and how type equivalence and type compatibility function in the language.[58]
Early programming languages often supported only built-in, numeric types such as the integer (signed and unsigned) and floating point (to support operations on real numbers that are not integers). Most programming languages support multiple sizes of floats (often called float and double) and integers depending on the size and precision required by the programmer. Storing an integer in a type that is too small to represent it leads to integer overflow. The most common way of representing negative numbers with signed types is twos complement, although ones complement is also used.[62] Other common types include Boolean—which is either true or false—and character—traditionally one byte, sufficient to represent all ASCII characters.[63]
Arrays are a data type whose elements, in many languages, must consist of a single type of fixed length. Other languages define arrays as references to data stored elsewhere and support elements of varying types.[64] Depending on the programming language, sequences of multiple characters, called strings, may be supported as arrays of characters or their own primitive type.[65] Strings may be of fixed or variable length, which enables greater flexibility at the cost of increased storage space and more complexity.[66] Other data types that may be supported include lists,[67] associative (unordered) arrays accessed via keys,[68] records in which data is mapped to names in an ordered structure,[69] and tuples—similar to records but without names for data fields.[70] Pointers store memory addresses, typically referencing locations on the heap where other data is stored.[71]
The simplest user-defined type is an ordinal type, often called an enumeration, whose values can be mapped onto the set of positive integers.[72] Since the mid-1980s, most programming languages also support abstract data types, in which the representation of the data and operations are hidden from the user, who can only access an interface.[73] The benefits of data abstraction can include increased reliability, reduced complexity, less potential for name collision, and allowing the underlying data structure to be changed without the client needing to alter its code.[74]
In static typing, all expressions have their types determined before a program executes, typically at compile-time.[59] Most widely used, statically typed programming languages require the types of variables to be specified explicitly. In some languages, types are implicit; one form of this is when the compiler can infer types based on context. The downside of implicit typing is the potential for errors to go undetected.[75] Complete type inference has traditionally been associated with functional languages such as Haskell and ML.[76]
With dynamic typing, the type is not attached to the variable but only the value encoded in it. A single variable can be reused for a value of a different type. Although this provides more flexibility to the programmer, it is at the cost of lower reliability and less ability for the programming language to check for errors.[77] Some languages allow variables of a union type to which any type of value can be assigned, in an exception to their usual static typing rules.[78]
In computing, multiple instructions can be executed simultaneously. Many programming languages support instruction-level and subprogram-level concurrency.[79] By the twenty-first century, additional processing power on computers was increasingly coming from the use of additional processors, which requires programmers to design software that makes use of multiple processors simultaneously to achieve improved performance.[80] Interpreted languages such as Python and Ruby do not support the concurrent use of multiple processors.[81] Other programming languages do support managing data shared between different threads by controlling the order of execution of key instructions via the use of semaphores, controlling access to shared data via monitor, or enabling message passing between threads.[82]
Many programming languages include exception handlers, a section of code triggered by runtime errors that can deal with them in two main ways:[83]
Some programming languages support dedicating a block of code to run regardless of whether an exception occurs before the code is reached; this is called finalization.[84]
One of the most important influences on programming language design has been computer architecture. Imperative languages, the most commonly used type, were designed to perform well on von Neumann architecture, the most common computer architecture.[89] In von Neumann architecture, the memory stores both data and instructions, while the CPU that performs instructions on data is separate, and data must be piped back and forth to the CPU. The central elements in these languages are variables, assignment, and iteration, which is more efficient than recursion on these machines.[90]
Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse.[citation needed] The birth of programming languages in the 1950s was stimulated by the desire to make a universal programming language suitable for all machines and uses, avoiding the need to write code for different computers.[91] By the early 1960s, the idea of a universal language was rejected due to the differing requirements of the variety of purposes for which code was written.[92]
Desirable qualities of programming languages include readability, writability, and reliability.[93] These features can reduce the cost of training programmers in a language, the amount of time needed to write and maintain programs in the language, the cost of compiling the code, and increase runtime performance.[94]
Programming language design often involves tradeoffs.[104] For example, features to improve reliability typically come at the cost of performance.[105] Increased expressivity due to a large number of operators makes writing code easier but comes at the cost of readability.[105]
Natural-language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs.[106] Alan Perlis was similarly dismissive of the idea.[107]
The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.
An implementation of a programming language is the conversion of a program into machine code that can be executed by the hardware. The machine code then can be executed with the help of the operating system.[111] The most common form of interpretation in production code is by a compiler, which translates the source code via an intermediate-level language into machine code, known as an executable. Once the program is compiled, it will run more quickly than with other implementation methods.[112] Some compilers are able to provide further optimization to reduce memory or computation usage when the executable runs, but increasing compilation time.[113]
Another implementation method is to run the program with an interpreter, which translates each line of software into machine code just before it executes. Although it can make debugging easier, the downside of interpretation is that it runs 10 to 100 times slower than a compiled executable.[114] Hybrid interpretation methods provide some of the benefits of compilation and some of the benefits of interpretation via partial compilation. One form this takes is just-in-time compilation, in which the software is compiled ahead of time into an intermediate language, and then into machine code immediately before execution.[115]
Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain-specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.[citation needed]
Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language,[116] and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.[117]
Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language. Some languages may make the transition from closed to open; for example, Erlang was originally Ericsson's internal programming language.[118]
Open source programming languages are particularly helpful for open science applications, enhancing the capacity for replication and code sharing.[119]
Thousands of different programming languages have been created, mainly in the computing field.[120]
Individual software projects commonly use five programming languages or more.[121]
Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers "do exactly what they are told to do", and cannot "understand" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.
A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives).[122] Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.
Determining which is the most widely used programming language is difficult since the definition of usage varies by context. One language may occupy the greater number of programmer hours, a different one has more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes;[124][125] Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time, and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.
Combining and averaging information from various internet sites, stackify.com reported the ten most popular programming languages (in descending order by overall popularity): Java, C, C++, Python, C#, JavaScript, VB .NET, R, PHP, and MATLAB.[129]
As of June 2024, the top five programming languages as measured by TIOBE index are Python, C++, C, Java and C#. TIOBE provide a list of top 100 programming languages according to popularity and update this list every month.[130]
A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate, or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC language has many dialects.
Programming languages are often placed into four main categories: imperative, functional, logic, and object oriented.[131]
Although markup languages are not programming languages, some have extensions that support limited programming. Additionally, there are special-purpose languages that are not easily compared to other programming languages.[135]
In computer science, empirical algorithmics (or experimental algorithmics) is the practice of using empirical methods to study the behavior of algorithms. The practice combines algorithm development and experimentation: algorithms are not just designed, but also implemented and tested in a variety of situations. In this process, an initial design of an algorithm is analyzed so that the algorithm may be developed in a stepwise manner.[1]
Methods from empirical algorithmics complement theoretical methods for the analysis of algorithms.[2] Through the principled application of empirical methods, particularly from statistics, it is often possible to obtain insights into the behavior of algorithms such as high-performance heuristic algorithms for hard combinatorial problems that are (currently) inaccessible to theoretical analysis.[3] Empirical methods can also be used to achieve substantial improvements in algorithmic efficiency.[4]
American computer scientist Catherine McGeoch identifies two main branches of empirical algorithmics: the first (known as empirical analysis) deals with the analysis and characterization of the behavior of algorithms, and the second (known as algorithm design or algorithm engineering) is focused on empirical methods for improving the performance of algorithms.[5] The former often relies on techniques and tools from statistics, while the latter is based on approaches from statistics, machine learning and optimization. Dynamic analysis tools, typically performance profilers, are commonly used when applying empirical methods for the selection and refinement of algorithms of various types for use in various contexts.[6][7][8]
Research in empirical algorithmics is published in several journals, including the ACM Journal on Experimental Algorithmics (JEA) and the Journal of Artificial Intelligence Research (JAIR). Besides Catherine McGeoch, well-known researchers in empirical algorithmics include Bernard Moret, Giuseppe F. Italiano, Holger H. Hoos, David S. Johnson, and Roberto Battiti.[9]
In the absence of empirical algorithmics, analyzing the complexity of an algorithm can involve various theoretical methods applicable to various situations in which the algorithm may be used.[10] Memory and cache considerations are often significant factors to be considered in the theoretical choice of a complex algorithm, or the approach to its optimization, for a given purpose.[11][12] Performance profiling is a dynamic program analysis technique typically used for finding and analyzing bottlenecks in an entire application's code[13][14][15] or for analyzing an entire application to identify poorly performing code.[16] A profiler can reveal the code most relevant to an application's performance issues.[17]
A profiler may help to determine when to choose one algorithm over another in a particular situation.[18] When an individual algorithm is profiled, as with complexity analysis, memory and cache considerations are often more significant than instruction counts or clock cycles; however, the profiler's findings can be considered in light of how the algorithm accesses data rather than the number of instructions it uses.[19]
Profiling may provide intuitive insight into an algorithm's behavior[20] by revealing performance findings as a visual representation.[21] Performance profiling has been applied, for example, during the development of algorithms for matching wildcards. Early algorithms for matching wildcards, such as Rich Salz' wildmat algorithm,[22] typically relied on recursion, a technique criticized on grounds of performance.[23] The Krauss matching wildcards algorithm was developed based on an attempt to formulate a non-recursive alternative using test cases[24] followed by optimizations suggested via performance profiling,[25] resulting in a new algorithmic strategy conceived in light of the profiling along with other considerations.[26] Profilers that collect data at the level of basic blocks[27] or that rely on hardware assistance[28] provide results that can be accurate enough to assist software developers in optimizing algorithms for a particular computer or situation.[29] Performance profiling can aid developer understanding of the characteristics of complex algorithms applied in complex situations, such as coevolutionary algorithms applied to arbitrary test-based problems, and may help lead to design improvements.[30]
In software engineering, profiling ("program profiling", "software profiling") is a form of dynamic program analysis that measures, for example, the space (memory) or time complexity of a program, the usage of particular instructions, or the frequency and duration of function calls.  Most commonly, profiling information serves to aid program optimization, and more specifically, performance engineering.
Profilers use a wide variety of techniques to collect data, including hardware interrupts, code instrumentation, instruction set simulation, operating system hooks, and performance counters.
Program analysis tools are extremely important for understanding program behavior. Computer architects need such tools to evaluate how well programs will perform on new architectures. Software writers need tools to analyze their programs and identify critical sections of code. Compiler writers often use such tools to find out how well their instruction scheduling or branch prediction algorithm is performing...
A profiler can be applied to an individual method or at the scale of a module or program, to identify performance bottlenecks by making long-running code obvious.[1] A profiler can be used to understand code from a timing point of view, with the objective of optimizing it to handle various runtime conditions[2] or various loads.[3] Profiling results can be ingested by a compiler that provides profile-guided optimization.[4] Profiling results can be used to guide the design and optimization of an individual algorithm; the Krauss matching wildcards algorithm is an example.[5] Profilers are built into some application performance management systems that aggregate profiling data to provide insight into transaction workloads in distributed applications.[6]
Some profilers operate by sampling. A sampling profiler probes the target program's call stack at regular intervals using operating system interrupts. Sampling profiles are typically less numerically accurate and specific, but allow the target program to run at near full speed.
Some commonly used[17] statistical profilers for Java/managed code are SmartBear Software's AQtime[18] and Microsoft's CLR Profiler.[19] Those profilers also support native code profiling, along with Apple Inc.'s Shark (OSX),[20] OProfile (Linux),[21] Intel VTune and Parallel Amplifier (part of Intel Parallel Studio), and Oracle Performance Analyzer,[22] among others.
In computer science, divide and conquer is an algorithm design paradigm. A divide-and-conquer algorithm recursively breaks down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.
The divide-and-conquer technique is the basis of efficient algorithms for many problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g., the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFT).[1]
Designing efficient divide-and-conquer algorithms can be difficult. As in mathematical induction, it is often necessary to generalize the problem to make it amenable to a recursive solution. The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.
The divide-and-conquer paradigm is often used to find an optimal solution of a problem. Its basic idea is to decompose a given problem into two or more similar, but simpler, subproblems, to solve them in turn, and to compose their solutions to solve the given problem. Problems of sufficient simplicity are solved directly. 
For example, to sort a given list of n natural numbers, split it into two lists of about n/2 numbers each, sort each of them in turn, and interleave both results appropriately to obtain the sorted version of the given list (see the picture). This approach is known as the merge sort algorithm.
The name "divide and conquer" is sometimes applied to algorithms that reduce each problem to only one sub-problem, such as the binary search algorithm for finding a record in a sorted list (or its analogue in numerical computing, the bisection algorithm for root finding).[2]  These algorithms can be implemented more efficiently than general divide-and-conquer algorithms; in particular, if they use tail recursion, they can be converted into simple loops.  Under this broad definition, however, every algorithm that uses recursion or loops could be regarded as a "divide-and-conquer algorithm".  Therefore, some authors consider that the name "divide and conquer" should be used only when each problem may generate two or more subproblems.[3] The name decrease and conquer has been proposed instead for the single-subproblem class.[4]
An important application of divide and conquer is in optimization,[example needed] where if the search space is reduced ("pruned") by a constant factor at each step, the overall algorithm has the same asymptotic complexity as the pruning step, with the constant depending on the pruning factor (by summing the geometric series); this is known as prune and search.
Early examples of these algorithms are primarily decrease and conquer – the original problem is successively broken down into single subproblems, and indeed can be solved iteratively.
Binary search, a decrease-and-conquer algorithm where the subproblems are of roughly half the original size, has a long history. While a clear description of the algorithm on computers appeared in 1946 in an article by John Mauchly, the idea of using a sorted list of items to facilitate searching dates back at least as far as Babylonia in 200 BC.[5] Another ancient decrease-and-conquer algorithm is the Euclidean algorithm to compute the greatest common divisor of two numbers by reducing the numbers to smaller and smaller equivalent subproblems, which dates to several centuries BC.
An early example of a divide-and-conquer algorithm with multiple subproblems is Gauss's 1805 description of what is now called the Cooley–Tukey fast Fourier transform (FFT) algorithm,[6] although he did not analyze its operation count quantitatively, and FFTs did not become widespread until they were rediscovered over a century later.
An early two-subproblem D&C algorithm that was specifically developed for computers and properly analyzed is the merge sort algorithm, invented by John von Neumann in 1945.[7]
Another notable example is the algorithm invented by Anatolii A. Karatsuba in 1960[8] that could multiply two n-digit numbers in 



O
(

n


log

2


⁡
3


)


{\displaystyle O(n^{\log _{2}3})}

 operations (in Big O notation). This algorithm disproved Andrey Kolmogorov's 1956 conjecture that 



Ω
(

n

2


)


{\displaystyle \Omega (n^{2})}

 operations would be required for that task.
As another example of a divide-and-conquer algorithm that did not originally involve computers, Donald Knuth gives the method a post office typically uses to route mail: letters are sorted into separate bags for different geographical areas, each of these bags is itself sorted into batches for smaller sub-regions, and so on until they are delivered.[5] This is related to a radix sort, described for punch-card sorting machines as early as 1929.[5]
The divide-and-conquer paradigm often helps in the discovery of efficient algorithms.  It was the key, for example, to Karatsuba's fast multiplication method, the quicksort and mergesort algorithms, the Strassen algorithm for matrix multiplication, and fast Fourier transforms.
In all these examples, the D&C approach led to an improvement in the asymptotic cost of the solution. For example, if (a) the base cases have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size 



n


{\displaystyle n}

, and (b) there is a bounded number 



p


{\displaystyle p}

 of sub-problems of size ~ 





n
p




{\displaystyle {\frac {n}{p}}}

 at each stage, then the cost of the divide-and-conquer algorithm will be 



O
(
n

log

p


⁡
n
)


{\displaystyle O(n\log _{p}n)}

.
For other types of divide-and-conquer approaches, running times can also be generalized. For example, when a) the work of splitting the problem and combining the partial solutions take 



c
n


{\displaystyle cn}

 time, where 



n


{\displaystyle n}

 is the input size and 



c


{\displaystyle c}

 is some constant; b) when 



n
<
2


{\displaystyle n<2}

, the algorithm takes time upper-bounded by 



c


{\displaystyle c}

, and c) there are 



q


{\displaystyle q}

 subproblems where each subproblem has size ~ 





n
2




{\displaystyle {\frac {n}{2}}}

. Then, the running times are as follows:
If, instead, the work of splitting the problem and combining the partial solutions take 



c

n

2




{\displaystyle cn^{2}}

 time, and there are 2 subproblems where each has size 





n
2




{\displaystyle {\frac {n}{2}}}

, then the running time of the divide-and-conquer algorithm is bounded by 



O
(

n

2


)


{\displaystyle O(n^{2})}

.[9]
Divide-and-conquer algorithms are naturally adapted for execution in multi-processor machines, especially shared-memory systems where the communication of data between processors does not need to be planned in advance because distinct sub-problems can be executed on different processors.
Divide-and-conquer algorithms naturally tend to make efficient use of memory caches. The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory. An algorithm designed to exploit the cache in this way is called cache-oblivious, because it does not contain the cache size as an explicit parameter.[10] Moreover, D&C algorithms can be designed for important algorithms (e.g., sorting, FFTs, and matrix multiplication) to be optimal cache-oblivious algorithms–they use the cache in a probably optimal way, in an asymptotic sense, regardless of the cache size. In contrast, the traditional approach to exploiting the cache is blocking, as in loop nest optimization, where the problem is explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache sizes of a particular machine.
In computations with rounded arithmetic, e.g. with floating-point numbers, a divide-and-conquer algorithm may yield more accurate results than a superficially equivalent iterative method. For example, one can add N numbers either by a simple loop that adds each datum to a single variable, or by a D&C algorithm called pairwise summation that breaks the data set into two halves, recursively computes the sum of each half, and then adds the two sums.  While the second method performs the same number of additions as the first and pays the overhead of the recursive calls, it is usually more accurate.[11]
Divide-and-conquer algorithms are naturally implemented as recursive procedures. In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the procedure call stack. A recursive function is a function that calls itself within its definition.
Divide-and-conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a stack, queue, or priority queue.  This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications — e.g. in breadth-first recursion and the branch-and-bound method for function optimization. This approach is also the standard solution in programming languages that do not provide support for recursive procedures.
In recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise, the execution may fail because of stack overflow.  D&C algorithms that are time-efficient often have relatively small recursion depth.  For example, the quicksort algorithm can be implemented so that it never requires more than 




log

2


⁡
n


{\displaystyle \log _{2}n}

 nested recursive calls to sort 



n


{\displaystyle n}

 items.
In any recursive algorithm, there is considerable freedom in the choice of the base cases, the small subproblems that are solved directly in order to terminate the recursion.
Choosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve. For example, a Fast Fourier Transform algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples, there is only one base case to consider, and it requires no processing.
On the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a hybrid algorithm. This strategy avoids the overhead of recursive calls that do little or no work and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion. A general procedure for a simple hybrid recursive algorithm is short-circuiting the base case, also known as arm's-length recursion. In this case, whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call. For example, in a tree, rather than recursing to a child node and then checking whether it is null, checking null before recursing; avoids half the function calls in some algorithms on binary trees. Since a D&C algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low. Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.
Thus, for example, many library implementations of quicksort will switch to a simple loop-based insertion sort (or similar) algorithm once the number of items to be sorted is sufficiently small. Note that, if the empty list were the only base case, sorting a list with 



n


{\displaystyle n}

 entries would entail maximally 



n


{\displaystyle n}

 quicksort calls that would do nothing but return immediately.  Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.
Alternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely unrolled into code that has no recursion, loops, or conditionals (related to the technique of partial evaluation).  For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes.[12] Source-code generation methods may be used to produce the large number of separate base cases desirable to implement this strategy efficiently.[12]
For some problems, the branched recursion may end up evaluating the same sub-problem many times over.  In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique which is commonly known as memoization.  Followed to the limit, it leads to bottom-up divide-and-conquer algorithms such as dynamic programming.
Dynamic programming is both a mathematical optimization method and an algorithmic paradigm. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.
In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.
If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems.[1] In the optimization literature this relationship is called the Bellman equation.
In terms of mathematical optimization, dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time.
The solution to this problem is an optimal control law or policy 





u


∗


=
h
(

x

(
t
)
,
t
)


{\displaystyle \mathbf {u} ^{\ast }=h(\mathbf {x} (t),t)}

, which produces an optimal trajectory 





x


∗




{\displaystyle \mathbf {x} ^{\ast }}

 and a cost-to-go function 




J

∗




{\displaystyle J^{\ast }}

. The latter obeys the fundamental equation of dynamic programming:
The dynamic programming approach to solve this problem involves breaking it apart into a sequence of smaller decisions. To do so, we define a sequence of value functions 




V

t


(
k
)


{\displaystyle V_{t}(k)}

, for 



t
=
0
,
1
,
2
,
…
,
T
,
T
+
1


{\displaystyle t=0,1,2,\ldots ,T,T+1}

 which represent the value of having any amount of capital k at each time t. There is (by assumption) no utility from having capital after death, 




V

T
+
1


(
k
)
=
0


{\displaystyle V_{T+1}(k)=0}

.
There are two key attributes that a problem must have in order for dynamic programming to be applicable: optimal substructure and overlapping sub-problems. If a problem can be solved by combining optimal solutions to non-overlapping sub-problems, the strategy is called "divide and conquer" instead.[1]  This is why merge sort and quick sort are not classified as dynamic programming problems.
Optimal substructure means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems. Such optimal substructures are usually described by means of recursion. For example, given a graph G=(V,E), the shortest path p from a vertex u to a vertex v exhibits optimal substructure: take any intermediate vertex w on this shortest path p. If p is truly the shortest path, then it can be split into sub-paths p1 from u to w and p2 from w to v such that these, in turn, are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in Introduction to Algorithms). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the Bellman–Ford algorithm or the Floyd–Warshall algorithm does.
Overlapping sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems. For example, consider the recursive formulation for generating the Fibonacci sequence: Fi = Fi−1 + Fi−2, with base case F1 = F2 = 1. Then F43 = F42 + F41, and F42 = F41 + F40. Now F41 is being solved in the recursive sub-trees of both F43 as well as F42. Even though the total number of sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once.
Some programming languages can automatically memoize the result of a function call with a particular set of arguments, in order to speed up call-by-name evaluation (this mechanism is referred to as call-by-need). Some languages make it possible portably (e.g. Scheme, Common Lisp, Perl or D). Some languages have automatic memoization  built in, such as tabled Prolog and J, which supports memoization with the M. adverb.[5] In any case, this is only possible for a referentially transparent function. Memoization is also encountered as an easily accessible design pattern within term-rewrite based languages such as Wolfram Language.
Dynamic programming is widely used in bioinformatics for tasks such as sequence alignment, protein folding, RNA structure prediction and protein-DNA binding. The first dynamic programming algorithms for protein-DNA binding were developed in the 1970s independently by Charles DeLisi in the US[6] and by Georgii Gurskii and Alexander Zasedatelev in the Soviet Union.[7] Recently these algorithms have become very popular in bioinformatics and computational biology, particularly in the studies of nucleosome positioning and transcription factor binding.
From a dynamic programming point of view, Dijkstra's algorithm for the shortest path problem  is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.[8][9][10]
Using dynamic programming in the calculation of the nth member of the Fibonacci sequence improves its performance greatly. Here is a naïve implementation, based directly on the mathematical definition:
In particular, fib(2) was calculated three times from scratch. In larger examples, many more values of fib, or subproblems, are recalculated, leading to an exponential time algorithm.
Dynamic programming makes it possible to count the number of solutions without visiting them all. Imagine backtracking values for the first row – what information would we require about the remaining rows, in order to be able to accurately count the solutions obtained for each first row value? We consider k × n boards, where 1 ≤ k ≤ n, whose 



k


{\displaystyle k}

 rows contain 



n

/

2


{\displaystyle n/2}

 zeros and 



n

/

2


{\displaystyle n/2}

 ones. The function f to which memoization is applied maps vectors of n pairs of integers to the number of admissible boards (solutions). There is one pair for each column, and its two components indicate respectively the number of zeros and ones that have yet to be placed in that column. We seek the value of 



f
(
(
n

/

2
,
n

/

2
)
,
(
n

/

2
,
n

/

2
)
,
…
(
n

/

2
,
n

/

2
)
)


{\displaystyle f((n/2,n/2),(n/2,n/2),\ldots (n/2,n/2))}

 (



n


{\displaystyle n}

 arguments or one vector of 



n


{\displaystyle n}

 elements). The process of subproblem creation involves iterating over every one of  







(


n

n

/

2



)






{\displaystyle {\tbinom {n}{n/2}}}

 possible assignments for the top row of the board, and going through every column, subtracting one from the appropriate element of the pair for that column, depending on whether the assignment for the top row contained a zero or a one at that position. If any one of the results is negative, then the assignment is invalid and does not contribute to the set of solutions (recursion stops). Otherwise, we have an assignment for the top row of the k × n board and recursively compute the number of solutions to the remaining (k − 1) × n board, adding the numbers of solutions for every admissible assignment of the top row and returning the sum, which is being memoized. The base case is the trivial subproblem, which occurs for a 1 × n board. The number of solutions for this board is either zero or one, depending on whether the vector is a permutation of n / 2 



(
0
,
1
)


{\displaystyle (0,1)}

 and n / 2 



(
1
,
0
)


{\displaystyle (1,0)}

 pairs or not.
Links to the MAPLE implementation of the dynamic programming approach may be found among the external links.
In genetics, sequence alignment is an important application where dynamic programming is essential.[12]  Typically, the problem consists of transforming one sequence into another using edit operations that replace, insert, or remove an element.  Each operation has an associated cost, and the goal is to find the sequence of edits with the lowest total cost.
The  number of moves required by this solution is 2n − 1. If the objective is to maximize the number of moves (without cycling) then the dynamic programming functional equation  is slightly more complicated and  3n − 1 moves are required.[13]
To derive a dynamic programming functional equation for this puzzle, let the state of the dynamic programming model be a pair s = (n,k), where
Thus, if we separately handle the case of 



n
=
1


{\displaystyle n=1}

, the algorithm would take 



O
(
n


k


)


{\displaystyle O(n{\sqrt {k}})}

 time.
Since 



f
(
t
,
n
)
≤
f
(
t
+
1
,
n
)


{\displaystyle f(t,n)\leq f(t+1,n)}

 for all 



t
≥
0


{\displaystyle t\geq 0}

, we can binary search on 



t


{\displaystyle t}

 to find 



x


{\displaystyle x}

, giving an 



O
(
n
log
⁡
k
)


{\displaystyle O(n\log k)}

 algorithm.[16]
Matrix chain multiplication is a well-known example that demonstrates utility of dynamic programming. For example,  engineering applications often have to multiply a chain of matrices. It is not surprising to find matrices of large dimensions, for example 100×100. Therefore, our task is to multiply matrices ⁠




A

1


,

A

2


,
.
.
.
.

A

n




{\displaystyle A_{1},A_{2},....A_{n}}

⁠. Matrix multiplication is not commutative, but is associative; and we can multiply only two matrices at a time. So, we can multiply this chain of matrices in many different ways, for example:
and so on. There are numerous ways to multiply this chain of matrices. They will all produce the same final result, however they will take more or less time to compute, based on which particular matrices are multiplied. If matrix A has dimensions m×n and matrix B has dimensions n×q, then matrix C=A×B will have dimensions m×q, and will require m*n*q scalar multiplications (using a simplistic matrix multiplication algorithm for purposes of illustration).
At this point, we have several choices, one of which is to design a dynamic programming algorithm that will split the problem into overlapping problems and calculate the optimal arrangement of parenthesis. The dynamic programming solution is presented below.
This algorithm will produce "tables" m[, ] and s[, ] that will have entries for all possible values of i and j. The final solution for the entire chain is m[1, n], with corresponding split at s[1, n]. Unraveling the solution will be recursive, starting from the top and continuing until we reach the base case, i.e. multiplication of single matrices.
Therefore, the next step is to actually split the chain, i.e. to place the parenthesis where they (optimally) belong. For this purpose we could use the following algorithm:
Of course, this algorithm is not useful for actual multiplication. This algorithm is just a user-friendly way to see what the result looks like.
The term dynamic programming was originally used in the 1940s by Richard Bellman to describe the process of solving problems where one needs to find the best decisions one after another.  By 1953, he refined this to the modern meaning, referring specifically to nesting smaller decision problems inside larger decisions,[17]  and the field was thereafter recognized by the IEEE as a systems analysis and engineering topic.  Bellman's contribution is remembered in the name of the Bellman equation, a central result of dynamic programming which restates an optimization problem in recursive form.
Bellman explains the reasoning behind the term dynamic programming in his autobiography, Eye of the Hurricane: An Autobiography:
I spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. An interesting question is, "Where did the name, dynamic programming, come from?" The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word "research". I'm not using the term lightly; I'm using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term research in his presence. You can imagine how he felt, then, about the term mathematical. The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word "programming". I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let's kill two birds with one stone. Let's take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it's impossible to use the word dynamic in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It's impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.
The word dynamic was chosen by Bellman to capture the time-varying aspect of the problems, and because it sounded impressive.[12] The word programming referred to the use of the method to find an optimal program, in the sense of a military schedule for training or logistics.  This usage is the same as that in the phrases linear programming and mathematical programming, a synonym for mathematical optimization.[18]
The above explanation of the origin of the term may be inaccurate: According to Russell and Norvig, the above story "cannot be strictly true, because his first paper using the term (Bellman, 1952) appeared before Wilson became Secretary of Defense in 1953."[19] Also, Harold J. Kushner stated in a speech that, "On the other hand, when I asked [Bellman] the same question, he replied that he was trying to upstage Dantzig's linear programming by adding dynamic. Perhaps both motivations were true."[20]
A software patent is a patent on a piece of software, such as a computer program, library, user interface, or algorithm. The validity of these patents can be difficult to evaluate, as software is often at once a product of engineering, something typically eligible for patents, and an abstract concept, which is typically not. This gray area, along with the difficulty of patent evaluation for intangible, technical works such as libraries and algorithms, makes software patents a frequent subject of controversy and litigation.
Different jurisdictions have radically different policies concerning software patents, including a blanket ban, no restrictions, or attempts to distinguish between purely mathematical constructs and "embodiments" of these constructs. For example, an algorithm itself may be judged unpatentable, but its use in software judged patentable.
These different countries and regional offices have different standards for granting patents. This is particularly true of software or computer-implemented inventions, especially where the software is implementing a business method.
On 21 May 1962, a British patent application entitled "A Computer Arranged for the Automatic Solution of Linear Programming Problems" was filed.[1] The invention was concerned with efficient memory management for the simplex algorithm, and could be implemented by purely software means. The patent struggled to establish that it represented a 'vendible product'. "The focus of attention shifted to look at the relationship between the [unpatentable] computer program and the [potentially patentable] programmed computer".[2] The patent was granted on August 17, 1966, and seems to be one of the first software patents, establishing the principle that the computer program itself was unpatentable and therefore covered by copyright law, while the computer program embedded in hardware was potentially patentable.[3]
Most countries place some limits on the patenting of inventions involving software, but there is no one legal definition of a software patent. For example, U.S. patent law excludes "abstract ideas", and this has been used to refuse some patents involving software. In Europe, "computer programs as such" are excluded from patentability, thus European Patent Office policy is consequently that a program for a computer is not patentable if it does not have the potential to cause a "technical effect" which is by now understood as a material effect (a "transformation of nature").[4] Substantive law regarding the patentability of software and computer-implemented inventions, and case law interpreting the legal provisions, are different under different jurisdictions.
In Australia, there is no particular exclusion for patents relating to software. The subject matter of an invention is patentable in Australia, if it is a manner of manufacture within the meaning of section 6 of the Statute of Monopolies.[5] The High Court of Australia has refrained from ruling on the precise definition of manner of manufacture stating that any such attempt is bound to fail for the policy reason of encouraging national development in fields that may be unpredictable.[6] In assessing whether an invention is a manner of manufacture, the High Court has relied on the inquiry of whether the subject of the claims defining the invention has as its end result an artificially created state of affairs.[6]
In principle, computer software is still a valid patentable subject matter in Australia. But, in circumstances where patents have been sought over software to merely implement abstract ideas or business methods, the courts and the Commissioner of Patents have resisted granting patent protection to such applications both as a matter of statutory interpretation and policy.
In China, the starting time of software patent is relatively late. Before 2006, software patents were basically not granted, and software and hardware had to be combined when applying for a patent. With the development of network technology and software technology, China's patent examination system has been constantly updated. Recently, the design idea of the software itself has been allowed to apply for patent separately, instead of requiring to be combined with hardware. However, software patent writing requirements are relatively high.
Software patents can be written as either a product or a method, depending on the standards of review. However, no matter what form it is written in, it is difficult to highlight the creativity of the scheme, which requires specific case analysis.
(2) Software to improve the internal performance of the computer, such as a software can improve the virtual memory of the computer;
The patent protection measures can be seen in the patent law and the regulations on the protection of computer software.
Within European Union member states, the EPO and other national patent offices have issued many patents for inventions involving software since the European Patent Convention (EPC) came into force in the late 1970s. Article 52 EPC excludes "programs for computers" from patentability (Art. 52(2)) to the extent that a patent application relates to a computer program "as such" (Art. 52(3)). This has been interpreted to mean that any invention that makes a non-obvious "technical contribution" or solves a "technical problem" in a non-obvious way is patentable even if that technical problem is solved by running a computer program.[12] When the EPO examines a patent application with questionable subject matter eligibility, their approach is to simply[dubious – discuss] disregard any ineligible portions or aspects and evaluate the rest.[13] This is notably different from the U.S. approach (see below).
Concerns have been raised by free software campaigners, such as the Free Software Foundation, that the Unified Patent Court will be much more open to patents generally and software patents in particular.[14]
In April 2013, the German Parliament adopted a joint motion "against the growing trend of patent offices to grant patents on software programs".[15]
In India, a clause to include software patents was quashed by the Indian Parliament in April 2005.[16] However, following publication of the new guidelines on the examination of computer-related inventions on 19 February 2016, the Office of the Controller General of Patents, Designs and Trade marks accepts applications for software patents, as long as the software is claimed in conjunction with a novel hardware.[17] On 30 June 2017, revised guidelines on the examination of computer related inventions were published. This 2017 guidelines provides clarity on patentability of software invention in India, i.e., the claimed computer-related invention needs to be ascertained whether it is of a technical nature involving technical advancement as compared to the existing knowledge or having economic significance or both, and is not subject to exclusion under Section 3 of the Patents Act.[18]
In 2019, the Court observed,
In today’s digital world, when most inventions are based on computer programs, it would be retrograde to argue that all such inventions would not be patentable. Innovation in the field of artificial intelligence, blockchain technologies and other digital products would be based on computer programs, however the same would not become nonpatentable inventions – simply for that reason. It is rare to see a product which is not based on a computer program. Whether they are cars and other automobiles, microwave ovens, washing machines, refrigerators, they all have some sort of computer programs in-built in them. Thus, the effect that such programs produce including in digital and electronic products is crucial in determining the test of patentability.
Software-related inventions are patentable. To qualify as an invention, however, there must be "a creation of technical ideas utilizing a law of nature"[21] although this requirement is typically met by "concretely realising the information processing performed by the software by using hardware resources".[22] Software-related inventions may be considered obvious if they involve the application of an operation known in other fields, the addition of a commonly known means or replacement by equivalent, the implementation in software of functions which were previously performed by hardware, or the systematisation of known human transactions.[23]
In New Zealand computer programs are excluded from patentability under the Patents Act 2013,[25] but guidelines permitting embedded software were added since the initial Patents Bill.[26] From 2013 computer programs 'as such' are excluded from patentability. The as such wording rules out only those software based patents where novelty lies solely in the software. Similar to Europe.[27]
In South Korea, software is considered patentable and many patents directed towards "computer programs" have been issued.[30] In 2006, Microsoft's sales of its "Office" suite were jeopardized due to a possible patent infringement.[31] A ruling by the Supreme Court of Korea found that patents directed towards automatic language translation within software programs were valid and possibly violated by its software.[31]
As like as 52(2) of the European Patent Convention (EPC), section 9 of the Thai Patent Act 1999 states that Thai patent law does not include software (or computer program) from patentability because the computer software is not considered as an “invention”, in which it is not the idea of the product itself. Hence, the software is considered as the manual or instruction that was controlled by users to perform the tasks.[32]
A software patents law in Thailand has been controversial debates among the economists and national developers’ overtime since there were two significant developments in the international patent law; (1) the European Union's attempt to harmonize national patent laws by the Proposal for a Directive of the European Parliament and Council on the patentability of computer-implemented inventions,[33] and (2) the US court decision to expand patent protection to business methods.[33] The opinions are divided into two sides. Dr. Tangkitvanich, the IT specialist of Thailand Development Research Institute (TDRI), raised his concern that Thailand is not in a good stage for a software patent as there were several flaws in patent rights. For example, the business method prevention has high tendency to hinder the growth in innovations especially for the infant software companies.[34] Moreover, the software patent may cause monopoly and innovation problems. “Monopoly will thwart innovations of new software products, particularly open-source software”, said by a group of Thai Economists. However, Dr. Hirapruk who is the Director of Software Park Thailand, on the other hand, provides his support on allowing the computer programs to be patentable: “Thailand had to provide a patent-right protection for computer software to ensure foreign high-tech investors that software producers' creativity would be secured from violations in Thailand”. As a result, Mr. Sribhibhadh, president of the Association of Thai Software Industry, emphasized that there will need to be a clear overview of the impact on the local industry if Thailand really had to fully implement the patent right protections.
The first software patent was issued June 19, 1968 to Martin Goetz for a data sorting algorithm.[36] The United States Patent and Trademark Office has granted patents that may be referred to as software patents since at least the early 1970s.[37] In Gottschalk v. Benson (1972), the United States Supreme Court ruled that a patent for a process should not be allowed if it would "wholly pre-empt the mathematical formula and in practical effect would be a patent on the algorithm itself", adding that "it is said that the decision precludes a patent for any program servicing a computer. We do not so hold."[38] In 1981, the Supreme Court stated that "a claim drawn to subject matter otherwise statutory does not become nonstatutory simply because it uses a mathematical formula, computer program, or digital computer" and a claim is patentable if it contains "a mathematical formula [and] implements or applies the formula in a structure or process which, when considered as a whole, is performing a function which the patent laws were designed to protect".[39] When a patent application is examined by the USPTO, the initial threshold question (for each claim) is whether the subject matter is eligible, so this is evaluated separately and prior to the other patentability criteria (novelty, nonobviousness).[40] This is notably different than the European approach (see above).
Due to different treatment of federal patent rights in different parts of the country, in 1982 the U.S. Congress created a new court (the Federal Circuit) to hear patent cases. Following several landmark decisions by this court, by the early 1990s the patentability of software was well established, and in 1996 the USPTO issued Final Computer Related Examination Guidelines stating that "A practical application of a computer-related invention is statutory subject matter. This requirement can be discerned from the variously phrased prohibitions against the patenting of abstract ideas, laws of nature or natural phenomena" (emphasis added).[41]
The emergence of the Internet and e-commerce led to many patents being applied for and being granted for business methods implemented in software and the question of whether business methods are statutory subject matter is a separate issue from the question of whether software is. Critics of the Federal Circuit believe that the non-obviousness standard is partly responsible for the large increase in patents for software and business methods.[42] There have been several successful enforcement trials in the United States, some of which are listed in the list of software patents article.
An issue with software patent intellectual property rights is typically revolved around deciding whether the company or inventor owns it.
A work for hire created after 1978 has copyright protection for 120 years from its creation date or 90 years from its publication date whichever comes first.[45] Patent protection for software lasts 20 years.[46]
In Indonesia, software cannot be protected by patents, until the implementation of the Law No. 13 Year 2016, Patent Law in Indonesia.[47] To begin evaluation, it is necessary to distinguish whether or not the application is considered an invention. Under Law No. 14 Year 2001, Article 1 of Patent Law in Indonesia,[48] application is considered as an invention if the activity is created to solve a particular conflict or problem in the technology sector. Furthermore, it can be executed in the medium of a new process or product or a developmental enhancement in a product or process. According to Law No. 14 Year 2001, Article 7 of Patent Law in Indonesia.,[48] an application can not be patented as an invention if the product or process contradicts or challenges the current regulations and rules, public order or ethics, and religious morality. In addition, if the application is treated as a method or theory in the scientific or mathematics, argued to be any type of living creatures, with the exception of micro-organisms, or is considered as an essential biological measure to produce plants or animals, the application is not a patentable invention.[48]
As software contains algorithms, it is deemed to be part of the field of mathematics; hence, software cannot be protected by patents in Indonesia. However, one way for the Indonesian Intellectual Property office to grant software patents in Indonesia is if the application has been patented in other nations, which have ratified the Patent Corporation Treaty (PCT). Therefore, in accordance to the regulations under the Patent Cooperation Treaty, a software will have a regional protection among the participating entities of World Intellectual Property Organization (WIPO).[49]
An important update was enacted on 26 August 2016, the Law No. 13 Year 2016, Patent Law in Indonesia.[47] This update is geared to encourage innovation and growth by augmenting the number of patents within the public and private sector in Indonesia. This update proposes an extension of protection for simple patent, which grants application for patents for new improvements or inventions to existing processes. Intangible inventions can also be patented; under the former law, simple patent is restricted for tangible inventions, which has a positive implication for software patents in Indonesia. Furthermore, these changes provide more protection to the pharmaceutical industry and encourage public access to medical knowledge. This can boost new software ideas and processes within the healthcare and pharmaceutical sector. This update provides a stronger protection of traditional knowledge. In addition, a significant update is the usability of electronic filling and electronic media. Under this new law, application can be made electronically.
In seeking to find a balance, different countries have different policies as to where the boundary between patentable and non-patentable software should lie. In Europe, a number of different proposals for setting a boundary line were put forward during the debate concerning the proposed Directive on the patentability of computer-implemented inventions, none of which were found acceptable by the various parties to the debate. Two particular suggestions for a hurdle that software must pass to be patentable include:
In the US, Ben Klemens, a Guest Scholar at the Brookings Institution, proposed that patents should be granted only to inventions that include a physical component that is by itself nonobvious.[54] This is based on Justice William Rehnquist's ruling in the U.S. Supreme Court case of Diamond v. Diehr that stated that "... insignificant postsolution activity will not transform an unpatentable principle into a patentable process."[55] By this rule, one would consider software loaded onto a stock PC to be an abstract algorithm with obvious postsolution activity, while a new circuit design implementing the logic would likely be a nonobvious physical device. Upholding an "insignificant postsolution activity" rule as per Justice Rehnquist's ruling would also eliminate most business method patents.
A common objection to software patents is that they relate to trivial inventions.[56] A patent on an invention that many people would easily develop independently of one another should not, it is argued, be granted since this impedes development.
Different countries have different ways of dealing with the question of inventive step and non-obviousness in relation to software patents. Europe uses an 'Inventive step test'; see the Inventive step requirement in Europe and, for instance, T 258/03.
There are a number of high-profile examples where the patenting of a data exchange standards forced another programming group to introduce an alternative format. For instance, the Portable Network Graphics (PNG) format was largely introduced to avoid the Graphics Interchange Format (GIF) patent problems, and Ogg Vorbis to avoid MP3. If it is discovered that these new suggested formats are themselves covered by existing patents, the final result may be a large number of incompatible formats. Creating such formats and supporting them costs money and creates inconvenience to users.
Under the European Patent Convention (EPC), and in particular its Article 52,[57] "programs for computers" are not regarded as inventions for the purpose of granting European patents,[58] but this exclusion from patentability only applies to the extent to which a European patent application or European patent relates to a computer program as such.[59] As a result of this partial exclusion, and despite the fact that the EPO subjects patent applications in this field to a much stricter scrutiny[60] when compared to their American counterpart, that does not mean that all inventions including some software are de jure not patentable.
Patent and copyright protection constitute two different means of legal protection which may cover the same subject matter, such as computer programs, since each of these two means of protection serves its own purpose.[61] Software is protected as works of literature under the Berne Convention. This allows the creator to prevent another entity from copying the program and there is generally no need to register code in order for it to be copyrighted.
Patents, on the other hand, give their owners the right to prevent others from using the technology defined by the patent claims, even if the technology was independently developed and there was no copying of a software or software code involved. In fact, one of the most recent EPO decisions[62] clarifies the distinction, stating that software is patentable, because it is basically only a technical method executed on a computer, which is to be distinguished from the program itself for executing the method, the program being merely an expression of the method, and thus being copyrighted.
Patents cover the underlying methodologies embodied in a given piece of software, or the function that the software is intended to serve, independent of the particular language or code that the software is written in. Copyright prevents the direct copying of some or all of a particular version of a given piece of software, but does not prevent other authors from writing their own embodiments of the underlying methodologies. Assuming a dataset meets certain criteria, copyright can also be used to prevent a given set of data from being copied while still allowing the author to keep the contents of said set of data a trade secret.[63]
Whether and how the numerus clausus principle shall apply to the legal hybrid software[64] to provide a judicious balance between property rights of the title holders and freedom rights of computing professionals[65] and society as a whole,[66] is in dispute.[67][68][69]
There is a debate over the extent to which software patents should be granted, if at all. Important issues concerning software patents include:
There is strong dislike in the free software community towards software patents. Much of this has been caused by free software or open source projects terminating[74] when the owners of patents covering aspects of a project demanded license fees that the project could not pay, or was not willing to pay, or offered licenses with terms that the project was unwilling to accept, or could not accept, because it conflicted with the free software license in use.[75]
Several patent holders have offered royalty-free patent licenses for a very small portion of their patent portfolios. Such actions have provoked only minor reaction from the free and open source software communities for reasons such as fear of the patent holder changing their mind or the license terms being so narrow as to have little use.[76] Companies that have done this include Apple,[77] IBM,[78] Microsoft,[79] Nokia,[80] Novell,[81] Red Hat,[82] and Sun (now Oracle).[83]
In 2006, Microsoft's pledge not to sue Novell Linux customers, openSUSE contributors, and free/open source software developers over patents[85] and the associated collaboration agreement with Novell[86] was met with disdain from the Software Freedom Law Center[87] while commentators from the Free Software Foundation stated that the agreement would not comply with GPLv3. Meanwhile, Microsoft has reached similar agreements with Dell and Samsung,[88] due to alleged patent infringements of the Linux operating system. Microsoft has also derived revenue from Android by making such agreements-not-to-sue with Android vendors.[89]
In the late 1990s, Unisys claimed to have granted royalty free licenses to hundreds of not-for-profit organizations that used the patented LZW compression method and, by extension, the GIF image format. However, this did not include most software developers and Unisys were "barraged" by negative and "sometimes obscene" emails from software developers.[90]
Patenting software is widespread in the US. As of 2015[update], approximately 500,000 patents had issued in the 23 classes of patents covering "computer implemented inventions" (see table).
Many software companies cross license their patents to each other. These agreements allow each party to practice the other party's patented inventions without the threat of being sued for patent infringement. Microsoft, for example, has agreements with IBM, Sun (now Oracle), SAP, Hewlett-Packard, Siemens, Cisco, Autodesk,[92] and recently Novell. Microsoft cross-licensed its patents with Sun, despite being direct competitors, and with Autodesk even though Autodesk has far fewer patents than Microsoft.
The ability to negotiate cross licensing agreements is a major reason that many software companies, including those providing open source software, file patents. As of June 2006, for example, Red Hat had developed a portfolio of 10 issued US patents, 1 issued European patent, 163 pending US patent applications, and 33 pending international PCT (Patent Cooperation Treaty) patent applications. Red Hat uses this portfolio to cross license with proprietary software companies so that they can preserve their freedom to operate.[82]
Other patent holders are in the business of inventing new "computer implemented inventions" and then commercializing the inventions by licensing the patents to other companies that manufacture the inventions. Walker Digital, for example, has generated a large patent portfolio from its research efforts, including the basic patent on the Priceline.com reverse auction technology. US universities also fall into this class of patent owners. They collectively generate about $1.4 billion per year through licensing the inventions they develop to both established and start up companies in all fields of technology, including software.[93]
The practice of acquiring patents merely to license them is controversial in the software industry. Companies that have this business model are pejoratively referred to as patent trolls. It is an integral part of the business model that patent licensing companies sue infringers that do not take a license. Furthermore, they may take advantage of the fact that many companies will pay a modest license fee (e.g. $100,000 to $1,000,000) for rights to a patent of questionable validity, rather than pay the high legal fees ($2,000,000 or more) to demonstrate in court that the patent is invalid.[citation needed]
The software patent debate is the argument about the extent to which, as a matter of public policy, it should be possible to patent software and computer-implemented inventions. Policy debate on software patents has been active for years.[1] The opponents to software patents have gained more visibility with fewer resources through the years than their pro-patent opponents.[2] Arguments and critiques have been focused mostly on the economic consequences of software patents.
One aspect of the debate has focused on the proposed European Union directive on the patentability of computer-implemented inventions, also known as the "CII Directive" or the "Software Patent Directive," which was ultimately rejected by the EU Parliament in July 2005.
There are several arguments commonly given in defense of software patents or defense of the patentability of computer-implemented inventions.
Copyright law protects unique expressions, while patent law protects inventions, which in the case of software, are algorithms; copyright cannot protect a novel means of accomplishing a function, merely the syntax of one such means.[7]
Software patents can afford smaller companies market protection by preventing larger companies from stealing work done by a smaller organization, leveraging their greater resources to go to market before the smaller company can.[9]
Hardware and software are sometimes interchangeable. If people can patent hardware, then ideas describing software implemented by that hardware should also be patentable.[10]
A program is the transcription of an algorithm in a programming language. Since every (Turing-complete) programming language implements Church's lambda calculus by virtue of the Church-Turing thesis, a program is thus the transcription of a mathematical function. Math is not patentable. Therefore, neither is software.[11]
A patent thicket is a dense web of patents that companies must decipher to develop new technology. There are various types of patent thickets such as when a single innovation is protected by multiple patent holders or when a product is covered by numerous patents. The consequences of patent thickets are increased difficulty of innovation, complex cross-licensing relations between companies, and discouragement of newcomers from entering the software industry.[12]
Several Supreme Court decisions since 2000, as well as the Federal Circuit and district court decisions interpreting and implementing them, have dramatically impacted the status of software patents in the United States. They have particularly affected many thousands of business-method patents that issued as a result of Federal Circuit decisions in the 1990s. The two principal Supreme Court decisions were Bilski v. Kappos and Alice v. CLS Bank, the latter of which confirmed the applicability of the earlier decision Mayo v. Prometheus to computer-related inventions in which a computer was used to implement an abstract principle or preexisting business practice. See also Software patents under United States patent law.
The Supreme Court's Bilski decision was criticized because of its lack of detailed guidance on how to determine whether a claim was directed to an abstract idea. Nonetheless, it provided some clarification and affirmed the Federal Circuit's taking a new direction in its software-related patent cases.
In Mayo v. Prometheus, the Supreme Court invalidated a patent on a diagnostic method, because it non-inventively implemented a natural principle; the Court drew on cases involving computer software and other abstract ideas. In this case, the Court was much more detailed in describing how to recognize a patent-ineligible claim to an abstract idea. The Mayo methodology has come to dominate patent-eligibility law.  It revived the approach of the Flook and Neilson cases, which is to treat the underlying principle, idea, or algorithm on which the claimed patent is based as if it were part of the prior art and to make patent eligibility turn on whether the implementation of it is inventive. This led to the "two-step" Alice test described next.
At the time the Mayo case was decided, there was some uncertainty over whether it applied only to natural principles (laws of nature) or more generally to patent eligibility of all abstract ideas and general principles, including those involved in software patents. The Alice decision confirmed that the test was general. The Alice case involved patents on electronic methods and computer programs for financial-trading systems on which trades between two parties who are to exchange payment are settled by a third party in ways that reduce the risk that one party performs while the other does not. The patents cover what amounts to a computerized escrow arrangement.
In the first step, the court must determine whether the patent claim under examination contains an abstract idea, such as an algorithm, method of computation, a Law of Nature or other general principle. If not, the claim is potentially patentable, subject to the other requirements of the patent code. If the answer is affirmative, the court must proceed to the next step.
The Alice decision met a mixed reception, but profoundly affected U.S. patent law. In its wake, as explained in the Wikipedia article on the case, courts invalidated vast numbers of so-called software and business-method patents (the overwhelming majority of those the United States Court of Appeals for the Federal Circuit considered) and the number of such patents issued has drastically fallen. The Alice decision has been widely criticized for its failure to specify in detail the boundaries of patent eligibility, but it has also been defended because its unanimity tends to stabilize decisional law in the field.[33]
After Alice, the Federal Circuit and district courts invalidated large numbers of business-method and software patents based on those courts' interpretations of Alice. Federal Circuit Judge William Bryson summed this up in these terms:
In computer science, recursion is a method of solving a computational problem where the solution depends on solutions to smaller instances of the same problem.[1][2] Recursion solves such recursive problems by using functions that call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.[3]
Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages (for instance, Clojure)[5] do not define any looping constructs but rely solely on recursion to repeatedly call code. It is proved in computability theory that these recursive-only languages are Turing complete; this means that they are as powerful (they can be used to solve the same problems) as imperative languages based on control structures such as while and for.
Repeatedly calling a function from within itself may cause the call stack to have a size equal to the sum of the input sizes of all involved calls. It follows that, for problems that can be solved easily by iteration, recursion is generally less efficient, and, for certain problems, algorithmic or compiler-optimization techniques such as tail call optimization may improve computational performance over a naive recursive implementation.
A common algorithm design tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method; when combined with a lookup table that stores the results of previously solved sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization.
Similarly recursive definitions are often used to model the structure of expressions and statements in programming languages.  Language designers often express grammars in a syntax such as Backus–Naur form; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition:
A coinductive data definition is one that specifies the operations that may be performed on a piece of data; typically, self-referential coinductive definitions are used for data structures of infinite size.
This is very similar to an inductive definition of lists of strings; the difference is that this definition specifies how to access the contents of the data structure—namely, via the accessor functions head and tail—and what those contents may be, whereas the inductive definition specifies how to create the structure and what it may be created from.
Corecursion is related to coinduction, and can be used to compute particular instances of (possibly) infinite objects.  As a programming technique, it is used most often in the context of lazy programming languages, and can be preferable to recursion when the desired size or precision of a program's output is unknown.  In such cases the program requires both a definition for an infinitely large (or infinitely precise) result, and a mechanism for taking a finite portion of that result.  The problem of computing the first n prime numbers is one that can be solved with a corecursive program (e.g. here).
Many well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it.  HtDP (How to Design Programs) refers to this kind as generative recursion.  Examples of generative recursion include: gcd, quicksort, binary search, mergesort, Newton's method, fractals, and adaptive integration.
On the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this.
Short-circuiting is primarily a concern when many base cases are encountered, such as Null pointers in a tree, which can be linear in the number of function calls, hence significant savings for O(n) algorithms; this is illustrated below for a depth-first search. Short-circuiting on a tree corresponds to considering a leaf (non-empty node with no children) as the base case, rather than considering an empty node as the base case. If there is only a single base case, such as in computing the factorial, short-circuiting provides only O(1) savings.
Recursive algorithms are often inefficient for small data, due to the overhead of repeated function calls and returns. For this reason efficient implementations of recursive algorithms often start with the recursive algorithm, but then switch to a different algorithm when the input becomes small. An important example is merge sort, which is often implemented by switching to the non-recursive insertion sort when the data is sufficiently small, as in the tiled merge sort. Hybrid recursive algorithms can often be further refined, as in Timsort, derived from a hybrid merge sort/insertion sort.
Recursion and iteration are equally expressive: recursion can be replaced by iteration with an explicit call stack, while iteration can be replaced with tail recursion. Which approach is preferable depends on the problem under consideration and the language used. In imperative programming, iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion. By contrast, in functional languages recursion is preferred, with tail recursion optimization leading to little overhead. Implementing an algorithm using iteration may not be easily achievable.
Most programming languages in use today allow the direct specification of recursive functions and procedures. When such a function is called, the program's runtime environment keeps track of the various instances of the function (often using a call stack, although other methods may be used). Every recursive function can be transformed into an iterative function by replacing recursive calls with iterative control constructs and simulating the call stack with a stack explicitly managed by the program.[9][10]
In some programming languages, the maximum size of the call stack is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms. Consequently, these languages sometimes place a limit on the depth of recursion to avoid stack overflows; Python is one such language.[13] Note the caveat below regarding the special case of tail recursion.
Because recursive algorithms can be subject to stack overflows, they may be vulnerable to pathological or malicious input.[14] Some malware specifically targets a program's call stack and takes advantage of the stack's inherently recursive nature.[15] Even in the absence of malware, a stack overflow caused by unbounded recursion can be fatal to the program, and exception handling logic may not prevent the corresponding process from being terminated.[16]
Multiply recursive problems are inherently recursive, because of prior state they need to track. One example is tree traversal as in depth-first search; though both recursive and iterative methods are used,[17] they contrast with list traversal and linear search in a list, which is a singly recursive and thus naturally iterative method. Other examples include divide-and-conquer algorithms such as Quicksort, and functions such as the Ackermann function. All of these algorithms can be implemented iteratively with the help of an explicit stack, but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution.
Recursive algorithms can be replaced with non-recursive counterparts.[18] One method for replacing recursive algorithms is to simulate them using heap memory in place of stack memory.[19] An alternative is to develop a replacement algorithm entirely based on non-recursive methods, which can be challenging.[20] For example, recursive algorithms for matching wildcards, such as Rich Salz' wildmat algorithm,[21] were once typical. Non-recursive algorithms for the same purpose, such as the Krauss matching wildcards algorithm, have been developed to avoid the drawbacks of recursion[22] and have improved only gradually based on techniques such as collecting tests and profiling performance.[23]
This factorial function can also be described without using recursion by making use of the typical looping constructs found in imperative programming languages:
The definition above translates straightforwardly to functional programming languages such as Scheme; this is an example of iteration implemented recursively.
The Euclidean algorithm, which computes the greatest common divisor of two integers, can be written recursively.
The recursive program above is tail-recursive; it is equivalent to an iterative algorithm, and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls.  Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls.  By maintaining its state entirely in the variables x and y and using a looping construct, the program avoids making recursive calls and growing the call stack.
The iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps.
The binary search algorithm is a method of searching a sorted array for a single element by cutting the array in half with each recursive pass.  The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for.
Recursion is used in this algorithm because with each pass a new array is created by cutting the old one in half.  The binary search procedure is then called recursively, this time on the new (and smaller) array.  Typically the array's size is adjusted by manipulating a beginning and ending index.  The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass.
An important application of recursion in computer science is in defining dynamic data structures such as lists and trees.  Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time.
"Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms."[27]
Because the struct node data structure is defined recursively, procedures that operate on it can be implemented naturally as recursive procedures.  The list_print procedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL).  For each node it prints the data element (an integer).  In the C implementation, the list remains unchanged by the list_print procedure.
The time efficiency of recursive algorithms can be expressed in a recurrence relation of Big O notation.  They can (usually) then be simplified into a single Big-O term.
The logical reading frees the reader from needing to know how the clause is used to solve problems. The clause can be used top-down, as in Prolog, to reduce problems to subproblems. Or it can be used bottom-up (or forwards), as in Datalog, to derive conclusions from conditions. This separation of concerns is a form of abstraction, which separates declarative knowledge from problem solving methods (see Algorithm#Algorithm = Logic + Control).[28]
A common mistake among programmers is not providing a way to exit a recursive function, often by omitting or incorrectly checking the base case, letting it run (at least theoretically) infinitely by endlessly calling itself recursively. This is called infinite recursion, and the program will never terminate. In practice, this typically exhausts the available stack space. In most programming environments, a program with infinite recursion will not really run forever. Eventually, something will break and the program will report an error.[29]
In computer science, a parallel algorithm, as opposed to a traditional serial algorithm, is an algorithm which can do multiple operations in a given time. It has been a tradition of computer science to describe serial algorithms in abstract machine models, often the one known as random-access machine. Similarly, many computer science researchers have used a so-called parallel random-access machine (PRAM) as a parallel abstract machine (shared-memory).[1][2]
Many parallel algorithms are executed concurrently – though in general concurrent algorithms are a distinct concept – and thus these concepts are often conflated, with which aspect of an algorithm is parallel and which is concurrent not being clearly distinguished. Further, non-parallel, non-concurrent algorithms are often referred to as "sequential algorithms", by contrast with concurrent algorithms.
Algorithms vary significantly in how parallelizable they are, ranging from easily parallelizable to completely unparallelizable. Further, a given problem may accommodate different algorithms, which may be more or less parallelizable.
Some problems are easy to divide up into pieces in this way – these are called embarrassingly parallel problems. Examples include many algorithms to solve Rubik's Cubes and find values which result in a given hash.[citation needed]
Some problems cannot be split up into parallel portions, as they require the results from a preceding step to effectively carry on with the next step – these are called inherently serial problems. Examples include iterative numerical methods, such as Newton's method, iterative solutions to the three-body problem, and most of the available algorithms to compute pi (π).[citation needed] Some sequential algorithms can be converted into parallel algorithms using automatic parallelization.[3]
In many cases developing an effective parallel algorithm for solution of some task requires attraction of new ideas and methods comparing to creating a sequential algorithm version. These are, for instance, practically important problems of searching a target element in data structures, evaluation of an algebraic expression, etc.[4]
Parallel algorithms on individual devices have become more common since the early 2000s because of substantial improvements in multiprocessing systems and the rise of multi-core processors. Up until the end of 2004, single-core processor performance rapidly increased via frequency scaling, and thus it was easier to construct a computer with a single fast core than one with many slower cores with the same throughput, so multicore systems were of more limited use. Since 2004 however, frequency scaling hit a wall, and thus multicore systems have become more widespread, making parallel algorithms of more general use.
The cost or complexity of serial algorithms is estimated in terms of the space (memory) and time (processor cycles) that they take. Parallel algorithms need to optimize one more resource, the communication between different processors. There are two ways parallel processors communicate, shared memory or message passing.
Shared memory processing needs additional locking for the data, imposes the overhead of additional processor and bus cycles, and also serializes some portion of the algorithm.
Message passing processing uses channels and message boxes but this communication adds transfer overhead on the bus, additional memory need for queues and message boxes and latency in the messages. Designs of parallel processors use special buses like crossbar so that the communication overhead will be small but it is the parallel algorithm that decides the volume of the traffic.
Another problem with parallel algorithms is ensuring that they are suitably load balanced, by ensuring that load (overall work) is balanced, rather than input size being balanced. For example, checking all numbers from one to a hundred thousand for primality is easy to split among processors; however, if the numbers are simply divided out evenly (1–1,000, 1,001–2,000, etc.), the amount of work will be unbalanced, as smaller numbers are easier to process by this algorithm (easier to test for primality), and thus some processors will get more work to do than the others, which will sit idle until the loaded processors complete.
A subtype of parallel algorithms, distributed algorithms, are algorithms designed to work in cluster computing and distributed computing environments, where additional concerns beyond the scope of "classical" parallel algorithms need to be addressed.
A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors. Distributed algorithms are used in different application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation.[1]
Distributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing. One of the major challenges in developing and implementing distributed algorithms is successfully coordinating the behavior of the independent parts of the algorithm in the face of processor failures and unreliable communications links. The choice of an appropriate distributed algorithm to solve a given problem depends on both the characteristics of the problem, and characteristics of the system the algorithm will run on such as the type and probability of processor or link failures, the kind of inter-process communication that can be performed, and the level of timing synchronization between separate processes.[1]
In computer science, a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states. Deterministic algorithms are by far the most studied and familiar kind of algorithm, as well as one of the most practical, since they can be run on real machines efficiently.
Formally, a deterministic algorithm computes a mathematical function; a function has a unique value for any input in its domain, and the algorithm is a process that produces this particular value as output.
Deterministic algorithms can be defined in terms of a state machine: a state describes what a machine is doing at a particular instant in time. State machines pass in a discrete manner from one state to another. Just after we enter the input, the machine is in its initial state or start state. If the machine is deterministic, this means that from this point onwards, its current state determines what its next state will be; its course through the set of states is predetermined. Note that a machine can be deterministic and still never stop or finish, and therefore fail to deliver a result.
A variety of factors can cause an algorithm to behave in a way which is not deterministic, or non-deterministic:
Although real programs are rarely purely deterministic, it is easier for humans as well as other programs to reason about programs that are. For this reason, most programming languages and especially functional programming languages make an effort to prevent the above events from happening except under controlled conditions.
The prevalence of multi-core processors has resulted in a surge of interest in determinism in parallel programming and challenges of non-determinism have been well documented.[1][2] A number of tools to help deal with the challenges have been proposed[3][4][5][6] to deal with deadlocks and race conditions.
It is advantageous, in some cases, for a program to exhibit nondeterministic behavior. The behavior of a card shuffling program used in a game of blackjack, for example, should not be predictable by players — even if the source code of the program is visible. The use of a pseudorandom number generator is often not sufficient to ensure that players are unable to predict the outcome of a shuffle. A clever gambler might guess precisely the numbers the generator will choose and so determine the entire contents of the deck ahead of time, allowing him to cheat; for example, the Software Security Group at Reliable Software Technologies was able to do this for an implementation of Texas Hold 'em Poker that is distributed by ASF Software, Inc, allowing them to consistently predict the outcome of hands ahead of time.[7] These problems can be avoided, in part, through the use of a cryptographically secure pseudo-random number generator, but it is still necessary for an unpredictable random seed to be used to initialize the generator. For this purpose, a source of nondeterminism is required, such as that provided by a hardware random number generator.
The mercury logic-functional programming language establishes different determinism categories for predicate modes as explained in the reference.[8][9]
In computer science and computer programming, a nondeterministic algorithm is an algorithm that, even for the same input, can exhibit different behaviors on different runs, as opposed to a deterministic algorithm.
Different models of computation give rise to different reasons that an algorithm may be non-deterministic, and different ways to evaluate its performance or correctness:
In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one.[1] Approximation algorithms naturally arise in the field of theoretical computer science as a consequence of the widely believed P ≠ NP conjecture. Under this conjecture, a wide class of optimization problems cannot be solved exactly in polynomial time. The field of approximation algorithms, therefore, tries to understand how closely it is possible to approximate optimal solutions to such problems in polynomial time. In an overwhelming majority of the cases, the guarantee of such algorithms is a multiplicative one expressed as an approximation ratio or approximation factor i.e., the optimal solution is always guaranteed to be within a (predetermined) multiplicative factor of the returned solution. However, there are also many approximation algorithms that provide an additive guarantee on the quality of the returned solution. A notable example of an approximation algorithm that provides both is the classic approximation algorithm of Lenstra, Shmoys and Tardos[2] for scheduling on unrelated parallel machines.
The design and analysis of approximation algorithms crucially involves a mathematical proof certifying the quality of the returned solutions in the worst case.[1] This distinguishes them from heuristics such as annealing or genetic algorithms, which find reasonably good solutions on some inputs, but provide no clear indication at the outset on when they may succeed or fail.
There is widespread interest in theoretical computer science to better understand the limits to which we can approximate certain famous optimization problems. For example, one of the long-standing open questions in computer science is to determine whether there is an algorithm that outperforms the 2-approximation for the Steiner Forest problem by Agrawal et al.[3] The desire to understand hard optimization problems from the perspective of approximability is motivated by the discovery of surprising mathematical connections and broadly applicable techniques to design algorithms for hard optimization problems. One well-known example of the former is the Goemans–Williamson algorithm for maximum cut, which solves a graph theoretic problem using high dimensional geometry.[4]
A simple example of an approximation algorithm is one for the minimum vertex cover problem, where the goal is to choose the smallest set of vertices such that every edge in the input graph contains at least one chosen vertex. One way to find a vertex cover is to repeat the following process: find an uncovered edge, add both its endpoints to the cover, and remove all edges incident to either vertex from the graph. As any vertex cover of the input graph must use a distinct vertex to cover each edge that was considered in the process (since it forms a matching), the vertex cover produced, therefore, is at most twice as large as the optimal one. In other words, this is a constant-factor approximation algorithm with an approximation factor of 2. Under the recent unique games conjecture, this factor is even the best possible one.[5]
NP-hard problems vary greatly in their approximability; some, such as the knapsack problem, can be approximated within a multiplicative factor 



1
+
ϵ


{\displaystyle 1+\epsilon }

, for any fixed 



ϵ
>
0


{\displaystyle \epsilon >0}

, and therefore produce solutions arbitrarily close to the optimum (such a family of approximation algorithms is called a polynomial-time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial, factor unless P = NP, as in the case of the maximum clique problem. Therefore, an important benefit of studying approximation algorithms is a fine-grained classification of the difficulty of various NP-hard problems beyond the one afforded by the theory of NP-completeness. In other words, although NP-complete problems may be equivalent (under polynomial-time reductions) to each other from the perspective of exact solutions, the corresponding optimization problems behave very differently from the perspective of approximate solutions.
By now there are several established techniques to design approximation algorithms. These include the following ones.
While approximation algorithms always provide an a priori worst case guarantee (be it additive or multiplicative), in some cases they also provide an a posteriori guarantee that is often much better. This is often the case for algorithms that work by solving a convex relaxation of the optimization problem on the given input. For example, there is a different approximation algorithm for minimum vertex cover that solves a linear programming relaxation to find a vertex cover that is at most twice the value of the relaxation. Since the value of the relaxation is never larger than the size of the optimal vertex cover, this yields another 2-approximation algorithm. While this is similar to the a priori guarantee of the previous approximation algorithm, the guarantee of the latter can be much better (indeed when the value of the LP relaxation is far from the size of the optimal vertex cover).
Approximation algorithms as a research area is closely related to and informed by inapproximability theory where the non-existence of efficient algorithms with certain approximation ratios is proved (conditioned on widely believed hypotheses such as the P ≠ NP conjecture) by means of reductions. In the case of the metric traveling salesman problem, the best known inapproximability result rules out algorithms with an approximation ratio less than 123/122 ≈ 1.008196 unless P = NP, Karpinski, Lampis, Schmied.[6] Coupled with the knowledge of the existence of Christofides' 1.5 approximation algorithm, this tells us that the threshold of approximability for metric traveling salesman (if it exists) is somewhere between 123/122 and 1.5.
While inapproximability results have been proved since the 1970s, such results were obtained by ad hoc means and no systematic understanding was available at the time. It is only since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set[7] and the famous PCP theorem,[8] that modern tools for proving inapproximability results were uncovered. The PCP theorem, for example, shows that Johnson's 1974 approximation algorithms for Max SAT, set cover, independent set and coloring all achieve the optimal approximation ratio, assuming P ≠ NP.[9]
Not all approximation algorithms are suitable for direct practical applications. Some involve solving non-trivial linear programming/semidefinite relaxations (which may themselves invoke the ellipsoid algorithm), complex data structures, or sophisticated algorithmic techniques, leading to difficult implementation issues or improved running time performance (over exact algorithms) only on impractically large inputs. Implementation and running time issues aside, the guarantees provided by approximation algorithms may themselves not be strong enough to justify their consideration in practice. Despite their inability to be used "out of the box" in practical applications, the ideas and insights behind the design of such algorithms can often be incorporated in other ways in practical algorithms. In this way, the study of even very expensive algorithms is not a completely theoretical pursuit as they can yield valuable insights.
In other cases, even if the initial results are of purely theoretical interest, over time, with an improved understanding, the algorithms may be refined to become more practical. One such example is the initial PTAS for Euclidean TSP by Sanjeev Arora (and independently by Joseph Mitchell) which had a prohibitive running time of 




n

O
(
1

/

ϵ
)




{\displaystyle n^{O(1/\epsilon )}}

 for a 



1
+
ϵ


{\displaystyle 1+\epsilon }

 approximation.[10] Yet, within a year these ideas were incorporated into a near-linear time 



O
(
n
log
⁡
n
)


{\displaystyle O(n\log n)}

 algorithm for any constant 



ϵ
>
0


{\displaystyle \epsilon >0}

.[11]
Specifically, having 




A

Π


(
i
)
∈

S

i




{\displaystyle A_{\Pi }(i)\in S_{i}}

, the algorithm has an approximation factor (or approximation ratio) of 



ρ
(
n
)


{\displaystyle \rho (n)}

 if 



∀
i
∈
I
 
s
.
t
.

|

i

|

=
n


{\displaystyle \forall i\in I\ s.t.|i|=n}

, we have:
The approximation can be proven tight (tight approximation) by demonstrating that there exist instances where the algorithm performs at the approximation limit, indicating the tightness of the bound. In this case, it's enough to construct an input instance designed to force the algorithm into a worst-case scenario.
For some approximation algorithms it is possible to prove certain properties about the approximation of th.e optimum result. For example, a ρ-approximation algorithm A is defined to be an algorithm for which it has been proven that the value/cost, f(x), of the approximate solution A(x) to an instance x will not be more (or less, depending on the situation) than a factor ρ times the value, OPT, of an optimum solution.
The factor ρ is called the relative performance guarantee. An approximation algorithm has an absolute performance guarantee or bounded error c, if it has been proven for every instance x that
where f(y) is the value/cost of the solution y for the instance x. Clearly, the performance guarantee is greater than or equal to 1 and equal to 1 if and only if y is an optimal solution. If an algorithm A guarantees to return solutions with a performance guarantee of at most r(n), then A is said to be an r(n)-approximation algorithm and has an approximation ratio of r(n). Likewise, a problem with an r(n)-approximation algorithm is said to be r(n)-approximable or have an approximation ratio of r(n).[12][13]
The absolute performance guarantee 





P


A




{\displaystyle \mathrm {P} _{A}}

 of some approximation algorithm A, where x refers to an instance of a problem, and where 




R

A


(
x
)


{\displaystyle R_{A}(x)}

 is the performance guarantee of A on x (i.e. ρ for problem instance x) is:
That is to say that it is the same as the absolute performance ratio, with a lower bound n on the size of problem instances. These two types of ratios are used because there exist algorithms where the difference between these two is significant.
In the literature, an approximation ratio for a maximization (minimization) problem of c - ϵ (min: c + ϵ) means that the algorithm has an approximation ratio of c ∓ ϵ  for arbitrary ϵ > 0 but that the ratio has not (or cannot) be shown for ϵ = 0. An example of this is the optimal inapproximability — inexistence of approximation — ratio of 7 / 8 + ϵ for satisfiable MAX-3SAT instances due to Johan Håstad.[14] As mentioned previously, when c = 1, the problem is said to have a polynomial-time approximation scheme.
An ϵ-term may appear when an approximation algorithm introduces a multiplicative error and a constant error while the minimum optimum of instances of size n goes to infinity as n does. In this case, the approximation ratio is c ∓ k / OPT = c ∓ o(1) for some constants c and k. Given arbitrary ϵ > 0, one can choose a large enough N such that the term k / OPT < ϵ for every n ≥ N. For every fixed ϵ, instances of size n < N can be solved by brute force, thereby showing an approximation ratio — existence of approximation algorithms with a guarantee — of c ∓ ϵ for every ϵ > 0.
In quantum computing, a quantum algorithm is an algorithm that runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation.[1][2] A classical (or non-quantum) algorithm is a finite sequence of instructions, or a step-by-step procedure for solving a problem, where each step or instruction can be performed on a classical computer. Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. Although all classical algorithms can also be performed on a quantum computer,[3]: 126   the term quantum algorithm is generally reserved for algorithms that seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.
Problems that are undecidable using classical computers remain undecidable using quantum computers.[4]: 127  What makes quantum algorithms interesting is that they might be able to solve some problems faster than classical algorithms because the quantum superposition and quantum entanglement that quantum algorithms exploit generally cannot be efficiently simulated on classical computers (see Quantum supremacy).
The best-known algorithms are Shor's algorithm for factoring and Grover's algorithm for searching an unstructured database or an unordered list. Shor's algorithm runs much (almost exponentially) faster than the best-known classical algorithm for factoring, the general number field sieve.[5] Grover's algorithm runs quadratically faster than the best possible classical algorithm for the same task,[6] a linear search.
Quantum algorithms are usually described, in the commonly used circuit model of quantum computation, by a quantum circuit that acts on some input qubits and terminates with a measurement. A quantum circuit consists of simple quantum gates, each of which acts on some finite number of qubits. Quantum algorithms may also be stated in other models of quantum computation, such as the Hamiltonian oracle model.[7]
Quantum algorithms can be categorized by the main techniques involved in the algorithm. Some commonly used techniques/ideas in quantum algorithms include phase kick-back, phase estimation, the quantum Fourier transform, quantum walks, amplitude amplification and topological quantum field theory. Quantum algorithms may also be grouped by the type of problem solved; see, e.g., the survey on quantum algorithms for algebraic problems.[8]
The quantum Fourier transform is the quantum analogue of the discrete Fourier transform, and is used in several quantum algorithms. The Hadamard transform is also an example of a quantum Fourier transform over an n-dimensional vector space over the field F2. The quantum Fourier transform can be efficiently implemented on a quantum computer using only a polynomial number of quantum gates.[citation needed]
The Deutsch–Jozsa algorithm solves a black-box problem that requires exponentially many queries to the black box for any deterministic classical computer, but can be done with a single query by a quantum computer. However, when comparing bounded-error classical and quantum algorithms, there is no speedup, since a classical probabilistic algorithm can solve the problem with a constant number of queries with small probability of error. The algorithm determines whether a function f is either constant (0 on all inputs or 1 on all inputs) or balanced (returns 1 for half of the input domain and 0 for the other half).
The Bernstein–Vazirani algorithm is the first quantum algorithm that solves a problem more efficiently than the best known classical algorithm. It was designed to create an oracle separation between BQP and BPP.
Simon's algorithm solves a black-box problem exponentially faster than any classical algorithm, including bounded-error probabilistic algorithms. This algorithm, which achieves an exponential speedup over all classical algorithms that we consider efficient, was the motivation for Shor's algorithm for factoring.
The quantum phase estimation algorithm is used to determine the eigenphase of an eigenvector of a unitary gate, given a quantum state proportional to the eigenvector and access to the gate.   The algorithm is frequently used as a subroutine in other algorithms.
Shor's algorithm solves the discrete logarithm problem and the integer factorization problem in polynomial time,[9] whereas the best known classical algorithms take super-polynomial time. It is unknown whether these problems are in P or NP-complete. It is also one of the few quantum algorithms that solves a non-black-box problem in polynomial time, where the best known classical algorithms run in super-polynomial time.
The abelian hidden subgroup problem is a generalization of many problems that can be solved by a quantum computer, such as Simon's problem, solving Pell's equation, testing the principal ideal of a ring R and factoring. There are efficient quantum algorithms known for the Abelian hidden subgroup problem.[10] The more general hidden subgroup problem, where the group is not necessarily abelian, is a generalization of the previously mentioned problems, as well as graph isomorphism and certain lattice problems. Efficient quantum algorithms are known for certain non-abelian groups. However, no efficient algorithms are known for the symmetric group, which would give an efficient algorithm for graph isomorphism[11] and the dihedral group, which would solve certain lattice problems.[12]
A Gauss sum is a type of exponential sum. The best known classical algorithm for estimating these sums takes exponential time. Since the discrete logarithm problem reduces to Gauss sum estimation, an efficient classical algorithm for estimating Gauss sums would imply an efficient classical algorithm for computing discrete logarithms, which is considered unlikely. However, quantum computers can estimate Gauss sums to polynomial precision in polynomial time.[13]
Amplitude amplification is a technique that allows the amplification of a chosen subspace of a quantum state. Applications of amplitude amplification usually lead to quadratic speedups over the corresponding classical algorithms. It can be considered as a generalization of Grover's algorithm.[citation needed]
Grover's algorithm searches an unstructured database (or an unordered list) with N entries for a marked entry, using only 



O
(


N


)


{\displaystyle O({\sqrt {N}})}

 queries instead of the 



O
(

N

)


{\displaystyle O({N})}

 queries required classically.[15] Classically, 



O
(

N

)


{\displaystyle O({N})}

 queries are required even allowing bounded-error probabilistic algorithms.
Theorists have considered a hypothetical generalization of a standard quantum computer that could access the histories of the hidden variables in Bohmian mechanics. (Such a computer is completely hypothetical and would not be a standard quantum computer, or even possible under the standard theory of quantum mechanics.) Such a hypothetical computer could implement a search of an N-item database in at most 



O
(


N

3



)


{\displaystyle O({\sqrt[{3}]{N}})}

 steps. This is slightly faster than the 



O
(


N


)


{\displaystyle O({\sqrt {N}})}

 steps taken by Grover's algorithm. However, neither search method would allow either model of quantum computer to solve NP-complete problems in polynomial time.[16]
Quantum counting solves a generalization of the search problem. It solves the problem of counting the number of marked entries in an unordered list, instead of just detecting whether one exists. Specifically, it counts the number of marked entries in an 



N


{\displaystyle N}

-element list with an error of at most 



ε


{\displaystyle \varepsilon }

 by making only 



Θ

(


ε

−
1




N

/

k



)



{\displaystyle \Theta \left(\varepsilon ^{-1}{\sqrt {N/k}}\right)}

 queries, where 



k


{\displaystyle k}

 is the number of marked elements in the list.[17][18] More precisely, the algorithm outputs an estimate 




k
′



{\displaystyle k'}

 for 



k


{\displaystyle k}

, the number of marked entries, with accuracy 




|

k
−

k
′


|

≤
ε
k


{\displaystyle |k-k'|\leq \varepsilon k}

.
A quantum walk is the quantum analogue of a classical random walk. A classical random walk can be described by a probability distribution over some states, while a quantum walk can be described by a quantum superposition over states. Quantum walks are known to give exponential speedups for some black-box problems.[19][20] They also provide polynomial speedups for many problems. A framework for the creation of quantum walk algorithms exists and is a versatile tool.[21]
The Boson Sampling Problem in an experimental configuration assumes[22] an input of bosons (e.g., photons) of moderate number that are randomly scattered into a large number of output modes, constrained by a defined unitarity. When individual photons are used, the problem is isomorphic to a multi-photon quantum walk.[23] The problem is then to produce a fair sample of the probability distribution of the output that depends on the input arrangement of bosons and the unitarity.[24] Solving this problem with a classical computer algorithm requires computing the permanent of the unitary transform matrix, which may take a prohibitively long time or be outright impossible. In 2014, it was proposed[25] that existing technology and standard probabilistic methods of generating single-photon states could be used as an input into a suitable quantum computable linear optical network and that sampling of the output probability distribution would be demonstrably superior using quantum algorithms. In 2015, investigation predicted[26] the sampling problem had similar complexity for inputs other than Fock-state photons and identified a transition in computational complexity from classically simulable to just as hard as the Boson Sampling Problem, depending on the size of coherent amplitude inputs.
The element distinctness problem is the problem of determining whether all the elements of a list are distinct. Classically, 



Ω
(
N
)


{\displaystyle \Omega (N)}

 queries are required for a list of size 



N


{\displaystyle N}

; however, it can be solved in 



Θ
(

N

2

/

3


)


{\displaystyle \Theta (N^{2/3})}

 queries on a quantum computer. The optimal algorithm was put forth by Andris Ambainis,[27] and Yaoyun Shi first proved a tight lower bound when the size of the range is sufficiently large.[28] Ambainis[29] and Kutin[30] independently (and via different proofs) extended that work to obtain the lower bound for all functions.
The triangle-finding problem is the problem of determining whether a given graph contains a triangle (a clique of size 3). The best-known lower bound for quantum algorithms is 



Ω
(
N
)


{\displaystyle \Omega (N)}

, but the best algorithm known requires O(N1.297) queries,[31] an improvement over the previous best O(N1.3) queries.[21][32]
A well studied formula is the balanced binary tree with only NAND gates.[33] This type of formula requires 



Θ
(

N

c


)


{\displaystyle \Theta (N^{c})}

 queries using randomness,[34] where 



c
=

log

2


⁡
(
1
+


33


)

/

4
≈
0.754


{\displaystyle c=\log _{2}(1+{\sqrt {33}})/4\approx 0.754}

. With a quantum algorithm, however, it can be solved in 



Θ
(

N

1

/

2


)


{\displaystyle \Theta (N^{1/2})}

 queries. No better quantum algorithm for this case was known until one was found for the unconventional Hamiltonian oracle model.[7] The same result for the standard setting soon followed.[35]
The problem is to determine if a black-box group, given by k generators, is commutative. A black-box group is a group with an oracle function, which must be used to perform the group operations (multiplication, inversion, and comparison with identity). The interest in this context lies in the query complexity, which is the number of oracle calls needed to solve the problem. The deterministic and randomized query complexities are 



Θ
(

k

2


)


{\displaystyle \Theta (k^{2})}

 and 



Θ
(
k
)


{\displaystyle \Theta (k)}

, respectively.[37] A quantum algorithm requires 



Ω
(

k

2

/

3


)


{\displaystyle \Omega (k^{2/3})}

 queries, while the best-known classical algorithm uses 



O
(

k

2

/

3


log
⁡
k
)


{\displaystyle O(k^{2/3}\log k)}

 queries.[38]
The idea that quantum computers might be more powerful than classical computers originated in Richard Feynman's observation that classical computers seem to require exponential time to simulate many-particle quantum systems, yet quantum many-body systems are able to "solve themselves."[41] Since then, the idea that quantum computers can simulate quantum physical processes exponentially faster than classical computers has been greatly fleshed out and elaborated. Efficient (i.e., polynomial-time) quantum algorithms have been developed for simulating both Bosonic and Fermionic systems,[42] as well as the simulation of chemical reactions beyond the capabilities of current classical supercomputers using only a few hundred qubits.[43] Quantum computers can also efficiently simulate topological quantum field theories.[44] In addition to its intrinsic interest, this result has led to efficient quantum algorithms for estimating quantum topological invariants such as Jones[45] and HOMFLY polynomials,[46] and the Turaev-Viro invariant of three-dimensional manifolds.[47]
In 2009, Aram Harrow, Avinatan Hassidim, and Seth Lloyd, formulated a quantum algorithm for solving linear systems. The algorithm estimates the result of a scalar measurement on the solution vector to a given linear system of equations.[48]
Provided that the linear system is sparse and has a low condition number 



κ


{\displaystyle \kappa }

, and that the user is interested in the result of a scalar measurement on the solution vector (instead of the values of the solution vector itself), then the algorithm has a runtime of 



O
(
log
⁡
(
N
)

κ

2


)


{\displaystyle O(\log(N)\kappa ^{2})}

, where 



N


{\displaystyle N}

 is the number of variables in the linear system. This offers an exponential speedup over the fastest classical algorithm, which runs in 



O
(
N
κ
)


{\displaystyle O(N\kappa )}

 (or 



O
(
N


κ


)


{\displaystyle O(N{\sqrt {\kappa }})}

 for positive semidefinite matrices).
Hybrid Quantum/Classical Algorithms combine quantum state preparation and measurement with classical optimization.[49] These algorithms generally aim to determine the ground-state eigenvector and eigenvalue of a Hermitian operator.
The quantum approximate optimization algorithm takes inspiration from quantum annealing, performing a discretized approximation of quantum annealing using a quantum circuit. It can be used to solve problems in graph theory.[50] The algorithm makes use of classical optimization of quantum operations to maximize an "objective function."
The variational quantum eigensolver (VQE) algorithm applies classical optimization to minimize the energy expectation value of an ansatz state to find the ground state of a Hermitian operator, such as a molecule's Hamiltonian.[51] It can also be extended to find excited energies of molecular Hamiltonians.[52]
The contracted quantum eigensolver (CQE) algorithm minimizes the residual of a contraction (or projection) of the Schrödinger equation onto the space of two (or more) electrons to find the ground- or excited-state energy and two-electron reduced density matrix of a molecule.[53]  It is based on classical methods for solving energies and two-electron reduced density matrices directly from the anti-Hermitian contracted Schrödinger equation.[54]
An algorithmic paradigm or algorithm design paradigm is a generic model or framework which underlies the design of a class of algorithms. An algorithmic paradigm is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program.[1][2]
In computer science, binary search, also known as half-interval search,[1] logarithmic search,[2] or binary chop,[3] is a search algorithm that finds the position of a target value within a sorted array.[4][5] Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array.
Binary search runs in logarithmic time in the worst case, making 



O
(
log
⁡
n
)


{\displaystyle O(\log n)}

 comparisons, where 



n


{\displaystyle n}

 is the number of elements in the array.[a][6] Binary search is faster than linear search except for small arrays. However, the array must be sorted first to be able to apply binary search. There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. However, binary search can be used to solve a wider range of problems, such as finding the next-smallest or next-largest element in the array relative to the target even if it is absent from the array.
There are numerous variations of binary search. In particular, fractional cascading speeds up binary searches for the same value in multiple arrays. Fractional cascading efficiently solves a number of search problems in computational geometry and in numerous other fields. Exponential search extends binary search to unbounded lists. The binary search tree and B-tree data structures are based on binary search.
Binary search works on sorted arrays. Binary search begins by comparing an element in the middle of the array with the target value. If the target value matches the element, its position in the array is returned. If the target value is less than the element, the search continues in the lower half of the array. If the target value is greater than the element, the search continues in the upper half of the array. By doing this, the algorithm eliminates the half in which the target value cannot lie in each iteration.[7]
Alternatively, the algorithm may take the ceiling of 






L
+
R

2




{\displaystyle {\frac {L+R}{2}}}

. This may change the result if the target value appears more than once in the array.
In the above procedure, the algorithm checks whether the middle element (



m


{\displaystyle m}

) is equal to the target (



T


{\displaystyle T}

) in every iteration. Some implementations leave out this check during each iteration. The algorithm would perform this check only when one element is left (when 



L
=
R


{\displaystyle L=R}

). This results in a faster comparison loop, as one comparison is eliminated per iteration, while it requires only one more iteration on average.[8]
The procedure may return any index whose element is equal to the target value, even if there are duplicate elements in the array. For example, if the array to be searched was 



[
1
,
2
,
3
,
4
,
4
,
5
,
6
,
7
]


{\displaystyle [1,2,3,4,4,5,6,7]}

 and the target was 



4


{\displaystyle 4}

, then it would be correct for the algorithm to either return the 4th (index 3) or 5th (index 4) element. The regular procedure would return the 4th element (index 3) in this case. It does not always return the first duplicate (consider 



[
1
,
2
,
4
,
4
,
4
,
5
,
6
,
7
]


{\displaystyle [1,2,4,4,4,5,6,7]}

 which still returns the 4th element). However, it is sometimes necessary to find the leftmost element or the rightmost element for a target value that is duplicated in the array. In the above example, the 4th element is the leftmost element of the value 4, while the 5th element is the rightmost element of the value 4. The alternative procedure above will always return the index of the rightmost element if such an element exists.[9]
In terms of iterations, no search algorithm that works only by comparing elements can exhibit better average and worst-case performance than binary search. The comparison tree representing binary search has the fewest levels possible as every level above the lowest level of the tree is filled completely.[b] Otherwise, the search algorithm can eliminate few elements in an iteration, increasing the number of iterations required in the average and worst case. This is the case for other search algorithms based on comparisons, as while they may work faster on some target values, the average performance over all elements is worse than binary search. By dividing the array in half, binary search ensures that the size of both subarrays are as similar as possible.[14]
Since binary search is the optimal algorithm for searching with comparisons, this problem is reduced to calculating the minimum internal path length of all binary trees with n nodes, which is equal to:[17]
Each iteration of the binary search procedure defined above makes one or two comparisons, checking if the middle element is equal to the target in each iteration. Assuming that each element is equally likely to be searched, each iteration makes 1.5 comparisons on average. A variation of the algorithm checks whether the middle element is equal to the target at the end of the search. On average, this eliminates half a comparison from each iteration. This slightly cuts the time taken per iteration on most computers. However, it guarantees that the search takes the maximum number of iterations, on average adding one iteration to the search. Because the comparison loop is performed only 



⌊

log

2


⁡
(
n
)
+
1
⌋


{\textstyle \lfloor \log _{2}(n)+1\rfloor }

 times in the worst case, the slight increase in efficiency per iteration does not compensate for the extra iteration for all but very large 



n


{\textstyle n}

.[c][18][19]
On most computer architectures, the processor has a hardware cache separate from RAM. Since they are located within the processor itself, caches are much faster to access but usually store much less data than RAM. Therefore, most processors store memory locations that have been accessed recently, along with memory locations close to it. For example, when an array element is accessed, the element itself may be stored along with the elements that are stored close to it in RAM, making it faster to sequentially access array elements that are close in index to each other (locality of reference). On a sorted array, binary search can jump to distant memory locations if the array is large, unlike algorithms (such as linear search and linear probing in hash tables) which access elements in sequence. This adds slightly to the running time of binary search for large arrays on most systems.[20]
Sorted arrays with binary search are a very inefficient solution when insertion and deletion operations are interleaved with retrieval, taking 



O
(
n
)


{\textstyle O(n)}

 time for each such operation. In addition, sorted arrays can complicate memory use especially when elements are often inserted into the array.[21] There are other data structures that support much more efficient insertion and deletion. Binary search can be used to perform exact matching and set membership (determining whether a target value is in a collection of values). There are data structures that support faster exact matching and set membership. However, unlike many other searching schemes, binary search can be used for efficient approximate matching, usually performing such matches in 



O
(
log
⁡
n
)


{\textstyle O(\log n)}

 time regardless of the type or structure of the values themselves.[22] In addition, there are some operations, like finding the smallest and largest element, that can be performed efficiently on a sorted array.[11]
Linear search is a simple search algorithm that checks every record until it finds the target value. Linear search can be done on a linked list, which allows for faster insertion and deletion than an array. Binary search is faster than linear search for sorted arrays except if the array is short, although the array needs to be sorted beforehand.[d][24] All sorting algorithms based on comparing elements, such as quicksort and merge sort, require at least 



O
(
n
log
⁡
n
)


{\textstyle O(n\log n)}

 comparisons in the worst case.[25] Unlike linear search, binary search can be used for efficient approximate matching. There are operations such as finding the smallest and largest element that can be done efficiently on a sorted array but not on an unsorted array.[26]
A binary search tree is a binary tree data structure that works based on the principle of binary search. The records of the tree are arranged in sorted order, and each record in the tree can be searched using an algorithm similar to binary search, taking on average logarithmic time. Insertion and deletion also require on average logarithmic time in binary search trees. This can be faster than the linear time insertion and deletion of sorted arrays, and binary trees retain the ability to perform all the operations possible on a sorted array, including range and approximate queries.[22][27]
Binary search trees lend themselves to fast searching in external memory stored in hard disks, as binary search trees can be efficiently structured in filesystems. The B-tree generalizes this method of tree organization. B-trees are frequently used to organize long-term storage such as databases and filesystems.[30][31]
For implementing associative arrays, hash tables, a data structure that maps keys to records using a hash function, are generally faster than binary search on a sorted array of records.[32] Most hash table implementations require only amortized constant time on average.[f][34] However, hashing is not useful for approximate matches, such as computing the next-smallest, next-largest, and nearest key, as the only information given on a failed search is that the target is not present in any record.[35] Binary search is ideal for such matches, performing them in logarithmic time. Binary search also supports approximate matches. Some operations, like finding the smallest and largest element, can be done efficiently on sorted arrays but not on hash tables.[22]
A related problem to search is set membership. Any algorithm that does lookup, like binary search, can also be used for set membership. There are other algorithms that are more specifically suited for set membership. A bit array is the simplest, useful when the range of keys is limited. It compactly stores a collection of bits, with each bit representing a single key within the range of keys. Bit arrays are very fast, requiring only 



O
(
1
)


{\textstyle O(1)}

 time.[36]  The Judy1 type of Judy array handles 64-bit keys efficiently.[37]
For approximate results, Bloom filters, another probabilistic data structure based on hashing, store a set of keys by encoding the keys using a bit array and multiple hash functions. Bloom filters are much more space-efficient than bit arrays in most cases and not much slower: with 



k


{\textstyle k}

 hash functions, membership queries require only 



O
(
k
)


{\textstyle O(k)}

 time. However, Bloom filters suffer from false positives.[g][h][39]
There exist data structures that may improve on binary search in some cases for both searching and other operations available for sorted arrays. For example, searches, approximate matches, and the operations available to sorted arrays can be performed more efficiently than binary search on specialized data structures such as van Emde Boas trees, fusion trees, tries, and bit arrays. These specialized data structures are usually only faster because they take advantage of the properties of keys with a certain attribute (usually keys that are small integers), and thus will be time or space consuming for keys that lack that attribute.[22] As long as the keys can be ordered, these operations can always be done at least efficiently on a sorted array regardless of the keys. Some structures, such as Judy arrays, use a combination of approaches to mitigate this while retaining efficiency and the ability to perform approximate matching.[37]
Uniform binary search stores, instead of the lower and upper bounds, the difference in the index of the middle element from the current iteration to the next iteration. A lookup table containing the differences is computed beforehand. For example, if the array to be searched is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], the middle element (



m


{\displaystyle m}

) would be 6. In this case, the middle element of the left subarray ([1, 2, 3, 4, 5]) is 3 and the middle element of the right subarray ([7, 8, 9, 10, 11]) is 9. Uniform binary search would store the value of 3 as both indices differ from 6 by this same amount.[40] To reduce the search space, the algorithm either adds or subtracts this change from the index of the middle element. Uniform binary search may be faster on systems where it is inefficient to calculate the midpoint, such as on decimal computers.[41]
Binary search has been generalized to work on certain types of graphs, where the target value is stored in a vertex instead of an array element. Binary search trees are one such generalization—when a vertex (node) in the tree is queried, the algorithm either learns that the vertex is the target, or otherwise which subtree the target would be located in. However, this can be further generalized as follows: given an undirected, positively weighted graph and a target vertex, the algorithm learns upon querying a vertex that it is equal to the target, or it is given an incident edge that is on the shortest path from the queried vertex to the target. The standard binary search algorithm is simply the case where the graph is a path. Similarly, binary search trees are the case where the edges to the left or right subtrees are given when the queried vertex is unequal to the target. For all undirected, positively weighted graphs, there is an algorithm that finds the target vertex in 



O
(
log
⁡
n
)


{\displaystyle O(\log n)}

 queries in the worst case.[48]
Noisy binary search algorithms solve the case where the algorithm cannot reliably compare elements of the array. For each pair of elements, there is a certain probability that the algorithm makes the wrong comparison. Noisy binary search can find the correct position of the target with a given probability that controls the reliability of the yielded position. Every noisy binary search procedure must make at least 



(
1
−
τ
)




log

2


⁡
(
n
)


H
(
p
)



−


10

H
(
p
)





{\displaystyle (1-\tau ){\frac {\log _{2}(n)}{H(p)}}-{\frac {10}{H(p)}}}

 comparisons on average, where 



H
(
p
)
=
−
p

log

2


⁡
(
p
)
−
(
1
−
p
)

log

2


⁡
(
1
−
p
)


{\displaystyle H(p)=-p\log _{2}(p)-(1-p)\log _{2}(1-p)}

 is the binary entropy function and 



τ


{\displaystyle \tau }

 is the probability that the procedure yields the wrong position.[49][50][51] The noisy binary search problem can be considered as a case of the Rényi-Ulam game,[52] a variant of Twenty Questions where the answers may be wrong.[53]
Classical computers are bounded to the worst case of exactly 



⌊

log

2


⁡
n
+
1
⌋


{\textstyle \lfloor \log _{2}n+1\rfloor }

 iterations when performing binary search. Quantum algorithms for binary search are still bounded to a proportion of 




log

2


⁡
n


{\textstyle \log _{2}n}

 queries (representing iterations of the classical procedure), but the constant factor is less than one, providing for a lower time complexity on quantum computers. Any exact quantum binary search procedure—that is, a procedure that always yields the correct result—requires at least 





1
π


(
ln
⁡
n
−
1
)
≈
0.22

log

2


⁡
n


{\textstyle {\frac {1}{\pi }}(\ln n-1)\approx 0.22\log _{2}n}

 queries in the worst case, where 



ln


{\textstyle \ln }

 is the natural logarithm.[54] There is an exact quantum binary search procedure that runs in 



4

log

605


⁡
n
≈
0.433

log

2


⁡
n


{\textstyle 4\log _{605}n\approx 0.433\log _{2}n}

 queries in the worst case.[55] In comparison, Grover's algorithm is the optimal quantum algorithm for searching an unordered list of elements, and it requires 



O
(


n


)


{\displaystyle O({\sqrt {n}})}

 queries.[56]
In 1946, John Mauchly made the first mention of binary search as part of the Moore School Lectures, a seminal and foundational college course in computing.[9] In 1957, William Wesley Peterson published the first method for interpolation search.[9][57] Every published binary search algorithm worked only for arrays whose length is one less than a power of two[i] until 1960, when Derrick Henry Lehmer published a binary search algorithm that worked on all arrays.[59] In 1962, Hermann Bottenbruch presented an ALGOL 60 implementation of binary search that placed the comparison for equality at the end, increasing the average number of iterations by one, but reducing to one the number of comparisons per iteration.[8] The uniform binary search was developed by A. K. Chandra of Stanford University in 1971.[9] In 1986, Bernard Chazelle and Leonidas J. Guibas introduced fractional cascading as a method to solve numerous search problems in computational geometry.[46][60][61]
When Jon Bentley assigned binary search as a problem in a course for professional programmers, he found that ninety percent failed to provide a correct solution after several hours of working on it, mainly because the incorrect implementations failed to run or returned a wrong answer in rare edge cases.[62] A study published in 1988 shows that accurate code for it is only found in five out of twenty textbooks.[63] Furthermore, Bentley's own implementation of binary search, published in his 1986 book Programming Pearls, contained an overflow error that remained undetected for over twenty years.  The Java programming language library implementation of binary search had the same overflow bug for more than nine years.[64]
This article was submitted to WikiJournal of Science for external academic peer review in 2018 (reviewer reports). The updated content was reintegrated into the Wikipedia page under a CC-BY-SA-3.0 license (2019). The version of record as reviewed is: 
Anthony Lin; et al. (2 July 2019). "Binary search algorithm" (PDF). WikiJournal of Science. 2 (1): 5. doi:10.15347/WJS/2019.005. ISSN 2470-6345. Wikidata Q81434400.
In computer science, graph traversal (also known as graph search) refers to the process of visiting (checking and/or updating) each vertex in a graph. Such traversals are classified by the order in which the vertices are visited. Tree traversal is a special case of graph traversal.
Thus, it is usually necessary to remember which vertices have already been explored by the algorithm, so that vertices are revisited as infrequently as possible (or in the worst case, to prevent the traversal from continuing indefinitely). This may be accomplished by associating each vertex of the graph with a "color" or "visitation" state during the traversal, which is then checked and updated as the algorithm visits each vertex. If the vertex has already been visited, it is ignored and the path is pursued no further; otherwise, the algorithm checks/updates the vertex and continues down its current path.
Several special cases of graphs imply the visitation of other vertices in their structure, and thus do not require that visitation be explicitly recorded during the traversal. An important example of this is a tree: during a traversal it may be assumed that all "ancestor" vertices of the current vertex (and others depending on the algorithm) have already been visited.  Both the depth-first and breadth-first graph searches are adaptations of tree-based algorithms, distinguished primarily by the lack of a structurally determined "root" vertex and the addition of a data structure to record the traversal's visitation state.
Note. — If each vertex in a graph is to be traversed by a tree-based algorithm (such as DFS or BFS), then the algorithm must be called at least once for each connected component of the graph. This is easily accomplished by iterating through all the vertices of the graph, performing the algorithm on each vertex that is still unvisited when examined.
A depth-first search (DFS) is an algorithm for traversing a finite graph. DFS visits the child vertices before visiting the sibling vertices; that is, it traverses the depth of any particular path before exploring its breadth. A stack (often the program's call stack via recursion) is generally used when implementing the algorithm.
The algorithm begins with a chosen "root" vertex; it then iteratively transitions from the current vertex to an adjacent, unvisited vertex, until it can no longer find an unexplored vertex to transition to from its current location. The algorithm then backtracks along previously visited vertices, until it finds a vertex connected to yet more uncharted territory.  It will then proceed down the new path as it had before, backtracking as it encounters dead-ends, and ending only when the algorithm has backtracked past the original "root" vertex from the very first step.
DFS is the basis for many graph-related algorithms, including topological sorts and planarity testing.
A breadth-first search (BFS) is another technique for traversing a finite graph. BFS visits the sibling vertices before visiting the child vertices, and a queue is used in the search process. This algorithm is often used to find the shortest path from one vertex to another.
The problem of graph exploration can be seen as a variant of graph traversal. It is an online problem, meaning that the information about the graph is only revealed during the runtime of the algorithm. A common model is as follows: given a connected graph G = (V, E) with non-negative edge weights. The algorithm starts at some vertex, and knows all incident outgoing edges and the vertices at the end of these edges—but not more. When a new vertex is visited, then again all incident outgoing edges and the vertices at the end are known. The goal is to visit all n vertices and return to the starting vertex, but the sum of the weights of the tour should be as small as possible. The problem can also be understood as a specific version of the travelling salesman problem, where the salesman has to discover the graph on the go.
For general graphs, the best known algorithms for both undirected and directed graphs is a simple greedy algorithm:
In computer science, a search algorithm is an algorithm designed to solve a search problem. Search algorithms work to retrieve information stored within particular data structure, or calculated in the search space of a problem domain, with either discrete or continuous values.
Although search engines use search algorithms, they belong to the study of information retrieval, not algorithmics.
The appropriate search algorithm to use often depends on the data structure being searched, and may also include prior knowledge about the data. Search algorithms can be made faster or more efficient by specially constructed database structures, such as search trees, hash maps, and database indexes.[1][2]
Search algorithms can be classified based on their mechanism of searching into three types of algorithms: linear, binary, and hashing. Linear search algorithms check every record for the one associated with a target key in a linear fashion.[3] Binary, or half-interval, searches repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order.[4] Digital search algorithms work based on the properties of digits in data structures by using numerical keys.[5] Finally, hashing directly maps keys to records based on a hash function.[6]
Algorithms are often evaluated by their computational complexity, or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O(log n), or logarithmic time. In simple terms, the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space.
Algorithms for searching virtual spaces are used in the constraint satisfaction problem, where the goal is to find a set of value assignments to certain variables that will satisfy specific mathematical equations and inequations / equalities.  They are also used when the goal is to find a variable assignment that will maximize or minimize a certain function of those variables.  Algorithms for these problems include the basic brute-force search (also called  "naïve" or "uninformed" search), and a variety of heuristics that try to exploit partial knowledge about the structure of this space, such as linear relaxation, constraint generation, and constraint propagation.
An important subclass are the local search methods, that view the elements of the search space as the vertices of a graph, with edges defined by a set of heuristics applicable to the case; and scan the space by moving from item to item along the edges, for example according to the steepest descent or best-first criterion, or in a stochastic search.  This category includes a great variety of general metaheuristic methods, such as simulated annealing, tabu search, A-teams, and genetic programming, that combine arbitrary heuristics in specific ways. The opposite of local search would be global search methods. This method is applicable when the search space is not limited and all aspects of the given network are available to the entity running the search algorithm.[7]
This class also includes various tree search algorithms, that view the elements as vertices of a tree, and traverse that tree in some special order.  Examples of the latter include the exhaustive methods such as depth-first search and breadth-first search, as well as various heuristic-based search tree pruning methods such as backtracking and branch and bound.  Unlike general metaheuristics, which at best work only in a probabilistic sense, many of these tree-search methods are guaranteed to find the exact or optimal solution, if given enough time. This is called "completeness".
Another important sub-class consists of algorithms for exploring the game tree of multiple-player games, such as chess or backgammon, whose nodes consist of all possible game situations that could result from the current situation.  The goal in these problems is to find the move that provides the best chance of a win, taking into account all possible moves of the opponent(s).  Similar problems occur when humans or machines have to make successive decisions whose outcomes are not entirely under one's control, such as in robot guidance or in marketing, financial, or military strategy planning.  This kind of problem — combinatorial search — has been extensively studied in the context of artificial intelligence. Examples of algorithms for this class are the minimax algorithm, alpha–beta pruning, and the A* algorithm and its variants.
An important and extensively studied subclass are the graph algorithms, in particular graph traversal algorithms, for finding specific sub-structures in a given graph — such as subgraphs, paths, circuits, and so on.  Examples include Dijkstra's algorithm, Kruskal's algorithm, the nearest neighbour algorithm, and Prim's algorithm.
Another important subclass of this category are the string searching algorithms, that search for patterns within strings. Two famous examples are the Boyer–Moore and Knuth–Morris–Pratt algorithms, and several algorithms based on the suffix tree data structure.
In 1953, American statistician Jack Kiefer devised Fibonacci search which can be used to find the maximum of a unimodal function and has many other applications in computer science.
There are also search methods designed for quantum computers, like Grover's algorithm, that are theoretically faster than linear or brute-force search even without the help of data structures or heuristics. While the ideas and applications behind quantum computers are still entirely theoretical, studies have been conducted with algorithms like Grover's that accurately replicate the hypothetical physical versions of quantum computing systems.[8]
In computing, a Monte Carlo algorithm is a randomized algorithm whose output may be incorrect with a certain (typically small) probability. Two examples of such algorithms are the Karger–Stein algorithm[1] and the Monte Carlo algorithm for minimum feedback arc set.[2]
Las Vegas algorithms are a dual of Monte Carlo algorithms and never return an incorrect answer. However, they may make random choices as part of their work. As a result, the time taken might vary between runs, even with the same input.
If there is a procedure for verifying whether the answer given by a Monte Carlo algorithm is correct, and the probability of a correct answer is bounded above zero, then with probability one, running the algorithm repeatedly while testing the answers will eventually give a correct answer. Whether this process is a Las Vegas algorithm depends on whether halting with probability one is considered to satisfy the definition.
While the answer returned by a deterministic algorithm is always expected to be correct, this is not the case for Monte Carlo algorithms. For decision problems, these algorithms are generally classified as either false-biased or true-biased. A false-biased Monte Carlo algorithm is always correct when it returns false; a true-biased algorithm is always correct when it returns true. While this describes algorithms with one-sided errors, others might have no bias; these are said to have two-sided errors. The answer they provide (either true or false) will be incorrect, or correct, with some bounded probability.
For instance, the Solovay–Strassen primality test is used to determine whether a given number is a prime number. It always answers true for prime number inputs; for composite inputs, it answers false with probability at least 1⁄2 and true with probability less than 1⁄2. Thus, false answers from the algorithm are certain to be correct, whereas the true answers remain uncertain; this is said to be a 1⁄2-correct false-biased algorithm.
For a Monte Carlo algorithm with one-sided errors, the failure probability can be reduced (and the success probability amplified) by running the algorithm k times. Consider again the Solovay–Strassen algorithm which is 1⁄2-correct false-biased. One may run this algorithm multiple times returning a false answer if it reaches a false response within k iterations, and otherwise returning true. Thus, if the number is prime then the answer is always correct, and if the number is composite then the answer is correct with probability at least 1−(1−1⁄2)k = 1−2−k.
For Monte Carlo decision algorithms with two-sided error, the failure probability may again be reduced by running the algorithm k times and returning the majority function of the answers.
The complexity class BPP describes decision problems that can be solved by polynomial-time Monte Carlo algorithms with a bounded probability of two-sided errors, and the complexity class RP describes problems that can be solved by a Monte Carlo algorithm with a bounded probability of one-sided error: if the correct answer is false, the algorithm always says so, but it may answer false incorrectly for some instances where the correct answer is true.[4] In contrast, the complexity class ZPP describes problems solvable by polynomial expected time Las Vegas algorithms. ZPP ⊆ RP ⊆ BPP, but it is not known whether any of these complexity classes is distinct from each other; that is, Monte Carlo algorithms may have more computational power than Las Vegas algorithms, but this has not been proven.[4] Another complexity class, PP, describes decision problems with a polynomial-time Monte Carlo algorithm that is more accurate than flipping a coin but where the error probability cannot necessarily be bounded away from 1⁄2.[4]
Randomized algorithms are primarily divided by its two main types, Monte Carlo and Las Vegas, however, these represent only a top of the hierarchy and can be further categorized.[4]
"Both Las Vegas and Monte Carlo are dealing with decisions, i.e., problems in their decision version."[4] "This however should not give a wrong impression and confine these algorithms to such problems—both types of randomized algorithms can be used on numerical problems as well, problems where the output is not simple ‘yes’/‘no’, but where one needs to receive a result that is numerical in nature."[4]
Previous table represents a general framework for Monte Carlo and Las Vegas randomized algorithms.[4] Instead of the mathematical symbol 



<


{\displaystyle <}

 one could use 



≤


{\displaystyle \leq }

, thus making probabilities in the worst case equal.[4]
Well-known Monte Carlo algorithms include the Solovay–Strassen primality test, the Baillie–PSW primality test, the Miller–Rabin primality test, and certain fast variants of the Schreier–Sims algorithm in computational group theory.
For algorithms that are a part of Stochastic Optimization (SO) group of algorithms, where probability is not known in advance and is empirically determined, it is sometimes possible to merge Monte Carlo and such an algorithm "to have both probability bound calculated in advance and a Stochastic Optimization component."[4] "Example of such an algorithm is Ant Inspired Monte Carlo."[4][5] In this way, "drawback of SO has been mitigated, and a confidence in a solution has been established."[4][5]
In computing, a Las Vegas algorithm is a randomized algorithm that always gives correct results; that is, it always produces the correct result or it informs about the failure. However, the runtime of a Las Vegas algorithm differs depending on the input. The usual definition of a Las Vegas algorithm includes the restriction that the expected runtime be finite, where the expectation is carried out over the space of random information, or entropy, used in the algorithm. An alternative definition requires that a Las Vegas algorithm always terminates (is effective), but may output a symbol not part of the solution space to indicate failure in finding a solution.[1] The nature of Las Vegas algorithms makes them suitable in situations where the number of possible solutions is limited, and where verifying the correctness of a candidate solution is relatively easy while finding a solution is complex.
Systematic search methods for computationally hard problems, such as some variants of the Davis–Putnam algorithm for propositional satisfiability (SAT), also utilize non-deterministic decisions, and can thus also be considered Las Vegas algorithms.[2]
Las Vegas algorithms were introduced by László Babai in 1979, in the context of the graph isomorphism problem, as a dual to Monte Carlo algorithms.[3] Babai[4] introduced the term "Las Vegas algorithm" alongside an example involving coin flips: the algorithm depends on a series of independent coin flips, and there is a small chance of failure (no result). However, in contrast to Monte Carlo algorithms, the Las Vegas algorithm can guarantee the correctness of any reported result.
As mentioned above, Las Vegas algorithms always return correct results. The code above illustrates this property. A variable k is generated randomly; after k is generated, k is used to index the array A. If this index contains the value 1, then k is returned; otherwise, the algorithm repeats this process until it finds 1. Although this Las Vegas algorithm is guaranteed to find the correct answer, it does not have a fixed runtime; due to the randomization (in line 3 of the above code), it is possible for arbitrarily much time to elapse before the algorithm terminates.
Las Vegas algorithms have different criteria for the evaluation based on the problem setting. These criteria are divided into three categories with different time limits since Las Vegas algorithms do not have set time complexity. Here are some possible application scenarios:
The run-time distribution (RTD) is the distinctive way to describe the run-time behavior of a Las Vegas algorithm.
Las Vegas algorithms arise frequently in search problems. For example, one looking for some information online might search related websites for the desired information. The time complexity thus ranges from getting "lucky" and finding the content immediately, to being "unlucky" and spending large amounts of time. Once the right website is found, then there is no possibility of error.[6]
In the case of average case, it is hard to determine since the analysis does not depend on the input distribution but on the random choices that the algorithm makes. The average of quicksort is computed over all possible random choices that the algorithm might make when choosing the pivot.
The eight queens problem is usually solved with a backtracking algorithm. However, a Las Vegas algorithm can be applied; in fact, it is more efficient than backtracking.
Note that the algorithm simply fails if a queen cannot be placed. But the process can be repeated and every time will generate different arrangement.[7]
The complexity class of decision problems that have Las Vegas algorithms with expected polynomial runtime is ZPP.
which is intimately connected with the way Las Vegas algorithms are sometimes constructed. Namely the class RP consists of all decision problems for which a randomized polynomial-time algorithm exists that always answers correctly when the correct answer is "no", but is allowed to be wrong with a certain probability bounded away from one when the answer is "yes". When such an algorithm exists for both a problem and its complement (with the answers "yes" and "no" swapped), the two algorithms can be run simultaneously and repeatedly: run each for a constant number of steps, taking turns, until one of them returns a definitive answer. This is the standard way to construct a Las Vegas algorithm that runs in expected polynomial time. Note that in general there is no worst case upper bound on the run time of a Las Vegas algorithm.
In order to make a Las Vegas algorithm optimal, the expected run time should be minimized. This can be done by:
Las Vegas algorithms can be contrasted with Monte Carlo algorithms, in which the resources used are bounded but the answer may be incorrect with a certain (typically small) probability. A Las Vegas algorithm can be converted into a Monte Carlo algorithm by running it for set time and generating a random answer when it fails to terminate. By an application of Markov's inequality, we can set the bound on the probability that the Las Vegas algorithm would go over the fixed limit.
If a deterministic way to test for correctness is available, then it is possible to turn a Monte Carlo algorithm into a Las Vegas algorithm. However, it is hard to convert a Monte Carlo algorithm to a Las Vegas algorithm without a way to test the algorithm. On the other hand, changing a Las Vegas algorithm to a Monte Carlo algorithm is easy. This can be done by running a Las Vegas algorithm for a specific period of time given by confidence parameter. If the algorithm finds the solution within the time, then it is success and if not then output can simply be "sorry".
In computer science, a selection algorithm is an algorithm for finding the 



k


{\displaystyle k}

th smallest value in a collection of ordered values, such as numbers. The value that it finds is called the 



k


{\displaystyle k}

th order statistic. Selection includes as special cases the problems of finding the minimum, median, and maximum element in the collection. Selection algorithms include quickselect, and the median of medians algorithm. When applied to a collection of 



n


{\displaystyle n}

 values, these algorithms take linear time, 



O
(
n
)


{\displaystyle O(n)}

 as expressed using big O notation. For data that is already structured, faster algorithms may be possible; as an extreme case, selection in an already-sorted array takes time 



O
(
1
)


{\displaystyle O(1)}

.
An algorithm for the selection problem takes as input a collection of values, and a number 



k


{\displaystyle k}

. It outputs the 



k


{\displaystyle k}

th smallest of these values, or, in some versions of the problem, a collection of the 



k


{\displaystyle k}

 smallest values. For this to be well-defined, it should be possible to sort the values into an order from smallest to largest; for instance, they may be integers, floating-point numbers, or some other kind of object with a numeric key. However, they are not assumed to have been already sorted. Often, selection algorithms are restricted to a comparison-based model of computation, as in comparison sort algorithms, where the algorithm has access to a comparison operation that can determine the relative ordering of any two values, but may not perform any other kind of arithmetic operations on these values.[1]
As a baseline algorithm, selection of the 



k


{\displaystyle k}

th smallest value in a collection of values can be performed by the following two steps:
The time for this method is dominated by the sorting step, which requires 



Θ
(
n
log
⁡
n
)


{\displaystyle \Theta (n\log n)}

 time using a comparison sort.[2][3] Even when integer sorting algorithms may be used, these are generally slower than the linear time that may be achieved using specialized selection algorithms. Nevertheless, the simplicity of this approach makes it attractive, especially when a highly-optimized sorting routine is provided as part of a runtime library, but a selection algorithm is not. For inputs of moderate size, sorting can be faster than non-random selection algorithms, because of the smaller constant factors in its running time.[4] This method also produces a sorted version of the collection, which may be useful for other later computations, and in particular for selection with other choices of 



k


{\displaystyle k}

.[3]
For a sorting algorithm that generates one item at a time, such as selection sort, the scan can be done in tandem with the sort, and the sort can be terminated once the 



k


{\displaystyle k}

th element has been found. One possible design of a consolation bracket in a single-elimination tournament, in which the teams who lost to the eventual winner play another mini-tournament to determine second place, can be seen as an instance of this method.[5] Applying this optimization to heapsort produces the heapselect algorithm, which can select the 



k


{\displaystyle k}

th smallest value in time 



O
(
n
+
k
log
⁡
n
)


{\displaystyle O(n+k\log n)}

.[6] This is fast when 



k


{\displaystyle k}

 is small relative to 



n


{\displaystyle n}

, but degenerates to 



O
(
n
log
⁡
n
)


{\displaystyle O(n\log n)}

 for larger values of 



k


{\displaystyle k}

, such as the choice 



k
=
n

/

2


{\displaystyle k=n/2}

 used for median finding.
Many methods for selection are based on choosing a special "pivot" element from the input, and using comparisons with this element to divide the remaining 



n
−
1


{\displaystyle n-1}

 input values into two subsets: the set 



L


{\displaystyle L}

 of elements less than the pivot, and the set 



R


{\displaystyle R}

 of elements greater than the pivot. The algorithm can then determine where the 



k


{\displaystyle k}

th smallest value is to be found, based on a comparison of 



k


{\displaystyle k}

 with the sizes of these sets. In particular, if 



k
≤

|

L

|



{\displaystyle k\leq |L|}

, the 



k


{\displaystyle k}

th smallest value is in 



L


{\displaystyle L}

, and can be found recursively by applying the same selection algorithm to 



L


{\displaystyle L}

. If 



k
=

|

L

|

+
1


{\displaystyle k=|L|+1}

, then the 



k


{\displaystyle k}

th smallest value is the pivot, and it can be returned immediately. In the remaining case, the 



k


{\displaystyle k}

th smallest value is in 



R


{\displaystyle R}

, and more specifically it is the element in position 



k
−

|

L

|

−
1


{\displaystyle k-|L|-1}

 of 



R


{\displaystyle R}

. It can be found by applying a selection algorithm recursively, seeking the value in this position in 



R


{\displaystyle R}

.[7]
As with the related pivoting-based quicksort algorithm, the partition of the input into 



L


{\displaystyle L}

 and 



R


{\displaystyle R}

 may be done by making new collections for these sets, or by a method that partitions a given list or array data type in-place. Details vary depending on how the input collection is represented.[8] The time to compare the pivot against all the other values is 



O
(
n
)


{\displaystyle O(n)}

.[7] However, pivoting methods differ in how they choose the pivot, which affects how big the subproblems in each recursive call will be. The efficiency of these methods depends greatly on the choice of the pivot. If the pivot is chosen badly, the running time of this method can be as slow as 



O
(

n

2


)


{\displaystyle O(n^{2})}

.[4]
The deterministic selection algorithms with the smallest known numbers of comparisons, for values of 



k


{\displaystyle k}

 that are far from 



1


{\displaystyle 1}

 or 



n


{\displaystyle n}

, are based on the concept of factories, introduced in 1976 by Arnold Schönhage, Mike Paterson, and Nick Pippenger.[18] These are methods that build partial orders of certain specified types, on small subsets of input values, by using comparisons to combine smaller partial orders. As a very simple example, one type of factory can take as input a sequence of single-element partial orders, compare pairs of elements from these orders, and produce as output a sequence of two-element totally ordered sets. The elements used as the inputs to this factory could either be input values that have not been compared with anything yet, or "waste" values produced by other factories. The goal of a factory-based algorithm is to combine together different factories, with the outputs of some factories going to the inputs of others, in order to eventually obtain a partial order in which one element (the 



k


{\displaystyle k}

th smallest) is larger than some 



k
−
1


{\displaystyle k-1}

 other elements and smaller than another 



n
−
k


{\displaystyle n-k}

 others. A careful design of these factories leads to an algorithm that, when applied to median-finding, uses at most 



2.942
n


{\displaystyle 2.942n}

 comparisons. For other values of 



k


{\displaystyle k}

, the number of comparisons is smaller.[19]
Parallel algorithms for selection have been studied since 1975, when Leslie Valiant introduced the parallel comparison tree model for analyzing these algorithms, and proved that in this model selection using a linear number of comparisons requires 



Ω
(
log
⁡
log
⁡
n
)


{\displaystyle \Omega (\log \log n)}

 parallel steps, even for selecting the minimum or maximum.[20] Researchers later found parallel algorithms for selection in 



O
(
log
⁡
log
⁡
n
)


{\displaystyle O(\log \log n)}

 steps, matching this bound.[21][22] In a randomized parallel comparison tree model it is possible to perform selection in a bounded number of steps and a linear number of comparisons.[23] On the more realistic parallel RAM model of computing, with exclusive read exclusive write memory access, selection can be performed in time 



O
(
log
⁡
n
)


{\displaystyle O(\log n)}

 with 



O
(
n

/

log
⁡
n
)


{\displaystyle O(n/\log n)}

 processors, which is optimal both in time and in the number of processors.[24] With concurrent memory access, slightly faster parallel time is possible in general,[25] and the 



log
⁡
n


{\displaystyle \log n}

 term in the time bound can be replaced by 



log
⁡
k


{\displaystyle \log k}

.[26]
When data is already organized into a data structure, it may be possible to perform selection in an amount of time that is sublinear in the number of values. As a simple case of this, for data already sorted into an array, selecting the 



k


{\displaystyle k}

th element may be performed by a single array lookup, in constant time.[27] For values organized into a two-dimensional array of size 



m
×
n


{\displaystyle m\times n}

, with sorted rows and columns, selection may be performed in time 



O


(


m
log
⁡
(
2
n

/

m
)


)




{\displaystyle O{\bigl (}m\log(2n/m){\bigr )}}

, or faster when 



k


{\displaystyle k}

 is small relative to the array dimensions.[27][28] For a collection of 



m


{\displaystyle m}

 one-dimensional sorted arrays, with 




k

i




{\displaystyle k_{i}}

 items less than the selected item in the 



i


{\displaystyle i}

th array, the time is 



O


(


m
+

∑

i
=
1


m


log
⁡
(

k

i


+
1
)


)




{\textstyle O{\bigl (}m+\sum _{i=1}^{m}\log(k_{i}+1){\bigr )}}

.[28]
Selection from data in a binary heap takes time 



O
(
k
)


{\displaystyle O(k)}

. This is independent of the size 



n


{\displaystyle n}

 of the heap, and faster than the 



O
(
k
log
⁡
n
)


{\displaystyle O(k\log n)}

 time bound that would be obtained from best-first search.[28][29] This same method can be applied more generally to data organized as any kind of heap-ordered tree (a tree in which each node stores one value in which the parent of each non-root node has a smaller value than its child). This method of performing selection in a heap has been applied to problems of listing multiple solutions to combinatorial optimization problems, such as finding the k shortest paths in a weighted graph, by defining a state space of solutions in the form of an implicitly defined heap-ordered tree, and then applying this selection algorithm to this tree.[30] In the other direction, linear time selection algorithms have been used as a subroutine in a priority queue data structure related to the heap, improving the time for extracting its 



k


{\displaystyle k}

th item from 



O
(
log
⁡
n
)


{\displaystyle O(\log n)}

 to 



O
(

log

∗


⁡
n
+
log
⁡
k
)


{\displaystyle O(\log ^{*}n+\log k)}

; here 




log

∗


⁡
n


{\displaystyle \log ^{*}n}

 is the iterated logarithm.[31]
For a collection of data values undergoing dynamic insertions and deletions, the order statistic tree augments a self-balancing binary search tree structure with a constant amount of additional information per tree node, allowing insertions, deletions, and selection queries that ask for the 



k


{\displaystyle k}

th element in the current set to all be performed in 



O
(
log
⁡
n
)


{\displaystyle O(\log n)}

 time per operation.[2] Going beyond the comparison model of computation, faster times per operation are possible for values that are small integers, on which binary arithmetic operations are allowed.[32] It is not possible for a streaming algorithm with memory sublinear in both 



n


{\displaystyle n}

 and 



k


{\displaystyle k}

 to solve selection queries exactly for dynamic data, but the count–min sketch can be used to solve selection queries approximately, by finding a value whose position in the ordering of the elements (if it were added to them) would be within 



ε
n


{\displaystyle \varepsilon n}

 steps of 



k


{\displaystyle k}

, for a sketch whose size is within logarithmic factors of 



1

/

ε


{\displaystyle 1/\varepsilon }

.[33]
The 



O
(
n
)


{\displaystyle O(n)}

 running time of the selection algorithms described above is necessary, because a selection algorithm that can handle inputs in an arbitrary order must take that much time to look at all of its inputs. If any one of its input values is not compared, that one value could be the one that should have been selected, and the algorithm can be made to produce an incorrect answer.[28] Beyond this simple argument, there has been a significant amount of research on the exact number of comparisons needed for selection, both in the randomized and deterministic cases.
The next simplest case is selecting the second-smallest. After several incorrect attempts, the first tight lower bound on this case was published in 1964 by Soviet mathematician Sergey Kislitsyn. It can be shown by observing that selecting the second-smallest also requires distinguishing the smallest value from the rest, and by considering the number 



p


{\displaystyle p}

 of comparisons involving the smallest value that an algorithm for this problem makes. Each of the 



p


{\displaystyle p}

 items that were compared to the smallest value is a candidate for second-smallest, and 



p
−
1


{\displaystyle p-1}

 of these values must be found larger than another value in a second comparison in order to rule them out as second-smallest.
With 



n
−
1


{\displaystyle n-1}

 values being the larger in at least one comparison, and 



p
−
1


{\displaystyle p-1}

 values being the larger in at least two comparisons, there are a total of at least 



n
+
p
−
2


{\displaystyle n+p-2}

 comparisons. An adversary argument, in which the outcome of each comparison is chosen in order to maximize 



p


{\displaystyle p}

 (subject to consistency with at least one possible ordering) rather than by the numerical values of the given items, shows that it is possible to force 



p


{\displaystyle p}

 to be at least 




log

2


⁡
n


{\displaystyle \log _{2}n}

. Therefore, the worst-case number of comparisons needed to select the second smallest is 



n
+
⌈

log

2


⁡
n
⌉
−
2


{\displaystyle n+\lceil \log _{2}n\rceil -2}

, the same number that would be obtained by holding a single-elimination tournament with a run-off tournament among the values that lost to the smallest value. However, the expected number of comparisons of a randomized selection algorithm can be better than this bound; for instance, selecting the second-smallest of six elements requires seven comparisons in the worst case, but may be done by a randomized algorithm with an expected number of 6.5 comparisons.[14]
More generally, selecting the 



k


{\displaystyle k}

th element out of 



n


{\displaystyle n}

 requires at least 



n
+
min
(
k
,
n
−
k
)
−
O
(
1
)


{\displaystyle n+\min(k,n-k)-O(1)}

 comparisons, in the average case, matching the number of comparisons of the Floyd–Rivest algorithm up to its 



o
(
n
)


{\displaystyle o(n)}

 term. The argument is made directly for deterministic algorithms, with a number of comparisons that is averaged over all possible permutations of the input values.[1] By Yao's principle, it also applies to the expected number of comparisons for a randomized algorithm on its worst-case input.[34]
For deterministic algorithms, it has been shown that selecting the 



k


{\displaystyle k}

th element requires 





(


1
+
H
(
k

/

n
)


)


n
+
Ω
(


n


)


{\displaystyle {\bigl (}1+H(k/n){\bigr )}n+\Omega ({\sqrt {n}})}

 comparisons, where 



H
(
x
)
=
x

log

2


⁡


1
x


+
(
1
−
x
)

log

2


⁡


1

1
−
x





{\displaystyle H(x)=x\log _{2}{\frac {1}{x}}+(1-x)\log _{2}{\frac {1}{1-x}}}

 is the binary entropy function.[35] The special case of median-finding has a slightly larger lower bound on the number of comparisons, at least 



(
2
+
ε
)
n


{\displaystyle (2+\varepsilon )n}

, for 



ε
≈

2

−
80




{\displaystyle \varepsilon \approx 2^{-80}}

.[36]
Knuth supplies the following triangle of numbers summarizing pairs of 



n


{\displaystyle n}

 and 



k


{\displaystyle k}

 for which the exact number of comparisons needed by an optimal selection algorithm is known. The 



n


{\displaystyle n}

th row of the triangle (starting with 



n
=
1


{\displaystyle n=1}

 in the top row) gives the numbers of comparisons for inputs of 



n


{\displaystyle n}

 values, and the 



k


{\displaystyle k}

th number within each row gives the number of comparisons needed to select the 



k


{\displaystyle k}

th smallest value from an input of that size. The rows are symmetric because selecting the 



k


{\displaystyle k}

th smallest requires exactly the same number of comparisons, in the worst case, as selecting the 



k


{\displaystyle k}

th largest.[14]
Python's standard library includes heapq.nsmallest and heapq.nlargest functions for returning the smallest or largest elements from a collection, in sorted order. The implementation maintains a binary heap, limited to holding 



k


{\displaystyle k}

 elements, and initialized to the first 



k


{\displaystyle k}

 elements in the collection. Then, each subsequent item of the collection may replace the largest or smallest element in the heap if it is smaller or larger than this element. The algorithm's memory usage is superior to heapselect (the former only holds 



k


{\displaystyle k}

 elements in memory at a time while the latter requires manipulating the entire dataset into memory).  Running time depends on data ordering.  The best case is 



O
(
(
n
−
k
)
+
k
log
⁡
k
)


{\displaystyle O((n-k)+k\log k)}

 for already sorted data.  The worst-case is 



O
(
n
log
⁡
k
)


{\displaystyle O(n\log k)}

 for reverse sorted data. In average cases, there are likely to be few heap updates and most input elements are processed with only a single comparison. For example, extracting the 100 largest or smallest values out of 10,000,000 random inputs makes 10,009,401 comparisons on average.[39]
Since 2017, Matlab has included maxk() and mink() functions, which return the maximal (minimal) 



k


{\displaystyle k}

 values in a vector as well as their indices. The Matlab documentation does not specify which algorithm these functions use or what their running time is.[40]
Quickselect was presented without analysis by Tony Hoare in 1965,[41] and first analyzed in a 1971 technical report by Donald Knuth.[11] The first known linear time deterministic selection algorithm is the median of medians method, published in 1973 by Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ron Rivest, and Robert Tarjan.[5] They trace the formulation of the selection problem to work of Charles L. Dodgson (better known as Lewis Carroll) who in 1883 pointed out that the usual design of single-elimination sports tournaments does not guarantee that the second-best player wins second place,[5][42] and to work of Hugo Steinhaus circa 1930, who followed up this same line of thought by asking for a tournament design that can make this guarantee, with a minimum number of games played (that is, comparisons).[5]
In computability theory and computational complexity theory, a reduction is an algorithm for transforming one problem into another problem. A sufficiently efficient reduction from one problem to another may be used to show that the second problem is at least as difficult as the first.
Intuitively, problem A is reducible to problem B, if an algorithm for solving problem B efficiently (if it existed) could also be used as a subroutine to solve problem A efficiently. When this is true, solving A cannot be harder than solving B. "Harder" means having a higher estimate of the required computational resources in a given context (e.g., higher time complexity, greater memory requirement, expensive need for extra hardware processor cores for a parallel solution compared to a single-threaded solution, etc.). The existence of a reduction from A to B can be written in the shorthand notation A ≤m B, usually with a subscript on the ≤ to indicate the type of reduction being used (m : mapping reduction, p : polynomial reduction).
In case of optimization (maximization or minimization) problems, we often think in terms of approximation-preserving reduction. Suppose we have two optimization problems such that instances of one problem can be mapped onto instances of the other, in a way that nearly optimal solutions to instances of the latter problem can be transformed back to yield nearly optimal solutions to the former. This way, if we have an optimization algorithm (or approximation algorithm) that finds near-optimal (or optimal) solutions to instances of problem B, and an efficient approximation-preserving reduction from problem A to problem B, by composition we obtain an optimization algorithm that yields near-optimal solutions to instances of problem A. Approximation-preserving reductions are often used to prove hardness of approximation results: if some optimization problem A is hard to approximate (under some complexity assumption) within a factor better than α for some α, and there is a β-approximation-preserving reduction from problem A to problem B, we can conclude that problem B is hard to approximate within factor α/β.
Linear programming (LP), also called linear optimization, is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements and objective are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).
More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polytope. A linear programming algorithm finds a point in the polytope where this function has the largest (or smallest) value if such a point exists.
Linear programming can be applied to various fields of study. It is widely used in mathematics and, to a lesser extent, in business, economics, and some engineering problems. There is a close connection between linear programs, eigenequations, John von Neumann's general equilibrium model, and structural equilibrium models (see dual linear program for details).[1]
[2]
[3]
Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.
In the late 1930s, Soviet mathematician Leonid Kantorovich and American economist Wassily Leontief independently delved into the practical applications of linear programming. Kantorovich focused on manufacturing schedules, while Leontief explored economic applications. Their groundbreaking work was largely overlooked for decades.
The turning point came during World War II when linear programming emerged as a vital tool. It found extensive use in addressing complex wartime challenges, including transportation logistics, scheduling, and resource allocation. Linear programming proved invaluable in optimizing these processes while considering critical constraints such as costs and resource availability.
Despite its initial obscurity, the wartime successes propelled linear programming into the spotlight. Post-WWII, the method gained widespread recognition and became a cornerstone in various fields, from operations research to economics. The overlooked contributions of Kantorovich and Leontief in the late 1930s eventually became foundational to the broader acceptance and utilization of linear programming in optimizing decision-making processes.[5]
From 1946 to 1947 George B. Dantzig independently developed general linear programming formulation to use for planning problems in the US Air Force.[8] In 1947, Dantzig also invented the simplex method that, for the first time efficiently, tackled the linear programming problem in most cases.[8] When Dantzig arranged a meeting with John von Neumann to discuss his simplex method, von Neumann immediately conjectured the theory of duality by realizing that the problem he had been working in game theory was equivalent.[8] Dantzig provided formal proof in an unpublished report "A Theorem on Linear Inequalities" on January 5, 1948.[6] Dantzig's work was made available to public in 1951. In the post-war years, many industries applied it in their daily planning.
Dantzig's original example was to find the best assignment of 70 people to 70 jobs. The computing power required to test all the permutations to select the best assignment is vast; the number of possible configurations exceeds the number of particles in the observable universe. However, it takes only a moment to find the optimum solution by posing the problem as a linear program and applying the simplex algorithm. The theory behind linear programming drastically reduces the number of possible solutions that must be checked.
The linear programming problem was first shown to be solvable in polynomial time by Leonid Khachiyan in 1979,[9] but a larger theoretical and practical breakthrough in the field came in 1984 when Narendra Karmarkar introduced a new interior-point method for solving linear-programming problems.[10]
Linear programming is a widely used field of optimization for several reasons. Many practical problems in operations research can be expressed as linear programming problems.[6] Certain special cases of linear programming, such as network flow problems and multicommodity flow problems, are considered important enough to have much research on specialized algorithms. A number of algorithms for other types of optimization problems work by solving linear programming problems as sub-problems. Historically, ideas from linear programming have inspired many of the central concepts of optimization theory, such as duality, decomposition, and the importance of convexity and its generalizations. Likewise, linear programming was heavily used in the early formation of microeconomics, and it is currently utilized in company management, such as planning, production, transportation, and technology. Although the modern management issues are ever-changing, most companies would like to maximize profits and minimize costs with limited resources. Google also uses linear programming to stabilize YouTube videos.[11]
Standard form is the usual and most intuitive form of describing a linear programming problem. It consists of the following three parts:
Suppose that a farmer has a piece of farm land, say L hectares, to be planted with either wheat or barley or some combination of the two. The farmer has F kilograms of fertilizer and P kilograms of pesticide. Every hectare of wheat requires F1 kilograms of fertilizer and P1 kilograms of pesticide, while every hectare of barley requires F2 kilograms of fertilizer and P2 kilograms of pesticide. Let S1 be the selling price of wheat and S2 be the selling price of barley, per hectare. If we denote the area of land planted with wheat and barley by x1 and x2 respectively, then profit can be maximized by choosing optimal values for x1 and x2. This problem can be expressed with the following linear programming problem in the standard form:
Linear programming problems can be converted into an augmented form in order to apply the common form of the simplex algorithm. This form introduces non-negative slack variables to replace inequalities with equalities in the constraints. The problems can then be written in the following block matrix form:
Every linear programming problem, referred to as a primal problem, can be converted into a dual problem, which provides an upper bound to the optimal value of the primal problem. In matrix form, we can express the primal problem as:
Covering and packing LPs commonly arise as a linear programming relaxation of a combinatorial problem and are important in the study of approximation algorithms.[12] For example, the LP relaxations of the set packing problem, the independent set problem, and the matching problem are packing LPs. The LP relaxations of the set cover problem, the vertex cover problem, and the dominating set problem are also covering LPs.
Otherwise, if a feasible solution exists and if the constraint set is bounded, then the optimum value is always attained on the boundary of the constraint set, by the maximum principle for convex functions (alternatively, by the minimum principle for concave functions) since linear functions are both convex and concave. However, some problems have distinct optimal solutions; for example, the problem of finding a feasible solution to a system of linear inequalities is a linear programming problem in which the objective function is the zero function (i.e., the constant function taking the value zero everywhere). For this feasibility problem with the zero-function for its objective-function, if there are two distinct solutions, then every convex combination of the solutions is a solution.
The vertices of the polytope are also called basic feasible solutions. The reason for this choice of name is as follows. Let d denote the number of variables. Then the fundamental theorem of linear inequalities implies (for feasible problems) that for every vertex x* of the LP feasible region, there exists a set of d (or fewer) inequality constraints from the LP such that, when we treat those d constraints as equalities, the unique solution is x*. Thereby we can study these vertices by means of looking at certain subsets of the set of all constraints (a discrete set), rather than the continuum of LP solutions. This principle underlies the simplex algorithm for solving linear programs.
The simplex algorithm, developed by George Dantzig in 1947, solves LP problems by constructing a feasible solution at a vertex of the polytope and then walking along a path on the edges of the polytope to vertices with non-decreasing values of the objective function until an optimum is reached for sure. In many practical problems, "stalling" occurs: many pivots are made with no increase in the objective function.[13][14] In rare practical problems, the usual versions of the simplex algorithm may actually "cycle".[14] To avoid cycles, researchers developed new pivoting rules.[15]
In practice, the simplex algorithm is quite efficient and can be guaranteed to find the global optimum if certain precautions against cycling are taken. The simplex algorithm has been proved to solve "random" problems efficiently, i.e. in a cubic number of steps,[16] which is similar to its behavior on practical problems.[13][17]
However, the simplex algorithm has poor worst-case behavior: Klee and Minty constructed a family of linear programming problems for which the simplex method takes a number of steps exponential in the problem size.[13][18][19] In fact, for some time it was not known whether the linear programming problem was solvable in polynomial time, i.e. of complexity class P.
Like the simplex algorithm of Dantzig, the criss-cross algorithm is a basis-exchange algorithm that pivots between bases. However, the criss-cross algorithm need not maintain feasibility, but can pivot rather from a feasible basis to an infeasible basis. The criss-cross algorithm does not have polynomial time-complexity for linear programming. Both algorithms visit all 2D corners of a (perturbed) cube in dimension D, the Klee–Minty cube, in the worst case.[15][20]
In contrast to the simplex algorithm, which finds an optimal solution by traversing the edges between vertices on a polyhedral set, interior-point methods move through the interior of the feasible region.
This is the first worst-case polynomial-time algorithm ever found for linear programming.  To solve a problem which has n variables and can be encoded in L input bits, this algorithm runs in 



O
(

n

6


L
)


{\displaystyle O(n^{6}L)}

 time.[9] Leonid Khachiyan solved this long-standing complexity issue in 1979 with the introduction of the ellipsoid method. The convergence analysis has (real-number) predecessors, notably the iterative methods developed by Naum Z. Shor and the approximation algorithms by Arkadi Nemirovski and D. Yudin.
Khachiyan's algorithm was of landmark importance for establishing the polynomial-time solvability of linear programs.  The algorithm was not a computational break-through, as the simplex method is more efficient for all but specially constructed families of linear programs.
However, Khachiyan's algorithm inspired new lines of research in linear programming. In 1984, N. Karmarkar proposed a projective method for linear programming.  Karmarkar's algorithm[10] improved on Khachiyan's[9] worst-case polynomial bound (giving 



O
(

n

3.5


L
)


{\displaystyle O(n^{3.5}L)}

). Karmarkar claimed that his algorithm was much faster in practical LP than the simplex method, a claim that created great interest in interior-point methods.[21] Since Karmarkar's discovery, many interior-point methods have been proposed and analyzed.
In 1987, Vaidya proposed an algorithm that runs in 



O
(

n

3


)


{\displaystyle O(n^{3})}

 time.[22]
In 1989, Vaidya developed an algorithm that runs in 



O
(

n

2.5


)


{\displaystyle O(n^{2.5})}

 time.[23] Formally speaking, the algorithm takes 



O
(
(
n
+
d

)

1.5


n
L
)


{\displaystyle O((n+d)^{1.5}nL)}

 arithmetic operations in the worst case, where 



d


{\displaystyle d}

 is the number of constraints, 



n


{\displaystyle n}

 is the number of variables, and 



L


{\displaystyle L}

 is the number of bits.
In 2015, Lee and Sidford showed that linear programming can be solved in 






O
~



(
(
n
n
z
(
A
)
+

d

2


)


d


L
)


{\displaystyle {\tilde {O}}((nnz(A)+d^{2}){\sqrt {d}}L)}

 time,[24] where 






O
~





{\displaystyle {\tilde {O}}}

 denotes the soft O notation, and 



n
n
z
(
A
)


{\displaystyle nnz(A)}

 represents the number of non-zero elements, and it remains taking 



O
(

n

2.5


L
)


{\displaystyle O(n^{2.5}L)}

 in the worst case.
In 2019, Cohen, Lee and Song improved the running time to 






O
~



(
(

n

ω


+

n

2.5
−
α

/

2


+

n

2
+
1

/

6


)
L
)


{\displaystyle {\tilde {O}}((n^{\omega }+n^{2.5-\alpha /2}+n^{2+1/6})L)}

 time, 



ω


{\displaystyle \omega }

 is the exponent of matrix multiplication and 



α


{\displaystyle \alpha }

 is the dual exponent of matrix multiplication.[25] 



α


{\displaystyle \alpha }

 is (roughly) defined to be the largest number such that one can multiply an 



n
×
n


{\displaystyle n\times n}

 matrix by a 



n
×

n

α




{\displaystyle n\times n^{\alpha }}

 matrix in 



O
(

n

2


)


{\displaystyle O(n^{2})}

 time. In a followup work by Lee, Song and Zhang, they reproduce the same result via a different method.[26] These two algorithms remain 






O
~



(

n

2
+
1

/

6


L
)


{\displaystyle {\tilde {O}}(n^{2+1/6}L)}

 when 



ω
=
2


{\displaystyle \omega =2}

 and 



α
=
1


{\displaystyle \alpha =1}

. The result due to Jiang, Song, Weinstein and Zhang improved 






O
~



(

n

2
+
1

/

6


L
)


{\displaystyle {\tilde {O}}(n^{2+1/6}L)}

 to 






O
~



(

n

2
+
1

/

18


L
)


{\displaystyle {\tilde {O}}(n^{2+1/18}L)}

.[27]
The current opinion is that the efficiencies of good implementations of simplex-based methods and interior point methods are similar for routine applications of linear programming. However, for specific types of LP problems, it may be that one type of solver is better than another (sometimes much better), and that the structure of the solutions generated by interior point methods versus simplex-based methods are significantly different with the support set of active variables being typically smaller for the latter one.[28]
There are several open problems in the theory of linear programming, the solution of which would represent fundamental breakthroughs in mathematics and potentially major advances in our ability to solve large-scale linear programs.
This closely related set of problems has been cited by Stephen Smale as among the 18 greatest unsolved problems of the 21st century.  In Smale's words, the third version of the problem "is the main unsolved problem of linear programming theory."  While algorithms exist to solve linear programming in weakly polynomial time, such as the ellipsoid methods and interior-point techniques, no algorithms have yet been found that allow strongly polynomial-time performance in the number of constraints and the number of variables.  The development of such algorithms would be of great theoretical interest, and perhaps allow practical gains in solving large LPs as well.
These questions relate to the performance analysis and development of simplex-like methods.  The immense efficiency of the simplex algorithm in practice despite its exponential-time theoretical performance hints that there may be variations of simplex that run in polynomial or even strongly polynomial time.  It would be of great practical and theoretical significance to know whether any such variants exist, particularly as an approach to deciding if LP can be solved in strongly polynomial time.
The simplex algorithm and its variants fall in the family of edge-following algorithms, so named because they solve linear programming problems by moving from vertex to vertex along edges of a polytope.  This means that their theoretical performance is limited by the maximum number of edges between any two vertices on the LP polytope.  As a result, we are interested in knowing the maximum graph-theoretical diameter of polytopal graphs.  It has been proved that all polytopes have subexponential diameter. The recent disproof of the Hirsch conjecture is the first step to prove whether any polytope has superpolynomial diameter. If any such polytopes exist, then no edge-following variant can run in polynomial time. Questions about polytope diameter are of independent mathematical interest.
Simplex pivot methods preserve primal (or dual) feasibility.  On the other hand, criss-cross pivot methods do not preserve (primal or dual) feasibility – they may visit primal feasible, dual feasible or primal-and-dual infeasible bases in any order.  Pivot methods of this type have been studied since the 1970s.[29]  Essentially, these methods attempt to find the shortest pivot path on the arrangement polytope under the linear programming problem.  In contrast to polytopal graphs, graphs of arrangement polytopes are known to have small diameter, allowing the possibility of strongly polynomial-time criss-cross pivot algorithm without resolving questions about the diameter of general polytopes.[15]
If all of the unknown variables are required to be integers, then the problem is called an integer programming (IP) or integer linear programming (ILP) problem.  In contrast to linear programming, which can be solved efficiently in the worst case, integer programming problems are in many practical situations (those with bounded variables) NP-hard. 0–1 integer programming or binary integer programming (BIP) is the special case of integer programming where variables are required to be 0 or 1 (rather than arbitrary integers). This problem is also classified as NP-hard, and in fact the decision version was one of Karp's 21 NP-complete problems.
If only some of the unknown variables are required to be integers, then the problem is called a mixed integer (linear) programming (MIP or MILP) problem.  These are generally also NP-hard because they are even more general than ILP programs.
Integral linear programs are of central importance in the polyhedral aspect of combinatorial optimization since they provide an alternate characterization of a problem. Specifically, for any problem, the convex hull of the solutions is an integral polyhedron; if this polyhedron has a nice/compact description, then we can efficiently find the optimal feasible solution under any linear objective. Conversely, if we can prove that a linear programming relaxation is integral, then it is the desired description of the convex hull of feasible (integral) solutions.
MINTO (Mixed Integer Optimizer, an integer programming solver which uses branch and bound algorithm) has publicly available source code[33] but is not open source.
In mathematical optimization, Dantzig's simplex algorithm (or simplex method) is a popular algorithm for linear programming.[1]
The name of the algorithm is derived from the concept of a simplex and was suggested by T. S. Motzkin.[2] Simplices are not actually used in the method, but one interpretation of it is that it operates on simplicial cones, and these become proper simplices with an additional constraint.[3][4][5][6] The simplicial cones in question are the corners (i.e., the neighborhoods of the vertices) of a geometric object called a polytope. The shape of this polytope is defined by the constraints applied to the objective function.
After Dantzig included an objective function as part of his formulation during mid-1947, the problem was mathematically more tractable. Dantzig realized that one of the unsolved problems that he had mistaken as homework in his professor Jerzy Neyman's class (and actually later solved), was applicable to finding an algorithm for linear programs. This problem involved finding the existence of Lagrange multipliers for general linear programs over a continuum of variables, each bounded between zero and one, and satisfying linear constraints expressed in the form of Lebesgue integrals. Dantzig later published his "homework" as a thesis to earn his doctorate. The column geometry used in this thesis gave Dantzig insight that made him believe that the Simplex method would be very efficient.[9]
It can also be shown that, if an extreme point is not a maximum point of the objective function, then there is an edge containing the point so that the value of the objective function is strictly increasing on the edge moving away from the point.[12] If the edge is finite, then the edge connects to another extreme point where the objective function has a greater value, otherwise the objective function is unbounded above on the edge and the linear program has no solution. The simplex algorithm applies this insight by walking along edges of the polytope to extreme points with greater and greater objective values. This continues until the maximum value is reached, or an unbounded edge is visited (concluding that the problem has no solution). The algorithm always terminates because the number of vertices in the polytope is finite; moreover since we jump between vertices always in the same direction (that of the objective function), we hope that the number of vertices visited will be small.[12]
The solution of a linear program is accomplished in two steps. In the first step, known as Phase I, a starting extreme point is found. Depending on the nature of the program this may be trivial, but in general it can be solved by applying the simplex algorithm to a modified version of the original program. The possible results of Phase I are either that a basic feasible solution is found or that the feasible region is empty. In the latter case the linear program is called infeasible. In the second step, Phase II, the simplex algorithm is applied using the basic feasible solution found in Phase I as a starting point. The possible results from Phase II are either an optimum basic feasible solution or an infinite edge on which the objective function is unbounded above.[13][14][15]
Let a linear program be given by a canonical tableau. The simplex algorithm proceeds by performing successive pivot operations each of which give an improved basic feasible solution; the choice of pivot element at each step is largely determined by the requirement that this pivot improves the solution.
If there is more than one column so that the entry in the objective row is positive then the choice of which one to add to the set of basic variables is somewhat arbitrary and several entering variable choice rules[20] such as Devex algorithm[21] have been developed.
By changing the entering variable choice rule so that it selects a column where the entry in the objective row is negative, the algorithm is changed so that it finds the minimum of the objective function rather than the maximum.
In general, a linear program will not be given in the canonical form and an equivalent canonical tableau must be found before the simplex algorithm can start. This can be accomplished by the introduction of artificial variables. Columns of the identity matrix are added as column vectors for these variables. If the b value for a constraint equation is negative, the equation is negated before adding the identity matrix columns.  This does not change the set of feasible solutions or the optimal solution, and it ensures that the slack variables will constitute an initial feasible solution.  The new tableau is in canonical form but it is not equivalent to the original problem. So a new objective function, equal to the sum of the artificial variables, is introduced and the simplex algorithm is applied to find the minimum; the modified linear program is called the Phase I problem.[23]
The simplex algorithm applied to the Phase I problem must terminate with a minimum value for the new objective function since, being the sum of nonnegative variables, its value is bounded below by 0. If the minimum is 0 then the artificial variables can be eliminated from the resulting canonical tableau producing a canonical tableau equivalent to the original problem. The simplex algorithm can then be applied to find the solution; this step is called Phase II. If the minimum is positive then there is no feasible solution for the Phase I problem where the artificial variables are all zero. This implies that the feasible region for the original problem is empty, and so the original problem has no solution.[13][14][24]
The tableau form used above to describe the algorithm lends itself to an immediate implementation in which the tableau is maintained as a rectangular (m + 1)-by-(m + n + 1) array. It is straightforward to avoid storing the m explicit columns of the identity matrix that will occur within the tableau by virtue of B being a subset of the columns of [A, I]. This implementation is referred to as the "standard simplex algorithm". The storage and computation overhead is such that the standard simplex method is a prohibitively expensive approach to solving large linear programming problems.
In each simplex iteration, the only data required are the first row of the tableau, the (pivotal) column of the tableau corresponding to the entering variable and the right-hand-side. The latter can be updated using the pivotal column and the first row of the tableau can be updated using the (pivotal) row corresponding to the leaving variable. Both the pivotal column and pivotal row may be computed directly using the solutions of linear systems of equations involving the matrix B and a matrix-vector product using A. These observations motivate the "revised simplex algorithm", for which implementations are distinguished by their invertible representation of B.[25]
In large linear-programming problems A is typically a sparse matrix and, when the resulting sparsity of B is exploited when maintaining its invertible representation, the revised simplex algorithm is much more efficient than the standard simplex method. Commercial simplex solvers are based on the revised simplex algorithm.[24][25][26][27][28]
If the values of all basic variables are strictly positive, then a pivot must result in an improvement in the objective value. When this is always the case no set of basic variables occurs twice and the simplex algorithm must terminate after a finite number of steps. Basic feasible solutions where at least one of the basic variables is zero are called degenerate and may result in pivots for which there is no improvement in the objective value. In this case there is no actual change in the solution but only a change in the set of basic variables. When several such pivots occur in succession, there is no improvement; in large industrial applications, degeneracy is common and such "stalling" is notable. 
Worse than stalling is the possibility the same set of basic variables occurs twice, in which case, the deterministic pivoting rules of the simplex algorithm will produce an infinite loop, or "cycle". While degeneracy is the rule in practice and stalling is common, cycling is rare in practice. A discussion of an example of practical cycling occurs in Padberg.[24] Bland's rule  prevents cycling and thus guarantees that the simplex algorithm always terminates.[24][29][30] Another pivoting algorithm, the criss-cross algorithm never cycles on linear programs.[31]
In 2014, it was proved[citation needed] that a particular variant of the simplex method is NP-mighty, i.e., it can be used to solve, with polynomial overhead, any problem in NP implicitly during the algorithm's execution.  Moreover, deciding whether a given variable ever enters the basis during the algorithm's execution on a given input, and determining the number of iterations needed for solving a given problem, are both NP-hard problems.[34] At about the same time it was shown that there exists an artificial pivot rule for which computing its output is PSPACE-complete.[35] In 2015, this was strengthened to show that computing the output of Dantzig's pivot rule is PSPACE-complete.[36]
Analyzing and quantifying the observation that the simplex algorithm is efficient in practice despite its exponential worst-case complexity has led to the development of other measures of complexity. The simplex algorithm has polynomial-time average-case complexity under various probability distributions, with the precise average-case performance of the simplex algorithm depending on the choice of a probability distribution for the random matrices.[37][38] Another approach to studying "typical phenomena" uses Baire category theory from general topology, and to show that (topologically) "most" matrices can be solved by the simplex algorithm in a polynomial number of steps.[citation needed]
Another method to analyze the performance of the simplex algorithm studies the behavior of worst-case scenarios under small perturbation – are worst-case scenarios stable under a small change (in the sense of structural stability), or do they become tractable? This area of research, called smoothed analysis, was introduced specifically to study the simplex method. Indeed, the running time of the simplex method on input with noise is polynomial in the number of variables and the magnitude of the perturbations.[39][40]
Other algorithms for solving linear-programming problems are described in the linear-programming article. Another basis-exchange pivoting algorithm is the criss-cross algorithm.[41][42] There are polynomial-time algorithms for linear programming that use interior point methods: these include Khachiyan's ellipsoidal algorithm, Karmarkar's projective algorithm, and path-following algorithms.[15] The Big-M method is an alternative strategy for solving a linear program, using a single-phase simplex.
Linear–fractional programming (LFP) is a generalization of linear programming (LP). In LP the objective function is a  linear function, while the objective function of a linear–fractional program is a ratio of two linear functions. In other words, a linear program is a fractional–linear program in which the denominator is the constant function having the value one everywhere. A linear–fractional program can be solved by a variant of the simplex algorithm[43][44][45][46] or by the criss-cross algorithm.[47]
An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. In many settings the term refers to integer linear programming (ILP), in which the objective function and the constraints (other than the integer constraints) are linear.
Integer programming is NP-complete. In particular, the special case of 0–1 integer linear programming, in which unknowns are binary, and only the restrictions must be satisfied, is one of Karp's 21 NP-complete problems.[1]
If some decision variables are not discrete, the problem is known as a mixed-integer programming problem.[2]
In integer linear programming, the canonical form is distinct from the standard form. An integer linear program in canonical form is expressed thus (note that it is the 




x



{\displaystyle \mathbf {x} }

 vector which is to be decided):[3]
The following is a reduction from minimum vertex cover to integer programming that will serve as the proof of NP-hardness.
Mixed-integer linear programming (MILP) involves problems in which only some of the variables, 




x

i




{\displaystyle x_{i}}

, are constrained to be integers, while other variables are allowed to be non-integers.
Zero–one linear programming (or binary integer programming) involves problems in which the variables are restricted to be either 0 or 1. Any bounded integer variable can be expressed as a combination of binary variables.[5] For example, given an integer variable, 



0
≤
x
≤
U


{\displaystyle 0\leq x\leq U}

, the variable can be expressed using 



⌊

log

2


⁡
U
⌋
+
1


{\displaystyle \lfloor \log _{2}U\rfloor +1}

 binary variables:
These considerations occur frequently in practice and so integer linear programming can be used in many applications areas, some of which are briefly described below.
Mixed-integer programming has many applications in industrial productions, including job-shop modelling. One important example happens in agricultural production planning and involves determining production yield for several crops that can share resources (e.g. land, labor, capital, seeds, fertilizer, etc.). A possible objective is to maximize the total production, without exceeding the available resources. In some cases, this can be expressed in terms of a linear program, but the variables must be constrained to be integer.
These problems involve service and vehicle scheduling in transportation networks.  For example, a problem may involve assigning buses or subways to individual routes so that a timetable can be met, and also to equip them with drivers. Here binary decision variables indicate whether a bus or subway is assigned to a route and whether a driver is assigned to a particular train or subway. The zero–one programming technique has been successfully applied to solve a project selection problem in which projects are mutually exclusive and/or technologically interdependent. It is used in a special case of integer programming, in which all the decision variables are integers. Variable can assume only the values zero or one.
While in general the solution to LP relaxation will not be guaranteed to be integral, if the ILP has the form 



max


c



T




x



{\displaystyle \max \mathbf {c} ^{\mathrm {T} }\mathbf {x} }

 such that 



A

x

=

b



{\displaystyle A\mathbf {x} =\mathbf {b} }

 where 



A


{\displaystyle A}

 and 




b



{\displaystyle \mathbf {b} }

 have all integer entries and 



A


{\displaystyle A}

 is totally unimodular, then every basic feasible solution is integral.  Consequently, the solution returned by the simplex algorithm is guaranteed to be integral.  To show that every basic feasible solution is integral, let 




x



{\displaystyle \mathbf {x} }

 be an arbitrary basic feasible solution . Since 




x



{\displaystyle \mathbf {x} }

 is feasible,
we know that 



A

x

=

b



{\displaystyle A\mathbf {x} =\mathbf {b} }

. Let 





x


0


=
[

x


n

1




,

x


n

2




,
⋯
,

x


n

j




]


{\displaystyle \mathbf {x} _{0}=[x_{n_{1}},x_{n_{2}},\cdots ,x_{n_{j}}]}

 be the elements corresponding to the basis columns for the basic solution 




x



{\displaystyle \mathbf {x} }

. By definition of a basis, there is some square submatrix 



B


{\displaystyle B}

 of




A


{\displaystyle A}

 with linearly independent columns such that 



B


x


0


=

b



{\displaystyle B\mathbf {x} _{0}=\mathbf {b} }

.
Since the columns of 



B


{\displaystyle B}

 are linearly independent and 



B


{\displaystyle B}

 is square, 



B


{\displaystyle B}

 is nonsingular,
and therefore by assumption, 



B


{\displaystyle B}

 is unimodular and so 



det
(
B
)
=
±
1


{\displaystyle \det(B)=\pm 1}

. Also, since 



B


{\displaystyle B}

 is nonsingular, it is invertible and therefore 





x


0


=

B

−
1



b



{\displaystyle \mathbf {x} _{0}=B^{-1}\mathbf {b} }

. By definition, 




B

−
1


=



B


a
d
j




det
(
B
)



=
±

B


a
d
j





{\displaystyle B^{-1}={\frac {B^{\mathrm {adj} }}{\det(B)}}=\pm B^{\mathrm {adj} }}

. Here 




B


a
d
j





{\displaystyle B^{\mathrm {adj} }}

 denotes the adjugate of 



B


{\displaystyle B}

 and is integral because 



B


{\displaystyle B}

 is integral.  Therefore,










⇒

B

−
1


=
±

B


a
d
j




 is integral.







⇒


x


0


=

B

−
1


b

 is integral.







⇒

Every basic feasible solution is integral.







{\displaystyle {\begin{aligned}&\Rightarrow B^{-1}=\pm B^{\mathrm {adj} }{\text{ is integral.}}\\&\Rightarrow \mathbf {x} _{0}=B^{-1}b{\text{ is integral.}}\\&\Rightarrow {\text{Every basic feasible solution is integral.}}\end{aligned}}}


Thus, if the matrix 



A


{\displaystyle A}

 of an ILP is totally unimodular, rather than use an ILP algorithm, the simplex method can be used to solve the LP relaxation and the solution will be integer.
When the matrix 



A


{\displaystyle A}

 is not totally unimodular, there are a variety of algorithms that can be used to solve integer linear programs exactly.  One class of algorithms are cutting plane methods, which work by solving the LP relaxation and then adding linear constraints that drive the solution towards being integer without excluding any integer feasible points.
Another class of algorithms are variants of the branch and bound method.  For example, the branch and cut method that combines both branch and bound and cutting plane methods.  Branch and bound algorithms have a number of advantages over algorithms that only use cutting planes.  One advantage is that the algorithms can be terminated early and as long as at least one integral solution has been found, a feasible, although not necessarily optimal, solution can be returned.  Further, the solutions of the LP relaxations can be used to provide a worst-case estimate of how far from optimality the returned solution is.  Finally, branch and bound methods can be used to return multiple optimal solutions.
Let V be the maximum absolute value of the coefficients in 



A


{\displaystyle A}

  and 




b



{\displaystyle \mathbf {b} }

. If n (the number of variables) is a fixed constant, then the feasibility problem can be solved in time polynomial in m and log V. This is trivial for the case n=1. The case n=2 was solved in 1981 by Herbert Scarf.[13] The general case was solved in 1983 by Hendrik Lenstra, combining ideas by László Lovász and Peter van Emde Boas.[14] Doignon's theorem asserts that an integer program is feasible whenever every subset of 




2

n




{\displaystyle 2^{n}}

 constraints is feasible; a method combining this result with algorithms for LP-type problems can be used to solve integer programs in time that is linear in 



m


{\displaystyle m}

 and fixed-parameter tractable (FPT) in 



n


{\displaystyle n}

, but possibly doubly exponential in 



n


{\displaystyle n}

, with no dependence on 



V


{\displaystyle V}

.[15]
In the special case of 0-1 ILP, Lenstra's algorithm is equivalent to complete enumeration: the number of all possible solutions is fixed (2n), and checking the feasibility of each solution can be done in time poly(m, log V). In the general case, where each variable can be an arbitrary integer, complete enumeration is impossible. Here, Lenstra's algorithm uses ideas from Geometry of numbers. It transforms the original problem into an equivalent one with the following property: either the existence of a solution 




x



{\displaystyle \mathbf {x} }

 is obvious, or the value of 




x

n




{\displaystyle x_{n}}

 (the n-th variable) belongs to an interval whose length is bounded by a function of n. In the latter case, the problem is reduced to a bounded number of lower-dimensional problems. The run-time complexity of the algorithm has been improved in several steps:
These algorithms can also be used for mixed integer linear programs (MILP) - programs in which some variables are integer and some variables are real.[23] The original algorithm of Lenstra[14]: Sec.5  has run-time 




2

O
(

n

3


)


⋅
p
o
l
y
(
d
,
L
)


{\displaystyle 2^{O(n^{3})}\cdot poly(d,L)}

, where n is the number of integer variables, d is the number of continuous variables, and L is the binary encoding size of the problem. Using techniques from later algorithms, the factor 




2

O
(

n

3


)




{\displaystyle 2^{O(n^{3})}}

 can be improved to 




2

O
(
n
log
⁡
n
)




{\displaystyle 2^{O(n\log n)}}

 or to 




n

n




{\displaystyle n^{n}}

.[23]
Since integer linear programming is NP-hard, many problem instances are intractable and so heuristic methods must be used instead.  For example, tabu search can be used to search for solutions to ILPs.[24]  To use tabu search to solve ILPs, moves can be defined as incrementing or decrementing an integer constrained variable of a feasible solution while keeping all other integer-constrained variables constant.  The unrestricted variables are then solved for. Short-term memory can consist of previously tried solutions while medium-term memory can consist of values for the integer constrained variables that have resulted in high objective values (assuming the ILP is a maximization problem). Finally, long-term memory can guide the search towards integer values that have not previously been tried.
There are also a variety of other problem-specific heuristics, such as the k-opt heuristic for the traveling salesman problem. A disadvantage of heuristic methods is that if they fail to find a solution, it cannot be determined whether it is because there is no feasible solution or whether the algorithm simply was unable to find one.  Further, it is usually impossible to quantify how close to optimal a solution returned by these methods are.
It is often the case that the matrix 



A


{\displaystyle A}

 that defines the integer program is sparse. In particular, this occurs when the matrix has a block structure, which is the case in many applications. The sparsity of the matrix can be measured as follows. The graph of 



A


{\displaystyle A}

 has vertices corresponding to columns of 



A


{\displaystyle A}

, and two columns form an edge if 



A


{\displaystyle A}

 has a row where both columns have nonzero entries. Equivalently, the vertices correspond to variables, and two variables form an edge if they share an inequality. The sparsity measure 



d


{\displaystyle d}

 of 



A


{\displaystyle A}

 is the minimum of the tree-depth of the graph of 



A


{\displaystyle A}

 and the tree-depth of the graph of the transpose of 



A


{\displaystyle A}

. Let 



a


{\displaystyle a}

 be the numeric measure of 



A


{\displaystyle A}

 defined as the maximum absolute value of any entry of 



A


{\displaystyle A}

. Let 



n


{\displaystyle n}

 be the number of variables of the integer program. Then it was shown in 2018[25] that integer programming can be solved in strongly polynomial and fixed-parameter tractable time parameterized by 



a


{\displaystyle a}

 and 



d


{\displaystyle d}

. That is, for some computable function 



f


{\displaystyle f}

 and some constant 



k


{\displaystyle k}

, integer programming can be solved in time 



f
(
a
,
d
)

n

k




{\displaystyle f(a,d)n^{k}}

. In particular, the time is independent of the right-hand side 



b


{\displaystyle b}

 and objective function 



c


{\displaystyle c}

. Moreover, in contrast to the classical result of Lenstra, where the number 



n


{\displaystyle n}

 of variables is a parameter, here the number 



n


{\displaystyle n}

 of variables is a variable part of the input.
In computer science, the Floyd–Warshall algorithm (also known as Floyd's algorithm, the Roy–Warshall algorithm, the Roy–Floyd algorithm, or the WFI algorithm) is an algorithm for finding shortest paths in a directed weighted graph with positive or negative edge weights (but with no negative cycles).[1][2] A single execution of the algorithm will find the lengths (summed weights) of shortest paths between all pairs of vertices. Although it does not return details of the paths themselves, it is possible to reconstruct the paths with simple modifications to the algorithm. Versions of the algorithm can also be used for finding the transitive closure of a relation 



R


{\displaystyle R}

, or (in connection with the Schulze voting system) widest paths between all pairs of vertices in a weighted graph.
The Floyd–Warshall algorithm is an example of dynamic programming, and was published in its currently recognized form by Robert Floyd in 1962.[3]  However, it is essentially the same as algorithms previously published by Bernard Roy in 1959[4] and also by Stephen Warshall in 1962[5] for finding the transitive closure of a graph,[6] and is closely related to Kleene's algorithm (published in 1956) for converting a deterministic finite automaton into a regular expression, with the difference being the use of a min-plus semiring.[7] The modern formulation of the algorithm as three nested for-loops was first described by Peter Ingerman, also in 1962.[8]
The Floyd–Warshall algorithm compares many possible paths through the graph between each pair of vertices. It is guaranteed to find all shortest paths and is able to do this with 



Θ
(

|

V


|


3


)


{\displaystyle \Theta (|V|^{3})}

 comparisons in a graph,[1][9] even though there may be 



Θ
(

|

V


|


2


)


{\displaystyle \Theta (|V|^{2})}

 edges in the graph.  It does so by incrementally improving an estimate on the shortest path between two vertices, until the estimate is optimal.
These formulas are the heart of the Floyd–Warshall algorithm. The algorithm works by first computing 




s
h
o
r
t
e
s
t
P
a
t
h

(
i
,
j
,
k
)


{\displaystyle \mathrm {shortestPath} (i,j,k)}

 for all 



(
i
,
j
)


{\displaystyle (i,j)}

 pairs for 



k
=
0


{\displaystyle k=0}

, then 



k
=
1


{\displaystyle k=1}

, then 



k
=
2


{\displaystyle k=2}

, and so on.  This process continues until 



k
=
N


{\displaystyle k=N}

, and we have found the shortest path for all 



(
i
,
j
)


{\displaystyle (i,j)}

 pairs using any intermediate vertices. Pseudocode for this basic version follows.
A negative cycle is a cycle whose edges sum to a negative value.  There is no shortest path between any pair of vertices 



i


{\displaystyle i}

, 



j


{\displaystyle j}

 which form part of a negative cycle,  because path-lengths from 



i


{\displaystyle i}

 to 



j


{\displaystyle j}

 can be arbitrarily small (negative).  For numerically meaningful output, the Floyd–Warshall algorithm assumes that there are no negative cycles.  Nevertheless, if there are negative cycles, the Floyd–Warshall algorithm can be used to detect them.  The intuition is as follows:
Hence, to detect negative cycles using the Floyd–Warshall algorithm, one can inspect the diagonal of the path matrix, and the presence of a negative number indicates that the graph contains at least one negative cycle.[9] During the execution of the algorithm, if there is a negative cycle, exponentially large numbers can appear, as large as 



Ω
(
⋅

6

n
−
1



w

m
a
x


)


{\displaystyle \Omega (\cdot 6^{n-1}w_{max})}

, where 




w

m
a
x




{\displaystyle w_{max}}

 is the largest absolute value of a negative edge in the graph. To avoid overflow/underflow problems one should check for negative numbers on the diagonal of the path matrix within the inner for loop of the algorithm.[10] Obviously, in an undirected graph a negative edge creates a negative cycle  (i.e., a closed walk) involving its incident vertices. Considering all edges of the above example graph as undirected, e.g. the vertex sequence 4 – 2 – 4 is a cycle with weight sum −2.
The Floyd–Warshall algorithm typically only provides the lengths of the paths between all pairs of vertices. With simple modifications, it is possible to create a method to reconstruct the actual path between any two endpoint vertices. While one may be inclined to store the actual path from each vertex to each other vertex, this is not necessary, and in fact, is very costly in terms of memory. Instead, we can use the shortest-path tree, which can be calculated for each node in 



Θ
(

|

E

|

)


{\displaystyle \Theta (|E|)}

 time using 



Θ
(

|

V

|

)


{\displaystyle \Theta (|V|)}

 memory, and allows us to efficiently reconstruct a directed path between any two connected vertices.
Let 



n


{\displaystyle n}

 be 




|

V

|



{\displaystyle |V|}

, the number of vertices. To find all 




n

2




{\displaystyle n^{2}}

 of 





s
h
o
r
t
e
s
t
P
a
t
h

(
i
,
j
,
k
)


{\displaystyle \mathrm {shortestPath} (i,j,k)}

 (for all 



i


{\displaystyle i}

 and 



j


{\displaystyle j}

) from those of





s
h
o
r
t
e
s
t
P
a
t
h

(
i
,
j
,
k
−
1
)


{\displaystyle \mathrm {shortestPath} (i,j,k-1)}

 requires 



Θ
(

n

2


)


{\displaystyle \Theta (n^{2})}

 operations. Since we begin with





s
h
o
r
t
e
s
t
P
a
t
h

(
i
,
j
,
0
)
=

e
d
g
e
C
o
s
t

(
i
,
j
)


{\displaystyle \mathrm {shortestPath} (i,j,0)=\mathrm {edgeCost} (i,j)}

 and compute the sequence of 



n


{\displaystyle n}

 matrices 




s
h
o
r
t
e
s
t
P
a
t
h

(
i
,
j
,
1
)


{\displaystyle \mathrm {shortestPath} (i,j,1)}

, 




s
h
o
r
t
e
s
t
P
a
t
h

(
i
,
j
,
2
)


{\displaystyle \mathrm {shortestPath} (i,j,2)}

, 



…


{\displaystyle \ldots }

, 




s
h
o
r
t
e
s
t
P
a
t
h

(
i
,
j
,
n
)


{\displaystyle \mathrm {shortestPath} (i,j,n)}

, each having a cost of 



Θ
(

n

2


)


{\displaystyle \Theta (n^{2})}

,
the total time complexity of the algorithm is 



n
⋅
Θ
(

n

2


)
=
Θ
(

n

3


)


{\displaystyle n\cdot \Theta (n^{2})=\Theta (n^{3})}

.[9][12]
For graphs with non-negative edge weights, Dijkstra's algorithm can be used to find all shortest paths from a single vertex with running time 



Θ
(

|

E

|

+

|

V

|

log
⁡

|

V

|

)


{\displaystyle \Theta (|E|+|V|\log |V|)}

. Thus, running Dijkstra starting at each vertex takes time 



Θ
(

|

E

|


|

V

|

+

|

V


|


2


log
⁡

|

V

|

)


{\displaystyle \Theta (|E||V|+|V|^{2}\log |V|)}

. Since 




|

E

|

=
O
(

|

V


|


2


)


{\displaystyle |E|=O(|V|^{2})}

, this yields a worst-case running time of repeated Dijkstra of 



O
(

|

V


|


3


)


{\displaystyle O(|V|^{3})}

. While this matches the asymptotic worst-case running time of the Floyd-Warshall algorithm, the constants involved matter quite a lot. When a graph is dense (i.e., 




|

E

|

≈

|

V


|


2




{\displaystyle |E|\approx |V|^{2}}

), the Floyd-Warshall algorithm tends to perform better in practice. When the graph is sparse (i.e., 




|

E

|



{\displaystyle |E|}

 is significantly smaller than 




|

V


|


2




{\displaystyle |V|^{2}}

), Dijkstra tends to dominate.
For sparse graphs with negative edges but no negative cycles, Johnson's algorithm can be used, with the same asymptotic running time as the repeated Dijkstra approach.
There are also known algorithms using fast matrix multiplication to speed up all-pairs shortest path computation in dense graphs, but these typically make extra assumptions on the edge weights (such as requiring them to be small integers).[16][17] In addition, because of the high constant factors in their running time, they would only provide a speedup over the Floyd–Warshall algorithm for very large graphs.
A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage.[1] In many problems, a greedy strategy does not produce an optimal solution, but a greedy heuristic can yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.
In mathematical optimization, greedy algorithms optimally solve combinatorial problems having the properties of matroids and give constant-factor approximations to optimization problems with the submodular structure.
Greedy algorithms produce good solutions on some mathematical problems, but not on others. Most problems for which they work will  have two properties:
A common technique for proving the correctness of greedy algorithms uses an inductive exchange argument.[3] The exchange argument demonstrates that any solution different from the greedy solution can be transformed into the greedy solution without degrading its quality. This proof pattern typically follows these steps:
Greedy algorithms fail to produce the optimal solution for many other problems and may even produce the unique worst possible solution. One example is the travelling salesman problem mentioned above: for each number of cities, there is an assignment of distances between the cities for which the nearest-neighbour heuristic produces the unique worst possible tour.[4]
For other possible examples, see horizon effect.
Greedy algorithms can be characterized as being 'short sighted', and also as 'non-recoverable'. They are ideal only for problems that have an 'optimal substructure'. Despite this, for many simple problems, the best-suited algorithms are greedy. It is important, however, to note that the greedy algorithm can be used as a selection algorithm to prioritize options within a search, or branch-and-bound algorithm. There are a few variations to the greedy algorithm:[5]
Greedy algorithms have a long history of study in combinatorial optimization and theoretical computer science. Greedy heuristics are known to produce suboptimal results on many problems,[6] and so natural questions are:
A matroid is a mathematical structure that generalizes the notion of linear independence from vector spaces to arbitrary sets. If an optimization problem has the structure of a matroid, then the appropriate greedy algorithm will solve it optimally.[7]
Suppose one wants to find a set 



S


{\displaystyle S}

 which maximizes 



f


{\displaystyle f}

. The greedy algorithm, which builds up a set 



S


{\displaystyle S}

 by incrementally adding the element which increases 



f


{\displaystyle f}

 the most at each step, produces as output a set that is at least 



(
1
−
1

/

e
)

max

X
⊆
Ω


f
(
X
)


{\displaystyle (1-1/e)\max _{X\subseteq \Omega }f(X)}

.[8] That is, greedy performs within a constant factor of 



(
1
−
1

/

e
)
≈
0.63


{\displaystyle (1-1/e)\approx 0.63}

 as good as the optimal solution.
Similar guarantees are provable when additional constraints, such as cardinality constraints,[9] are imposed on the output, though often slight variations on the greedy algorithm are required. See [10] for an overview.
Other problems for which the greedy algorithm gives a strong guarantee, but not an optimal solution, include
Many of these problems have matching lower bounds; i.e., the greedy algorithm does not perform better than the guarantee in the worst case.
Greedy algorithms typically (but not always) fail to find the globally optimal solution because they usually do not operate exhaustively on all the data. They can make commitments to certain choices too early, preventing them from finding the best overall solution later. For example, all known greedy coloring algorithms for the graph coloring problem and all other NP-complete problems do not consistently find optimum solutions. Nevertheless, they are useful because they are quick to think up and often give good approximations to the optimum.
If a greedy algorithm can be proven to yield the global optimum for a given problem class, it typically becomes the method of choice because it is faster than other optimization methods like dynamic programming. Examples of such greedy algorithms are Kruskal's algorithm and Prim's algorithm for finding minimum spanning trees and the algorithm for finding optimum Huffman trees.
Greedy algorithms appear in network routing as well.  Using greedy routing, a message is forwarded to the neighbouring node which is "closest" to the destination. The notion of a node's location (and hence "closeness") may be determined by its physical location, as in geographic routing used by ad hoc networks.  Location may also be an entirely artificial construct as in small world routing and distributed hash table.
Kruskal's algorithm[1] finds a minimum spanning forest of an undirected edge-weighted graph. If the graph is connected, it finds a minimum spanning tree. It is a greedy algorithm that in each step adds to the forest the lowest-weight edge that will not form a cycle.[2] The key steps of the algorithm are sorting and the use of a disjoint-set data structure to detect cycles. Its running time is dominated by the time to sort all of the graph edges by their weight.
This algorithm was first published by Joseph Kruskal in 1956,[3] and was rediscovered soon afterward by Loberman & Weinberger (1957).[4] Other algorithms for this problem include Prim's algorithm, Borůvka's algorithm, and the reverse-delete algorithm.
At the termination of the algorithm, the forest forms a minimum spanning forest of the graph. If the graph is connected, the forest has a single component and forms a minimum spanning tree.
The following code is implemented with a disjoint-set data structure. It represents the forest F as a set of undirected edges, and uses the disjoint-set data structure to efficiently determine whether two vertices are part of the same tree.
For a graph with E edges and V vertices, Kruskal's algorithm can be shown to run in time O(E log E) time, with simple data structures. This time bound is often written instead as O(E log V), which is equivalent for graphs with no isolated vertices, because for these graphs V/2 ≤ E < V2 and the logarithms of V and E are again within a constant factor of each other.
To achieve this bound, first sort the edges by weight using a comparison sort in O(E log E) time. Once sorted, it is possible to loop through the edges in sorted order in constant time per edge. Next, use a disjoint-set data structure, with a set of vertices for each component, to keep track of which vertices are in which components. Creating this structure, with a separate set for each vertex, takes V operations and O(V) time. The final iteration through all edges performs two find operations and possibly one union operation per edge. These operations take amortized time O(α(V)) time per operation, giving worst-case total time O(E α(V)) for this loop, where α is the extremely slowly growing inverse Ackermann function. This part of the time bound is much smaller than the time for the sorting step, so the total time for the algorithm can be simplified to the time for the sorting step.
In cases where the edges are already sorted, or where they have small enough integer weight to allow integer sorting algorithms such as counting sort or radix sort to sort them in linear time, the disjoint set operations are the slowest remaining part of the algorithm and the total time is O(E α(V)).
The proof consists of two parts. First, it is proved that the algorithm produces a spanning tree. Second, it is proved that the constructed spanning tree is of minimal weight.
Let 



G


{\displaystyle G}

 be a connected, weighted graph and let 



Y


{\displaystyle Y}

 be the subgraph of 



G


{\displaystyle G}

 produced by the algorithm. 



Y


{\displaystyle Y}

 cannot have a cycle, as by definition an edge is not added if it results in a cycle. 



Y


{\displaystyle Y}

 cannot be disconnected, since the first encountered edge that joins two components of 



Y


{\displaystyle Y}

 would have been added by the algorithm. Thus, 



Y


{\displaystyle Y}

 is a spanning tree of 



G


{\displaystyle G}

.
We show that the following proposition P is true by induction: If F is the set of edges chosen at any stage of the algorithm, then there is some minimum spanning tree that contains F and none of the edges rejected by the algorithm.
Kruskal's algorithm is inherently sequential and hard to parallelize. It is, however, possible to perform the initial sorting of the edges in parallel or, alternatively, to use a parallel implementation of a binary heap to extract the minimum-weight edge in every iteration.[5]
As parallel sorting is possible in time 



O
(
n
)


{\displaystyle O(n)}

 on 



O
(
log
⁡
n
)


{\displaystyle O(\log n)}

 processors,[6] the runtime of Kruskal's algorithm can be reduced to O(E α(V)), where α again is the inverse of the single-valued Ackermann function.
A variant of Kruskal's algorithm, named Filter-Kruskal, has been described by Osipov et al.[7] and is better suited for parallelization. The basic idea behind Filter-Kruskal is to partition the edges in a similar way to quicksort and filter out edges that connect vertices of the same tree to reduce the cost of sorting. The following pseudocode demonstrates this.
Finally, other variants of a parallel implementation of Kruskal's algorithm have been explored. Examples include a scheme that uses helper threads to remove edges that are definitely not part of the MST in the background,[8] and a variant which runs the sequential algorithm on p subgraphs, then merges those subgraphs until only one, the final MST, remains.[9]
In computer science, Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex.
The algorithm was developed in 1930 by Czech mathematician Vojtěch Jarník[1] and later rediscovered and republished by computer scientists Robert C. Prim in 1957[2] and Edsger W. Dijkstra in 1959.[3] Therefore, it is also sometimes called the Jarník's algorithm,[4] Prim–Jarník algorithm,[5] Prim–Dijkstra algorithm[6]
or the DJP algorithm.[7]
Other well-known algorithms for this problem include Kruskal's algorithm and Borůvka's algorithm.[8] These algorithms find the minimum spanning forest in a possibly disconnected graph; in contrast, the most basic form of Prim's algorithm only finds minimum spanning trees in connected graphs. However, running Prim's algorithm separately for each connected component of the graph, it can also be used to find the minimum spanning forest.[9] In terms of their asymptotic time complexity, these three algorithms are equally fast for sparse graphs, but slower than other more sophisticated algorithms.[7][6]
However, for graphs that are sufficiently dense, Prim's algorithm can be made to run in linear time, meeting or improving the time bounds for other algorithms.[10]
As described above, the starting vertex for the algorithm will be chosen arbitrarily, because the first iteration of the main loop of the algorithm will have a set of vertices in Q that all have equal weights, and the algorithm will automatically start a new tree in F when it completes a spanning tree of each connected component of the input graph. The algorithm may be modified to start with any particular vertex s by setting C[s] to be a number smaller than the other values of C (for instance, zero), and it may be modified to only find a single spanning tree rather than an entire spanning forest (matching more closely the informal description) by stopping whenever it encounters another vertex flagged as having no associated edge.
Different variations of the algorithm differ from each other in how the set Q is implemented: as a simple linked list or array of vertices, or as a more complicated priority queue data structure. This choice leads to differences in the time complexity of the algorithm. In general, a priority queue will be quicker at finding the vertex v with minimum cost, but will entail more expensive updates when the value of C[w] changes.
The time complexity of Prim's algorithm depends on the data structures used for the graph and for ordering the edges by weight, which can be done using a priority queue. The following table shows the typical choices:
A simple implementation of Prim's, using an adjacency matrix or an adjacency list graph representation and linearly searching an array of weights to find the minimum weight edge to add, requires O(|V|2) running time. However, this running time can be greatly improved by using heaps to implement finding minimum weight edges in the algorithm's inner loop.
Using a simple binary heap data structure, Prim's algorithm can now be shown to run in time O(|E| log |V|) where |E| is the number of edges and |V| is the number of vertices. Using a more sophisticated Fibonacci heap, this can be brought down to O(|E| + |V| log |V|), which is asymptotically faster when the graph is dense enough that |E| is ω(|V|), and linear time when |E| is at least |V| log |V|. For graphs of even greater density (having at least |V|c edges for some c > 1), Prim's algorithm can be made to run in linear time even more simply, by using a d-ary heap in place of a Fibonacci heap.[10][11]
Let P be a connected, weighted graph.  At every iteration of Prim's algorithm, an edge must be found that connects a vertex in a subgraph to a vertex outside the subgraph.  Since  P is connected, there will always be a path to every vertex.  The output Y of Prim's algorithm is a tree, because the edge and vertex added to tree Y are connected. Let Y1 be a minimum spanning tree of graph P. If Y1=Y then Y is a minimum spanning tree. Otherwise, let e be the first edge added during the construction of tree Y that is not in tree Y1, and V be the set of vertices connected by the edges added before edge e.  Then one endpoint of edge e is in set V and the other is not.  Since tree Y1 is a spanning tree of graph P, there is a path in tree Y1 joining the two endpoints.  As one travels along the path, one must encounter an edge f joining a vertex in set V to one that is not in set V.  Now, at the iteration when edge e was added to tree Y, edge f could also have been added and it would be added instead of edge e if its weight was less than e, and since edge f was not added, we conclude that
The main loop of Prim's algorithm is inherently sequential and thus not parallelizable. However, the inner loop, which determines the next edge of minimum weight that does not form a cycle, can be parallelized by dividing the vertices and edges between the available processors.[12] The following pseudocode demonstrates this.
This algorithm can generally be implemented on distributed machines[12] as well as on shared memory machines.[13] The running time is 



O
(





|

V


|


2





|

P

|





)
+
O
(

|

V

|

log
⁡

|

P

|

)


{\displaystyle O({\tfrac {|V|^{2}}{|P|}})+O(|V|\log |P|)}

, assuming that the reduce and broadcast operations can be performed in 



O
(
log
⁡

|

P

|

)


{\displaystyle O(\log |P|)}

.[12] A variant of Prim's algorithm for shared memory machines, in which Prim's sequential algorithm is being run in parallel, starting from different vertices, has also been explored.[14] It should, however, be noted that more sophisticated algorithms exist to solve the distributed minimum spanning tree problem in a more efficient manner.
Borůvka's algorithm is a greedy algorithm for finding a minimum spanning tree in a graph,
or a minimum spanning forest in the case of a graph that is not connected.
It was first published in 1926 by Otakar Borůvka as a method of constructing an efficient electricity network for Moravia.[1][2][3]
The algorithm was rediscovered by Choquet in 1938;[4] again by Florek,  Łukasiewicz, Perkal, Steinhaus, and Zubrzycki in 1951;[5] and again by Georges Sollin in 1965.[6] This algorithm is frequently called Sollin's algorithm, especially in the parallel computing literature.
The algorithm begins by finding the minimum-weight edge incident to each vertex of the graph, and adding all of those edges to the forest.
Then, it repeats a similar process of finding the minimum-weight edge from each tree constructed so far to a different tree, and adding all of those edges to the forest.
Each repetition of this process reduces the number of trees, within each connected component of the graph, to at most half of this former value,
so after logarithmically many repetitions the process finishes. When it does, the set of edges it has added forms the minimum spanning forest.
The following pseudocode illustrates a basic implementation of Borůvka's algorithm.
In the conditional clauses, every edge uv is considered cheaper than "None".  The purpose of the completed variable is to determine whether the forest F is yet a spanning forest.
Borůvka's algorithm can be shown to take O(log V) iterations of the outer loop until it terminates, and therefore to run in time O(E log V), where E is the number of edges, and V is the number of vertices in G (assuming E ≥ V). In planar graphs, and more generally in families of graphs closed under graph minor operations, it can be made to run in linear time, by removing all but the cheapest edge between each pair of components after each stage of the algorithm.[7]
Other algorithms for this problem include Prim's algorithm and Kruskal's algorithm. Fast parallel algorithms can be obtained by combining Prim's algorithm with Borůvka's.[8]
A faster randomized minimum spanning tree algorithm based in part on Borůvka's algorithm due to Karger, Klein, and Tarjan runs in expected O(E) time.[9]  The best known (deterministic) minimum spanning tree algorithm by Bernard Chazelle is also based in part on Borůvka's and runs in O(E α(E,V)) time, where α is the inverse Ackermann function.[10] These randomized and deterministic algorithms combine steps of Borůvka's algorithm, reducing the number of components that remain to be connected, with steps of a different type that reduce the number of edges between pairs of components.
In mathematical optimization and computer science, heuristic (from Greek εὑρίσκω "I find, discover"[1]) is a technique designed for problem solving more quickly when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.
A heuristic function, also simply called a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution.[2]
Heuristics may produce results by themselves, or they may be used in conjunction with optimization algorithms to improve their efficiency (e.g., they may be used to generate good seed values).
Results about NP-hardness in theoretical computer science make heuristics the only viable option for a variety of complex optimization problems that need to be routinely solved in real-world applications.
Heuristics underlie the whole field of Artificial Intelligence and the computer simulation of thinking, as they may be used in situations where there are no known algorithms.[3]
so as to select the order to draw using a pen plotter. TSP is known to be NP-hard so an optimal solution for even a moderate size problem is difficult to solve. Instead, the greedy algorithm can be used to give a good but not optimal solution (it is an approximation to the optimal answer) in a reasonably short amount of time. The greedy algorithm heuristic says to pick whatever is currently the best next step regardless of whether that prevents (or even makes impossible) good steps later. It is a heuristic in the sense that practice indicates it is a good enough solution, while theory indicates that there are better solutions (and even indicates how much better, in some cases).[4]
Another example of heuristic making an algorithm faster occurs in certain search problems. Initially, the heuristic tries every possibility at each step, like the full-space search algorithm. But it can stop the search at any time if the current possibility is already worse than the best solution already found. In such search problems, a heuristic can be used to try good choices first so that bad paths can be eliminated early (see alpha–beta pruning). In the case of best-first search algorithms, such as A* search, the heuristic improves the algorithm's convergence while maintaining its correctness as long as the heuristic is admissible.
Antivirus software often uses heuristic rules for detecting viruses and other forms of malware. Heuristic scanning looks for code and/or behavioral patterns common to a class or family of viruses, with different sets of rules for different viruses. If a file or executing process is found to contain matching code patterns and/or to be performing that set of activities, then the scanner infers that the file is infected. The most advanced part of behavior-based heuristic scanning is that it can work against highly randomized self-modifying/mutating (polymorphic) viruses that cannot be easily detected by simpler string scanning methods. Heuristic scanning has the potential to detect future viruses without requiring the virus to be first detected somewhere else, submitted to the virus scanner developer, analyzed, and a detection update for the scanner provided to the scanner's users.
In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA).[1] Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems via biologically inspired operators such as selection, crossover, and mutation.[2] Some examples of GA applications include optimizing decision trees for better performance, solving sudoku puzzles,[3] hyperparameter optimization, and causal inference.[4]
In a genetic algorithm, a population of candidate solutions (called individuals, creatures, organisms, or phenotypes) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its chromosomes or genotype) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.[5]
The evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a generation. In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.
A standard representation of each candidate solution is as an array of bits (also called bit set or bit string).[5] Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple crossover operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in genetic programming and graph-form representations are explored in evolutionary programming; a mix of both linear chromosomes and trees is explored in gene expression programming.
In some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e.g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used.
Although crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms.[citation needed]
It is worth tuning parameters such as the mutation probability, crossover probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to genetic drift (which is non-ergodic in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless elitist selection is employed. An adequate population size ensures sufficient genetic diversity for the problem at hand, but can lead to a waste of computational resources if set to a value larger than required.
Genetic algorithms are simple to implement, but their behavior is difficult to understand. In particular, it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:
Despite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold.[12][13] Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs' efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.[14][15][16]
The practical use of a genetic algorithm has limitations, especially as compared to alternative optimization algorithms:
The simplest algorithm represents each chromosome as a bit string. Typically, numeric parameters can be represented by integers, though it is possible to use floating point representations. The floating point representation is natural to evolution strategies and evolutionary programming. The notion of real-valued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by John Henry Holland in the 1970s. This theory is not without support though, based on theoretical and experimental results (see below). The basic algorithm performs crossover and mutation at the bit level. Other variants treat the chromosome as a list of numbers which are indexes into an instruction table, nodes in a linked list, hashes, objects, or any other imaginable data structure. Crossover and mutation are performed so as to respect data element boundaries. For most data types, specific variation operators can be designed. Different chromosomal data types seem to work better or worse for different specific problem domains.
An expansion of the Genetic Algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome.[21] This particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters. For instance, in problems of cascaded controller tuning, the internal loop controller structure can belong to a conventional regulator of three parameters, whereas the external loop could implement a linguistic controller (such as a fuzzy system) which has an inherently different description. This particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section, and it is a useful tool for the modelling and simulation of complex adaptive systems, especially evolution processes.
Parallel implementations of genetic algorithms come in two flavors. Coarse-grained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes. Fine-grained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction.
Other variants, like genetic algorithms for online optimization problems, introduce time-dependence or noise in the fitness function.
Genetic algorithms with adaptive parameters (adaptive genetic algorithms, AGAs) is another significant and promising variant of genetic algorithms. The probabilities of crossover (pc) and mutation (pm) greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain. Researchers have analyzed GA convergence analytically.[23][24]
Instead of using fixed values of pc and pm, AGAs utilize the population information in each generation and adaptively adjust the pc and pm in order to maintain the population diversity as well as to sustain the convergence capacity. In AGA (adaptive genetic algorithm),[25] the adjustment of pc and pm depends on the fitness values of the solutions. There are more examples of AGA variants: Successive zooming method is an early example of improving convergence.[26] In CAGA (clustering-based adaptive genetic algorithm),[27] through the use of clustering analysis to judge the optimization states of the population, the adjustment of pc and pm depends on these optimization states. Recent approaches use more abstract variables for deciding pc and pm. Examples are dominance & co-dominance principles[28] and LIGA (levelized interpolative genetic algorithm), which combines a flexible GA with modified A* search to tackle search space anisotropicity.[29]
A number of variations have been developed to attempt to improve performance of GAs on problems with a high degree of fitness epistasis, i.e. where the fitness of a solution consists of interacting subsets of its variables. Such algorithms aim to learn (before exploiting) these beneficial phenotypic interactions. As such, they are aligned with the Building Block Hypothesis in adaptively reducing disruptive recombination. Prominent examples of this approach include the mGA,[31] GEMGA[32] and LLGA.[33]
Problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs[citation needed]. GAs have also been applied to engineering.[34] Genetic algorithms are often applied as an approach to solve global optimization problems.
As a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a Markov chain).
Examples of problems solved by genetic algorithms include: mirrors designed to funnel sunlight to a solar collector,[35] antennae designed to pick up radio signals in space,[36] walking methods for computer figures,[37] optimal design of aerodynamic bodies in complex flowfields[38]
[I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology adds another level of complexity between you and your problem. Second, genetic algorithms take a very long time on nontrivial problems. [...] [T]he analogy with evolution—where significant progress require [sic] millions of years—can be quite appropriate.
I have never encountered any problem where genetic algorithms seemed to me the right way to attack it. Further, I have never seen any computational results reported using genetic algorithms that have favorably impressed me. Stick to simulated annealing for your heuristic search voodoo needs.
In 1950, Alan Turing proposed a "learning machine" which would parallel the principles of evolution.[40] Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey.[41][42] His 1954 publication was not widely noticed. Starting in 1957,[43] the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970)[44] and Crosby (1973).[45] Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms.[46] Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).[47]
Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game,[48] artificial evolution only became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies.[49][50][51][52] Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.
In the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes.[53][circular reference] 
In 1989, Axcelis, Inc. released Evolver, the world's first commercial GA product for desktop computers. The New York Times technology writer John Markoff wrote[54] about Evolver in 1990, and it remained the only interactive commercial genetic algorithm until 1995.[55] Evolver was sold to Palisade in 1997, translated into several languages, and is currently in its 6th version.[56] Since the 1990s, MATLAB has built in three derivative-free optimization heuristic algorithms (simulated annealing, particle swarm optimization, genetic algorithm) and two direct search algorithms (simplex search, pattern search).[57]
An algorithm is fundamentally a set of rules or defined procedures that is typically designed and used to solve a specific problem or a broad set of problems.
Broadly, algorithms define process(es), sets of rules, or methodologies that are to be followed in calculations, data processing, data mining, pattern recognition, automated reasoning or other problem-solving operations. With the increasing automation of services, more and more decisions are being made by algorithms. Some general examples are; risk assessments, anticipatory policing, and pattern recognition technology.[1]
Algorithm aversion is defined as a "biased assessment of an algorithm which manifests in negative behaviors and attitudes towards the algorithm compared to a human agent."[1] This phenomenon describes the tendency of humans to reject advice or recommendations from an algorithm in situations where they would accept the same advice if it came from a human.
Algorithms, particularly those utilizing machine learning methods or artificial intelligence (AI), play a growing role in decision-making across various fields. Examples include recommender systems in e-commerce for identifying products a customer might like and AI systems in healthcare that assist in diagnoses and treatment decisions. Despite their proven ability to outperform humans in many contexts, algorithmic recommendations are often met with resistance or rejection, which can lead to inefficiencies and suboptimal outcomes.
The study of algorithm aversion is critical as algorithms become increasingly embedded in our daily lives. Factors such as perceived accountability, lack of transparency, and skepticism towards machine judgment contribute to this aversion. Conversely, there are scenarios where individuals are more likely to trust and follow algorithmic advice over human recommendations, a phenomenon referred to as algorithm appreciation.[2] Understanding these dynamics is essential for improving human-algorithm interactions and fostering greater acceptance of AI-driven decision-making.
Algorithm aversion manifests in various domains where algorithms are employed to assist or replace human decision-making. Below are examples from diverse contexts, highlighting situations where people tend to resist algorithmic advice or decisions:
Algorithmic agents used in recruitment are often perceived as less capable of fulfilling relational roles, such as providing emotional support or career development. While algorithms are trusted for transactional tasks like salary negotiations, human recruiters are favored for relational tasks due to their perceived ability to connect on an emotional level.[4]
Consumers generally react less favorably to decisions made by algorithms compared to those made by humans. For example, when a decision results in a positive outcome, consumers find it harder to internalize the result if it comes from an algorithm. Conversely, negative outcomes tend to elicit similar responses regardless of whether the decision was made by an algorithm or a human.[5]
Cultural norms play a significant role in algorithm aversion. In individualistic cultures, such as in the United States, there is a higher tendency to reject algorithmic recommendations due to an emphasis on autonomy and personalized decision-making. In contrast, collectivist cultures, such as in India, exhibit lower aversion, particularly when familiarity with algorithms is higher or when decisions align with societal norms.[8]
Algorithms are less trusted for tasks involving moral or emotional judgment, such as ethical dilemmas or empathetic decision-making. For example, individuals may reject algorithmic decisions in scenarios where they perceive moral stakes to be high, such as autonomous vehicle decisions or medical life-or-death situations.[9]
Algorithm aversion arises from a combination of psychological, task-related, cultural, and design-related factors. These mechanisms interact to shape individuals' negative perceptions and behaviors toward algorithms, even in cases where algorithmic performance is objectively superior to human decision-making.
Individuals often feel a heightened sense of accountability when using algorithmic advice compared to human advice. This stems from the belief that, if a decision goes wrong, they will be solely responsible because an algorithm lacks the capacity to share blame. By contrast, decisions made with human input are perceived as more collaborative, allowing for shared accountability. For example, users are less likely to rely on algorithmic recommendations in high-stakes domains like healthcare or financial advising, where the repercussions of errors are significant.[8]
People with an internal locus of control, who believe they have direct influence over outcomes, are more reluctant to trust algorithms. They may perceive algorithmic decision-making as undermining their autonomy, preferring human input that feels more modifiable or personal. Conversely, individuals with an external locus of control, who attribute outcomes to external forces, may accept algorithmic decisions more readily, viewing algorithms as neutral and effective tools. This tendency is particularly evident in decision-making contexts where users seek to maintain agency.[10]
Neurotic individuals are more prone to anxiety and fear of uncertainty, making them less likely to trust algorithms. This aversion may be fueled by concerns about the perceived "coldness" of algorithms or their inability to account for nuanced emotional factors. For example, in emotionally sensitive tasks like healthcare or recruitment, neurotic individuals may reject algorithmic inputs in favor of human recommendations, even when the algorithm performs equally well or better.[11]
The nature of the task significantly influences algorithm aversion. For routine and low-risk tasks, such as recommending movies or predicting product preferences, users are generally comfortable relying on algorithms. However, for high-stakes or subjective tasks, such as making medical diagnoses, financial decisions, or moral judgments, algorithm aversion increases. Users perceive these tasks as requiring empathy, ethical reasoning, or nuanced understanding—qualities that they believe algorithms lack. This disparity highlights why algorithms are better received in technical fields (e.g., logistics) but face resistance in human-centric domains.[5]
People's reactions to algorithmic decisions are influenced by the nature of the decision outcome. When algorithms deliver positive results, users are more likely to trust and accept them. However, when outcomes are negative, users are more inclined to reject algorithms and attribute blame to their use. This phenomenon is linked to the perception that algorithms lack accountability, unlike human decision-makers, who can offer justifications or accept responsibility for failures.[5]
Cultural norms significantly shape attitudes toward algorithmic decision-making. In individualistic cultures, such as the United States, people value autonomy and personalization, making them more skeptical of algorithmic systems that they perceive as impersonal or rigid. Conversely, in collectivist cultures like India, individuals are more likely to accept algorithmic recommendations, particularly when these systems align with group norms or social expectations. Familiarity with algorithms in collectivist societies also reduces aversion, as users view algorithms as tools to reinforce societal goals rather than threats to individual autonomy.[8]
Cultural norms and values significantly impact algorithm acceptance. Individualistic cultures, such as those in the United States, tend to display higher algorithm aversion due to an emphasis on autonomy, personal agency, and distrust of generalized systems. On the other hand, collectivist cultures, such as in India, exhibit greater acceptance of algorithms, particularly when familiarity is high and the decision aligns with societal norms. These differences highlight the importance of tailoring algorithmic systems to align with cultural expectations.[8]
The role of organizations in supporting and explaining the use of algorithms can greatly influence aversion levels. When organizations actively promote algorithmic tools and provide training on their usage, employees are less likely to resist them. Transparency about how algorithms support decision-making processes fosters trust and reduces anxiety, particularly in high-stakes or workplace settings.[1]
Algorithm aversion is higher for autonomous systems that make decisions independently (performative algorithms) compared to advisory systems that provide recommendations but allow humans to retain final decision-making power. Users tend to view advisory algorithms as supportive tools that enhance their control, whereas autonomous algorithms may be perceived as threatening to their authority or ability to intervene.[1]
Algorithms are often perceived as lacking human-specific skills, such as empathy or moral reasoning. This perception leads to greater aversion in tasks involving subjective judgment, ethical dilemmas, or emotional interactions. Users are generally more accepting of algorithms in objective, technical tasks where human qualities are less critical.[1]
In high-stakes or expertise-intensive tasks, users tend to favor human experts over algorithms. This preference stems from the belief that human experts can account for context, nuance, and situational complexity in ways that algorithms cannot. Algorithm aversion is particularly pronounced when humans with expertise are available as an alternative to the algorithm.[1]
Users are more likely to reject algorithms when the alternative is their own input or the input of someone they know and relate to personally. In contrast, when the alternative is an anonymous or distant human agent, algorithms may be viewed more favorably. This preference for closer, more relatable human agents highlights the importance of perceived social connection in algorithmic decision acceptance.[1]
A lack of transparency in algorithmic systems, often referred to as the "black box" problem, creates distrust among users. Without clear explanations of how decisions are made, users may feel uneasy relying on algorithmic outputs, particularly in high-stakes scenarios. For instance, transparency in medical AI systems—such as providing explanations for diagnostic recommendations—can significantly improve trust and reduce aversion. Transparent algorithms empower users by demystifying decision-making processes, making them feel more in control.[10]
Users are generally less forgiving of algorithmic errors than human errors, even when the frequency of errors is lower for algorithms. This heightened scrutiny stems from the belief that algorithms should be "perfect" or error-free, unlike humans, who are expected to make mistakes. However, algorithms that demonstrate the ability to learn from their mistakes and adapt over time can foster greater trust. For example, users are more likely to accept algorithms in financial forecasting if they observe improvements based on feedback.[10]
Designing algorithms with human-like traits, such as avatars, conversational interfaces, or relatable language, can reduce aversion by making interactions feel more natural and personal. For instance, AI-powered chatbots with empathetic communication styles are better received in customer service than purely mechanical interfaces. This design strategy helps mitigate the perception that algorithms are "cold" or impersonal, encouraging users to engage with them more comfortably.[7]
The format in which algorithms present their recommendations significantly affects user trust. Systems that use conversational or audio interfaces are generally more trusted than those relying solely on textual outputs, as they create a sense of human-like interaction.[12]
Algorithms that provide clear, concise, and well-organized explanations of their recommendations are more likely to gain user acceptance. Systems that offer detailed yet accessible insights into their decision-making process are perceived as more reliable and trustworthy.[10]
Many individuals harbor an ingrained skepticism toward algorithms, particularly when they lack familiarity with the system or its capabilities. Early negative experiences with algorithms can entrench this distrust, making it difficult to rebuild confidence. Even when algorithms perform better, this bias often persists, leading to outright rejection.[8]
People often display a preference for human decisions over algorithmic ones, particularly for positive outcomes. Yalsin et al. highlighted that individuals are more likely to internalize favorable decisions made by humans, attributing success to human expertise or effort. In contrast, decisions made by algorithms are viewed as impersonal, reducing the sense of achievement or satisfaction. This favoritism contributes to a persistent bias against algorithmic systems, even when their performance matches or exceeds that of humans.[5]
People may also be averse to using algorithms if doing so conveys negative information about the human's ability.[13] This can occur if humans have private information about their own ability.
Algorithms are often capable of outperforming humans or performing tasks much more cost-effectively.[14][15] Despite this, algorithm aversion persists due to a range of psychological, cultural, and design-related factors. To mitigate resistance and build trust, researchers and practitioners have proposed several strategies.
One effective way to reduce algorithmic aversion is by incorporating a human-in-the-loop approach, where the human decision-maker retains control over the final decision. This approach addresses concerns about agency and accountability by positioning algorithms as advisory tools rather than autonomous decision-makers.
Algorithms can provide recommendations while leaving the ultimate decision-making authority with humans. This allows users to view algorithms as supportive rather than threatening. For example, in healthcare, AI systems can suggest diagnoses or treatments, but the human doctor makes the final call.
Integrating humans into algorithmic processes fosters a sense of collaboration and encourages users to engage with the system more openly. This method is particularly effective in domains where human intuition and context are critical, such as recruitment, education, and financial planning.
Transparency is crucial for overcoming algorithm aversion, as it helps to build trust and reduce the "black box" effect that often causes discomfort among users. Providing explanations about how algorithms work enables users to understand and evaluate their recommendations. Transparency can take several forms, such as global explanations that describe the overall functioning of an algorithm, case-specific explanations that clarify why a particular recommendation was made, or confidence levels that highlight the algorithm's certainty in its decisions. For example, in financial advising, transparency about how investment recommendations are generated can increase user confidence in the system. Explainable AI (XAI) methods, such as visualizations of decision pathways or feature importance metrics, make these explanations accessible and comprehensible, allowing users to make informed decisions about whether to trust the algorithm.[1]
Familiarizing users with algorithms through training can significantly reduce aversion, especially for those who are unfamiliar or skeptical. Training programs that simulate real-world interactions with algorithms allow users to see their capabilities and limitations firsthand. For instance, healthcare professionals using diagnostic AI systems can benefit from hands-on training that demonstrates how the system arrives at recommendations and how to interpret its outputs. Such training helps bridge knowledge gaps and demystifies algorithms, making users more comfortable with their use. Furthermore, repeated interactions and feedback loops help users build trust in the system over time. Financial incentives, such as rewards for accurate decisions made with the help of algorithms, have also been shown to encourage users to engage more readily with these systems.[16]
Allowing users to interact with and adjust algorithmic outputs can greatly enhance their sense of control, which is a key factor in overcoming aversion. For example, interactive interfaces that let users modify parameters, simulate outcomes, or personalize recommendations make algorithms feel less rigid and more adaptable. Providing confidence thresholds that users can adjust—such as setting stricter criteria for medical diagnoses—further empowers them to feel involved in the decision-making process. Feedback mechanisms are another important feature, as they allow users to provide input or correct errors, fostering a sense of collaboration between the user and the algorithm. These design features not only reduce resistance but also demonstrate that algorithms are flexible tools rather than fixed, inflexible systems.
Personalization is another critical factor in reducing algorithm aversion. Algorithms that adapt to individual preferences or contexts are more likely to gain user acceptance. For instance, recommendation systems in e-commerce that learn a user's shopping habits over time are often trusted more than generic systems. Customization features, such as the ability to prioritize certain factors (e.g., cost or sustainability in product recommendations), further enhance user satisfaction by aligning outputs with their unique needs. In healthcare, personalized AI systems that incorporate a patient's medical history and specific conditions are better received than generalized tools. By tailoring outputs to the user's preferences and circumstances, algorithms can foster greater engagement and trust.
Studies do not consistently show people demonstrating bias against algorithms and sometimes show the opposite, preferring advice from an algorithm instead of a human. This effect is called algorithm appreciation.[17][18] Results are mixed, showing that people sometimes seem to prefer advice that comes from an algorithm instead of a human.
Algorithm engineering focuses on the design, analysis, implementation, optimization, profiling and experimental evaluation of computer algorithms, bridging the gap between algorithmics theory and practical applications of algorithms in software engineering.[1]
It is a general methodology for algorithmic research.[2]
But also, promising algorithmic approaches have been neglected due to difficulties in mathematical analysis.[2]
The term "algorithm engineering" was first used with specificity in 1997, with the first Workshop on Algorithm Engineering (WAE97), organized by Giuseppe F. Italiano.[4]
Algorithm engineering does not intend to replace or compete with algorithm theory, but tries to enrich, refine and reinforce its formal approaches with experimental algorithmics (also called empirical algorithmics).
This way it can provide new insights into the efficiency and performance of algorithms in cases where
Some researchers describe algorithm engineering's methodology as a cycle consisting of algorithm design, analysis, implementation and experimental evaluation, joined by further aspects like machine models or realistic inputs.
They argue that equating algorithm engineering with experimental algorithmics is too limited, because viewing design and analysis, implementation and experimentation as separate activities ignores the crucial feedback loop between those elements of algorithm engineering.[2]
While specific applications are outside the methodology of algorithm engineering, they play an important role in shaping realistic models of the problem and the underlying machine, and supply real inputs and other design parameters for experiments.[2]
Compared to algorithm theory, which usually focuses on the asymptotic behavior of algorithms, algorithm engineers need to keep further requirements in mind: Simplicity of the algorithm, implementability in programming languages on real hardware, and allowing code reuse.
Additionally, constant factors of algorithms have such a considerable impact on real-world inputs that sometimes an algorithm with worse asymptotic behavior performs better in practice due to lower constant factors.
Some problems can be solved with heuristics and randomized algorithms in a simpler and more efficient fashion than with deterministic algorithms. Unfortunately, this makes even simple randomized algorithms difficult to analyze because there are subtle dependencies to be taken into account.[2]
Huge semantic gaps between theoretical insights, formulated algorithms, programming languages and hardware pose a challenge to efficient implementations of even simple algorithms, because small implementation details can have rippling effects on execution behavior.
The only reliable way to compare several implementations of an algorithm is to spend an considerable amount of time on tuning and profiling, running those algorithms on multiple architectures, and looking at the generated machine code.[2]
Implementations of algorithms used for experiments differ in significant ways from code usable in applications.
While the former prioritizes fast prototyping, performance and instrumentation for measurements during experiments, the latter requires thorough testing, maintainability, simplicity, and tuning for particular classes of inputs.[2]
Stable, well-tested algorithm libraries like LEDA play an important role in technology transfer by speeding up the adoption of new algorithms in applications. 
Such libraries reduce the required investment and risk for practitioners, because it removes the burden of understanding and implementing the results of academic research.
The 1997 Workshop on Algorithm Engineering (WAE'97) was held in Venice (Italy) on September 11–13, 1997. The Third International Workshop on Algorithm Engineering (WAE'99) was held in London, UK in July 1999.[6]
The first Workshop on Algorithm Engineering and Experimentation (ALENEX99) was held in Baltimore, Maryland on January 15–16, 1999.[7] It was sponsored by DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science (at Rutgers University), with additional support from SIGACT, the ACM Special Interest Group on Algorithms and Computation Theory, and SIAM, the Society for Industrial and Applied Mathematics.[7]
Algorithmic bias describes systematic and repeatable errors in a computer system that create "unfair" outcomes, such as "privileging" one category over another in ways different from the intended function of the algorithm.
Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm.[2]  For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect "systematic and unfair" discrimination.[3] This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (proposed 2018) and the Artificial Intelligence Act (proposed 2021, approved 2024).
As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; by how features and labels are chosen; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.[4]
Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single "algorithm" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.
Algorithms are difficult to define,[5] but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output.[6]: 13  For a rigorous technical introduction, see Algorithms. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence.[7]: 14–15  By analyzing and processing data, algorithms are the backbone of search engines,[8] social media websites,[9] recommendation engines,[10] online retail,[11] online advertising,[12] and more.[13]
Contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality.[14]: 2 [15]: 563 [16]: 294 [17] The term algorithmic bias describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as biased.[18]: 332  This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on).
Bias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a database according to human-designed cataloging criteria.[19]: 3  Next, programmers assign priorities, or hierarchies, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded.[19]: 4  Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers.[19]: 8  Other algorithms may reinforce stereotypes and preferences as they process and display "relevant" data for human users, for example, by selecting information based on previous choices of a similar user or group of users.[19]: 6
Beyond assembling and processing data, bias can emerge as a result of design.[20] For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores).[21]: 36  Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. These criteria could present unanticipated outcomes for search results, such as with flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths.[20] Algorithms may also display an uncertainty bias, offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations.[22]: 4
The earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book Computer Power and Human Reason, artificial intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded.[23]: 149
An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with "foreign-sounding names" based on historical trends in admissions.[25] While many schools at the time employed similar biases in their selection process, St. George was most notable for automating said bias through the use of an algorithm, thus gaining the attention of people on a much wider scale.
In recent years, as algorithms increasingly rely on machine learning methods applied to real-world data, algorithmic bias has become more prevalent due to inherent biases within the data itself. For instance, facial recognition systems have been shown to misidentify individuals from marginalized groups at significantly higher rates than white individuals, highlighting how biases in training datasets manifest in deployed systems.[26] A 2018 study by Joy Buolamwini and Timnit Gebru found that commercial facial recognition technologies exhibited error rates of up to 35% when identifying darker-skinned women, compared to less than 1% for lighter-skinned men.[27]
Algorithmic biases are not only technical failures but often reflect systemic inequities embedded in historical and societal data. Researchers and critics, such as Cathy O'Neil in her book Weapons of Math Destruction (2016), emphasize that these biases can amplify existing social inequalities under the guise of objectivity. O'Neil argues that opaque, automated decision-making processes in areas such as credit scoring, predictive policing, and education can reinforce discriminatory practices while appearing neutral or scientific.[28]
Though well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze.[29] The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten.[30]: 115  In theory, these biases may create new patterns of behavior, or "scripts", in relationship to specific technologies as the code interacts with other elements of society.[31] Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests.[32]: 180
The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist,[33]: 15  a process described by author Clay Shirky as "algorithmic authority".[34] Shirky uses the term to describe "the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources", such as search results.[34] This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as "trending" or "popular" may be created based on significantly wider criteria than just their popularity.[19]: 14
Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans.[33]: 16 [35]: 6  This can have the effect of reducing alternative options, compromises, or flexibility.[33]: 16  Sociologist Scott Lash has critiqued algorithms as a new form of "generative power", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors.[36]: 71
While blind adherence to algorithmic decisions is a concern, an opposite issue arises when human decision-makers exhibit "selective adherence" to algorithmic advice. In such cases, individuals accept recommendations that align with their preexisting beliefs and disregard those that do not, thereby perpetuating existing biases and undermining the fairness objectives of algorithmic interventions. Consequently, incorporating fair algorithmic tools into decision-making processes does not automatically eliminate human biases.[37]
Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft, which have co-created a working group named Fairness, Accountability,
and Transparency in Machine Learning.[38]: 115  Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences.[38]: 117  In recent years, the study of the Fairness, Accountability,
and Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAccT.[39] Critics have suggested that FAT initiatives cannot serve effectively as independent watchdogs when many are funded by corporations building the systems being studied.[40]
Pre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious.[18]: 334 [16]: 294  Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines.[24]: 17  Encoding pre-existing bias into software can preserve social and institutional bias, and, without correction, could be replicated in all future uses of that algorithm.[30]: 116 [35]: 8
An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new British citizens after the 1981 British Nationality Act.[18]: 341  The program accurately reflected the tenets of the law, which stated that "a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not."[18]: 341 [41]: 375  In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed.[18]: 342
Another source of bias, which has been called "label choice bias",[42] arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients[43] Solutions to the "label choice bias" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.[42]
Machine learning bias refers to systematic and unfair disparities in the output of machine learning algorithms. These biases can manifest in various ways and are often a reflection of the data used to train these algorithms. Here are some key aspects:
Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[51][52]
Racial bias refers to the tendency of machine learning models to produce outcomes that unfairly discriminate against or stereotype individuals based on race or ethnicity. This bias often stems from training data that reflects historical and systemic inequalities. For example, AI systems used in hiring, law enforcement, or healthcare may disproportionately disadvantage certain racial groups by reinforcing existing stereotypes or underrepresenting them in key areas. Such biases can manifest in ways like facial recognition systems misidentifying individuals of certain racial backgrounds or healthcare algorithms underestimating the medical needs of minority patients. Addressing racial bias requires careful examination of data, improved transparency in algorithmic processes, and efforts to ensure fairness throughout the AI development lifecycle.[53][54]
Technical bias emerges through limitations of a program, computational power, its design, or other constraint on the system.[18]: 332  Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display.[18]: 336  Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list.[18]: 332
A decontextualized algorithm uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines.[18]: 332  The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime.[15]: 574
Lastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury.[18]: 332  Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection.[33]: 21–22
Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts.[18]: 334  Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms.[18]: 334, 336  This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion.[32]: 179 [16]: 294  Similarly, problems may emerge when training data (the samples "fed" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world.[55]
In 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP).[18]: 338  The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference.[18]: 338 [56]
Unpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data.[22]: 6  In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care.[57][clarification needed]
Emergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand.[18]: 334  These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society.[32]: 179
Apart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of British immigration law.[18]: 342
Emergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm.[58][59] For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public.[60] The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods.[58][61][62] The Human Rights Data Analysis Group, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.[59] Another well known example of such an algorithm exhibiting such behavior is COMPAS, a software that determines an individual's likelihood of becoming a criminal offender. The software is often criticized for labeling Black individuals as criminals much more likely than others, and then feeds the data back into itself in the event individuals become registered criminals, further enforcing the bias created by the dataset the algorithm is acting on.
Recommender systems such as those used to recommend online videos or news articles can create feedback loops.[63] When users click on content that is suggested by algorithms, it influences the next set of suggestions.[64] Over time this may lead to users entering a filter bubble and being unaware of important or useful content.[65][66]
Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.[67]: 2 [18]: 331
A series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have "no means of competing" if an algorithm, with or without intent, boosted page listings for a rival candidate.[69] Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted.[70] Legal scholar Jonathan Zittrain has warned that this could create a "digital gerrymandering" effect in elections, "the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users", if intentionally manipulated.[71]: 335
Web search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, "lesbian". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, "Top 25 Sexiest Women Athletes" articles displayed as first-page results in searches for "women athletes".[75]: 31  In 2017, Google adjusted these results along with others that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content.[76] Other examples include the display of higher-paying jobs to male applicants on job search websites.[77] Researchers have also identified that machine translation exhibits a strong tendency towards male defaults.[78] In particular, this is observed in fields linked to unbalanced gender distribution, including STEM occupations.[79] In fact, current machine translation systems fail to reproduce the real world distribution of female workers.[80]
In 2015, Amazon.com turned off an AI system it developed to screen job applications when they realized it was biased against women.[81] The recruitment tool excluded applicants who attended all-women's colleges and resumes that included the word "women's".[82] A similar problem emerged with music streaming services—In 2019, it was discovered that the recommender system algorithm used by Spotify was biased against women artists.[83] Spotify's song recommendations suggested more male artists over women artists.
Algorithms have been criticized as a method for obscuring racial prejudices in decision-making.[84][85][86]: 158  Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases.[87] For example, black people are likely to receive longer sentences than white people who committed the same crime.[88][89] This could potentially mean that a system amplifies the original biases in the data.
In 2015, Google apologized when a couple of black users complained that an image-identification algorithm in its Photos application identified them as gorillas.[90] In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking.[91] Such examples are the product of bias in biometric data sets.[90] Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.[86]: 154  Speech recognition technology can have different accuracies depending on the user's accent. This may be caused by the a lack of training data for speakers of that accent.[92]
Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name.[93] A 2015 study also found that Black and Asian people are assumed to have lesser functioning lungs due to racial and occupational exposure data not being incorporated into the prediction algorithm's model of lung function.[94][95]
In 2019, a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about $1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases.[96]
A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on "creditworthiness" which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities.[97][non-primary source needed]
Algorithms already have numerous applications in legal systems. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than the average COMPAS-assigned risk level of white defendants, and that black defendants are twice as likely to be erroneously assigned the label "high-risk" as white defendants.[99][100]
One example is the use of risk assessments in criminal sentencing in the United States and parole hearings, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime.[101] For the time period starting in 1920 and ending in 1970, the nationality of a criminal's father was a consideration in those risk assessment scores.[102]: 4  Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.[101]
In the pretrial detention context, a law review article argues that algorithmic risk assessments violate 14th Amendment Equal Protection rights on the basis of race, since the algorithms are argued to be facially discriminatory, to result in disparate treatment, and to not be narrowly tailored.[104]
In 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents.[105] The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing "Muslims" would be blocked, while posts denouncing "Radical Muslims" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the "children" subset of blacks, rather than "all blacks", whereas "all white men" would trigger a block, because whites and males are not considered subsets.[105] Facebook was also found to allow ad purchasers to target "Jew haters" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.[106]
While algorithms are used to track and block hate speech, some were found to be 1.5 times more likely to flag information posted by Black users and 2.2 times likely to flag information as hate speech if written in African American English.[107] Without context for slurs and epithets, even when used by communities which have re-appropriated them, were flagged.[108]
Another instance in a study found that 85 out of 100 examined subreddits tended to remove various norm violations, including misogynistic slurs and racist hate speech, highlighting the prevalence of such content in online communities.[109] As platforms like Reddit update their hate speech policies, they must balance free expression with the protection of marginalized communities, emphasizing the need for context-sensitive moderation and nuanced algorithms.[109]
Surveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times.[15]: 572  The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database; if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender.[110] However, even audits of these image-recognition systems are ethically fraught, and some scholars have suggested the technology's context will always have a disproportionate impact on communities whose actions are over-surveilled.[111] For example, a 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites.[32]: 190  A 2018 study found that facial recognition software most likely accurately identified light-skinned (typically European) males, with slightly lower accuracy rates for light-skinned females. Dark-skinned males and females were significanfly less likely to be accurately identified by facial recognition software. These disparities are attributed to the under-representation of darker-skinned participants in data sets used to develop this software.[112][113]
In 2011, users of the gay hookup application Grindr reported that the Android store's recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately related homosexuality with pedophilia. Writer Mike Ananny criticized this association in The Atlantic, arguing that such associations further stigmatized gay men.[114] In 2009, online retailer Amazon de-listed 57,000 books after an algorithmic change expanded its "adult content" blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel Brokeback Mountain.[115][19]: 5 [116]
Facial recognition technology has been seen to cause problems for transgender individuals. In 2018, there were reports of Uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built-in security measure. As a result of this, some of the accounts of trans Uber drivers were suspended which cost them fares and potentially cost them a job, all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning.[118] Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models, an instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos, which created an issue of violation of privacy.[119]
There has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individual's sexual orientation based on their facial images.[120] The model in the study predicted a correct distinction between gay and straight men 81% of the time, and a correct distinction between gay and straight women 74% of the time. This study resulted in a backlash from the LGBTQIA community, who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being "outed" against their will.[121]
While the modalities of algorithmic fairness have been judged on the basis of different aspects of bias – like gender, race and socioeconomic status, disability often is left out of the list.[122][123] The marginalization people with disabilities currently face in society is being translated into AI systems and algorithms, creating even more exclusion[124][125]
The shifting nature of disabilities and its subjective characterization, makes it more difficult to computationally address. The lack of historical depth in defining disabilities, collecting its incidence and prevalence in questionnaires, and establishing recognition add to the controversy and ambiguity in its quantification and calculations.  The definition of disability has been long debated shifting from a medical model to a social model of disability most recently, which establishes that disability is a result of the mismatch between people's interactions and barriers in their environment, rather than impairments and health conditions. Disabilities can also be situational or temporary,[126] considered in a constant state of flux. Disabilities are incredibly diverse,[127] fall within a large spectrum, and can be unique to each individual. People's identity can vary based on the specific types of disability they experience, how they use assistive technologies, and who they support.  The high level of variability across people's experiences greatly personalizes how a disability can manifest. Overlapping identities and intersectional experiences[128] are excluded from statistics and datasets,[129] hence underrepresented and nonexistent in training data.[130] Therefore, machine learning models are trained inequitably and artificial intelligent systems perpetuate more algorithmic bias.[131] For example, if people with speech impairments are not included in training voice control features and smart AI assistants –they are unable to use the feature or the responses received from a Google Home or Alexa are extremely poor.
Given the stereotypes and stigmas that still exist surrounding disabilities, the sensitive nature of revealing these identifying characteristics also carries vast privacy challenges. As disclosing disability information can be taboo and drive further discrimination against this population, there is a lack of explicit disability data available for algorithmic systems to interact with. People with disabilities face additional harms and risks with respect to their social support, cost of health insurance, workplace discrimination and other basic necessities upon disclosing their disability status. Algorithms are further exacerbating this gap by recreating the biases that already exist in societal systems and structures.[132][133]
While users generate results that are "completed" automatically, Google has failed to remove sexist and racist autocompletion text. For example, Algorithms of Oppression: How Search Engines Reinforce Racism Safiya Noble notes an example of the search for "black girls", which was reported to result in pornographic images. Google claimed it was unable to erase those pages unless they were considered unlawful.[134]
Several problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.[14]: 5 [135][136]
Literature on algorithmic bias has focused on the remedy of fairness, but definitions of fairness are often incompatible with each other and the realities of machine learning optimization.[137][138] For example, defining fairness as an "equality of outcomes" may simply refer to a system producing the same result for all people, while fairness defined as "equality of treatment" might explicitly consider differences between individuals.[139]: 2  As a result, fairness is sometimes described as being in conflict with the accuracy of a model, suggesting innate tensions between the priorities of social welfare and the priorities of the vendors designing these systems.[140]: 2  In response to this tension, researchers have suggested more care to the design and use of systems that draw on potentially biased algorithms, with "fairness" defined for specific applications and contexts.[141]
Algorithmic processes are complex, often exceeding the understanding of the people who use them.[14]: 2 [142]: 7  Large-scale operations may not be understood even by those involved in creating them.[143] The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code's input or output.[32]: 183  Social scientist Bruno Latour has identified this process as blackboxing, a process in which "scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become."[144] Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.[145]: 92
An example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013.[146] Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms.[30]: 118  Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.[7]: 22
Additional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms.[147]: 367 [142]: 7  One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do.[14]: 5 
Companies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user.[14]: 5
Commercial algorithms are proprietary, and may be treated as trade secrets.[14]: 2 [142]: 7 [32]: 183  Treating algorithms as trade secrets protects companies, such as search engines, where a transparent algorithm might reveal tactics to manipulate search rankings.[147]: 366  This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function.[7]: 20  Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output.[147]: 369  Other critics, such as lawyer and activist Katarzyna Szymielewicz, have suggested that the lack of transparency is often disguised as a result of algorithmic complexity, shielding companies from disclosing or investigating its own algorithmic processes.[148]
Some practitioners have tried to estimate and impute these missing sensitive categorizations in order to allow bias mitigation, for example building systems to infer ethnicity from names,[150] however this can introduce other forms of bias if not undertaken with care.[151] Machine learning researchers have drawn upon cryptographic privacy-enhancing technologies such as secure multi-party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext.[152]
Algorithmic bias does not only include protected categories, but can also concern characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and removing the bias from such a system is more difficult.[153] Furthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities.[154]
There have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent fields focus on tools which are typically applied to the (training) data used by the program rather than the algorithm's internal processes. These methods may also analyze a program's output and its usefulness and therefore may involve the analysis of its confusion matrix (or table of confusion).[156][157][158][159][160][161][162][163][164] Explainable AI to detect algorithm Bias is a suggested way to detect the existence of bias in an algorithm or learning model.[165] Using machine learning to detect bias is called, "conducting an AI audit", where the "auditor" is an algorithm that goes through the AI model and the training data to identify biases.[166]
Ensuring that an AI tool such as a classifier is free from bias is more difficult than just removing the sensitive information
from its input signals, because this is typically implicit in other signals. For example, the hobbies, sports and schools attended
by a job candidate might reveal their gender to the software, even when this is removed from the analysis. Solutions to this
problem involve ensuring that the intelligent agent does not have any information that could be used to reconstruct the protected
and sensitive information about the subject, as first demonstrated in[167] where a deep learning network was simultaneously trained to learn a task while at the same time being completely agnostic about the protected feature. A simpler method was proposed in the context of word embeddings, and involves removing information that is correlated with the protected characteristic.[168]
Currently[when?], a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or end users) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the Software & Systems Engineering Standards Committee,[169] a committee chartered by the IEEE Computer Society. A draft of the standard is expected to be submitted for balloting in June 2019.[170][171]The standard was published in January 2025.[172]
In 2022, the IEEE released a standard aimed at specifying methodologies to help creators of algorithms address issues of bias and promote transparency regarding the function and potential effects of their algorithms. The project, initially approved in February 2017, was sponsored by the Software & Systems Engineering Standards Committee,[173] a committee under the IEEE Computer Society. The standard provides guidelines for articulating transparency to authorities or end users and mitigating algorithmic biases.[170][171][174]
Ethics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results.[175] Such solutions include the consideration of the "right to understanding" in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed.[176] Toward this end, a movement for "Explainable AI" is already underway within organizations such as DARPA, for reasons that go beyond the remedy of bias.[177] Price Waterhouse Coopers, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results.[178]
An initial approach towards transparency included the open-sourcing of algorithms.[179] Software code can be looked into and improvements can be proposed through source-code-hosting facilities. However, this approach doesn't necessarily produce the intended effects. Companies and organizations can share all possible documentation and code, but this does not establish transparency if the audience doesn't understand the information given. Therefore, the role of an interested critical audience is worth exploring in relation to transparency. Algorithms cannot be held accountable without a critical audience.[180]
From a regulatory perspective, the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias.[181] This includes legislating expectations of due diligence on behalf of designers of these algorithms, and creating accountability when private actors fail to protect the public interest, noting that such rights may be obscured by the complexity of determining responsibility within a web of complex, intertwining processes.[182] Others propose the need for clear liability insurance mechanisms.[183]
Amid concerns that the design of AI systems is primarily the domain of white, male engineers,[184] a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems.[176][155] For example, just 12% of machine learning engineers are women,[185] with black AI leaders pointing to a "diversity crisis" in the field.[186] Groups like Black in AI and Queer in AI are attempting to create more inclusive spaces in the AI community and work against the often harmful desires of corporations that control the trajectory of AI research.[187] Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality, and have called for applying a more deliberate lens of intersectionality to the design of algorithms.[188][189]: 4  Researchers at the University of Cambridge have argued that addressing racial diversity is hampered by the "whiteness" of the culture of AI.[190]
Integrating interdisciplinarity and collaboration in developing of AI systems can play a critical role in tackling algorithmic bias. Integrating insights, expertise, and perspectives from disciplines outside of computer science can foster a better understanding of the impact data driven solutions have on society. An example of this in AI research is PACT or Participatory Approach to enable Capabilities in communiTies, a proposed framework for facilitating collaboration when developing AI driven solutions concerned with social impact.[191] This framework identifies guiding principals for stakeholder participation when working on AI for Social Good (AI4SG) projects. PACT attempts to reify the importance of decolonizing and power-shifting efforts in the design of human-centered AI solutions. An academic initiative in this regard is the Stanford University's Institute for Human-Centered Artificial Intelligence which aims to foster multidisciplinary collaboration. The mission of the institute is to advance artificial intelligence (AI) research, education, policy and practice to improve the human condition.[192]
The GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71,[195] noting that
Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of recitals.[196] While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law,[195] its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes.[197]
The United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used.[198] Many policies are self-enforced or controlled by the Federal Trade Commission.[198] In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan,[199] which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to "design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases". Intended only as guidance, the report did not create any legal precedent.[200]: 26
In 2017, New York City passed the first algorithmic accountability bill in the United States.[201] The bill, which went into effect on January 1, 2018, required "the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems."[202] The task force is required to present findings and recommendations for further regulatory action in 2019.[203]
On February 11, 2019, according to Executive Order 13859, the federal government unveiled the "American AI Initiative," a comprehensive strategy to maintain U.S. leadership in artificial intelligence. The initiative highlights the importance of sustained AI research and development, ethical standards, workforce training, and the protection of critical AI technologies.[204] This aligns with broader efforts to ensure transparency, accountability, and innovation in AI systems across public and private sectors. Furthermore, on October 30, 2023, the President signed Executive Order 14110, which emphasizes the safe, secure, and trustworthy development and use of artificial intelligence (AI). The order outlines a coordinated, government-wide approach to harness AI's potential while mitigating its risks, including fraud, discrimination, and national security threats. An important point in the commitment is promoting responsible innovation and collaboration across sectors to ensure that AI benefits society as a whole.[205] With this order, President Joe Biden mandated the federal government to create best practices for companies to optimize AI's benefits and minimize its harms.[206]
On July 31, 2018, a draft of the Personal Data Bill was presented.[207] The draft proposes standards for the storage, processing and transmission of data. While it does not use the term algorithm, it makes for provisions for "harm resulting from any processing or any kind of processing undertaken by the fiduciary". It defines "any denial or withdrawal of a service, benefit or good resulting from an evaluative decision about the data principal" or "any discriminatory treatment" as a source of harm that could arise from improper use of data. It also makes special provisions for people of "Intersex status".[208]
Algorithms (or, at the very least, formal sets of rules) have been used to compose music for centuries; the procedures used to plot voice-leading in Western counterpoint, for example, can often be reduced to algorithmic determinacy. The term can be used to describe music-generating techniques that run without ongoing human intervention, for example through the introduction of chance procedures. However through live coding and other interactive interfaces, a fully human-centric approach to algorithmic composition is possible.[1]
Some algorithms or data that have no immediate musical relevance are used by composers[2] as creative inspiration for their music. Algorithms such as fractals, L-systems, statistical models, and even arbitrary data (e.g. census figures, GIS coordinates, or magnetic field measurements) have been used as source materials.
Compositional algorithms are usually classified by the specific programming techniques they use. The results of the process can then be divided into 1) music composed by computer and 2) music composed with the aid of computer. Music may be considered composed by computer when the algorithm is able to make choices of its own during the creation process.
Another way to sort compositional algorithms is to examine the results of their compositional processes. Algorithms can either 1) provide notational information (sheet music or MIDI) for other instruments or 2) provide an independent way of sound synthesis (playing the composition by itself). There are also algorithms creating both notational data and sound synthesis.
One way to categorize compositional algorithms is by their structure and the way of processing data, as seen in this model of six partly overlapping types:[3]
This is an approach to music synthesis that involves "translating" information from an existing non-musical medium into a new sound. The translation can be either rule-based or stochastic. For example, when translating a picture into sound, a JPEG image of a horizontal line may be interpreted in sound as a constant pitch, while an upwards-slanted line may be an ascending scale. Oftentimes, the software seeks to extract concepts or metaphors from the medium, (such as height or sentiment) and apply the extracted information to generate songs using the ways music theory typically represents those concepts. Another example is the translation of text into music,[4][5] which can approach composition by extracting sentiment (positive or negative) from the text using machine learning methods like sentiment analysis and represents that sentiment in terms of chord quality such as minor (sad) or major (happy) chords in the musical output generated.
Mathematical models are based on mathematical equations and random events. The most common way to create compositions through mathematics is stochastic processes. In stochastic models a piece of music is composed as a result of non-deterministic methods. The compositional process is only partially controlled by the composer by weighting the possibilities of random events. Prominent examples of stochastic algorithms are Markov chains and various uses of Gaussian distributions. Stochastic algorithms are often used together with other algorithms in various decision-making processes.
Music has also been composed through natural phenomena. These chaotic models create compositions from the harmonic and inharmonic phenomena of nature. For example, since the 1970s fractals have been studied also as models for algorithmic composition.
When generating well defined styles, music can be seen as a combinatorial optimization problem, whereby the aim is to find the right combination of notes such that the objective function is minimized. This objective function typically contains rules of a particular style, but could be learned using machine learning methods such as Markov models.[8] Researchers have generated music using a myriad of different optimization methods, including integer programming,[9] variable neighbourhood search,[10] and evolutionary methods as mentioned in the next subsection.
Evolutionary methods of composing music are based on genetic algorithms.[11] The composition is being built by the means of evolutionary process. Through mutation and natural selection, different solutions evolve towards a suitable musical piece. Iterative action of the algorithm cuts out bad solutions and creates new ones from those surviving the process. The results of the process are supervised by the critic, a vital part of the algorithm controlling the quality of created compositions.
Learning systems are programs that have no given knowledge of the genre of music they are working with. Instead, they collect the learning material by themselves from the example material supplied by the user or programmer. The material is then processed into a piece of music similar to the example material. This method of algorithmic composition is strongly linked to algorithmic modeling of style,[14] machine improvisation, and such studies as cognitive science and the study of neural networks. Assayag and Dubnov[15] proposed a variable length Markov model to learn motif and phrase continuations of different length. Marchini and Purwins[16] presented a system that learns the structure of an audio recording of a rhythmical percussion fragment using unsupervised clustering and variable length Markov chains and that synthesizes musical variations from it.
Programs based on a single algorithmic model rarely succeed in creating aesthetically satisfying results. For that reason algorithms of different type are often used together to combine the strengths and diminish the weaknesses of these algorithms. Creating hybrid systems for music composition has opened up the field of algorithmic composition and created also many brand new ways to construct compositions algorithmically. The only major problem with hybrid systems is their growing complexity and the need of resources to combine and test these algorithms.[17]
Another approach, which can be called computer-assisted composition, is to algorithmically create certain structures for finally "hand-made" compositions. As early as in the 1960s, Gottfried Michael Koenig developed computer programs Project 1 and Project 2 for aleatoric music, the output of which was sensibly structured "manually" by means of performance instructions. In the 2000s, Andranik Tangian developed a computer algorithm to determine the time event structures for rhythmic canons and rhythmic fugues,[18][19]
which were then worked out into harmonic compositions Eine kleine Mathmusik I and Eine kleine Mathmusik II; for scores and recordings see.[20]
Algorithmic entities refer to autonomous algorithms that operate without human control or interference. Recently, attention is being given to the idea of algorithmic entities being granted (partial or full) legal personhood. Professor Shawn Bayern[1][2] and Professor Lynn M. LoPucki[3] popularized through their papers the idea of having algorithmic entities that obtain legal personhood and the accompanying rights and obligations.
Academics and politicians have been discussing over the last few years whether it is possible to have a legal algorithmic entity, meaning that an algorithm or AI is granted legal personhood. In most countries, the law only recognizes natural or real persons and legal persons. The main argument is that behind every legal person (or layers of legal persons), there is eventually a natural person.[4]
The general consensus is that AI in any case cannot be regarded as a natural or real person and that granting AI (legal) personhood at this stage is unwanted from a societal point of view. However, the academic and public discussions continue as AI software becomes more sophisticated and companies are increasingly implementing artificial intelligence to assist in all aspects of business and society. This leads to some scholars to wonder whether AI should be granted legal personhood as it is not unthinkable to one day have a sophisticated algorithm capable of managing a firm completely independent of human interventions.[6]
Brown argues that the question of whether legal personhood for AI may be granted is tied directly to the issue of whether AI can or should even be allowed to legally own property. [7] Brown “concludes that that legal personhood is the best approach for AI to own personal property.” [8] This is an especially important inquiry since many scholars already recognize AI as having possession and control of some digital assets or even data. AI can also create written text, photo, art, and even algorithms, though ownership of these works is not currently granted to AI in any country because it is not recognized as a legal person.
In Europe, certain academics from different countries have started to look at the possibilities in their respective jurisdictions. Bayern et al. (2017)  compared the UK, Germany and Switzerland to the findings of Bayern (2016) earlier for the US to see whether such “loopholes” in the law exist there as well to set up an algorithmic entity.[10]
In reaction to the European Parliament's request, the European Commission set up a High Level Expert Group to tackle issues and take initiative in a number of subjects relating to automation, robotics and AI. The High Level Expert Group released a draft document for AI ethical guidelines[15] and a document defining AI  in December 2018.[16] The document on ethical guidelines was opened for consultation and received extensive feedback.[17] The European Commission is taking a careful approach legislating AI by emphasizing on ethics, but at the same time – as the EU is behind in AI research to the United States and China – focusing on how to narrow the gap with competitors by creating a more inviting regulatory framework for AI research and development.[18] Giving (limited) legal personality to AI or even allow certain forms of algorithmic entities might create an extra edge.[19]
High-level synthesis (HLS), sometimes referred to as C synthesis, electronic system-level (ESL) synthesis, algorithmic synthesis, or behavioral synthesis, is an automated design process that takes an abstract behavioral specification of a digital system and finds a register-transfer level structure that realizes the given behavior.[1][2][3]
Hardware can be designed at varying levels of abstraction. The commonly used levels of abstraction are gate level, register-transfer level (RTL), and algorithmic level.
While logic synthesis uses an RTL description of the design, high-level synthesis works at a higher level of abstraction, starting with an algorithmic description in a high-level language such as SystemC and ANSI C/C++. The designer typically develops the module functionality and the interconnect protocol. The high-level synthesis tools handle the micro-architecture and transform untimed or partially timed functional code into fully timed RTL implementations, automatically creating cycle-by-cycle detail for hardware implementation.[6] The (RTL) implementations are then used directly in a conventional logic synthesis flow to create a gate-level implementation.
Early academic work extracted scheduling, allocation, and binding as the basic steps for high-level-synthesis. Scheduling partitions the algorithm in control steps that are used to define the states in the finite-state machine. Each control step contains one small section of the algorithm that can be performed in a single clock cycle in the hardware. Allocation and binding maps the instructions and variables to the hardware components, multiplexers, registers and wires of the data path.
In 2006, an efficient and scalable "SDC modulo scheduling" technique was developed on control and data flow graphs [9] and was later extended to pipeline scheduling.[10] This technique uses the integer linear programming formulation. But it shows that the underlying constraint matrix is totally unimodular (after approximating the resource constraints). Thus, the problem can be solved in polynomial time optimally using a linear programming solver in polynomial time. This work was inducted to the FPGA and Reconfigurable Computing Hall of Fame 2022.[11]
The SDC scheduling algorithm was implemented in the xPilot HLS system[12] developed at UCLA,[13] and later licensed to the AutoESL Design Technologies, a spin-off from UCLA. AutoESL was acquired by Xilinx (now part of AMD) in 2011,[11] and the HLS tool developed by AutoESL became the base of Xilinx HLS solutions, Vivado HLS and Vitis HLS, widely used for FPGA designs.
The high-level synthesis process consists of a number of activities. Various high-level synthesis tools perform these activities in different orders using different algorithms. Some high-level synthesis tools combine some of these activities or perform them iteratively to converge on the desired solution.[15]
In general, an algorithm can be performed over many clock cycles with few hardware resources, or over fewer clock cycles using a larger number of ALUs, registers and memories.  Correspondingly, from one algorithmic description, a variety of hardware microarchitectures can be generated by an HLS compiler according to the directives given to the tool.  This is the same trade off of execution speed for hardware complexity as seen when a given program is run on conventional processors of differing performance, yet all running at roughly the same clock frequency.
In mathematics and computer science, an algorithmic technique[1] is a general approach for implementing a process or computation.[2]
There are several broadly recognized algorithmic techniques that offer a proven method or process for designing and constructing algorithms. Different techniques may be used depending on the objective, which may include searching, sorting, mathematical optimization, constraint satisfaction, categorization, analysis, and prediction.[3]
Dynamic programming is a systematic technique in which a complex problem is decomposed recursively into smaller, overlapping subproblems for solution. Dynamic programming stores the results of the overlapping sub-problems locally using an optimization technique called memoization.[6]
Learning techniques employ statistical methods to perform categorization and analysis without explicit programming. Supervised learning, unsupervised learning, reinforcement learning, and deep learning techniques are included in this category.[11]
Recursion is a general technique for designing an algorithm that calls itself with a progressively simpler part of the task down to one or more base cases with defined results.[14][15]
Algorithmic topology, or computational topology, is a subfield of topology with an overlap with areas of computer science, in particular, computational geometry and computational complexity theory.
A primary concern of algorithmic topology, as its name suggests, is to develop efficient algorithms for solving problems that arise naturally in fields such as computational geometry, graphics, robotics, social science, structural biology, and chemistry, using methods from computable topology.[1][2][3]
A large family of algorithms concerning 3-manifolds revolve around normal surface theory, which is a phrase that encompasses several techniques to turn problems in 3-manifold theory into integer linear programming problems.
At present the JSJ decomposition has not been implemented algorithmically in computer software.  Neither has the compression-body decomposition.  There are some very popular and successful heuristics, such as SnapPea which has much success computing approximate hyperbolic structures on triangulated 3-manifolds.  It is known that the full classification of 3-manifolds can be done algorithmically,[9] in fact, it is known that deciding whether two closed, oriented 3-manifolds given by triangulations (simplicial complexes) are equivalent (homeomorphic) is elementary recursive.[10] This generalizes the result on 3-sphere recognition.
Determining whether or not a knot is trivial is known to be in the complexity classes NP[12] as well as co-NP.[13] The problem of determining the genus of a knot in a 3-manifold is NP-complete;[14] however, while  NP remains an upper bound on the complexity of determining the genus of a knot in R3 or S3, as of 2006 it was unknown whether the algorithmic problem of determining the genus of a knot in those particular 3-manifolds was still NP-hard.[14]
Computation of homology groups of cell complexes reduces to bringing the boundary matrices into Smith normal form. Although this is a completely solved problem algorithmically, there are various technical obstacles to efficient computation for large complexes. There are two central obstacles. Firstly, the basic Smith form algorithm has cubic complexity in the size of the matrix involved since it uses row and column operations which makes it unsuitable for large cell complexes. Secondly, the intermediate matrices which result from the application of the Smith form algorithm get filled-in even if one starts and ends with sparse matrices.
Introduction to Algorithms is a book on computer programming by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. The book is described by its publisher as "the leading algorithms text in universities worldwide as well as the standard reference for professionals".[1] It is commonly cited as a reference for algorithms in published papers, with over 10,000 citations documented on CiteSeerX,[2] and over 70,000 citations on Google Scholar as of 2024.[3] The book sold half a million copies during its first 20 years,[4] and surpassed a million copies sold in 2022.[5] Its fame has led to the common use of the abbreviation "CLRS" (Cormen, Leiserson, Rivest, Stein), or, in the first edition, "CLR" (Cormen, Leiserson, Rivest).[6]
In the preface, the authors write about how the book was written to be comprehensive and useful in both teaching and professional environments. Each chapter focuses on an algorithm, and discusses its design techniques and areas of application. Instead of using a specific programming language, the algorithms are written in pseudocode. The descriptions focus on the aspects of the algorithm itself, its mathematical properties, and emphasize efficiency.[7]
The first edition of the textbook did not include Stein as an author, and thus the book became known by the initialism CLR. It included two chapters ("Arithmetic Circuits" & "Algorithms for Parallel Computers") that were dropped in the second edition. After the addition of the fourth author in the second edition, many began to refer to the book as "CLRS". This first edition of the book was also known as "The Big White Book (of Algorithms)." With the second edition, the predominant color of the cover changed to green, causing the nickname to be shortened to just "The Big Book (of Algorithms)."[8] The third edition was published in August 2009. The fourth edition was published in April 2022, which has colors added to improve visual presentations.[9]
Government by algorithm[1] (also known as algorithmic regulation,[2] regulation by algorithms, algorithmic governance,[3][4] algocratic governance, algorithmic legal order or algocracy[5]) is an alternative form of government or social ordering where the usage of computer algorithms is applied to regulations, law enforcement, and generally any aspect of everyday life such as transportation or land registration.[6][7][8][9][10] The term "government by algorithm" has appeared in academic literature as an alternative for "algorithmic governance" in 2013.[11] A related term, algorithmic regulation, is defined as setting the standard, monitoring and modifying behaviour by means of computational algorithms – automation of judiciary is in its scope.[12] In the context of blockchain, it is also known as blockchain governance.[13]
Government by algorithm raises new challenges that are not captured in the e-government literature and the practice of public administration.[14] Some sources equate cyberocracy, which is a hypothetical form of government that rules by the effective use of information,[15][16][17] with algorithmic governance, although algorithms are not the only means of processing information.[18][19] Nello Cristianini and Teresa Scantamburlo argued that the combination of a human society and certain regulation algorithms (such as reputation-based scoring) forms a social machine.[20]
In 1962, the director of the Institute for Information Transmission Problems of the Russian Academy of Sciences in Moscow (later Kharkevich Institute),[21] Alexander Kharkevich, published an article in the journal "Communist" about a computer network for processing information and control of the economy.[22][23] In fact, he proposed to make a network like the modern Internet for the needs of algorithmic governance (Project OGAS). This created a serious concern among CIA analysts.[24] In particular, Arthur M. Schlesinger Jr. warned that "by 1970 the USSR may have a radically new production technology, involving total enterprises or complexes of industries, managed by closed-loop, feedback control employing self-teaching computers".[24]
Since the 2000s, algorithms have been designed and used to automatically analyze surveillance videos.[34]
Sometimes the "rules" aren't really even rules. Gordon Bruce, the former CIO of the city of Honolulu, explained to me that when he entered government from the private sector and tried to make changes, he was told, "That's against the law." His reply was "OK. Show me the law." "Well, it isn't really a law. It's a regulation." "OK. Show me the regulation." "Well, it isn't really a regulation. It's a policy that was put in place by Mr. Somebody twenty years ago." "Great. We can change that!" [...] Laws should specify goals, rights, outcomes, authorities, and limits. If specified broadly, those laws can stand the test of time. Regulations, which specify how to execute those laws in much more detail, should be regarded in much the same way that programmers regard their code and algorithms, that is, as a constantly updated toolset to achieve the outcomes specified in the laws. [...] It's time for government to enter the age of big data. Algorithmic regulation is an idea whose time has come.[38]
In 2017, Ukraine's Ministry of Justice ran experimental government auctions using blockchain technology to ensure transparency and hinder corruption in governmental transactions.[39] "Government by Algorithm?" was the central theme introduced at Data for Policy 2017 conference held on 6–7 September 2017 in London.[40]
A smart city is an urban area where collected surveillance data is used to improve various operations. Increase in computational power allows more automated decision making and replacement of public agencies by algorithmic governance.[41] In particular, the combined use of artificial intelligence and blockchains for IoT may lead to the creation of sustainable smart city ecosystems.[42] Intelligent street lighting in Glasgow is an example of successful government application of AI algorithms.[43] A study of smart city initiatives in the US shows that it requires public sector as a main organizer and coordinator, the private sector as a technology and infrastructure provider, and universities as expertise contributors.[44]
Tim O'Reilly suggested that data sources and reputation systems combined in algorithmic regulation can outperform traditional regulations.[38] For instance, once taxi-drivers are rated by passengers, the quality of their services will improve automatically and "drivers who provide poor service are eliminated".[38] O'Reilly's suggestion is based on the control-theoreric concept of feed-back loop—improvements and disimprovements of reputation enforce desired behavior.[20] The usage of feedback-loops for the management of social systems has already been suggested in management cybernetics by Stafford Beer before.[50]
Smart contracts, cryptocurrencies, and decentralized autonomous organization are mentioned as means to replace traditional ways of governance.[54][55][10] Cryptocurrencies are currencies which are enabled by algorithms without a governmental central bank.[56] Central bank digital currency often employs similar technology, but is differentiated from the fact that it does use a central bank. It is soon to be employed by major unions and governments such as the European Union and China. Smart contracts are self-executable contracts, whose objectives are the reduction of need in trusted governmental intermediators, arbitrations and enforcement costs.[57][58] A decentralized autonomous organization is an organization represented by smart contracts that is transparent, controlled by shareholders and not influenced by a central government.[59][60][61] Smart contracts have been discussed for use in such applications as use in (temporary) employment contracts[62][63] and automatic transfership of funds and property (i.e. inheritance, upon registration of a death certificate).[64][65][66][67] Some countries such as Georgia and Sweden have already launched blockchain programs focusing on property (land titles and real estate ownership)[39][68][69][70] Ukraine is also looking at other areas too such as state registers.[39]
According to a study of Stanford University, 45% of the studied US federal agencies have experimented with AI and related machine learning (ML) tools up to 2020.[1] US federal agencies counted the number of artificial intelligence applications, which are listed below.[1] 53% of these applications were produced by in-house experts.[1] Commercial providers of residual applications include Palantir Technologies.[71]
In 2012, NOPD started a collaboration with Palantir Technologies in the field of predictive policing.[72] Besides Palantir's Gotham software, other similar (numerical analysis software) used by police agencies (such as the NCRIC) include SAS.[73]
In the fight against money laundering, FinCEN employs the FinCEN Artificial Intelligence System (FAIS) since 1995.[74][75]
US Department of Homeland Security has employed the software ATLAS, which run on Amazon Cloud. It scanned more than 16.5 million records of naturalized Americans and flagged approximately 124,000 of them for manual analysis and review by USCIS officers regarding denaturalization.[77][78] They were flagged due to potential fraud, public safety and national security issues. Some of the scanned data came from Terrorist Screening Database and National Crime Information Center.
The NarxCare is a US software,[79] which combines data from the prescription registries of various U.S. states[80][81] and uses machine learning to generate various three-digit "risk scores" for prescriptions of medications and an overall "Overdose Risk Score", collectively referred to as Narx Scores,[82] in a process that potentially includes EMS and criminal justice data[79] as well as court records.[83]
In Estonia, artificial intelligence is used in its e-government to make it more automated and seamless. A virtual assistant will guide citizens through any interactions they have with the government. Automated and proactive services "push" services to citizens at key events of their lives (including births, bereavements, unemployment). One example is the automated registering of babies when they are born.[84] Estonia's X-Road system will also be rebuilt to include even more privacy control and accountability into the way the government uses citizen's data.[85]
Government procurement audit software can also be used.[88][89] Audits are performed in some countries after subsidies have been received.
Judges' decisions in Australia are supported by the "Split Up" software in cases of determining the percentage of a split after a divorce.[95] COMPAS software is used in the USA to assess the risk of recidivism in courts.[96][97] According to the statement of Beijing Internet Court, China is the first country to create an internet court or cyber court.[98][99][100] The Chinese AI judge is a virtual recreation of an actual female judge. She "will help the court's judges complete repetitive basic work, including litigation reception, thus enabling professional practitioners to focus better on their trial work".[98] Also, Estonia plans to employ artificial intelligence to decide small-claim cases of less than €7,000.[101]
Lawbots can perform tasks that are typically done by paralegals or young associates at law firms. One such technology used by US law firms to assist in legal research is from ROSS Intelligence,[102] and others vary in sophistication and dependence on scripted algorithms.[103] Another legal technology chatbot application is DoNotPay.
Due to the COVID-19 pandemic in 2020, in-person final exams were impossible for thousands of students.[104] The public high school Westminster High employed algorithms to assign grades. UK's Department for Education also employed a statistical calculus to assign final grades in A-levels, due to the pandemic.[105]
Besides use in grading, software systems like AI were used in preparation for college entrance exams.[106]
In 2018, an activist named Michihito Matsuda ran for mayor in the Tama city area of Tokyo as a human proxy for an artificial intelligence program.[110] While election posters and campaign material used the term robot, and displayed stock images of a feminine android, the "AI mayor" was in fact a machine learning algorithm trained using Tama city datasets.[111] The project was backed by high-profile executives Tetsuzo Matsumoto of Softbank and Norio Murakami of Google.[112] Michihito Matsuda came third in the election, being defeated by Hiroyuki Abe.[113] Organisers claimed that the 'AI mayor' was programmed to analyze citizen petitions put forward to the city council in a more 'fair and balanced' way than human politicians.[114]
In Cannes, France, monitoring software has been used on footage shot by CCTV cameras, allowing to monitor their compliance to local social distancing and mask wearing during the COVID-19 pandemic. The system does not store identifying data, but rather allows to alert city authorities and police where breaches of the mask and mask wearing rules are spotted (allowing fining to be carried out where needed). The algorithms used by the monitoring software can be incorporated into existing surveillance systems in public spaces (hospitals, stations, airports, shopping centres, ...) [132]
Tsunamis can be detected by Tsunami warning systems. They can make use of AI.[140][141] Floodings can also be detected using AI systems.[142] Wildfires can be predicted using AI systems.[143][144] Wildfire detection is possible by AI systems (i.e. through satellite data, aerial imagery, and GPS phone personnel position) and can help in the evacuation of people during wildfires,[145] to investigate how householders responded in wildfires[146] and spotting wildfire in real time using computer vision.[147][148] Earthquake detection systems are now improving alongside the development of AI technology through measuring seismic data and implementing complex algorithms to improve detection and prediction rates.[149][150][151] Earthquake monitoring, phase picking, and seismic signal detection have developed through AI algorithms of deep-learning, analysis, and computational models.[152] Locust breeding areas can be approximated using machine learning, which could help to stop locust swarms in an early phase.[153]
Algorithmic regulation is supposed to be a system of governance where more exact data, collected from citizens via their smart devices and computers, is used to more efficiently organize human life as a collective.[154][155] As Deloitte estimated in 2017, automation of US government work could save 96.7 million federal hours annually, with a potential savings of $3.3 billion; at the high end, this rises to 1.2 billion hours and potential annual savings of $41.1 billion.[156]
There are potential risks associated with the use of algorithms in government. Those include algorithms becoming susceptible to bias,[157] a lack of transparency in how an algorithm may make decisions,[158] and the accountability for any such decisions.[158] According to a 2016's book Weapons of Math Destruction, algorithms and big data are suspected to increase inequality due to opacity, scale and damage.[159]
There is also a serious concern that gaming by the regulated parties might occur, once more transparency is brought into the decision making by algorithmic governance, regulated parties might try to manipulate their outcome in own favor and even use adversarial machine learning.[1][20] According to Harari, the conflict between democracy and dictatorship is seen as a conflict of two different data-processing systems—AI and algorithms may swing the advantage toward the latter by processing enormous amounts of information centrally.[160]
In 2018, the Netherlands employed an algorithmic system SyRI (Systeem Risico Indicatie) to detect citizens perceived as being high risk for committing welfare fraud, which quietly flagged thousands of people to investigators.[161] This caused a public protest. The district court of Hague shut down SyRI referencing Article 8 of the European Convention on Human Rights (ECHR).[162]
In 2020, algorithms assigning exam grades to students in the UK sparked open protest under the banner "Fuck the algorithm."[105] This protest was successful and the grades were taken back.[165]
In 2020, the US government software ATLAS, which run on Amazon Cloud, sparked uproar from activists and Amazon's own employees.[166]
In 2021, Eticas Foundation launched a database of governmental algorithms called Observatory of Algorithms with Social Impact (OASI).[167]
An initial approach towards transparency included the open-sourcing of algorithms.[168] Software code can be looked into and improvements can be proposed through source-code-hosting facilities.
A 2019 poll conducted by IE University's Center for the Governance of Change in Spain found that 25% of citizens from selected European countries were somewhat or totally in favor of letting an artificial intelligence make important decisions about how their country is run.[169] The following table lists the results by country:
A 2021 poll by IE University indicates that 51% of Europeans are in favor of reducing the number of national parliamentarians and reallocating these seats to an algorithm. This proposal has garnered substantial support in Spain (66%), Italy (59%), and Estonia (56%). Conversely, the citizens of Germany, the Netherlands, the United Kingdom, and Sweden largely oppose the idea.[171] The survey results exhibit significant generational differences. Over 60% of Europeans aged 25-34 and 56% of those aged 34-44 support the measure, while a majority of respondents over the age of 55 are against it. International perspectives also vary: 75% of Chinese respondents support the proposal, whereas 60% of Americans are opposed.[171]
The novels Daemon (2006) and Freedom™ (2010) by Daniel Suarez describe a fictional scenario of global algorithmic regulation.[173] Matthew De Abaitua's If Then imagines an algorithm supposedly based on "fairness" recreating a premodern rural economy.[174]
Regulation of algorithms, or algorithmic regulation, is the creation of laws, rules and public sector policies for promotion and regulation of algorithms, particularly in artificial intelligence and machine learning.[1][2][3] For the subset of AI algorithms, the term regulation of artificial intelligence is used. The regulatory and  policy landscape for artificial intelligence (AI) is an emerging issue in jurisdictions globally, including in the European Union.[4] Regulation of AI is considered necessary to both encourage AI and manage associated risks, but challenging.[5] Another emerging topic is the regulation of blockchain algorithms (Use of the smart contracts must be regulated) and is mentioned along with regulation of AI algorithms.[6] Many countries have enacted regulations of high frequency trades, which is shifting due to technological progress into the realm of AI algorithms.[citation needed]
The motivation for regulation of algorithms is the apprehension of losing control over the algorithms, whose impact on human life increases. Multiple countries have already introduced regulations in case of automated credit score calculation—right to explanation is mandatory for those algorithms.[7][8]  For example, The IEEE has begun developing a new standard to explicitly address ethical issues and the values of potential future users.[9] Bias, transparency, and ethics concerns have emerged with respect to the use of algorithms in diverse domains ranging from criminal justice[10] to healthcare[11]—many fear that artificial intelligence could replicate existing social inequalities along race, class, gender, and sexuality lines.
In 2016, Joy Buolamwini founded Algorithmic Justice League after a personal experience with biased facial detection software in order to raise awareness of the social implications of artificial intelligence through art and research.[12]
In 2017 Elon Musk advocated regulation of algorithms in the context of the existential risk from artificial general intelligence.[13][14][15] According to NPR, the Tesla CEO was "clearly not thrilled" to be advocating for government scrutiny that could impact his own industry, but believed the risks of going completely without oversight are too high: "Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation."[13]
In response, some politicians expressed skepticism about the wisdom of regulating a technology that is still in development.[14] Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO Brian Krzanich has argued that artificial intelligence is in its infancy and that it is too early to regulate the technology.[15] Instead of trying to regulate the technology itself, some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.[16] One suggestion has been for the development of a global governance board to regulate AI development.[17] In 2020, the European Union published its draft strategy paper for promoting and regulating AI.[18]
Algorithmic tacit collusion is a legally dubious antitrust practise committed by means of algorithms, which the courts are not able to prosecute.[19] This danger concerns scientists and regulators in EU, US and beyond.[19] European Commissioner Margrethe Vestager mentioned an early example of algorithmic tacit collusion in her speech on "Algorithms and Collusion" on March 16, 2017, described as follows:[20]
"A few years ago, two companies were selling a textbook called The Making of a Fly. One of those sellers used an algorithm which essentially matched its rival’s price. That rival had an algorithm which always set a price 27% higher than the first. The result was that prices kept spiralling upwards, until finally someone noticed what was going on, and adjusted the price manually. By that time, the book was selling – or rather, not selling – for 23 million dollars a copy."
In 2018, the Netherlands employed an algorithmic system SyRI (Systeem Risico Indicatie) to detect citizens perceived being high risk for committing welfare fraud, which quietly flagged thousands of people to investigators.[21] This caused a public protest. The district court of Hague shut down SyRI referencing Article 8 of the European Convention on Human Rights (ECHR).[22]
In 2020, algorithms assigning exam grades to students in the UK sparked open protest under the banner "Fuck the algorithm."[23] This protest was successful and the grades were taken back.[24]
AI law and regulations can be divided into three main topics, namely governance of autonomous intelligence systems, responsibility and accountability for the systems, and privacy and safety issues.[5] The development of public sector strategies for management and regulation of AI has been increasingly deemed necessary at the local, national,[25] and international levels[18] and in fields from public service management[26] to law enforcement,[18] the financial sector,[25] robotics,[27] the military,[28] and international law.[29][30]  There are many concerns that there is not enough visibility and monitoring of AI in these sectors.[31]  In the United States financial sector, for example, there have been calls for the Consumer Financial Protection Bureau to more closely examine source code and algorithms when conducting audits of financial institutions' non-public data.[32]
In the United States, on January 7, 2019, following an Executive Order on 'Maintaining American Leadership in Artificial Intelligence', the White House's Office of Science and Technology Policy released a draft Guidance for Regulation of Artificial Intelligence Applications, which includes ten principles for United States agencies when deciding whether and how to regulate AI.[33][34] In response, the National Institute of Standards and Technology has released a position paper,[35] the National Security Commission on Artificial Intelligence has published an interim report,[36] and the Defense Innovation Board has issued recommendations on the ethical use of AI.[37]
In April 2016, for the first time in more than two decades, the European Parliament adopted a set of comprehensive regulations for the collection, storage, and use of personal information, the General Data Protection Regulation (GDPR)1 (European Union, Parliament and Council 2016).[6]  The GDPR's policy on the right of citizens to receive an explanation for algorithmic decisions highlights the pressing importance of human interpretability in algorithm design.[38]
In 2016, China published a position paper questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue,[29] and leading to proposals for global regulation.[39] In the United States, steering on regulating security-related AI is provided by the National Security Commission on Artificial Intelligence.[40]
In 2017, the U.K. Vehicle Technology and Aviation Bill imposes liability on the owner of an uninsured automated vehicle when driving itself and makes provisions for cases where the owner has made “unauthorized alterations” to the vehicle or failed to update its software. Further ethical issues arise when, e.g., a self-driving car swerves to avoid a pedestrian and causes a fatal accident.[41]
There is a concept of algorithm certification emerging as a method of regulating algorithms. Algorithm certification involves auditing whether the algorithm used during the life cycle 1) conforms to the protocoled requirements (e.g., for correctness, completeness, consistency, and accuracy); 2) satisfies the standards, practices, and conventions; and 3) solves the right problem (e.g., correctly model physical laws), and satisfies the intended use and user needs in the operational environment.[9]
In 1942, author Isaac Asimov addressed regulation of algorithms by introducing the fictional Three Laws of Robotics:
The main alternative to regulation is a ban, and the banning of algorithms is presently highly unlikely. However, in Frank Herbert's Dune universe, thinking machines is a collective term for artificial intelligence, which were completely destroyed and banned after a revolt known as the Butlerian Jihad:[50]
The NIST Dictionary of Algorithms and Data Structures[1] is a reference work maintained by the U.S. National Institute of Standards and Technology. It defines a large number of terms relating to algorithms and data structures. For algorithms and data structures not necessarily mentioned here, see list of algorithms and list of data structures.
In computer programming, several language mechanisms exist for exception handling. The term exception is typically used to denote a data structure storing information about an exceptional condition. One mechanism to transfer control, or raise an exception, is known as a throw; the exception is said to be thrown. Execution is transferred to a catch.
Programming languages differ substantially in their notion of what an exception is. Exceptions can be used to represent and handle abnormal, unpredictable, erroneous situations, but also as flow control structures to handle normal situations. For example, Python's iterators throw StopIteration exceptions to signal that there are no further items produced by the iterator.[1] There is disagreement within many languages as to what constitutes idiomatic usage of exceptions. For example, Joshua Bloch states that Java's exceptions should only be used for exceptional situations,[2] but Kiniry observes that Java's built-in FileNotFoundException is not at all an exceptional event.[3] Similarly, Bjarne Stroustrup, author of C++, states that C++ exceptions should only be used for error handling, as this is what they were designed for,[4] but Kiniry observes that many modern languages such as Ada, C++,
Modula-3, ML and OCaml, Python, and Ruby use exceptions for flow control. Some languages such as Eiffel, C#, Common Lisp, and Modula-2 have made a concerted effort to restrict their usage of exceptions, although this is done on a social rather than technical level.[3]
Software exception handling continued to be developed in the 1960s and 1970s. LISP 1.5 (1958-1961)[5] allowed exceptions to be raised by the ERROR pseudo-function, similarly to errors raised by the interpreter or compiler. Exceptions were caught by the ERRORSET keyword, which returned NIL in case of an error, instead of terminating the program or entering the debugger.[6]
PL/I introduced its own form of exception handling circa 1964, allowing interrupts to be handled with ON units.[7]
MacLisp observed that ERRSET and ERR were used not only for error raising, but for non-local control flow, and thus added two new keywords, CATCH and THROW (June 1972).[8] The cleanup behavior now generally called "finally" was introduced in NIL (New Implementation of LISP) in the mid- to late-1970s as UNWIND-PROTECT.[9] This was then adopted by Common Lisp. Contemporary with this was dynamic-wind in Scheme, which handled exceptions in closures. The first papers on structured exception handling were Goodenough (1975a) and Goodenough (1975b).[10] Exception handling was subsequently widely adopted by many programming languages from the 1980s onward.
Also common is a related clause (finally or ensure) that is executed whether an exception occurred or not, typically to release resources acquired within the body of the exception-handling block. Notably, C++ does not provide this construct, recommending instead the Resource Acquisition Is Initialization (RAII) technique which frees resources using destructors.[12] According to a 2008 paper by Westley Weimer and George Necula, the syntax of the try...finally blocks in Java is a contributing factor to software defects. When a method needs to handle the acquisition and release of 3–5 resources, programmers are apparently unwilling to nest enough blocks due to readability concerns, even when this would be a correct solution. It is possible to use a single try...finally block even when dealing with multiple resources, but that requires a correct use of sentinel values, which is another common source of bugs for this type of problem.[13]: 8:6–8:7
He backed this statement with experience from several operating systems. The key example was Cedar/Mesa: It was written by people who liked and used resumption, but after ten years of use, there was only one use of resumption left in the half million line system – and that was a context inquiry. Because resumption wasn't actually necessary for such a context inquiry, they removed it and found a significant speed increase in that part of the system. In each and every case where resumption had been used it had – over the ten years – become a problem and a more appropriate design had replaced it. Basically, every use of resumption had represented a failure to keep separate levels of abstraction disjoint.[10]
Exception-handling languages with resumption include Common Lisp with its Condition System, PL/I, Dylan, R,[17] and Smalltalk. However, the majority of newer programming languages follow C++ and use termination semantics.
The implementation of exception handling in programming languages typically involves a fair amount of support from both a code generator and the runtime system accompanying a compiler. (It was the addition of exception handling to C++ that ended the useful lifetime of the original C++ compiler, Cfront.[18])  Two schemes are most common. The first, dynamic registration, generates code that continually updates structures about the program state in terms of exception handling.[19]  Typically, this adds a new element to the stack frame layout that knows what handlers are available for the function or method associated with that frame; if an exception is thrown, a pointer in the layout directs the runtime to the appropriate handler code. This approach is compact in terms of space, but adds execution overhead on frame entry and exit. It was commonly used in many Ada implementations, for example, where complex generation and runtime support was already needed for many other language features. Microsoft's 32-bit Structured Exception Handling (SEH) uses this approach with a separate exception stack.[20] Dynamic registration, being fairly straightforward to define, is amenable to proof of correctness.[21]
Other definitional and implementation schemes have been proposed as well. For languages that support metaprogramming, approaches that involve no overhead at all (beyond the already present support for reflection) have been advanced.[26]
The "Safe Exception Handling principle" as introduced by Bertrand Meyer in Object-Oriented Software Construction then holds that there are only two meaningful ways a routine can react when an exception occurs:
Here is an example expressed in Eiffel syntax. It assumes that a routine send_fast is normally the better way to send a message, but it may fail, triggering an exception; if so, the algorithm next uses send_slow, which will fail less often. If send_slow fails, the routine send as a whole should fail, causing the caller to get an exception.
Contemporary applications face many design challenges when considering exception handling strategies. Particularly in modern enterprise level applications, exceptions must often cross process boundaries and machine boundaries. Part of designing a solid exception handling strategy is recognizing when a process has failed to the point where it cannot be economically handled by the software portion of the process.[27]
Kiniry writes that "As any Java programmer knows, the volume of try catch code in a typical Java application is sometimes larger than the comparable code necessary for explicit formal parameter and return value checking in other languages that do not have checked exceptions. In fact, the general consensus among in-the-trenches Java programmers is that dealing with checked exceptions is nearly as unpleasant a task as writing documentation. Thus, many programmers report that they “resent” checked exceptions.".[3] Martin Fowler has written "...on the whole I think that exceptions are good, but Java checked exceptions are more trouble than they are worth."[36] As of 2006 no major programming language has followed Java in adding checked exceptions.[36] For example, C# does not require or allow declaration of any exception specifications, with the following posted by Eric Gunnerson:[37][3][36]
"Examination of small programs leads to the conclusion that requiring exception specifications could both enhance developer productivity and enhance code quality, but experience with large software projects suggests a different result – decreased productivity and little or no increase in code quality."
The roots of checked exceptions go back to the CLU programming language's notion of exception specification.[43] A function could raise only exceptions listed in its type, but any leaking exceptions from called functions would automatically be turned into the sole runtime exception, failure, instead of resulting in compile-time error.[44] Later, Modula-3 had a similar feature.[45] These features don't include the compile time checking that is central in the concept of checked exceptions.[43]
Early versions of the C++ programming language included an optional mechanism similar to checked exceptions, called exception specifications. By default any function could throw any exception, but this could be limited by a throw clause added to the function signature, that specified which exceptions the function may throw. Exception specifications were not enforced at compile-time. Violations resulted in the global function std::unexpected being called.[46]  An empty exception specification could be given, which indicated that the function will throw no exception. This was not made the default when exception handling was added to the language because it would have required too much modification of existing code, would have impeded interaction with code written in other languages, and would have tempted programmers into writing too many handlers at the local level.[46] Explicit use of empty exception specifications could, however, allow C++ compilers to perform significant code and stack layout optimizations that are precluded when exception handling may take place in a function.[23]  Some analysts viewed the proper use of exception specifications in C++ as difficult to achieve.[47] This use of exception specifications was included in C++98 and C++03, deprecated in the 2012 C++ language standard (C++11),[48] and was removed from the language in C++17. A function that will not throw any exceptions can now be denoted by the noexcept keyword.
An uncaught exceptions analyzer exists for the OCaml programming language.[49] The tool reports the set of raised exceptions as an extended type signature. But, unlike checked exceptions, the tool does not require any syntactic annotations and is external (i.e. it is possible to compile and run a program without having checked the exceptions).
The point of exception handling routines is to ensure that the code can handle error conditions. In order to establish that exception handling routines are sufficiently robust, it is necessary to present the code with a wide spectrum of invalid or unexpected inputs, such as can be created via software fault injection and mutation testing (that is also sometimes referred to as fuzz testing). One of the most difficult types of software for which to write exception handling routines is protocol software, since a robust protocol implementation must be prepared to receive input that does not comply with the relevant specification(s).
In order to ensure that meaningful regression analysis can be conducted throughout a software development lifecycle process, any exception handling testing should be highly automated, and the test cases must be generated in a scientific, repeatable fashion. Several commercially available systems exist that perform such testing.
Programming languages typically avoid or restrict asynchronous exception handling, for example C++ forbids raising exceptions from signal handlers, and Java has deprecated the use of its ThreadDeath exception that was used to allow one thread to stop another one.[52] Another feature is a semi-asynchronous mechanism that raises an asynchronous exception only during certain operations of the program. For example, Java's Thread.interrupt() only affects the thread when the thread calls an operation that throws InterruptedException.[53] The similar POSIX pthread_cancel API has race conditions which make it impossible to use safely.[54]
Exception handling is often not handled correctly in software, especially when there are multiple sources of exceptions; data flow analysis of 5 million lines of Java code found over 1300 exception handling defects.[13]
Citing multiple prior studies by others (1999–2004) and their own results, Weimer and Necula wrote that a significant problem with exceptions is that they "create hidden control-flow paths that are difficult for programmers to reason about".[13]: 8:27  "While try-catch-finally is conceptually simple, it has the most complicated execution description in the language specification [Gosling et al. 1996] and requires four levels of nested “if”s in its official English description. In short, it contains a large number of corner cases that programmers often overlook."[13]: 8:13–8:14
Tony Hoare in 1980 described the Ada programming language as having "...a plethora of features and notational conventions, many of them unnecessary and some of them, like exception handling, even dangerous. [...] Do not allow this language in its present state to be used in applications where reliability is critical [...]. The next rocket to go astray as a result of a programming language error may not be an exploratory space rocket on a harmless trip to Venus: It may be a nuclear warhead exploding over one of our own cities."[58]
In computer programming, a programming language implementation is a system for executing computer programs. There are two general approaches to programming language implementation:[1]
A virtual machine is a special type of interpreter that interprets bytecode.[2] Bytecode is a portable low-level code similar to machine code, though it is generally executed on a virtual machine instead of a physical machine.[4] To improve their efficiencies, many programming languages such as Java,[4] Python,[5] and C#[6] are compiled to bytecode before being interpreted.
A compiler translates programs written in one language into another language. Most compilers are organized into three stages: a front end, an optimizer, and a back end. The front end is responsible for understanding the program. It makes sure a program is valid and transforms it into an intermediate representation, a data structure used by the compiler to represent the program. The optimizer improves the intermediate representation to increase the speed or reduce the size of the executable which is ultimately produced by the compiler. The back end converts the optimized intermediate representation into the output language of the compiler.[7]
Many combinations of interpretation and compilation are possible, and many modern programming language implementations include elements of both. For example, the Smalltalk programming language is conventionally implemented by compilation into bytecode, which is then either interpreted or compiled by a virtual machine. Since Smalltalk bytecode is run on a virtual machine, it is portable across different hardware platforms.[8]
Programming languages can have multiple implementations. Different implementations can be written in different languages and can use different methods to compile or interpret code. For example, implementations of Python include: [9]
Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages. Programming language theory is closely related to other fields including mathematics, software engineering, and linguistics.
In some ways, the history of programming language theory predates even the development of programming languages themselves. The lambda calculus, developed by Alonzo Church and Stephen Cole Kleene in the 1930s, is considered by some to be the world's first programming language, even though it was intended to model computation rather than being a means for programmers to describe algorithms to a computer system. Many modern functional programming languages have been described as providing a "thin veneer" over the lambda calculus,[2] and many are easily described in terms of it.
The first programming language to be invented was Plankalkül, which was designed by Konrad Zuse in the 1940s, but not publicly known until 1972 (and not implemented until 1998). The first widely known and successful high-level programming language was FORTRAN (Stands for Formula Translation), developed from 1954 to 1957 by a team of IBM researchers led by John Backus. The success of FORTRAN led to the formation of a committee of scientists to develop a "universal" computer language; the result of their effort was ALGOL 58. Separately, John McCarthy of MIT developed Lisp, the first language with origins in academia to be successful. With the success of these initial efforts, programming languages became an active topic of research in the 1960s and beyond.
There are several fields of study that either lie within programming language theory, or which have a profound influence on it; many of these have considerable overlap. In addition, PLT makes use of many other branches of mathematics, including computability theory, category theory, and set theory.
Formal semantics is the formal specification of the behaviour of computer programs and programming languages. Three common approaches to describe the semantics or "meaning" of a computer program are denotational semantics, operational semantics and axiomatic semantics.
Type theory is the study of type systems; which are "a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute".[4] Many programming languages are distinguished by the characteristics of their type systems.
Comparative programming language analysis seeks to classify programming languages into different types based on their characteristics; broad categories of programming languages are often known as programming paradigms.
Metaprogramming is the generation of higher-order programs which, when executed, produce programs (possibly in a different language, or in a subset of the original language) as a result.
Run-time systems refer to the development of programming language runtime environments and their components, including virtual machines, garbage collection, and foreign function interfaces.
Conferences are the primary venue for presenting research in programming languages. The most well known conferences include the Symposium on Principles of Programming Languages (POPL), Programming Language Design and Implementation (PLDI), the International Conference on Functional Programming (ICFP), the International Conference on Object Oriented Programming, Systems, Languages and Applications (OOPSLA) and the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS).
Notable journals that publish PLT research include the ACM Transactions on Programming Languages and Systems (TOPLAS), Journal of Functional Programming (JFP), Journal of Functional and Logic Programming, and Higher-Order and Symbolic Computation.
A first-generation programming language (1GL) is a machine-level programming language and belongs to the low-level programming languages.[1]
A first generation (programming) language (1GL) is a grouping of programming languages that are machine level languages used to program first-generation computers. Originally, no translator was used to compile or assemble the first-generation language. The first-generation programming instructions were entered through the front panel switches of the computer system.
The main advantage of programming in 1GL is that the code can run very fast and very efficiently, precisely because the instructions are executed directly by the central processing unit (CPU). One of the main disadvantages of programming in a low level language is that when an error occurs, the code is not as easy to fix.
Modern day programmers still occasionally use machine level code, especially when programming lower level functions of the system, such as drivers, interfaces with firmware and hardware devices. Modern tools such as native-code compilers are used to produce machine level from a higher-level language.
The label of second-generation programming language (2GL) is a generational way to categorize assembly languages.[1][2][3]  They belong to the low-level programming languages.
The term was coined to provide a distinction from higher level machine independent third-generation programming languages (3GLs) (such as COBOL, C, or JavaScript) and earlier first-generation programming languages (machine code)[2][4]
In modern programs, second generation assembly languages are rarely used.[5] Programming in second generation languages may yield speed benefits, but several disadvantages have led to its decline:
The vast majority of programs are written in a third-generation programming language or a fourth-generation programming language. Assembly's main advantage, speed, has degraded by the fact that well written C code can often be as fast or even faster than handwritten assembly.[2][6]
Second generation languages are perhaps most significant in their place in computing history. For a long time, Second generation assembly languages were the only good option for development for many machines, such as the NES or the Commodore 64.[7][8] Second generation languages represented a massive step away from the tradition of programmers conforming to the needs of a machine, and the first step towards the machine accommodating for the programmer, a phenomenon that would be repeated in all subsequent programming language generations.[2][3]
A third-generation programming language (3GL) is a high-level computer programming language that tends to be more machine-independent and programmer-friendly than the machine code of the first-generation and assembly languages of the second-generation, while having a less specific focus to the fourth and fifth generations.[1] Examples of common and historical third-generation programming languages are ALGOL, BASIC, C, COBOL, Fortran, Java, and Pascal.
Most popular general-purpose languages today, such as C, C++, C#, Java, BASIC and Pascal, are also third-generation languages, although each of these languages can be further subdivided into other categories based on other contemporary traits. Most 3GLs support structured programming. Many support object-oriented programming. Traits like these are more often used to describe a language rather than just being a 3GL.
These programs could run on different machines (they are portable) so they were machine-independent. As new, more abstract languages have been developed, however, the concept of high- and low-level languages have become rather relative. Many of the early "high-level" languages are now considered relatively low-level in comparison to languages such as Python, Ruby, and Common Lisp, which have some features of fourth-generation programming languages and were called very high-level programming languages in the 1990s.[2][3]
ML (Meta Language) is a general-purpose, high-level, functional programming language. It is known for its use of the polymorphic Hindley–Milner type system, which automatically assigns the data types of most expressions without requiring explicit type annotations (type inference), and ensures type safety; there is a formal proof that a well-typed ML program does not cause runtime type errors.[1] ML provides pattern matching for function arguments, garbage collection, imperative programming, call-by-value and currying. While a general-purpose programming language, ML is used heavily in programming language research and is one of the few languages to be completely specified and verified using formal semantics. Its types and pattern matching make it well-suited and commonly used to operate on other formal languages, such as in compiler writing, automated theorem proving, and formal verification.
ML can be referred to as an impure functional language, because although it encourages functional programming, it does allow side-effects[3] (like languages such as Lisp, but unlike a purely functional language such as Haskell). Like most programming languages, ML uses eager evaluation, meaning that all subexpressions are always evaluated, though lazy evaluation can be achieved through the use of closures. Thus, infinite streams can be created and used as in Haskell, but their expression is indirect.
The function also relies on pattern matching, an important part of ML programming. Note that parameters of a function are not necessarily in parentheses but separated by spaces. When the function's argument is 0 (zero) it will return the integer 1 (one). For all other cases the second line is tried. This is the recursion, and executes the function again until the base case is reached.
In computer programming, a parameter or a  formal argument is a special kind of variable used in a subroutine to refer to one of the pieces of data provided as input to the subroutine.[a][1] These pieces of data are the values[2][3][4] of the arguments (often called actual arguments or actual parameters) with which the subroutine is going to be called/invoked. An ordered list of parameters is usually included in the definition of a subroutine, so that, each time the subroutine is called, its arguments for that call are evaluated, and the resulting values can be assigned to the corresponding parameters.
Unlike argument in usual mathematical usage, the argument in computer science is the actual input expression passed/supplied to a function, procedure, or routine in the invocation/call statement, whereas the parameter is the variable inside the implementation of the subroutine. For example, if one defines the add subroutine as def add(x, y): return x + y, then x, y are parameters, while if this is called as add(2, 3), then 2, 3 are the arguments. Variables (and expressions thereof) from the calling context can be arguments: if the subroutine is called as a = 2; b = 3; add(a, b) then the variables a, b are the arguments, not the values 2, 3. See the Parameters and arguments section for more information.
The following program in the C programming language defines a function that is named "SalesTax" and has one parameter named "price". The type of price is "double" (i.e. a double-precision floating point number). The function's return type is also a double.
The terms parameter and argument may have different meanings in different programming languages. Sometimes they are used interchangeably, and the context is used to distinguish the meaning.  The term parameter (sometimes called formal parameter) is often used to refer to the variable as found in the function declaration, while argument (sometimes called actual parameter) refers to the actual input supplied at a function call statement. For example, if one defines a function as def f(x): ..., then x is the parameter, and if it is called by a = ...; f(a) then a is the argument. A parameter is an (unbound) variable, while the argument can be a literal or variable or more complex expression involving literals and variables. In case of call by value, what is passed to the function is the value of the argument – for example, f(2) and a = 2; f(a) are equivalent calls – while in call by reference, with a variable as argument, what is passed is a reference to that variable - even though the syntax for the function call could stay the same.[5] The specification for pass-by-reference or pass-by-value would be made in the function declaration and/or definition.
Within the Eiffel software development method and language, the terms argument and parameter have distinct uses established by convention. The term argument is used exclusively in reference to a routine's inputs,[7] and the term parameter is used exclusively in type parameterization for generic classes.[8]
In strongly typed programming languages, each parameter's type must be specified in the procedure declaration.  Languages using type inference attempt to discover the types automatically from the function's body and usage. Dynamically typed programming languages defer type resolution until run-time. Weakly typed languages perform little to no type resolution, relying instead on the programmer for correctness.
Some programming languages such as Ada, C++, Clojure,[citation needed] Common Lisp,[9] Fortran 90,[10] Python, Ruby, Tcl, and Windows PowerShell[citation needed] allow for a default argument to be explicitly or implicitly given in a subroutine's declaration. This allows the caller to omit that argument when calling the subroutine. If the default argument is explicitly given, then that value is used if it is not provided by the caller. If the default argument is implicit (sometimes by using a keyword such as Optional) then the language provides a well-known value (such as null, Empty, zero, an empty string, etc.) if a value is not provided by the caller.
Some programming languages—such as Ada and Windows PowerShell—allow subroutines to have named parameters. This allows the calling code to be more self-documenting. It also provides more flexibility to the caller, often allowing the order of the arguments to be changed, or for arguments to be omitted as needed.
In lambda calculus, each function has exactly one parameter. What is thought of as functions with multiple parameters is usually represented in lambda calculus as a function which takes the first argument, and returns a function which takes the rest of the arguments; this is a transformation known as currying. Some programming languages, like ML and Haskell, follow this scheme. In these languages, every function has exactly one parameter, and what may look like the definition of a function of multiple parameters, is actually syntactic sugar for the definition of a function that returns a function, etc. Function application is left-associative in these languages as well as in lambda calculus, so what looks like an application of a function to multiple arguments is correctly evaluated as the function applied to the first argument, then the resulting function applied to the second argument, etc.
Output parameters are often discouraged in modern programming, essentially as being awkward, confusing, and too low-level – commonplace return values are considerably easier to understand and work with.[18] Notably, output parameters involve functions with side effects (modifying the output parameter) and are semantically similar to references, which are more confusing than pure functions and values, and the distinction between output parameters and input/output parameters can be subtle. Further, since in common programming styles most parameters are simply input parameters, output parameters and input/output parameters are unusual and hence susceptible to misunderstanding.
The usual alternative to output parameters in C and related languages is to return a single data structure containing all return values.[13] For example, given a structure encapsulating width and height, one can write:
C# (/ˌsiː ˈʃɑːrp/ see SHARP)[b] is a general-purpose high-level programming language supporting multiple paradigms. C# encompasses static typing,[16]: 4  strong typing, lexically scoped, imperative, declarative, functional, generic,[16]: 22  object-oriented (class-based), and component-oriented programming disciplines.[17]
The principal inventors of the C# programming language were Anders Hejlsberg, Scott Wiltamuth, and Peter Golde from Microsoft.[17] It was first widely distributed in July 2000[17] and was later approved as an international standard by Ecma (ECMA-334) in 2002 and ISO/IEC (ISO/IEC 23270 and 20619[c]) in 2003. Microsoft introduced C# along with .NET Framework and Microsoft Visual Studio, both of which are technically speaking, closed-source. At the time, Microsoft had no open-source products. Four years later, in 2004, a free and open-source project called Microsoft Mono began, providing a cross-platform compiler and runtime environment for the C# programming language. A decade later, Microsoft released Visual Studio Code (code editor), Roslyn (compiler), and the unified .NET platform (software framework), all of which support C# and are free, open-source, and cross-platform. Mono also joined Microsoft but was not merged into .NET.
Hejlsberg is C#'s principal designer and lead architect at Microsoft, and was previously involved with the design of Turbo Pascal, Embarcadero Delphi (formerly CodeGear Delphi, Inprise Delphi and Borland Delphi), and Visual J++. In interviews and technical papers, he has stated that flaws[25] in most major programming languages (e.g. C++, Java, Delphi, and Smalltalk) drove the fundamentals of the Common Language Runtime (CLR), which, in turn, drove the design of the C# language.
James Gosling, who created the Java programming language in 1994, and Bill Joy, a co-founder of Sun Microsystems, the originator of Java, called C# an "imitation" of Java; Gosling further said that "[C# is] sort of Java with reliability, productivity and security deleted."[26][27] 
In July 2000, Hejlsberg said that C# is "not a Java clone" and is "much closer to C++" in its design.[28]
Since the release of C# 2.0 in November 2005, the C# and Java languages have evolved on increasingly divergent trajectories, becoming two quite different languages. One of the first major departures came with the addition of generics to both languages, with vastly different implementations. C# uses of reification to provide "first-class" generic objects that can be used like any other class, with code generation performed at class-load time.[29]
Furthermore, C# has added several major features to accommodate functional-style programming, culminating in the LINQ extensions released with C# 3.0 and its supporting framework of lambda expressions, extension methods, and anonymous types.[30]  These features enable C# programmers to use functional programming techniques, such as closures, when it is advantageous to their application. The LINQ extensions and the functional imports help developers reduce the amount of boilerplate code included in common tasks such as querying a database, parsing an XML file, or searching through a data structure, shifting the emphasis onto the actual program logic to help improve readability and maintainability.[31]
Due to technical limits of display (standard fonts, browsers, etc.), and most keyboard layouts lacking a sharp symbol (U+266F ♯ MUSIC SHARP SIGN (&sharp;)), the number sign (U+0023 # NUMBER SIGN (&num;)) was chosen to approximate the sharp symbol in the written name of the programming language.[40]
This convention is reflected in the ECMA-334 C# Language Specification.[17]
The "sharp" suffix has been used by a number of other Microsoft .NET compatible/compliant languages that are variants of existing languages, including J# (a .NET language also designed by Microsoft that is derived from Java 1.1), A# (from Ada), and the functional programming language F#.[41] The original implementation of Eiffel for .NET was called Eiffel#,[42] a name retired since the full Eiffel language is now supported. The suffix has also been used for libraries, such as Gtk# (a .NET wrapper for GTK and other GNOME libraries) and Cocoa# (a wrapper for Cocoa).
By design, C# is the programming language that most directly reflects the underlying Common Language Infrastructure (CLI).[70]  Most of its intrinsic types correspond to value-types implemented by the CLI (Common Language Infrastructure) framework. However, the language specification does not state the code generation requirements of the compiler: that is, it does not state that a C# compiler must target a Common Language Runtime (CLR), or generate Common Intermediate Language (CIL), or generate any other specific format. Some C# compilers can also generate machine code like traditional compilers of Objective-C, C, C++, Assembly and Fortran.[71][72]
C# supports a strict Boolean data type, bool. Statements that take conditions, such as while and if, require an expression of a type that implements the true operator, such as the Boolean type. While C++ also has a Boolean type, it can be freely converted to and from integers, and expressions such as if (a) require only that a is convertible to bool, allowing a to be an int, or a pointer. C# disallows this "integer meaning true or false" approach, on the grounds that forcing programmers to use expressions that return exactly bool can prevent certain types of programming mistakes such as if (a = b) (use of assignment = instead of equality ==).
C# has the ability to utilize LINQ through the .NET Framework. A developer can query a variety of data sources, provided the IEnumerable<T> interface is implemented on the object. This includes XML documents, an ADO.NET dataset, and SQL databases.[96]
Generics introduced a massive new feature in .NET that allowed developers to create type-safe data structures. This shift is particularly important in the context of converting legacy systems, where updating to generics can significantly enhance performance and maintainability by replacing outdated data structures with more efficient, type-safe alternatives.[113]
In August 2001, Microsoft, Hewlett-Packard and Intel co-sponsored the submission of specifications for C# as well as the Common Language Infrastructure (CLI) to the standards organization Ecma International. In December 2001, ECMA released ECMA-334 C# Language Specification. C# became an ISO/IEC standard in 2003 (ISO/IEC 23270:2003 - Information technology — Programming languages — C#). ECMA had previously adopted equivalent specifications as the 2nd edition of C#, in December 2002. In June 2005, ECMA approved edition 3 of the C# specification, and updated ECMA-334. Additions included partial classes, anonymous methods, nullable types, and generics (somewhat similar to C++ templates). In July 2005, ECMA submitted to ISO/IEC JTC 1/SC 22, via the latter's Fast-Track process, the standards and related TRs. This process usually takes 6–9 months.
I do consider assignment statements and pointer variables to be among computer science's "most valuable treasures."
In computer science, a pointer is an object in many programming languages that stores a memory address. This can be that of another value located in computer memory, or in some cases, that of memory-mapped computer hardware. A pointer references a location in memory, and obtaining the value stored at that location is known as dereferencing the pointer. As an analogy, a page number in a book's index could be considered a pointer to the corresponding page; dereferencing such a pointer would be done by flipping to the page with the given page number and reading the text found on that page. The actual format and content of a pointer variable is dependent on the underlying computer architecture.
Using pointers significantly improves performance for repetitive operations, like traversing iterable data structures (e.g. strings, lookup tables, control tables, linked lists, and tree structures). In particular, it is often much cheaper in time and space to copy and dereference pointers than it is to copy and access the data to which the pointers point.
Pointers are also used to hold the addresses of entry points for called subroutines in procedural programming and for run-time linking to dynamic link libraries (DLLs). In object-oriented programming, pointers to functions are used for binding methods, often using virtual method tables.
A pointer is a simple, more concrete implementation of the more abstract reference data type. Several languages, especially low-level languages, support some type of pointer, although some have more restrictions on their use than others. While "pointer" has been used to refer to references in general, it more properly applies to data structures whose interface explicitly allows the pointer to be manipulated (arithmetically via pointer arithmetic) as a memory address, as opposed to a magic cookie or capability which does not allow such.[citation needed] Because pointers allow both protected and unprotected access to memory addresses, there are risks associated with using them, particularly in the latter case. Primitive pointers are often stored in a format similar to an integer; however, attempting to dereference or "look up" such a pointer whose value is not a valid memory address could cause a program to crash (or contain invalid data). To alleviate this potential problem, as a matter of type safety, pointers are considered a separate type parameterized by the type of data they point to, even if the underlying representation is an integer. Other measures may also be taken (such as validation and bounds checking), to verify that the pointer variable contains a value that is both a valid memory address and within the numerical range that the processor is capable of addressing.
In 1955, Soviet Ukrainian computer scientist Kateryna Yushchenko created the Address programming language that made possible indirect addressing and addresses of the highest rank – analogous to pointers. This language was widely used on the Soviet Union computers. However, it was unknown outside the Soviet Union and usually Harold Lawson is credited with the invention, in 1964, of the pointer.[2] In 2000, Lawson was presented the Computer Pioneer Award by the IEEE "[f]or inventing the pointer variable and introducing this concept into PL/I, thus providing for the first time, the capability to flexibly treat linked lists in a general-purpose high-level language".[3] His seminal paper on the concepts appeared in the June 1967 issue of CACM entitled: PL/I List Processing. According to the Oxford English Dictionary, the word pointer first appeared in print as a stack pointer in a technical memorandum by the System Development Corporation.
A pointer is a programming concept used in computer science to reference or point to a memory location that stores a value or an object. It is essentially a variable that stores the memory address of another variable or data structure rather than storing the data itself.
Pointers are commonly used in programming languages that support direct memory manipulation, such as C and C++. They allow programmers to work with memory directly, enabling efficient memory management and more complex data structures. By using pointers, you can access and modify data located in memory, pass data efficiently between functions, and create dynamic data structures like linked lists, trees, and graphs.
References serve as a level of indirection: A pointer's value determines which memory address (that is, which datum) is to be used in a calculation. Because indirection is a fundamental aspect of algorithms, pointers are often expressed as a fundamental data type in programming languages; in statically (or strongly) typed programming languages, the type of a pointer determines the type of the datum to which the pointer points.
On the other hand, some systems have more units of memory than there are addresses. In this case, a more complex scheme such as memory segmentation or paging is employed to use different parts of the memory at different times. The last incarnations of the x86 architecture support up to 36 bits of physical memory addresses, which were mapped to the 32-bit linear address space through the PAE paging mechanism. Thus, only 1/16 of the possible total memory may be accessed at a time. Another example in the same computer family was the 16-bit protected mode of the 80286 processor, which, though supporting only 16 MB of physical memory, could access up to 1 GB of virtual memory, but the combination of 16-bit address and segment registers made accessing more than 64 KB in one data structure cumbersome.
Pointers are directly supported without restrictions in languages such as PL/I, C, C++, Pascal, FreeBASIC, and implicitly in most assembly languages. They are used mainly to construct references, which in turn are fundamental to construct nearly all data structures, and to pass data between different parts of a program.
In functional programming languages that rely heavily on lists, data references are managed abstractly by using primitive constructs like cons and the corresponding elements car and cdr, which can be thought of as specialised pointers to the first and second components of a cons-cell. This gives rise to some of the idiomatic "flavour" of functional programming. By structuring data in such cons-lists, these languages facilitate recursive means for building and processing data—for example, by recursively accessing the head and tail elements of lists of lists; e.g. "taking the car of the cdr of the cdr". By contrast, memory management based on pointer dereferencing in some approximation of an array of memory addresses facilitates treating variables as slots into which data can be assigned imperatively.
When dealing with arrays, the critical lookup operation typically involves a stage called address calculation which involves constructing a pointer to the desired data element in the array. In other data structures, such as linked lists, pointers are used as references to explicitly tie one piece of the structure to another.
When setting up data structures like lists, queues and trees, it is necessary to have pointers to help manage how the structure is implemented and controlled. Typical examples of pointers are start pointers, end pointers, and stack pointers. These pointers can either be absolute (the actual physical address or a virtual address in virtual memory) or relative (an offset from an absolute start address ("base") that typically uses fewer bits than a full address, but will usually require one additional arithmetic operation to resolve).
Relative addresses are a form of manual memory segmentation, and share many of its advantages and disadvantages. A two-byte offset, containing a 16-bit, unsigned integer, can be used to provide relative addressing for up to 64 KiB (216 bytes) of a data structure. This can easily be extended to 128, 256 or 512 KiB if the address pointed to is forced to be aligned on a half-word, word or double-word boundary (but, requiring an additional "shift left" bitwise operation—by 1, 2 or 3 bits—in order to adjust the offset by a factor of 2, 4 or 8, before its addition to the base address). Generally, though, such schemes are a lot of trouble, and for convenience to the programmer absolute addresses (and underlying that, a flat address space) is preferred.
The definition with references, however, is type-checked and does not use potentially confusing signal values. For this reason, data structures in C are usually dealt with via wrapper functions, which are carefully checked for correctness.
Assigning addresses to pointers is an invaluable tool when programming microcontrollers. Below is a simple example declaring a pointer of type int and initialising it to a hexadecimal address in this example the constant 0x7FFF:
Control tables that are used to control program flow usually make extensive use of pointers. The pointers, usually embedded in a table entry, may, for instance, be used to hold the entry points to subroutines to be executed, based on certain conditions defined in the same table entry. The pointers can however be simply indexes to other separate, but associated, tables comprising an array of the actual addresses or the addresses themselves (depending upon the programming language constructs available). They can also be used to point to earlier table entries (as in loop processing) or forward to skip some table entries (as in a switch or "early" exit from a loop). For this latter purpose, the "pointer" may simply be the table entry number itself and can be transformed into an actual address by simple arithmetic.
As a pointer allows a program to attempt to access an object that may not be defined, pointers can be the origin of a variety of programming errors. However, the usefulness of pointers is so great that it can be difficult to perform programming tasks without them. Consequently, many languages have created constructs designed to provide some of the useful features of pointers without some of their pitfalls, also sometimes referred to as pointer hazards. In this context, pointers that directly address memory (as used in this article) are referred to as raw pointers, by contrast with smart pointers or other variants.
One major problem with pointers is that as long as they can be directly manipulated as a number, they can be made to point to unused addresses or to data which is being used for other purposes. Many languages, including most functional programming languages and recent imperative programming languages like Java, replace pointers with a more opaque type of reference, typically referred to as simply a reference, which can only be used to refer to objects and not manipulated as numbers, preventing this type of error. Array indexing is handled as a special case.
The Rust programming language introduces a borrow checker, pointer lifetimes, and an optimisation based around option types for null pointers to eliminate pointer bugs, without resorting to garbage collection.
A dangling pointer is a pointer that does not point to a valid object and consequently may make a program crash or behave oddly. In the Pascal or C programming languages, pointers that are not specifically initialized may point to unpredictable addresses in memory.
An autorelative pointer is a pointer whose value is interpreted as an offset from the address of the pointer itself; thus, if a data structure has an autorelative pointer member that points to some portion of the data structure itself, then the data structure may be relocated in memory without having to update the value of the auto relative pointer.[12]
In some languages, a pointer can reference another pointer, requiring multiple dereference operations to get to the original value. While each level of indirection may add a performance cost, it is sometimes necessary in order to provide correct behavior for complex data structures. For example, in C it is typical to define a linked list in terms of an element that contains a pointer to the next element of the list:
In some languages, a pointer can reference executable code, i.e., it can point to a function, method, or procedure. A function pointer will store the address of a function to be invoked. While this facility can be used to call functions dynamically, it is often a favorite technique of virus and other malicious software writers.
Primarily for languages which do not support pointers explicitly but do support arrays, the array can be thought of and processed as if it were the entire memory range (within the scope of the particular array) and any index to it can be thought of as equivalent to a general-purpose register in assembly language (that points to the individual bytes but whose actual value is relative to the start of the array, not its absolute address in memory).
Assuming the array is, say, a contiguous 16 megabyte character data structure, individual bytes (or a string of contiguous bytes within the array) can be directly addressed and manipulated using the name of the array with a 31 bit unsigned integer as the simulated pointer (this is quite similar to the C arrays example shown above). Pointer arithmetic can be simulated by adding or subtracting from the index, with minimal additional overhead compared to genuine pointer arithmetic.
While powerful, pointer arithmetic can be a source of computer bugs. It tends to confuse novice programmers, forcing them into different contexts: an expression can be an ordinary arithmetic one or a pointer arithmetic one, and sometimes it is easy to mistake one for the other. In response to this, many modern high-level computer languages (for example Java) do not permit direct access to memory using addresses. Also, the safe C dialect Cyclone addresses many of the issues with pointers. See C programming language for more discussion.
In the C# programming language, pointers are supported by either marking blocks of code that include pointers with the unsafe keyword, or by using the System.Runtime.CompilerServices assembly provisions for pointer access. 
The syntax is essentially the same as in C++, and the address pointed can be either managed or unmanaged memory. However, pointers to managed memory (any pointer to a managed object) must be declared using the fixed keyword, which prevents the garbage collector from moving the pointed object as part of memory management while the pointer is in scope, thus keeping the pointer address valid.
The COBOL programming language supports pointers to variables. Primitive or group (record) data objects declared within the LINKAGE SECTION of a program are inherently pointer-based, where the only memory allocated within the program is space for the address of the data item (typically a single memory word). In program source code, these data items are used just like any other WORKING-STORAGE variable, but their contents are implicitly accessed indirectly through their LINKAGE pointers.
The PL/I language provides full support for pointers to all data types (including pointers to structures), recursion, multitasking, string handling, and extensive built-in functions. PL/I was quite a leap forward compared to the programming languages of its time.[citation needed] PL/I pointers are untyped, and therefore no casting is required for pointer dereferencing or assignment. The declaration syntax for a pointer is DECLARE xxx POINTER;, which declares a pointer named "xxx". Pointers are used with BASED variables. A based variable can be declared with a default locator (DECLARE xxx BASED(ppp); or without (DECLARE xxx BASED;), where xxx is a based variable, which may be an element variable, a structure, or an array, and ppp is the default pointer). Such a variable can be address without an explicit pointer reference (xxx=1;, or may be addressed with an explicit reference to the default locator (ppp), or to any other pointer (qqq->xxx=1;).
The D programming language is a derivative of C and C++ which fully supports C pointers and C typecasting.
The Eiffel object-oriented language employs value and reference semantics without pointer arithmetic. Nevertheless, pointer classes are provided. They offer pointer arithmetic, typecasting, explicit memory management,
interfacing with non-Eiffel software, and other features.
There is no explicit representation of pointers in Java. Instead, more complex data structures like objects and arrays are implemented using references. The language does not provide any explicit pointer manipulation operators. It is still possible for code to attempt to dereference a null reference (null pointer), however, which results in a run-time exception being thrown. The space occupied by unreferenced memory objects is recovered automatically by garbage collection at run-time.[20]
The Perl programming language supports pointers, although rarely used, in the form of the pack and unpack functions. These are intended only for simple interactions with compiled OS libraries. In all other cases, Perl uses references, which are typed and do not allow any form of pointer arithmetic. They are used to construct complex data structures.[24]
In object-oriented programming, a class defines the shared aspects of objects created from the class. The capabilities of a class differ between programming languages, but generally the shared aspects consist of state (variables) and behavior (methods) that are each either associated with a particular object or with all objects of that class.[1][2]
An object expresses data type as an interface – the type of each member variable and the signature of each member function (method). A class defines an implementation of an interface, and instantiating the class results in an object that exposes the implementation via the interface.[3] In the terms of type theory, a class is an implementation‍—‌a concrete data structure and collection of subroutines‍—‌while a type is an interface. Different (concrete) classes can produce objects of the same (abstract) type (depending on type system). For example, the type (interface) Stack might be implemented by SmallStack that is fast for small stacks but scales poorly and ScalableStack that scales well but has high overhead for small stacks.
Some programming languages such as Eiffel support specification of invariants as part of the definition of the class, and enforce them through the type system. Encapsulation of state is necessary for being able to enforce the invariants of the class.
Every class implements (or realizes) an interface by providing structure and behavior. Structure consists of data and state, and behavior consists of code that specifies how methods are implemented.[8] There is a distinction between the definition of an interface and the implementation of that interface; however, this line is blurred in many programming languages because class declarations both define and implement an interface. Some languages, however, provide features that separate interface and implementation. For example, an abstract class can define an interface without providing an implementation.
In languages that support access specifiers, the interface of a class is considered to be the set of public members of the class, including both methods and attributes (via implicit getter and setter methods); any private members or internal data structures are not intended to be depended on by external code and thus are not part of the interface.
Object-oriented programming methodology dictates that the operations of any interface of a class are to be independent of each other. It results in a layered design where clients of an interface use the methods declared in the interface. An interface places no requirements for clients to invoke the operations of one interface in any particular order. This approach has the benefit that client code can assume that the operations of an interface are available for use whenever the client has access to the object.[9]
The various object-oriented programming languages enforce member accessibility and visibility to various degrees, and depending on the language's type system and compilation policies, enforced at either compile time or runtime. For example, the Java language does not allow client code that accesses the private data of a class to compile.[12] In the C++ language, private methods are visible, but not accessible in the interface; however, they may be made invisible by explicitly declaring fully abstract classes that represent the interfaces of the class.[13]
Most object-oriented programming languages allow the programmer to specify which classes are considered abstract and will not allow these to be instantiated. For example, in Java, C# and PHP, the keyword abstract is used.[26][27] In C++, an abstract class is a class having at least one abstract method given by the appropriate syntax in that language (a pure virtual function in C++ parlance).[25]
Software reuse is also a major benefit of using Object classes. Classes facilitate re-use via inheritance and interfaces. When a new behavior is required it can often be achieved by creating a new class and having that class inherit the default behaviors and data of its superclass and then tailoring some aspect of the behavior or data accordingly. Re-use via interfaces (also known as methods) occurs when another object wants to invoke (rather than create a new kind of) some object class. This method for re-use removes many of the common errors that can make their way into software when one program re-uses code from another.[52]
As a data type, a class is usually considered as a compile time construct.[53] A language or library may also support prototype or factory metaobjects that represent runtime information about classes, or even represent metadata that provides access to reflective programming (reflection) facilities and ability to manipulate data structure formats at runtime. Many languages distinguish this kind of run-time type information about classes from a class on the basis that the information is not needed at runtime. Some dynamic languages do not make strict distinctions between runtime and compile time constructs, and therefore may not distinguish between metaobjects and classes.
In contrast to creating an object from a class, some programming contexts support object creation by copying (cloning) a prototype object.[54]
In object-oriented programming, inheritance is the mechanism of basing an object or class upon another object (prototype-based inheritance) or class (class-based inheritance), retaining similar implementation. Also defined as deriving new classes (sub classes) from existing ones such as super class or base class and then forming them into a hierarchy of classes. In most class-based object-oriented languages like C++, an object created through inheritance, a "child object", acquires all the properties and behaviors of the "parent object", with the exception of: constructors, destructors, overloaded operators and friend functions of the base class. Inheritance allows programmers to create classes that are built upon existing classes,[1] to specify a new implementation while maintaining the same behaviors (realizing an interface), to reuse code and to independently extend original software via public classes and interfaces. The relationships of objects or classes through inheritance give rise to a directed acyclic graph.
An inherited class is called a subclass of its parent class or super class. The term inheritance is loosely used for both class-based and prototype-based programming, but in narrow use the term is reserved for class-based programming (one class inherits from another), with the corresponding technique in prototype-based programming being instead called delegation (one object delegates to another). Class-modifying inheritance patterns can be pre-defined according to simple network interface parameters such that inter-language compatibility is preserved.[2][3]
In 1966, Tony Hoare presented some remarks on records, and in particular, the idea of record subclasses, record types with common properties but discriminated by a variant tag and having fields private to the variant.[8] Influenced by this, in 1967 Ole-Johan Dahl and Kristen Nygaard presented a design that allowed specifying objects that belonged to different classes but had common properties. The common properties were collected in a superclass, and each superclass could itself potentially have a superclass. The values of a subclass were thus compound objects, consisting of some number of prefix parts belonging to various superclasses, plus a main part belonging to the subclass. These parts were all concatenated together.[9] The attributes of a compound object would be accessible by dot notation. This idea was first adopted in the Simula 67 programming language.[10] The idea then spread to Smalltalk, C++, Java, Python, and many other languages.
Some languages also support the inheritance of other constructs. For example, in Eiffel, contracts that define the specification of a class are also inherited by heirs. The superclass establishes a common interface and foundational functionality, which specialized subclasses can inherit, modify, and supplement. The software inherited by a subclass is considered reused in the subclass. A reference to an instance of a class may actually be referring to one of its subclasses. The actual class of the object being referenced is impossible to predict at compile-time. A uniform interface is used to invoke the member functions of objects of a number of different classes. Subclasses may replace superclass functions with entirely new functions that must share the same method signature.
A non-subclassable class has no subclasses, so it can be easily deduced at compile time that references or pointers to objects of that class are actually referencing instances of that class and not instances of subclasses (they do not exist) or instances of superclasses (upcasting a reference type violates the type system). Because the exact type of the object being referenced is known before execution, early binding (also called static dispatch) can be used instead of late binding (also called dynamic dispatch), which requires one or more virtual method table lookups depending on whether multiple inheritance or only single inheritance are supported in the programming language that is being used.
Many object-oriented programming languages permit a class or object to replace the implementation of an aspect—typically a behavior—that it has inherited. This process is called overriding. Overriding introduces a complication: which version of the behavior does an instance of the inherited class use—the one that is part of its own class, or the one from the parent (base) class? The answer varies between programming languages, and some languages provide the ability to indicate that a particular behavior is not to be overridden and should behave as defined by the base class. For instance, in C#, the base method or property can only be overridden in a subclass if it is marked with the virtual, abstract, or override modifier, while in programming languages such as Java, different methods can be called to override other methods.[15] An alternative to overriding is hiding the inherited code.
In most quarters, class inheritance for the sole purpose of code reuse has fallen out of favor.[citation needed] The primary concern is that implementation inheritance does not provide any assurance of polymorphic substitutability—an instance of the reusing class cannot necessarily be substituted for an instance of the inherited class. An alternative technique, explicit delegation, requires more programming effort, but avoids the substitutability issue.[citation needed] In C++ private inheritance can be used as a form of implementation inheritance without substitutability. Whereas public inheritance represents an "is-a" relationship and delegation represents a "has-a" relationship, private (and protected) inheritance can be thought of as an "is implemented in terms of" relationship.[16]
In programming languages that do not support inheritance as a subtyping mechanism, the relationship between a base class and a derived class is only a relationship between implementations (a mechanism for code reuse), as compared to a relationship between types. Inheritance, even in programming languages that support inheritance as a subtyping mechanism, does not necessarily entail behavioral subtyping. It is entirely possible to derive a class whose object will behave incorrectly when used in a context where the parent class is expected; see the Liskov substitution principle.[18] (Compare connotation/denotation.) In some OOP languages, the notions of code reuse and subtyping coincide because the only way to declare a subtype is to define a new class that inherits the implementation of another.
Implementation inheritance is controversial among programmers and theoreticians of object-oriented programming since at least the 1990s. Among them are the authors of Design Patterns, who advocate interface inheritance instead, and favor composition over inheritance. For example, the decorator pattern (as mentioned above) has been proposed to overcome the static nature of inheritance between classes. As a more fundamental solution to the same problem, role-oriented programming introduces a distinct relationship, played-by, combining properties of inheritance and composition into a new concept.[citation needed]
According to Allen Holub, the main problem with implementation inheritance is that it introduces unnecessary coupling in the form of the "fragile base class problem":[6] modifications to the base class implementation can cause inadvertent behavioral changes in subclasses. Using interfaces avoids this problem because no implementation is shared, only the API.[19] Another way of stating this is that "inheritance breaks encapsulation".[20] The problem surfaces clearly in open object-oriented systems such as frameworks, where client code is expected to inherit from system-supplied classes and then substituted for the system's classes in its algorithms.[6]
Reportedly, Java inventor James Gosling has spoken against implementation inheritance, stating that he would not include it if he were to redesign Java.[19] Language designs that decouple inheritance from subtyping (interface inheritance) appeared as early as 1990;[21] a modern example of this is the Go programming language.
A fifth-generation programming language (5GL) is  a high-level programming language based on problem-solving using constraints given to the program, rather than using an algorithm written by a programmer.[1] Most constraint-based and logic programming languages and some other declarative languages are fifth-generation languages.
While fourth-generation programming languages are designed to build specific programs, fifth-generation languages are designed to make the computer solve a given problem without the programmer. This way, the user only needs to worry about what problems need to be solved and what conditions need to be met, without worrying about how to implement a routine or algorithm to solve them. Fifth-generation languages are used mainly in artificial intelligence research. OPS5 and Mercury are examples of fifth-generation languages,[2] as is ICAD, which was built upon Lisp.  KL-ONE is an example of a related idea, a frame language.
In the 1980s, fifth-generation languages were considered to be the way of the future, and some predicted that they would replace procedural programming with constraint based programming for all tasks that could be framed as a series of logical constraints.[3] Most notably, from 1982 to 1993, Japan[4][5] put much research and money into their fifth-generation computer systems project, hoping to design a massive computer network of machines using these tools.
However, as larger programs were built, the flaws of the approach became more apparent. It turns out that, given a set of constraints defining a particular problem, deriving an efficient algorithm to solve it is a very difficult problem in itself. This crucial step cannot yet be automated and still requires the insight of a human programmer.
Service-oriented programming (SOP) is a programming paradigm that uses "services" as the unit of computer work, to design and implement integrated business applications and mission critical software programs. Services can represent steps of business processes and thus one of the main applications of this paradigm is the cost-effective delivery of standalone or composite business applications that can "integrate from the inside-out". It inherently promotes service-oriented architecture (SOA), however, it is not the same as SOA. While SOA focuses on communication between systems using "services",[1] SOP provides a new technique to build agile application modules using in-memory services as the unit of work.
An in-memory service in SOP can be transparently externalized as a web service operation. Due to language and platform independent Web Service standards, SOP embraces all existing programming paradigms, languages and platforms.  In SOP, the design of the programs pivot around the semantics of service calls, logical routing and data flow description across well-defined service interfaces.  All SOP program modules are encapsulated as services and a service can be composed of other nested services in a hierarchical manner with virtually limitless depth to this service stack hierarchy. A composite service can also contain programming constructs some of which are specific and unique to SOP. A service can be an externalized system component that is accessed via any proprietary API or web service standards utilizing an in-memory plug-in technique.
While SOP supports the basic programming constructs for sequencing, selection and iteration, it is differentiated with a slew of new programming constructs that provide built-in native ability geared towards data list manipulation, data integration, automated multithreading of service modules, declarative context management and synchronization of services.  SOP design enables programmers to semantically synchronize the execution of services in order to guarantee that it is correct, or to declare a service module as a transaction boundary with automated commit/rollback behavior.
SOP concepts provide a robust base for a semantic approach to programming integration and application logic.  There are three significant benefits to this approach:
In SOP, in-memory software modules are strictly encapsulated through well-defined service interfaces that can be externalized on-demand as web service operations. This minimal unit of encapsulation maximizes the opportunities for reusability within other in-memory service modules as well as across existing and legacy software assets.  By using service interfaces for information hiding, SOP extends the service-oriented design principles used in SOA to achieve separation of concerns across in-memory service modules.
A service interface in SOP is an in-memory object that describes a well-defined software task with well-defined input and output data structures. Service interfaces can be grouped into packages. An SOP service interface can be externalized as a WSDL operation and a single service or a package of services can be described using WSDL. Furthermore, service interfaces can be assigned to one or many service groups based on shared properties.
A service listener receives service requests. It is a pluggable in-memory interface that abstracts the communication protocol for incoming service requests made to the SOP runtime environment such as the SVM. Through this abstract layer, the SOP runtime environment can be virtually embedded within the memory address of any traditional programming environment or application service.
One of the most important characteristic of SOP is that it can support a fully semantic-based approach to programming. Furthermore, this semantic-based approach can be layered into a visual environment built on top of a fully metadata-driven layer for storing the service interface and service module definitions. Furthermore, if the SOP runtime is supported by a SVM capable of interpreting the metadata layer, the need for automatic code generation can be eliminated.  The result is tremendous productivity gain during development, ease of testing and significant agility in deployment.
A composite service implementation is the semantic definition of a service module based on SOP techniques and concepts.  If you look inside of a black-boxed interface definition of a composite service, you may see other service interfaces connected to each other and connected to SOP programming constructs. A Composite service has a recursive definition meaning that any service inside ("inner service") may be another atomic or composite service. An inner service may be a recursive reference to the same containing composite service.
SOP supports the basic programming constructs for sequencing, selection and iteration as well as built-in, advance behavior.  Furthermore, SOP supports semantic constructs for automatic data mapping, translation, manipulation and flow across inner services of a composite service.
Logical selection is accomplished through data-driven branching constructs and other configurable constructs.  In general, configurable constructs are services built into the SOP platform with inputs and outputs that can assume the input/output shape of other connected services.  For example, a configurable construct used for filtering output data of services can take a list of Sales orders, Purchase orders or any other data structure, and filter its data based on user declared filter properties stored on the interface of that instance of the filter construct.  In this example, the structure to be filtered becomes the input of the particular instance of the filter construct and the same structure representing the filtered data becomes the output of the configurable construct.
A composite service can be declared to loop.  The loop can be bound by a fixed number of iterations with an optional built-in delay between iterations and it can dynamically terminate using a "service exit with success" or "service exit with failure" construct inside of the looping composite service.  Furthermore, any service interface can automatically run in a loop or "foreach" mode, if it is supplied with two or more input components upon automatic preparation. This behavior is supported at design-time when a data list structure from one service is connected to a service that takes a single data structure (i.e. non-plural) as its input. If a runtime property of the composite service interface is declared to support "foreach" in parallel, then the runtime automation environment can automatically multi-thread the loop and run it in parallel. This is an example of how SOP programming constructs provide built-in advanced functionality.
Exception handling is a run-time error in Java. Exception handling in SOP is simply accomplished by connecting the failure port of inner services to another inner service, or to a programming construct. "Exit with failure" and "exit with success" constructs are examples of constructs used for exception handling.  If no action is taken on the failure port of a service, then the outer (parent) service will automatically fail and the standard output messages from the failed inner service will automatically bubble up to the standard output of the parent.
An atomic service is an in-memory extension of the SOP runtime environment through a service native interface (SNI) it is essentially a plug-in mechanism. For example, if SOP is automated through an SVM, a service plug-in is dynamically loaded into the SVM when any associated service is consumed. An example of a service plug-in would be a SOAP communicator plug-in that can on-the-fly translate any in-memory service input data to a Web Service SOAP request, post it to a service producer, and then translate the corresponding SOAP response to in-memory output data on the service.  Another example of a service plug-in is a standard database SQL plug-in that supports data access, modification and query operations.  A further example that can help establish the fundamental importance of atomic services and service plug-ins is using a service invoker as a service plug-in to transparently virtualize services across different instances of an SOP platform.  This unique, component-level virtualization is termed "service grid virtualization" in order to distinguish it from traditional application, or process-level virtualization.
In addition to the ability to call any service, Service Request Events and Shared Memory are two of the SOP built-in mechanisms provided for inter-service communication. The consumption of a service is treated as an Event in SOP. SOP provides a correlation-based event mechanism that results in the pre-emption of a running composite that has declared, through a "wait" construct, the need to wait for one or more other service consumption events to happen with specified input data values. The execution of the composite service continues when services are consumed with specific correlation key inputs associated with the wait construct.  SOP also provides a shared memory space with access control where services can access and update a well-defined data structure that is similar to the input/output structure of services.  The shared memory mechanism within SOP can be programmatically accessed through service interfaces.
In SOP, customizations are managed through an inventive feature called Service Overrides. Through this feature, a service implementation can be statically or dynamically overridden by one of many possible implementations at runtime.  This feature is analogous to polymorphism in object-oriented programming. Each possible override implementation can be associated to one or more override configuration portfolios in order to manage activation of groups of related overrides throughout different SOP application installations at the time of deployment.
The term service-oriented programming was first published in 2002 by Alberto Sillitti, Tullio Vernazza and Giancarlo Succi in a book called "Software Reuse: Methods, Techniques, and Tools."  SOP, as described above, reflects some aspects of the use of the term proposed by  Sillitti, Vernazza and Succi.
F# (pronounced F sharp) is a general-purpose, high-level, strongly typed, multi-paradigm programming language that encompasses functional, imperative, and object-oriented programming methods. It is most often used as a cross-platform Common Language Infrastructure (CLI) language on .NET, but can also generate JavaScript[8] and graphics processing unit (GPU) code.[9]
F# is developed by the F# Software Foundation,[10] Microsoft and open contributors. An open source, cross-platform compiler for F# is available from the F# Software Foundation.[11] F# is a fully supported language in Visual Studio[12] and JetBrains Rider.[13] Plug-ins supporting F# exist for many widely used editors including Visual Studio Code, Vim, and Emacs.
F# is a member of the ML language family and originated as a .NET Framework implementation of a core of the programming language OCaml.[5][6] It has also been influenced by C#, 
Python, Haskell,[4] Scala and Erlang.
F# uses an open development and engineering process. 
The language evolution process is managed by Don Syme from Microsoft Research as the benevolent dictator for life (BDFL) for the language design, together with the F# Software Foundation.
Earlier versions of the F# language were designed by Microsoft and Microsoft Research using a closed development process.
F# originates from Microsoft Research, Cambridge, UK. The language was originally designed and implemented by Don Syme,[5] according to whom in the fsharp team, they say the F is for "Fun".[39] 
Andrew Kennedy contributed to the design of units of measure.[5] The Visual F# Tools for Visual Studio are developed by Microsoft.[5] The F# Software Foundation developed the F# open-source compiler and tools, incorporating the open-source compiler implementation provided by the Microsoft Visual F# Tools team.[10]
F# is a strongly typed functional-first language with a large number of capabilities that are normally found only in functional programming languages, while supporting object-oriented features available in C#. Together, these features allow F# programs to be written in a completely functional style and also allow functional and object-oriented styles to be mixed.
New types are defined using the type keyword. For functional programming, F# provides tuple, record, discriminated union, list, option, and result types.[52] A tuple represents a set of n values, where n ≥ 0. The value n is called the arity of the tuple. A 3-tuple would be represented as (A, B, C), where A, B, and C are values of possibly different types. A tuple can be used to store values only when the number of values is known at design-time and stays constant during execution.
F# supports lambda functions and closures.[52] All functions in F# are first class values and are immutable.[52] Functions can be curried. Being first-class values, functions can be passed as arguments to other functions. Like other functional programming languages, F# allows function composition using the >> and << operators.
Also, F# supports access to all CLI types and objects such as those defined in the System.Collections.Generic namespace defining imperative data structures.
Like other Common Language Infrastructure (CLI) languages, F# can use CLI types through object-oriented programming.[52] F# support for object-oriented programming in expressions includes:
F# supports asynchronous programming through asynchronous workflows.[55] An asynchronous workflow is defined as a sequence of commands inside an async{ ... }, as in
Parallel programming is supported partly through the Async.Parallel, Async.Start and other operations that run asynchronous blocks in parallel.
Parallel programming is also supported through the Array.Parallel functional programming operators in the F# standard library, direct use of the System.Threading.Tasks task programming model, the direct use of .NET thread pool and .NET threads and through dynamic translation of F# code to alternative parallel execution engines such as GPU[9] code.
In F#, you can assign units of measure, such as meters or kilograms, to floating point, unsigned integer[58] and signed integer values. This allows the compiler to check that arithmetic involving these values is dimensionally consistent, helping to prevent common programming mistakes by ensuring that, for instance, lengths aren't mistakenly added to times.
F# allows some forms of syntax customizing via metaprogramming to support embedding custom domain-specific languages within the F# language, particularly through computation expressions.[52]
F# includes a feature for run-time meta-programming called quotations.[60] A quotation expression evaluates to an abstract syntax tree representation of the F# expressions. Similarly, definitions labelled with the [<ReflectedDefinition>] attribute can also be accessed in their quotation form. F# quotations are used for various purposes including to compile F# code into JavaScript[8] and GPU[9] code. Quotations represent their F# code expressions as data for use by other parts of the program while requiring it to be syntactically correct F# code.
F# 3.0 introduced a form of compile-time meta-programming through statically extensible type generation called F# type providers.[61] F# type providers allow the F# compiler and tools to be extended with components that provide type information to the compiler on-demand at compile time. F# type providers have been used to give strongly typed access to connected information sources in a scalable way, including to the Freebase knowledge graph.[62]
The combination of type providers, queries and strongly typed functional programming is known as information rich programming.[64]
F# supports a variation of the actor programming model through the in-memory implementation of lightweight asynchronous agents. For example, the following code defines an agent and posts 2 messages:
Among others, F# is used for quantitative finance programming,[70] energy trading and portfolio optimization,[71] machine learning,[72] business intelligence[73] and social gaming on Facebook.[74]
The F# open-source community includes the F# Software Foundation[10] and the F# Open Source Group at GitHub.[11] Popular open-source F# projects include:
Rust is a general-purpose programming language emphasizing performance, type safety, and concurrency. It enforces memory safety, meaning that all references point to valid memory. It does so without a conventional garbage collector; instead, memory safety errors and data races are prevented by the "borrow checker", which tracks the object lifetime of references at compile time.
Rust does not enforce a programming paradigm, but was influenced by ideas from functional programming, including immutability, higher-order functions, algebraic data types, and pattern matching. It also supports object-oriented programming via structs, enums, traits, and methods. It is popular for systems programming.[13][14][15]
Software developer Graydon Hoare created Rust as a personal project while working at Mozilla Research in 2006. Mozilla officially sponsored the project in 2009. In the years following the first stable release in May 2015, Rust was adopted by companies including Amazon, Discord, Dropbox, Google (Alphabet), Meta, and Microsoft. In December 2022, it became the first language other than C and assembly to be supported in the development of the Linux kernel.
Rust has been noted for its rapid adoption, and has been studied in programming language theory research.
During the early years, the Rust compiler was written in about 38,000 lines of OCaml.[17]: 15:34 [19] Early Rust contained features such as explicit object-oriented programming via an obj keyword (later removed),[17]: 10:08  and a typestates system that would allow variables of a type to be tracked along with state changes (such as going from uninitialized to initialized).[17]: 13:12
Rust's expansion and consolidation was influenced by developers coming from C++ (e.g., low-level performance of features), scripting languages (e.g., Cargo and package management), and functional programming (e.g., type systems development).[17]: 30:50
On February 26, 2024, the U.S. White House released a 19-page press report urging software development to move to memory-safe programming languages; specifically, moving away from C and C++ and encouraging languages like C#, Go, Java, Ruby, Swift, and Rust.[41][42] The report was widely interpreted as increasing interest in Rust.[43][44] The report was released through the Office of the National Cyber Director.[41][45]
Rust's syntax is similar to that of C and C++,[46][47] although many of its features were influenced by functional programming languages such as OCaml.[48] Hoare has described Rust as targeted at frustrated C++ developers and emphasized features such as safety, control of memory layout, and concurrency.[18] Safety in Rust includes the guarantees of memory safety, type safety, and lack of data races.
User-defined types are created with the struct or enum keywords. The struct keyword is used to denote a record type that groups multiple related values.[67] enums can take on different variants at runtime, with its capabilities similar to algebraic data types found in functional programming languages.[68] Both records and enum variants can contain fields with different types.[69] Alternative names, or aliases, for the same type can be defined with the type keyword.[70]
Because of these ownership rules, Rust types are known as linear or affine types, meaning each value can be used exactly once. This enforces a form of software fault isolation as the owner of a value is solely responsible for its correctness and deallocation.[94]
The Rust ecosystem includes its compiler, its standard library, and additional components for software development. Component installation is typically managed by rustup, a Rust toolchain installer developed by the Rust project.[134]
The Rust standard library defines and implements many widely used custom data types, including core data structures such as Vec, Option, and HashMap, as well as smart pointer types. Rust also provides a way to exclude most of the standard library using the attribute #![no_std]; this enables applications, such as embedded devices, which want to remove dependency code or provide their own core data structures. Internally, the standard library is divided into three parts, core, alloc, and std, where std and alloc are excluded by #![no_std].[139]
Rust is used in software across different domains. Components from the Servo browser engine (funded by Mozilla and Samsung) were incorporated in the Gecko browser engine underlying Firefox.[159] In January 2023, Google (Alphabet) announced support for using third party Rust libraries in Chromium.[160][161]
Rust is used in several backend software projects of large web services. OpenDNS, a DNS resolution service owned by Cisco, uses Rust internally.[162][163] Amazon Web Services uses Rust in "performance-sensitive components" of its several services. In 2019, AWS open-sourced Firecracker, a virtualization solution primarily written in Rust.[164] Microsoft Azure IoT Edge, a platform used to run Azure services on IoT devices, has components implemented in Rust.[165] Microsoft also uses Rust to run containerized modules with WebAssembly and Kubernetes.[166] Cloudflare, a company providing content delivery network services, used Rust to build a new web proxy named Pingora for increased performance and efficiency.[167] The npm package manager used Rust for its production authentication service in 2019.[168][169][170]
In operating systems, the Rust for Linux project, launched in 2020, merged initial support into the Linux kernel version 6.1 in late 2022.[171][172][173] The project is active with a team of 6-7 developers, and has added additional Rust code with kernel releases from 2022 to 2024,[174] aiming to demonstrate the minimum viability of the project and resolve key compatibility blockers.[171][175] The first drivers written in Rust were merged into the kernel for version 6.8.[171] The Android developers used Rust in 2021 to rewrite existing components.[176][177] Microsoft has rewritten parts of Windows in Rust.[178] The r9 project aims to re-implement Plan 9 from Bell Labs in Rust.[179] Rust has been used in the development of new operating systems such as Redox, a "Unix-like" operating system and microkernel,[180] Theseus, an experimental operating system with modular state management,[181][182] and most of Fuchsia.[183] Rust is also used for command-line tools and operating system components, including stratisd, a file system manager[184][185] and COSMIC, a desktop environment by System76.[186]
Discord, an instant messaging software company, rewrote parts of its system in Rust for increased performance in 2020. In the same year, Dropbox announced that its file synchronization had been rewritten in Rust. Facebook (Meta) used Rust to redesign its system that manages source code for internal projects.[16]
In the 2024 Stack Overflow Developer Survey, 12.6% of respondents had recently done extensive development in Rust.[190] The survey named Rust the "most admired programming language" every year from 2016 to 2024 (inclusive), based on the number of existing developers interested in continuing to work in the same language.[191][note 7] In 2024, Rust was the 6th "most wanted technology", with 28.7% of developers not currently working in Rust expressing an interest in doing so.[190]
Rust has been studied in academic research, both for properties of the language itself as well as the utility the language provides for writing software used for research. Its features around safety[192][152] and performance[193] have been examined.
In a journal article published to Proceedings of the International Astronomical Union, astrophysicists Blanco-Cuaresma and Bolmont re-implemented programs responsible for simulating multi-planet systems in Rust, and found it to be a competitive programming language for its "speed and accuracy".[14] Likewise, an article published on Nature shared several stories of bioinformaticians using Rust for its performance and safety.[140] However, both articles have cited Rust's unique concepts, including its ownership system, being difficult to learn as one of the main drawbacks to adopting Rust.
Go is a high-level general purpose programming language that is statically typed and compiled. It is known for the simplicity of its syntax and the efficiency of development that it enables by the inclusion of a large standard library supplying many needs for common projects.[12] It was designed at Google[13] in 2007 by Robert Griesemer, Rob Pike, and Ken Thompson, and publicly announced in November of 2009.[4] It is syntactically similar to C, but also has memory safety, garbage collection, structural typing,[7] and CSP-style concurrency.[14] It is often referred to as Golang to avoid ambiguity and because of its former domain name, golang.org, but its proper name is Go.[15]
Go was designed at Google in 2007 to improve programming productivity in an era of multicore, networked machines and large codebases.[22] The designers wanted to address criticisms of other languages in use at Google, but keep their useful characteristics:[23]
Although the design of most languages concentrates on innovations in syntax, semantics, or typing, Go is focused on the software development process itself. ... The principal unusual property of the language itself—concurrency—addressed problems that arose with the proliferation of multicore CPUs in the 2010s. But more significant was the early work that established fundamentals for packaging, dependencies, build, test, deployment, and other workaday tasks of the software development world, aspects
that are not usually foremost in language design.[32]
The lack of support for generic programming in initial versions of Go drew considerable criticism.[37] The designers expressed an openness to generic programming and noted that built-in functions were in fact type-generic, but are treated as special cases; Pike called this a weakness that might be changed at some point.[38] The Google team built at least one compiler for an experimental Go dialect with generics, but did not release it.[39]
In August 2018, the Go principal contributors published draft designs for generic programming and error handling and asked users to submit feedback.[40][41] However, the error handling proposal was eventually abandoned.[42]
Go uses a go1.[major].[patch] versioning format, such as go1.24.0 and each major Go release is supported until there are two newer major releases. Unlike most software, Go calls the second number in a version the major, i.e., in go1.24.0 the 24 is the major version. [48] This is because Go plans to never reach 2.0, prioritizing backwards compatibility over potential breaking changes.[49]
For a pair of types K, V, the type map[K]V is the type mapping type-K keys to type-V values, though Go Programming Language specification does not give any performance guarantees or implementation requirements for map types. Hash tables are built into the language, with special syntax and built-in functions. chan  T is a channel that allows sending values of type T between concurrent Go processes.[67]
The second are its interfaces, which provides runtime polymorphism.[74]: 266  Interfaces are a class of types and provide a limited form of structural typing in the otherwise nominal type system of Go. An object which is of an interface type is also of another type, much like C++ objects being simultaneously of a base and derived class. The design of Go interfaces was inspired by protocols from the Smalltalk programming language.[75] Multiple sources use the term duck typing when describing Go interfaces.[76][77] Although the term duck typing is not precisely defined and therefore not wrong, it usually implies that type conformance is not statically checked. Because conformance to a Go interface is checked statically by the Go compiler (except when performing a type assertion), the Go authors prefer the term structural typing.[78]
Concurrency-related structural conventions of Go (channels and alternative channel inputs) are derived from Tony Hoare's communicating sequential processes model. Unlike previous concurrent programming languages such as Occam or Limbo (a language on which Go co-designer Rob Pike worked),[104] Go does not provide any built-in notion of safe or verifiable concurrency.[105] While the communicating-processes model is favored in Go, it is not the only one: all goroutines in a program share a single address space. This means that mutable objects and pointers can be shared between goroutines; see § Lack of data race safety, below.
Although Go's concurrency features are not aimed primarily at parallel processing,[94] they can be used to program shared-memory multi-processor machines. Various studies have been done into the effectiveness of this approach.[106] One of these studies compared the size (in lines of code) and speed of programs written by a seasoned programmer not familiar with the language and corrections to these programs by a Go expert (from Google's development team), doing the same for Chapel, Cilk and Intel TBB. The study found that the non-expert tended to write divide-and-conquer algorithms with one go statement per recursion, while the expert wrote distribute-work-synchronize programs using one goroutine per processor core. The expert's programs were usually faster, but also longer.[107]
Go's approach to concurrency can be summarized as "don't communicate by sharing memory; share memory by communicating".[108] There are no restrictions on how goroutines access shared data, making data races possible. Specifically, unless a program explicitly synchronizes via channels or other means, writes from one goroutine might be partly, entirely, or not at all visible to another, often with no guarantees about ordering of writes.[105] Furthermore, Go's internal data structures like interface values, slice headers, hash tables, and string headers are not immune to data races, so type and memory safety can be violated in multithreaded programs that modify shared instances of those types without synchronization.[109][110] Instead of language support, safe concurrent programming thus relies on conventions; for example, Chisnall recommends an idiom called "aliases xor mutable", meaning that passing a mutable value (or pointer) over a channel signals a transfer of ownership over the value to its receiver.[96]: 155  The gc toolchain has an optional data race detector that can check for unsynchronized access to shared memory during runtime since version 1.1,[111] additionally a best-effort race detector is also included by default since version 1.6 of the gc runtime for access to the map data type.[112]
Popular applications include: Caddy, a web server that automates the process of setting up HTTPS,[136] Docker, which provides a platform for containerization, aiming to ease the complexities of software development and deployment,[137] Kubernetes, which automates the deployment, scaling, and management of containerized applications,[138] CockroachDB, a distributed SQL database engineered for scalability and strong consistency,[139] and Hugo, a static site generator that prioritizes speed and flexibility, allowing developers to create websites efficiently.[140]
Go was named Programming Language of the Year by the TIOBE Programming Community Index in its first year, 2009, for having a larger 12-month increase in popularity (in only 2 months, after its introduction in November) than any other language that year, and reached 13th place by January 2010,[143] surpassing established languages like Pascal. By June 2015, its ranking had dropped to below 50th in the index, placing it lower than COBOL and Fortran.[144] But as of January 2017, its ranking had surged to 13th, indicating significant growth in popularity and adoption. Go was again awarded TIOBE Programming Language of the Year in 2016.[145]
On November 10, 2009, the day of the general release of the language, Francis McCabe, developer of the Go! programming language (note the exclamation point), requested a name change of Google's language to prevent confusion with his language, which he had spent 10 years developing.[155] McCabe raised concerns that "the 'big guy' will end up steam-rollering over" him, and this concern resonated with the more than 120 developers who commented on Google's official issues thread saying they should change the name, with some[156] even saying the issue contradicts Google's motto of: Don't be evil.[157]
Swift is a high-level general-purpose, multi-paradigm, compiled programming language created by Chris Lattner in 2010 for Apple Inc. and maintained by the open-source community. Swift compiles to machine code and uses an LLVM-based compiler. Swift was first released in June 2014[11] and the Swift toolchain has shipped in Xcode since Xcode version 6, released in September 2014.
Apple intended Swift to support many core concepts associated with Objective-C, notably dynamic dispatch, widespread late binding, extensible programming, and similar features, but in a "safer" way, making it easier to catch software bugs; Swift has features addressing some common programming errors like null pointer dereferencing and provides syntactic sugar to help avoid the pyramid of doom. Swift supports the concept of protocol extensibility, an extensibility system that can be applied to types, structs and classes, which Apple promotes as a real change in programming paradigms they term "protocol-oriented programming"[12] (similar to traits and type classes).[13]
Swift was introduced at Apple's 2014 Worldwide Developers Conference (WWDC).[14] It underwent an upgrade to version 1.2 during 2014 and a major upgrade to Swift 2 at WWDC 2015. It was initially a proprietary language, but version 2.2 was made open-source software under the Apache License 2.0 on December 3, 2015, for Apple's platforms and Linux.[15][16]
Swift 4.0, released in 2017, introduced several changes to some built-in classes and structures. Code written with previous versions of Swift can be updated using the migration functionality built into Xcode. Swift 5, released in March 2019, introduced a stable binary interface on Apple platforms, allowing the Swift runtime to be incorporated into Apple operating systems. It is source compatible with Swift 4.[20]
Development of Swift started in July 2010 by Chris Lattner, with the eventual collaboration of many other programmers at Apple. Swift was motivated by the need for a replacement for Apple's earlier programming language Objective-C, which had been largely unchanged since the early 1980s and lacked modern language features. Swift took language ideas "from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list".[7] On June 2, 2014, the Apple Worldwide Developers Conference (WWDC) application became the first publicly released app written with Swift.[27] A beta version of the programming language was released to registered Apple developers at the conference, but the company did not promise that the final version of Swift would be source code compatible with the test version. Apple planned to make source code converters available if needed for the full release.[27]
The Swift Programming Language, a free 500-page manual, was also released at WWDC, and is available on the Apple Books Store and the official website.[28]
Swift won first place for Most Loved Programming Language in the Stack Overflow Developer Survey 2015[36] and second place in 2016.[37]
The platforms Swift supports are Apple's operating systems (Darwin, iOS, iPadOS, macOS, tvOS, watchOS), Linux, Windows, and Android.[52][53]
Swift is a general purpose programming language that employs modern programming-language theory concepts and strives to present a simple, yet powerful syntax. Swift incorporates innovations and conventions from various programming languages, with notable inspiration from Objective-C, which it replaced as the primary development language on Apple Platforms.
Swift was designed to be safe and friendly to new programmers while not sacrificing speed. By default Swift manages all memory automatically and ensures variables are always initialized before use. Array accesses are checked for out-of-bounds errors and integer operations are checked for overflow. Parameter names allow creating clear APIs. Protocols define interfaces that types may adopt, while extensions allow developers to add more function to existing types. Swift enables object-oriented programming with the support for classes, subtyping, and method overriding. Optionals allow nil values to be handled explicitly and safely. Concurrent programs can be written using async/await syntax, and actors isolate shared mutable state in order to eliminate data races.[72][73]
Both concepts have their advantages and disadvantages. Objects are useful when the data is large, like the description of a window or the contents of a document. In these cases, access to that data is provided by copying a 32- or 64-bit value, versus copying an entire data structure. However, smaller values like integers are the same size as pointers (typically both are one word), so there is no advantage to passing a pointer, versus passing the value.
The programmer is free to choose which semantics are more appropriate for each data structure in the application. Larger structures like windows would be defined as classes, allowing them to be passed around as pointers. Smaller structures, like a 2D point, can be defined as structs, which will be pass-by-value and allow direct access to their internal data with no indirection or reference counting. The performance improvement inherent to the pass-by-value concept is such that Swift uses these types for almost all common data types, including Int and Double, and types normally represented by objects, like String and Array.[84] Using value types can result in significant performance improvements in user applications as well.[85]
In Objective-C, and most other languages implementing the protocol concept, it is up to the programmer to ensure that the required methods are implemented in each class.[88] Swift adds the ability to add these methods using extensions, and to use generic programming (generics) to implement them. Combined, these allow protocols to be written once and support a wide variety of instances. Also, the extension mechanism can be used to add protocol conformance to an object that does not list that protocol in its definition.[89]
A key element of the Swift system is its ability to be cleanly debugged and run within the development environment, using a read–eval–print loop (REPL), giving it interactive properties more in common with the scripting abilities of Python than traditional system programming languages. The REPL is further enhanced with playgrounds, interactive views running within the Xcode environment or Playgrounds app that respond to code or debugger changes on-the-fly.[102] Playgrounds allow programmers to add in Swift code along with markdown documentation. Programmers can step through code and add breakpoints using LLDB either in a console or an IDE like Xcode.
A second free implementation of Swift that targets Cocoa, Microsoft's Common Language Infrastructure (.NET Framework, now .NET), and the Java and Android platform exists as part of the Elements Compiler from RemObjects Software.[109]
Zig, also known as Ziglang, [11] is an imperative, general-purpose, statically typed, compiled system programming language designed by Andrew Kelley.[12] It is free and open-source software, released under an MIT License.[13]
A major goal of the language is to improve on the C language,[14][15] (also taking inspiration from Rust[16][17]), with the intent of being even smaller and simpler to program in, while offering more functionality.[18] The improvements in language simplicity relate to flow control, function calls, library imports, variable declaration and Unicode support. Further, the language makes no use of macros or preprocessor instructions. Features adopted from modern languages include the addition of compile time generic programming data types, allowing functions to work on a variety of data, along with a small set of new compiler directives to allow access to the information about those types using reflective programming (reflection).[19] Like C, Zig omits garbage collection, and has manual memory management.[19] To help eliminate the potential errors that arise in such systems, it includes option types, a simple syntax for using them, and a unit testing framework built into the language. Zig has many features for low-level programming, notably packed structs (structs without padding between fields), arbitrary-width integers[20] and multiple pointer types.[21]
The main drawback of the system is that, although Zig has a growing community, as of 2025, it remains a new language with areas for improvement in maturity, ecosystem and tooling.[22] Also the learning curve for Zig can be steep, especially for those unfamiliar with low-level programming concepts.[22] The availability of learning resources is limited for complex use cases, though this is gradually improving as interest and adoption increase.[22] Other challenges mentioned by the reviewers are interoperability with other languages (extra effort to manage data marshaling and communication is required), as well as manual memory deallocation (disregarding proper memory management results directly in memory leaks).[22]
The development is funded by the Zig Software Foundation (ZSF), a non-profit corporation with Andrew Kelley as president, which accepts donations and hires multiple full-time employees.[23][24][25] Zig has very active contributor community, and is still in its early stages of development.[26] Despite this, a Stack Overflow survey in 2024 found that Zig software developers earn salaries of $103,000 USD per year on average, making it one of the best-paying programming languages.[27] However, only 0.83% reported they were proficient in Zig.[26]
A common solution to these problems is a garbage collector (GC), which examines the program for pointers to previously allocated memory, and removing any blocks that no longer have anything pointing to them. Although this greatly reduces, or even eliminates, memory errors, GC systems are relatively slow compared to manual memory management[citation needed], and have unpredictable performance that makes them unsuited to systems programming. Another solution is automatic reference counting (ARC), which implements the same basic concept of identifying blocks of disused memory, but does so at pointer creation and destruction time by maintaining the number of pointers to a block, meaning there is no need to perform exhaustive pointer searches, which are rendered unnecessary at the cost of adding reference counter adjustment overhead to every pointer creation and destruction operation.[29]
In this code, the allocator variable is passed a struct that describes what code should perform the allocation, and the repeat function returns either the resulting string or, using the optional type as indicated by the !, an Allocator.Error. By directly expressing the allocator as an input, memory allocation is never "hidden" within another function, it is always exposed to the API by the function that is ultimately calling for the memory to be allocated. No allocations are performed inside Zig’s standard library. Further, as the struct can point to anything, one can use alternative allocators, even ones written in the program. This can allow, for instance, small-object allocators that do not use the operating system functions that normally allocate an entire memory page.[32]
Zig treats cross-compiling as a first-class use-case of the language.[19] This means any Zig compiler can compile runnable binaries for any of its target platforms, of which there are dozens. These include not only widely-used modern systems like ARM and x86-64, but also PowerPC, SPARC, MIPS, RISC-V, LoongArch64 and even the IBM z/Architectures (S390). The toolchain can compile to any of these targets without installing additional software, all the needed support is in the basic system.[32] The experimental support is also provided for less known platforms like AMD and Nvidia GPUs or PlayStation 4 and 5 (with various degree of support).
Cross-compilation is also available for variety of the operating systems (mostly desktop ones). Popular UNIX-like ones and Windows are officially supported (and documented), but (minimal) applications can and have been made for Android (with Android NDK) or iOS.
Zig uses LLVM (written in C++) as a backend for optimization. Since version 0.10 the Zig compiler is written in the Zig programming language, i.e., it is a self-hosting compiler. The self-hosted linker is tightly coupled with the self-hosted compiler.
Carbon is an experimental programming language designed for connectiveness with C++.[2] The project is open-source and was started at Google. Google engineer Chandler Carruth first introduced Carbon at the CppNorth conference in Toronto in July 2022. He stated that Carbon was created to be a C++ successor.[3][1][4] The language is expected to have an experimental MVP version 0.1 in 2025 and a production-ready version 1.0 after 2027.[5]
Ring is a dynamically typed, general-purpose programming language. It can be embedded in C/C++ projects, extended using C/C++ code or used as a standalone language.[5] The supported programming paradigms are imperative, procedural, object-oriented, functional, meta, declarative using nested structures, and natural programming.[6][7] The language is portable (Windows, Linux, macOS, Android,[8] WebAssembly,[9] etc.)[10] and can be used to create console, GUI, web, game and mobile applications.[11][12][13][14]
In 2009, Mahmoud Samir Fayed created a minor domain-specific language called Supernova that focuses on User interface (UI) creation and uses some ideas related to Natural Language Programming, then he realized the need for a new language that is general-purpose and can increase the productivity of natural language creation. Ring aims to offer a language focused on helping the developer with building natural interfaces and declarative DSLs.[15][16][17][18]
Ring supports object-oriented programming (classes, objects, composition, inheritance, encapsulation, etc.)[26]
The compiler and the virtual machine are designed using visual programming through the Programming Without Coding Technology software then the C code is generated.
Ring had a rapid rise and fall in popularity as measured by the TIOBE Programming Community Index. In February 2018, Ring broke into the top 50 for the first time (position 45).[34] As of October 2020, Ring holds position 93 on the TIOBE index.[35] Ring is listed by GitHub in the list of programming languages that are actively developed.[36]
Ring critics pointed to some features in Ring that are not common in widely used programming languages.[37]
Julia is a high-level, general-purpose[16] dynamic programming language, designed to be fast and productive,[17] for e.g. data science, artificial intelligence, machine learning, modeling and simulation, most commonly used for numerical analysis and computational science.[18][19][20]
Distinctive aspects of Julia's design include a type system with parametric polymorphism and the use of multiple dispatch as a core programming paradigm, a default just-in-time (JIT) compiler[16][21] (with support for ahead-of-time compilation[22][23][24]) and an efficient (multi-threaded) garbage collection implementation.[25] Notably Julia does not support classes with encapsulated methods and instead it relies on structs with generic methods/functions not tied to them.
Three of the Julia co-creators are the recipients of the 2019 James H. Wilkinson Prize for Numerical Software (awarded every four years) "for the creation of Julia, an innovative environment for the creation of high-performance tools that enable the analysis and solution of computational science problems."[38] Also, Alan Edelman, professor of applied mathematics at MIT, has been selected to receive the 2019 IEEE Computer Society Sidney Fernbach Award "for outstanding breakthroughs in high-performance computing, linear algebra, and computational science and for contributions to the Julia programming language."[39]
In June 2017, Julia Computing raised US$4.6 million in seed funding from General Catalyst and Founder Collective,[66] the same month was "granted $910,000 by the Alfred P. Sloan Foundation to support open-source Julia development, including $160,000 to promote diversity in the Julia community",[67] and in December 2019 the company got $1.1 million funding from the US government to "develop a neural component machine learning tool to reduce the total energy consumption of heating, ventilation, and air conditioning (HVAC) systems in buildings".[68] In July 2021, Julia Computing announced they raised a $24 million Series A round led by Dorilton Ventures,[69] which also owns Formula 1 team Williams Racing, that partnered with Julia Computing. Williams' Commercial Director said: "Investing in companies building best-in-class cloud technology is a strategic focus for Dorilton and Julia's versatile platform, with revolutionary capabilities in simulation and modelling, is hugely relevant to our business. We look forward to embedding Julia Computing in the world's most technologically advanced sport".[70] In June 2023, JuliaHub received (again, now under its new name) a $13 million strategic new investment led by AE Industrial Partners HorizonX ("AEI HorizonX"). AEI HorizonX is a venture capital investment platform formed in partnership with The Boeing Company, which uses Julia.[71] Tim Holy's work (at Washington University in St. Louis's Holy Lab) on Julia 1.9 (improving responsiveness) was funded by the Chan Zuckerberg Initiative.
Julia is a general-purpose programming language,[72] while also originally designed for numerical/technical computing. It is also useful for low-level systems programming,[73] as a specification language,[74] high-level synthesis (HLS) tool (for hardware, e.g. FPGAs),[75] and for web programming[76] at both server[77][78] and client[79][80] side.
Multiple dispatch (also termed multimethods in Lisp) is a generalization of single dispatch –  the polymorphic mechanism used in common object-oriented programming (OOP) languages, such as Python, C++, Java, JavaScript, and Smalltalk –  that uses inheritance. In Julia, all concrete types are subtypes of abstract types, directly or indirectly subtypes of the Any type, which is the top of the type hierarchy. Concrete types can not themselves be subtyped the way they can in other languages; composition is used instead (see also inheritance vs subtyping).
Most programming paradigms can be implemented using Julia's homoiconic macros and packages. Julia's syntactic macros (used for metaprogramming), like Lisp macros, are more powerful than text-substitution macros used in the preprocessor of some other languages such as C, because they work at the level of abstract syntax trees (ASTs). Julia's macro system is hygienic, but also supports deliberate capture when desired (like for anaphoric macros) using the esc construct.
Julia draws inspiration from various dialects of Lisp, including Scheme and Common Lisp, and it shares many features with Dylan, also a multiple-dispatch-oriented dynamic language (which features an infix syntax rather than a Lisp-like prefix syntax, while in Julia "everything"[82] is an expression), and with Fortress, another numerical programming language (which features multiple dispatch and a sophisticated parametric type system). While Common Lisp Object System (CLOS) adds multiple dispatch to Common Lisp, not all functions are generic functions.
The REPL can be extended with additional modes, and has been with packages, e.g. with an SQL mode,[103] for database access, and RCall.jl adds an R mode, to work with the R language.[104]
Julia has packages supporting markup languages such as HTML (and also for HTTP), XML, JSON and BSON, and for databases (such as PostgreSQL,[107] Mongo,[108] Oracle, including for TimesTen,[109] MySQL, SQLite, Microsoft SQL Server,[108] Amazon Redshift, Vertica, ODBC) and web use in general.[110][111]
While Julia requires an operating system by default, and has no official support to run without or on embedded system platforms such as Arduino, Julia code has still been run on it, with some limitations, i.e. on a baremetal 16 MHz 8-bit (ATmega328P) AVR-microcontroller Arduino with 2 KB RAM (plus 32 KB of flash memory).[134][135]
In computing, a visual programming language (visual programming system, VPL, or, VPS), also known as diagrammatic programming,[1][2] graphical programming or block coding, is a programming language that lets users create programs by manipulating program elements graphically rather than by specifying them textually.[3] A VPL allows programming with visual expressions, spatial arrangements of text and graphic symbols, used either as elements of syntax or secondary notation. For example, many VPLs are based on the idea of "boxes and arrows", where boxes or other screen objects are treated as entities, connected by arrows, lines or arcs which represent relations. VPLs are generally the basis of low-code development platforms.
VPLs may be further classified, according to the type and extent of visual expression used, into icon-based languages, form-based languages, and diagram languages. Visual programming environments provide graphical or iconic elements which can be manipulated by users in an interactive way according to some specific spatial grammar for program construction.
The general goal of VPLs is to make programming more accessible to novices and to support programmers at three different levels[4]
As of 2005, current developments try to integrate the visual programming approach with dataflow programming languages to either have immediate access to the program state, resulting in online debugging, or automatic program generation and documentation. Dataflow languages also allow automatic parallelization, which is likely to become one of the greatest programming challenges of the future.[5]
The Visual Basic, Visual C#, Visual J# etc. languages of the Microsoft Visual Studio integrated development environment (IDE) are not visual programming languages: the representation of algorithms etc. is textual even though the IDE embellishes the editing and debugging activities with a rich user interface. A similar consideration applies to most other rapid application development environments which typically support a form designer and sometimes also have graphical tools to illustrate (but not define) control flow and data dependencies.
The following list is not mutually exclusive, as some visual programming environments may incorporate elements from multiple paradigms. The choice of visual programming paradigm often depends on the specific requirements of the application or the preferences of the users or the developers.
Most of the VPLs are designed for education or domain-specific usage where the target users are novice programmers. But there are some research projects try to provide a general-purpose visual programming language that can be used by mainstream programmers in any software project instead of using textual programming languages (like C, C++, Java, etc.).
For example, research projects such as Envision [8][9] and PWCT[10] are designed to achieve this goal. 
It's common for a VPL to be developed using a textual programming language. Developing general-purpose VPLs allows the other way around. For example, a new textual programming language Compiler and Virtual Machine have been developed and maintained using visual programming for many years.[11] This approach increased the abstraction level, thereby hiding unnecessary details. [12] This increased level of abstraction enables developers to concentrate on the program's overall structure and functionality instead of being entangled in the details of code syntax. [13][14]
Many modern video games make use of behavior trees, which are in principle a family of simple programming languages designed to model behaviors for non-player characters. The behaviors are modeled as trees, and are often edited in graphical editors.
Scratch is a high-level, block-based visual programming language and website aimed primarily at children as an educational tool, with a target audience of ages 8 to 16.[9][10] Users on the site can create projects on the website using a block-like interface. Scratch was conceived and designed through collaborative National Science Foundation grants awarded to Mitchel Resnick and Yasmin Kafai.[11] Scratch is developed by the MIT Media Lab and has been translated into 70+ languages, being used in most parts of the world. Scratch is taught and used in after-school centers, schools, and colleges, as well as other public knowledge institutions. As of 15 February 2023, community statistics on the language's official website show more than 123 million projects shared by over 103 million users, and more than 95 million monthly website visits.[12] Overall, more than 1,140,000,000 projects have been created in total, with the site reaching its 1,000,000,000th project on April 12th, 2024.
In Scratch, extensions add extra blocks and features that can be used in projects. In Scratch 2.0, the extensions were all hardware-based and Pen was a normal category. Software-based extensions were added in Scratch 3.0, such as text-to-speech voices, along with some new hardware-based extensions like the micro:bit. The extensions are listed below.
Scratch is used in many different settings: schools, museums, libraries, community centers, and homes.[19][20][21] Scratch is designed primarily for users aged 8–16, but it is used by all ages and has a sizeable adult user base as of 2009.[10][22] This wide outreach has created many surrounding communities, both physical and digital.[12] In April 2020, the Tiobe ranking of the world's programming languages included Scratch in the top 20. According to Tiobe, there are 50 million projects written in Scratch, and every month one million new projects are added.[23]
Scratch is used as the introductory language because the creation of interesting programs is relatively easy, and skills learned can be applied to other programming languages such as Python and Java.
Scratch is not exclusively for creating games. With the provided visuals, programmers can create animations, text, stories, music, art, and more. There are already many programs that students can use to learn topics in math, history, and even photography. Scratch allows teachers to create conceptual and visual lessons and science lab assignments with animations that help visualize difficult concepts. Within the social sciences, instructors can create quizzes, games, and tutorials with interactive elements. Using Scratch allows young people to understand the logic of programming and how to creatively build and collaborate.[24]
In higher education, Scratch is used in the first week of Harvard University's CS50 introductory computer science course.[27][28]
In 3.0, Scratch blocks are implemented using Blockly, a JavaScript library developed by Google for creating block-based visual programming languages.[45][46]
In the early 2000s, the  MIT Media Lab's Lifelong Kindergarten group (LLK) was developing visual programming languages targeted towards children.[50] In 2003, Mitchel Resnick, Yasmin Kafai, and John Maeda were awarded a National Science Foundation grant for the development of a new programming environment for children to express themselves with code.[11] The LLK, led by Mitchel Resnick, in partnership with Yasmin Kafai's team at UCLA worked closely with Computer Clubhouses in Boston and Los Angeles to develop Scratch, grounding its design in the practices and social dynamics of these after-school youth centers.[11] It started as a basic programming language, with no labeled categories and no green flag.[51] Similar to AgentSheets[52] Scratch employed concepts of Tactile Programming later known as blocks-based programming. Scratch was made with the intention to teach kids to program.[51]
In Scratch 2.0, the stage area is on the left side, with the programming blocks palette in the middle, and the coding area on the right. Extensions are in the "More Blocks" section of the palette.[22]
Scratch uses event-driven programming with multiple active objects called sprites.[14] Sprites can be drawn, as vector or bitmap graphics, from scratch in a simple editor that is part of Scratch, or can be imported from external sources. Scratch 3.0 only supports one-dimensional arrays, known as "lists", and floating-point scalars and strings are supported but with limited string manipulation ability. There is a strong contrast between the powerful multimedia functions and multi-threaded programming style and the rather limited scope of the Scratch programming language.
A more advanced visual programming language inspired by Scratch is Snap!, featuring first class procedures (their mathematical foundations are also called lambda calculus), first-class lists (including lists of lists), and first-class truly object-oriented sprites with prototyping inheritance, and nestable sprites, which are not part of Scratch.[83] Snap! (previously "BYOB") was developed by Jens Mönig[84][85] with documentation provided by Brian Harvey[86][87] from University of California, Berkeley and has been used to teach "The Beauty and Joy of Computing" introductory course in CS for non-CS-major students.[88] Both of them were members of the Scratch Team before designing "Snap!".[89][22]
Ballerina is an open source general-purpose programming language designed by WSO2 for cloud-era application programmers.[2]
In programming language theory, semantics is the rigorous mathematical study of the meaning of programming languages.[1] Semantics assigns computational meaning to valid strings in a programming language syntax. It is closely related to, and often crosses over with, the semantics of mathematical proofs.
A semantic definition of a programming language, in our approach, is founded on a syntactic definition. It must specify which of the phrases in a syntactically correct program represent commands, and what conditions must be imposed on an interpretation in the neighborhood of each command.
It has close links with other areas of computer science such as programming language design, type theory, compilers and interpreters, program verification and model checking.
In programming language theory, semantics is the rigorous mathematical study of the meaning of programming languages.[1] Semantics assigns computational meaning to valid strings in a programming language syntax. It is closely related to, and often crosses over with, the semantics of mathematical proofs.
A semantic definition of a programming language, in our approach, is founded on a syntactic definition. It must specify which of the phrases in a syntactically correct program represent commands, and what conditions must be imposed on an interpretation in the neighborhood of each command.
It has close links with other areas of computer science such as programming language design, type theory, compilers and interpreters, program verification and model checking.
In computer programming, one of the many ways that programming languages are colloquially classified is whether the language's type system makes it strongly typed or weakly typed (loosely typed). However, there is no precise technical definition of what the terms mean and different authors disagree about the implied meaning of the terms and the relative rankings of the "strength" of the type systems of mainstream programming languages.[1] For this reason, writers who wish to write unambiguously about type systems often eschew the terms "strong typing" and "weak typing" in favor of specific expressions such as "type safety".
"Strong typing" generally refers to use of programming language types in order to both capture invariants of the code, and ensure its correctness, and definitely exclude certain classes of programming errors. Thus there are many "strong typing" disciplines used to achieve these goals.
Some programming languages make it easy to use a value of one type as if it were a value of another type. This is sometimes described as "weak typing".
Some programming languages expose pointers as if they were numeric values, and allow users to perform arithmetic on them. These languages are sometimes referred to as "weakly typed", since pointer arithmetic can be used to bypass the language's type system.
Some programming languages support untagged unions, which allow a value of one type to be viewed as if it were a value of another type.
In Luca Cardelli's article Typeful Programming,[7] a "strong type system" is described as one in which there is no possibility of an unchecked runtime type error. In other writing, the absence of unchecked run-time errors is referred to as safety or type safety; Tony Hoare's early papers call this property security.[8]
Some of these definitions are contradictory, others are merely conceptually independent, and still others are special cases (with additional constraints) of other, more "liberal" (less strong) definitions. Because of the wide divergence among these definitions, it is possible to defend claims about most programming languages that they are either strongly or weakly typed. For instance:
In some operating systems, e.g. Burroughs/Unisys MCP,[2] and OS/360 and successors,[3] memory is managed by the operating system.[note 1] In other operating systems, e.g. Unix-like operating systems, memory is managed at the application level.
The specific dynamic memory allocation algorithm implemented can impact performance significantly. A study conducted in 1994 by Digital Equipment Corporation illustrates the overheads involved for a variety of allocators. The lowest average instruction path length required to allocate a single memory slot was 52 (as measured with an instruction level profiler on a variety of software).[1]
Since the precise location of the allocation is not known in advance, the memory is accessed indirectly, usually through a pointer reference. The specific algorithm used to organize the memory area and allocate and deallocate chunks is interlinked with the kernel, and may use any of the following methods:
In many programming language implementations, the runtime environment for the program automatically allocates memory in the call stack for non-static local variables of a subroutine, called automatic variables, when the subroutine is called, and automatically releases that memory when the subroutine is exited. Special declarations may allow local variables to retain values between invocations of the procedure, or may allow local variables to be accessed by other subroutines. The automatic allocation of local variables makes recursion possible, to a depth limited by available memory.
In virtual memory systems the operating system limits how a process can access the memory. This feature, called memory protection, can be used to disallow a process to read or write to memory that is not allocated to it, preventing malicious or malfunctioning code in one program from interfering with the operation of another.
An operating system manages various resources in the computing system. The memory subsystem is the system element for managing memory. The memory subsystem combines the hardware memory resource and the MCP OS software that manages the resource.
Donald Knuth describes a similar system in Section 2.5 ‘Dynamic Storage Allocation’ of ‘Fundamental Algorithms’.[disputed – discuss]
In computer science, a data structure is a data organization and storage format that is usually chosen for efficient access to data.[1][2][3] More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data,[4] i.e., it is an algebraic structure about data.
Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.[5]
Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval,[6] while compiler implementations usually use hash tables to look up identifiers.[7]
Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.[8]
Data structures can be implemented using a variety of programming languages and techniques, but they all share the common goal of efficiently organizing and storing data.[9] Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself. This approach to data structuring has profound implications for the efficiency and scalability of algorithms. For instance, the contiguous memory allocation in arrays facilitates rapid access and modification operations, leading to optimized performance in sequential data processing scenarios.[10]
The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).[11]
There are numerous types of data structures, generally built upon simpler primitive data types. Well known examples are:[12]
Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C (a direct descendant of BCPL) and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.[14][15]
Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and the Microsoft .NET Framework.
Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java, and Smalltalk, typically use classes for this purpose.
Many known data structures have concurrent versions which allow multiple computing threads to access a single concrete instance of a data structure simultaneously.[16]
In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple threads and avoid critical section problems in a concurrent system such as a multitasking operating system. Semaphores are a type of synchronization primitive. A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) depending on programmer-defined conditions.
The semaphore concept was invented by Dutch computer scientist Edsger Dijkstra in 1962 or 1963,[1] when Dijkstra and his team were developing an operating system for the Electrologica X8. That system eventually became known as the THE multiprogramming system.
Many operating systems provide efficient semaphore primitives that unblock a waiting process when the semaphore is incremented. This means that processes do not waste time checking the semaphore value unnecessarily.
If the implementation does not ensure atomicity of the increment, decrement, and comparison operations, there is a risk of increments or decrements being forgotten, or of the semaphore value becoming negative. Atomicity may be achieved by using a machine instruction that can read, modify, and write the semaphore in a single operation. Without such a hardware instruction, an atomic operation may be synthesized by using a software mutual exclusion algorithm. On uniprocessor systems, atomic operations can be ensured by temporarily suspending preemption or disabling hardware interrupts. This approach does not work on multiprocessor systems where it is possible for two programs sharing a semaphore to run on different processors at the same time. To solve this problem in a multiprocessor system, a locking variable can be used to control access to the semaphore. The locking variable is manipulated using a test-and-set-lock command.
The "Passing the baton" pattern[3][4][5] proposed by Gregory R. Andrews is a generic scheme to solve many complex concurrent programming problems in which multiple processes compete for the same resource with complex access conditions (such as satisfying specific priority criteria or avoiding starvation). Given a shared resource, the pattern requires a private "priv" semaphore (initialized to zero) for each process (or class of processes) involved and a single mutual exclusion "mutex" semaphore (initialized to one). 
The pseudo-code for each process is:
In ALGOL 68, the Linux kernel,[11] and in some English textbooks, the V and P operations are called, respectively, up and down. In software engineering practice, they are often called signal and wait,[12] release and acquire[12] (standard Java library),[13] or post and pend. Some texts call them vacate and procure to match the original Dutch initials.[14][15]
Programming languages are typically created by designing a form of representation of a computer program, and writing an implementation for the developed concept, [1] usually an interpreter or compiler. Interpreters are designed to read programs, usually in some variation of a text format, and perform actions based on what it reads, whereas compilers convert code to lower level.[2]
In programming language design, there are a wide variety of factors to consider. Some factors may be mutually exclusive (e.g. security versus speed). It may be necessary to consider whether a programming language will perform better interpreted, or compiled, if a language should be dynamically or statically typed, if inheritance will be in, and the general syntax of the language.[3] Many factors involved with the design of a language can be decided on by the goals behind the language. It's important to consider the target audience of a language, its unique features and its purpose.[4] It is good practice to look at what existing languages lack, or make difficult, to make sure a language serves a purpose.[4]
Processes of making a programming language may differ from developer to developer; however, here is a general process of how one might create a programming language, which includes common concepts:
Natural-language programming (NLP) is an ontology-assisted way of programming in terms of natural-language sentences, e.g. English.[1] A structured document with Content, sections and subsections for explanations of sentences forms a NLP document, which is actually a computer program. Natural language programming is not to be mixed up with natural language interfacing or voice control where a program is first written and then communicated with through natural language using an interface added on. In NLP the functionality of a program is organised only for the definition of the meaning of sentences. For instance, NLP can be used to represent all the knowledge of an autonomous robot. Having done so, its tasks can be scripted by its users so that the robot can execute them autonomously while keeping to prescribed rules of behaviour as determined by the robot's user. Such robots are called transparent robots [2] as their reasoning  is transparent to users and this develops trust in robots.   Natural language use and natural-language user interfaces include Inform 7, a natural programming language for making interactive fiction, Shakespeare, an esoteric natural programming language in the style of the plays of William Shakespeare, and Wolfram Alpha, a computational knowledge engine, using natural-language input.[citation needed] Some methods for program synthesis are based on natural-language programming.[3]
The smallest unit of statement in NLP is a sentence.  Each sentence is stated in terms of concepts from the underlying ontology, attributes in that ontology and named objects in capital letters.  In an NLP text every sentence unambiguously compiles into a procedure call in the underlying high-level programming language such as MATLAB, Octave, SciLab, Python, etc.
Symbolic languages such as Wolfram Language are capable of interpreted processing of queries by sentences. This can allow interactive requests such as that implemented in Wolfram Alpha.[4][5] The difference between these and NLP is that the latter builds up a single program or a library of routines that are programmed through natural language sentences using an ontology that defines the available data structures in a high level programming language.
A natural-language program is a precise formal description of some procedure that its author created. It is human readable and it can also be read by a suitable software agent.  For example, a web page in an NLP format can be read by a software personal assistant agent to a person and she or he can ask the agent to execute some sentences, i.e. carry out some task or answer a question. There is a reader agent available for English interpretation of HTML based NLP documents that a person can run on her personal computer .
A set of NLP sentences, with associated ontology defined, can also be used as a pseudo code that does not provide the details in any underlying high level programming language. In such an application the sentences used become high level abstractions (conceptualisations) of computing procedures that are computer language and machine independent.
Researchers have started to experiment with natural language programming environments that use plain language prompts and then use AI (specifically large language models) to turn natural language into formal code. For example Spatial Pixel created a natural language programming environment to turn natural language into P5.js code through OpenAI's API. In 2021 OpenAI developed a natural language programming environment for their programming large language model called Codex.
In computer programming, a programming language specification (or standard or definition) is a documentation artifact that defines a programming language so that users and implementors can agree on what programs in that language mean. Specifications are typically detailed and formal, and primarily used by implementors, with users referring to them in case of ambiguity; the C++ specification is frequently cited by users, for instance, due to the complexity. Related documentation includes a programming language reference, which is intended expressly for users, and a programming language rationale, which explains why the specification is written as it is; these are typically more informal than a specification.
Not all major programming languages have specifications, and languages can exist and be popular for decades without a specification. A language may have one or more implementations, whose behavior acts as a de facto standard, without this behavior being documented in a specification. Perl (through Perl 5) is a notable example of a language without a specification, while PHP was only specified in 2014, after being in use for 20 years.[1] A language may be implemented and then specified, or specified and then implemented, or these may develop together, which is usual practice today. This is because implementations and specifications provide checks on each other: writing a specification requires precisely stating the behavior of an implementation, and implementation checks that a specification is possible, practical and consistent. Writing a specification before an implementation has largely been avoided since ALGOL 68 (1968), due to unexpected difficulties in implementation when implementation is deferred. However, languages are still occasionally implemented and gain popularity without a formal specification: an implementation is essential for use, while a specification is desirable but not essential (informally, "code talks").
The syntax of a programming language represents the definition of acceptable words, i.e., formal parameters and rules upon which to decide whether a given code is valid in respect to the language. On that note, the language syntax usually consists of a combination of the following three construction components:
Formulating a rigorous semantics of a large, complex, practical programming language is a daunting task even for experienced specialists, and the resulting specification can be difficult for anyone but experts to understand. The following are some of the ways in which programming language semantics can be described; all languages use at least one of these description methods, and some languages combine more than one[5]
The imprecision of natural language as a vehicle for describing programming language semantics can lead to problems with interpreting the specification. For example, the semantics of Java threads were specified in English, and it was later discovered that the specification did not provide adequate guidance for implementors.[7]
A reference implementation is a single implementation of a programming language that is designated as authoritative. The behavior of this implementation is held to define the proper behavior of a program written in the language. This approach has several attractive properties. First, it is precise, and requires no human interpretation: disputes as to the meaning of a program can be settled simply by executing the program on the reference implementation (provided that the implementation behaves deterministically for that program).
Nevertheless, several languages have successfully used the reference implementation approach. For example, the Perl interpreter is considered to define the authoritative behavior of Perl programs. In the case of Perl, the open-source model of software distribution has contributed to the fact that nobody has ever produced another implementation of the language, so the issues involved in using a reference implementation to define the language semantics are moot.
Defining the semantics of a programming language in terms of a test suite involves writing a number of example programs in the language, and then describing how those programs ought to behave—perhaps by writing down their correct outputs. The programs, plus their outputs, are called the "test suite" of the language. Any correct language implementation must then produce exactly the correct outputs on the test suite programs.
The software release life cycle is the process of developing, testing, and distributing a software product (e.g., an operating system). It typically consists of several stages, such as pre-alpha, alpha, beta, and release candidate, before the final version, or "gold", is released to the public.
Pre-alpha refers to the early stages of development, when the software is still being designed and built. Alpha testing is the first phase of formal testing, during which the software is tested internally using white-box techniques. Beta testing is the next phase, in which the software is tested by a larger group of users, typically outside of the organization that developed it. The beta phase is focused on reducing impacts on users and may include usability testing.
After beta testing, the software may go through one or more release candidate phases, in which it is refined and tested further, before the final version is released.
Some software, particularly in the internet and technology industries, is released in a perpetual beta state, meaning that it is continuously being updated and improved, and is never considered to be a fully completed product. This approach allows for a more agile development process and enables the software to be released and used by users earlier in the development cycle.
Pre-alpha refers to all activities performed during the software project before formal testing. These activities can include requirements analysis, software design, software development, and unit testing. In typical open source development, there are several types of pre-alpha versions. Milestone versions include specific sets of functions and are released as soon as the feature is complete.[citation needed]
The alpha phase of the release life cycle is the first phase of software testing (alpha is the first letter of the Greek alphabet, used as the number 1). In this phase, developers generally test the software using white-box techniques. Additional validation is then performed using black-box or gray-box techniques, by another testing team. Moving to black-box testing inside the organization is known as alpha release.[1][2]
Alpha software is not thoroughly tested by the developer before it is released to customers. Alpha software may contain serious errors, and any resulting instability could cause crashes or data loss.[3] Alpha software may not contain all of the features that are planned for the final version.[4] In general, external availability of alpha software is uncommon for proprietary software, while open source software often has publicly available alpha versions. The alpha phase usually ends with a feature freeze, indicating that no more features will be added to the software. At this time, the software is said to be feature-complete. A beta test is carried out following acceptance testing at the supplier's site (the alpha test) and immediately before the general release of the software as a product.[5]
A feature-complete (FC) version of a piece of software has all of its planned or primary features implemented but is not yet final due to bugs, performance or stability issues.[6] This occurs at the end of alpha testing in development.
Usually, feature-complete software still has to undergo beta testing and bug fixing, as well as performance or stability enhancement before it can go to release candidate, and finally gold status.
Beta, named after the second letter of the Greek alphabet, is the software development phase following alpha. A beta phase generally begins when the software is feature-complete but likely to contain several known or unknown bugs.[7] Software in the beta phase will generally have many more bugs in it than completed software and speed or performance issues, and may still cause crashes or data loss. The focus of beta testing is reducing impacts on users, often incorporating usability testing. The process of delivering a beta version to the users is called beta release and is typically the first time that the software is available outside of the organization that developed it. Software beta releases can be either open or closed, depending on whether they are openly available or only available to a limited audience. Beta version software is often useful for demonstrations and previews within an organization and to prospective customers. Some developers refer to this stage as a preview, preview release, prototype, technical preview or technology preview (TP),[8] or early access.
Beta testers are people who actively report issues with beta software. They are usually customers or representatives of prospective customers of the organization that develops the software. Beta testers tend to volunteer their services free of charge but often receive versions of the product they test, discounts on the release version, or other incentives.[9][10]
Some software is kept in so-called perpetual beta, where new features are continually added to the software without establishing a final "stable" release. As the Internet has facilitated the rapid and inexpensive distribution of software, companies have begun to take a looser approach to the use of the word beta.[11]
Developers may release either a closed beta, or an open beta; closed beta versions are released to a restricted group of individuals for a user test by invitation, while open beta testers are from a larger group, or anyone interested. Private beta could be suitable for the software that is capable of delivering value but is not ready to be used by everyone either due to scaling issues, lack of documentation or still missing vital features. The testers report any bugs that they find, and sometimes suggest additional features they think should be available in the final version.
Some software products (e.g. Linux distributions like Debian) also have long-term support (LTS) releases which are based on full releases that have already been tried and tested and receive only security updates.[citation needed]
Once released, the software is generally known as a "stable release". The formal term often depends on the method of release: physical media, online release, or a web application.[12]
The term "release to manufacturing" (RTM), also known as "going gold", is a term used when a software product is ready to be delivered. This build may be digitally signed, allowing the end user to verify the integrity and authenticity of the software purchase. The RTM build is known as the "gold master" or GM[13] is sent for mass duplication or disc replication if applicable. The terminology is taken from the audio record-making industry, specifically the process of mastering. RTM precedes general availability (GA) when the product is released to the public. A golden master build (GM) is typically the final build of a piece of software in the beta stages for developers. Typically, for iOS, it is the final build before a major release, however, there have been a few exceptions.
RTM is typically used in certain retail mass-production software contexts—as opposed to a specialized software production or project in a commercial or government production and distribution—where the software is sold as part of a bundle in a related computer hardware sale and typically where the software and related hardware is ultimately to be available and sold on mass/public basis at retail stores to indicate that the software has met a defined quality level and is ready for mass retail distribution. RTM could also mean in other contexts that the software has been delivered or released to a client or customer for installation or distribution to the related hardware end user computers or machines. The term does not define the delivery mechanism or volume; it only states that the quality is sufficient for mass distribution. The deliverable from the engineering organization is frequently in the form of a golden master media used for duplication or to produce the image for the web.
General availability (GA) is the marketing stage at which all necessary commercialization activities have been completed and a software product is available for purchase, depending, however, on language, region, and electronic vs. media availability.[14] Commercialization activities could include security and compliance tests, as well as localization and worldwide availability. The time between RTM and GA can take from days to months before a generally available release can be declared, due to the time needed to complete all commercialization activities required by GA. At this stage, the software has "gone live".
Release to the Web (RTW) or Web release is a means of software delivery that utilizes the Internet for distribution. No physical media are produced in this type of release mechanism by the manufacturer. Web releases have become more common as Internet usage has grown.[citation needed]
During its supported lifetime, the software is sometimes subjected to service releases, patches or service packs, sometimes also called "interim releases" or "maintenance releases" (MR). For example, Microsoft released three major service packs for the 32-bit editions of Windows XP and two service packs for the 64-bit editions.[15] Such service releases contain a collection of updates, fixes, and enhancements, delivered in the form of a single installable package. They may also implement new features. Some software is released with the expectation of regular support. Classes of software that generally involve protracted support as the norm include anti-virus suites and massively multiplayer online games. Continuing with this Windows XP example, Microsoft did offer paid updates for five more years after the end of extended support. This means that support ended on April 8, 2019.[16]
When software is no longer sold or supported, the product is said to have reached end-of-life, to be discontinued, retired, deprecated, abandoned, or obsolete, but user loyalty may continue its existence for some time, even long after its platform is obsolete—e.g., the Common Desktop Environment[17] and Sinclair ZX Spectrum.[18]
Usage of the "alpha/beta" test terminology originated at IBM.[citation needed] Similar terminologies for IBM's software development were used by people involved with IBM from at least the 1950s (and probably earlier). "A" test was the verification of a new product before the public announcement. The "B" test was the verification before releasing the product to be manufactured. The "C" test was the final test before the general availability of the product. As software became a significant part of IBM's offerings, the alpha test terminology was used to denote the pre-announcement test and the beta test was used to show product readiness for general availability. Martin Belsky, a manager on some of IBM's earlier software projects claimed to have invented the terminology. IBM dropped the alpha/beta terminology during the 1960s, but by then it had received fairly wide notice. The usage of "beta test" to refer to testing done by customers was not done in IBM. Rather, IBM used the term "field test".
Major public betas developed afterward, with early customers having purchased a "pioneer edition" of the WordVision word processor for the IBM PC for $49.95. In 1984, Stephen Manes wrote that "in a brilliant marketing coup, Bruce and James Program Publishers managed to get people to pay for the privilege of testing the product."[19] In September 2000, a boxed version of Apple's Mac OS X Public Beta operating system was released.[20] Between September 2005 and May 2006, Microsoft released community technology previews (CTPs) for Windows Vista.[21] From 2009 to 2011, Minecraft was in public beta.
In February 2005, ZDNet published an article about the phenomenon of a beta version often staying for years and being used as if it were at the production level.[22] It noted that Gmail and Google News, for example, had been in beta for a long time although widely used; Google News left beta in January 2006, followed by Google Apps (now named Google Workspace), including Gmail, in July 2009.[12] Since the introduction of Windows 8, Microsoft has called pre-release software a preview rather than beta. All pre-release builds released through the Windows Insider Program launched in 2014 are termed "Insider Preview builds". "Beta" may also indicate something more like a release candidate, or as a form of time-limited demo, or marketing technique.[23]
Java is a high-level, general-purpose, memory-safe, object-oriented programming language. It is intended to let programmers write once, run anywhere (WORA),[16] meaning that compiled Java code can run on all platforms that support Java without the need to recompile.[17] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture. The syntax of Java is similar to C and C++, but has fewer low-level facilities than either of them. The Java runtime provides dynamic capabilities (such as reflection and runtime code modification) that are typically not available in traditional compiled languages.
Java gained popularity shortly after its release, and has been a popular programming language since then.[18] Java was the third most popular programming language in 2022[update] according to GitHub.[19] Although still widely popular, there has been a gradual decline in use of Java in recent years with other languages using JVM gaining popularity.[20]
Java was designed by James Gosling at Sun Microsystems. It was released in May 1995 as a core component of Sun's Java platform. The original and reference implementation Java compilers, virtual machines, and class libraries were released by Sun under proprietary licenses. As of May 2007, in compliance with the specifications of the Java Community Process, Sun had relicensed most of its Java technologies under the GPL-2.0-only license. Oracle, which bought Sun in 2010, offers its own HotSpot Java Virtual Machine. However, the official reference implementation is the OpenJDK JVM, which is open-source software used by most developers and is the default JVM for almost all Linux distributions.
In 1997, Sun Microsystems approached the ISO/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process.[27][28][29] Java remains a de facto standard, controlled through the Java Community Process.[30] At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.
On November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software (FOSS), under the terms of the GPL-2.0-only license. On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.[31]
Java software runs on most devices from laptops to data centers, game consoles to scientific supercomputers.[36]
One design goal of Java is portability, which means that programs written for the Java platform must run similarly on any combination of hardware and operating system with adequate run time support. This is achieved by compiling the Java language code to an intermediate representation called Java bytecode, instead of directly to architecture-specific machine code. Java bytecode instructions are analogous to machine code, but they are intended to be executed by a virtual machine (VM) written specifically for the host hardware. End-users commonly use a Java Runtime Environment (JRE) installed on their device for standalone Java applications or a web browser for Java applets.
Some platforms offer direct hardware support for Java; there are micro controllers that can run Java bytecode in hardware instead of a software Java virtual machine,[53] and some ARM-based processors could have hardware support for executing Java bytecode through their Jazelle option, though support has mostly been dropped in current implementations of ARM.
Having solved the memory management problem does not relieve the programmer of the burden of handling properly other kinds of resources, like network or database connections, file handles, etc., especially in the presence of exceptions.
The syntax of Java is largely influenced by C++ and C. Unlike C++, which combines the syntax for structured, generic, and object-oriented programming, Java was built almost exclusively as an object-oriented language.[17] All code is written inside classes, and every data item is an object, with the exception of the primitive data types, (i.e. integers, floating-point numbers, boolean values, and characters), which are not objects for performance reasons. Java reuses some popular aspects of C++ (such as the printf method).
JavaFX is a software platform for creating and delivering desktop applications, as well as rich web applications that can run across a wide variety of devices. JavaFX is intended to replace Swing as the standard graphical user interface (GUI) library for Java SE, but since JDK 11 JavaFX has not been in the core JDK and instead in a separate module.[64] JavaFX has support for desktop computers and web browsers on Microsoft Windows, Linux, and macOS. JavaFX does not have support for native OS look and feels.[65]
Criticisms directed at Java include the implementation of generics,[67] speed,[50] the handling of unsigned numbers,[68] the implementation of floating-point arithmetic,[69] and a history of security vulnerabilities in the primary Java VM implementation HotSpot.[70]
Developers have criticized the complexity and verbosity of the Java Persistence API (JPA), a standard part of Java EE. This has led to increased adoption of higher-level abstractions like Spring Data JPA, which aims to simplify database operations and reduce boilerplate code. The growing popularity of such frameworks suggests limitations in the standard JPA implementation's ease-of-use for modern Java development.[71]
The Oracle implementation is packaged into two different distributions: The Java Runtime Environment (JRE) which contains the parts of the Java SE platform required to run Java programs and is intended for end users, and the Java Development Kit (JDK), which is intended for software developers and includes development tools such as the Java compiler, Javadoc, Jar, and a debugger. Oracle has also released GraalVM, a high performance Java dynamic compiler and interpreter.
The Java programming language requires the presence of a software platform in order for compiled programs to be executed.
Oracle supplies the Java platform for use with Java. The Android SDK is an alternative software platform, used primarily for developing Android applications with its own GUI system.
The Java language is a key pillar in Android, an open source mobile operating system. Although Android, built on the Linux kernel, is written largely in C, the Android SDK uses the Java language as the basis for Android applications but does not use any of its standard GUI, SE, ME or other established Java standards.[83] The bytecode language supported by the Android SDK is incompatible with Java bytecode and runs on its own virtual machine, optimized for low-memory devices such as smartphones and tablet computers. Depending on the Android version, the bytecode is either interpreted by the Dalvik virtual machine or compiled into native code by the Android Runtime.
Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks.[1][2] It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.
Auxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term software development is used for this larger overall process – with the terms programming, implementation, and coding reserved for the writing and editing of code per se. Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.
Code-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.[7]
The first computer program is generally dated to 1843 when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.[8] However, Charles Babbage himself had written a program for the AE in 1837.[9][10]
These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier[16] by allowing programmers to specify calculations by entering a formula using infix notation.
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.
Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.[22] Some of these factors include:
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.
The academic field and the engineering practice of computer programming are concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into orders using Big O notation, which expresses resource use—such as execution time or memory consumption—in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.
Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic programming languages.
It is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language,[23] the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers[24] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).
Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.
Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in "high-level" languages than in "low-level" ones.
Programming languages are essential for software development. They are the building blocks for all software, from the simplest applications to the most sophisticated ones.
Learning to program has a long history related to professional standards and practices, academic initiatives and curriculum, and commercial books and materials for students, self-taught learners, hobbyists, and others who desire to create or customize software for personal use. Since the 1960s, learning to program has taken on the characteristics of a popular movement, with the rise of academic disciplines, inspirational leaders, collective identities, and strategies to grow the movement and make institutionalize change.[26] Through these social ideals and educational agendas, learning to code has become important not just for scientists and engineers, but for millions of citizens who have come to believe that creating software is beneficial to society and its members.
In 1957, there were approximately 15,000 computer programmers employed in the U.S., a figure that accounts for 80% of the world's active developers. In 2014, there were approximately 18.5 million professional programmers in the world, of which 11 million can be considered professional and 7.5 million student or hobbyists.[27] Before the rise of the commercial Internet in the mid-1990s, most programmers learned about software construction through books, magazines, user groups, and informal instruction methods, with academic coursework and corporate training playing important roles for professional workers.[28]
When high-level languages arrived, they were introduced by numerous books and materials that explained language keywords, managing program flow, working with data, and other concepts. These languages included FLOW-MATIC, COBOL, FORTRAN, ALGOL, Pascal, BASIC, and C. An example of an early programming primer from these years is Marshal H. Wrubel's A Primer of Programming for Digital Computers (1959), which included step-by-step instructions for filling out coding sheets, creating punched cards, and using the keywords in IBM's early FORTRAN system.[29] Daniel McCracken's A Guide to FORTRAN Programming (1961) presented FORTRAN to a larger audience, including students and office workers.
In 1961, Alan Perlis suggested that all university freshmen at Carnegie Technical Institute take a course in computer programming.[30] His advice was published in the popular technical journal Computers and Automation, which became a regular source of information for professional programmers.
Early and influential programmer's guides included John G. Kemeny and Thomas E. Kurtz's BASIC Programming (1967), Kathleen Jensen and Niklaus Wirth's The Pascal User Manual and Report (1971), and Brian Kernighan and Dennis Ritchie's The C Programming Language (1978). Similar books for popular audiences (but with a much lighter tone) included Bob Albrecht's My Computer Loves Me When I Speak BASIC (1972), Al Kelley and Ira Pohl's A Book on C (1984), and Dan Gookin's C for Dummies (1994).
Beyond language-specific primers, there were numerous books and academic journals that introduced professional programming practices. Many were designed for university courses in computer science, software engineering, or related disciplines. Donald Knuth's The Art of Computer Programming (1968 and later), presented hundreds of computational algorithms and their analysis. The Elements of Programming Style (1974), by Brian W. Kernighan and P. J. Plauger, concerned itself with programming style, the idea that programs should be written not only to satisfy the compiler but human readers. Jon Bentley's Programming Pearls (1986) offered practical advice about the art and craft of programming in professional and academic contexts. Texts specifically designed for students included Doug Cooper and Michael Clancy's Oh Pascal! (1982), Alfred Aho's Data Structures and Algorithms (1983), and Daniel Watt's Learning with Logo (1983).
As personal computers became mass-market products, thousands of trade books and magazines sought to teach professional, hobbyist, and casual users to write computer programs. A sample of these learning resources includes BASIC Computer Games, Microcomputer Edition (1978), by David Ahl; Programming the Z80 (1979), by Rodnay Zaks; Programmer's CP/M Handbook (1983), by Andy Johnson-Laird; C Primer Plus (1984), by Mitchell Waite and The Waite Group; The Peter Norton Programmer's Guide to the IBM PC (1985), by Peter Norton; Advanced MS-DOS (1986), by Ray Duncan; Learn BASIC Now (1989), by Michael Halvorson and David Rygymr; Programming Windows (1992 and later), by Charles Petzold; Code Complete: A Practical Handbook for Software Construction (1993), by Steve McConnell; and Tricks of the Game-Programming Gurus (1994), by André LaMothe.
The PC software industry spurred the creation of numerous book publishers that offered programming primers and tutorials, as well as books for advanced software developers.[31] These publishers included Addison-Wesley, IDG, Macmillan Inc., McGraw-Hill, Microsoft Press, O'Reilly Media, Prentice Hall, Sybex, Ventana Press, Waite Group Press, Wiley, Wrox Press, and Ziff-Davis.
Computer magazines and journals also provided learning content for professional and hobbyist programmers. A partial list of these resources includes Amiga World, Byte (magazine), Communications of the ACM, Computer (magazine), Compute!, Computer Language (magazine), Computers and Electronics, Dr. Dobb's Journal, IEEE Software, Macworld, PC Magazine, PC/Computing, and UnixWorld.
Between 2000 and 2010, computer book and magazine publishers declined significantly as providers of programming instruction, as programmers moved to Internet resources to expand their access to information. This shift brought forward new digital products and mechanisms to learn programming skills. During the transition, digital books from publishers transferred information that had traditionally been delivered in print to new and expanding audiences.[32]
Important Internet resources for learning to code included blogs, wikis, videos, online databases, subscription sites, and custom websites focused on coding skills. New commercial resources included YouTube videos, Lynda.com tutorials (later LinkedIn Learning), Khan Academy, Codecademy, GitHub, W3Schools, and numerous coding bootcamps.
Most software development systems and game engines included rich online help resources, including integrated development environments (IDEs), context-sensitive help, APIs, and other digital resources. Commercial software development kits (SDKs) also provided a collection of software development tools and documentation in one installable package.
Commercial and non-profit organizations published learning websites for developers, created blogs, and established newsfeeds and social media resources about programming. Corporations like Apple, Microsoft, Oracle, Google, and Amazon built corporate websites providing support for programmers, including resources like the Microsoft Developer Network (MSDN). Contemporary movements like Hour of Code (Code.org) show how learning to program has become associated with digital learning strategies, education agendas, and corporate philanthropy.
Although programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language.[33][34]
R is a programming language for statistical computing and data visualization. It has been adopted in the fields of data mining, bioinformatics and data analysis.[9]
R software is open-source and free software. It is licensed by the GNU Project and available under the GNU General Public License.[3] It is written primarily in C, Fortran, and R itself. Precompiled executables are provided for various operating systems.
R was started by professors Ross Ihaka and Robert Gentleman as a programming language to teach introductory statistics at the University of Auckland.[10] The language was inspired by the S programming language, with most S programs able to run unaltered in R.[6] The language was also inspired by Scheme's lexical scoping, allowing for local variables.[1]
Base packages are immediately available when starting R and provide the necessary syntax and commands for programming, computing, graphics production, basic arithmetic, and statistical functionality.[19]
The Task Views on the CRAN web site list packages in fields such as causal inference, finance, genetics, high-performance computing, machine learning, medical imaging, meta-analysis, social sciences, and spatial statistics.
General purpose programming languages that support R include Java via the Rserve socket server, and .NET C# (website).
The R Journal is an open access, academic journal which features short to medium-length articles on the use and development of R. It includes articles on packages, programming tips, CRAN news, and foundation news.
The R language has native support for object-oriented programming. There are two native frameworks, the so-called S3 and S4 systems. The former, being more informal, supports single dispatch on the first argument and objects are assigned to a class by just setting a "class" attribute in each object. The latter is a Common Lisp Object System (CLOS)-like system of formal classes (also derived from S) and generic methods that supports multiple dispatch and multiple inheritance[41]
Forth is a stack-oriented programming language and interactive integrated development environment designed by Charles H. "Chuck" Moore and first used by other programmers in 1970. Although not an acronym, the language's name in its early years was often spelled in all capital letters as FORTH. The FORTH-79 and FORTH-83 implementations, which were not written by Moore, became de facto standards, and an official technical standard of the language was published in 1994 as ANS Forth. A wide range of Forth derivatives existed before and after ANS Forth. The free and open-source software Gforth implementation is actively maintained, as are several commercially supported systems.
Forth has often been used to bring up new hardware. Forth was the first resident software on the new Intel 8086 chip in 1978, and MacFORTH was the first resident development system for the Macintosh 128K in 1984.[6]
Ashton-Tate's RapidFile (1986), a flat-file database program, and VP-Planner[12] from Paperback Software International (1983), a spreadsheet program competing with Lotus 1-2-3, were written in Forth.
The Forth programming language was used in the production of Star Wars: Episode IV – A New Hope.[citation needed] Specifically, it was used to control the Dykstraflex motion control system, which was created by John Dykstra and his team at Industrial Light & Magic. This system was instrumental in creating the complex and precise camera movements required for the space battle scenes and the trench run sequences on the Death Star.
The Dykstraflex system allowed for the pre-programming of camera movements, which could then be repeated exactly for multiple takes. This was crucial for combining live-action footage with model shots and other visual effects. Forth was chosen for its efficiency and its ability to handle real-time processing required for these intricate tasks.
Forth evolved from Charles H. Moore's personal programming system, which had been in continuous development since 1968.[6][14] Forth was first exposed to other programmers in the early 1970s, starting with Elizabeth Rather at the United States National Radio Astronomy Observatory (NRAO).[6] After their work at NRAO, Charles Moore and Elizabeth Rather formed FORTH, Inc. in 1973, refining and porting Forth systems to dozens of other platforms in the next decade.
Moore saw Forth as a successor to compile-link-go third-generation programming languages, or software for "fourth generation" hardware. He recalls how the name was coined:[15]
Forth was popular in the early 1980s,[17] because it was well suited to the limited memory of microcomputers. The ease of implementing the language led to many implementations.[18] The Jupiter ACE home computer has Forth in its ROM-resident operating system. Insoft GraFORTH is a version of Forth with graphics extensions for the Apple II.[19]
The FLOOR5 word is equivalent to this function written in the C programming language using the conditional operator '?:'
Forth's grammar has no official specification. Instead, it is defined by a simple algorithm. The interpreter reads a line of input from the user input device, which is then parsed for a word using spaces as a delimiter; some systems recognise additional whitespace characters. When the interpreter finds a word, it looks the word up in the dictionary. If the word is found, the interpreter executes the code associated with the word, and then returns to parse the rest of the input stream. If the word isn't found, the word is assumed to be a number and an attempt is made to convert it into a number and push it on the stack; if successful, the interpreter continues parsing the input stream. Otherwise, if both the lookup and the number conversion fail, the interpreter prints the word followed by an error message indicating that the word is not recognised, flushes the input stream, and waits for new user input.[27]
Most Forth systems run under a host operating system such as Microsoft Windows, Linux or a version of Unix and use the host operating system's file system for source and data files; the ANSI Forth Standard describes the words used for I/O. All modern Forth systems use normal text files for source, even if they are embedded. An embedded system with a resident compiler gets its source via a serial line.
Classic Forth systems traditionally use neither operating system nor file system. Instead of storing code in files, source code is stored in disk blocks written to physical disk addresses. The word BLOCK is employed to translate the number of a 1K-sized block of disk space into the address of a buffer containing the data, which is managed automatically by the Forth system. Block use has become rare since the mid-1990s. In a hosted system those blocks too are allocated in a normal file in any case.
Other non-standard facilities include a mechanism for issuing calls to the host OS or windowing systems, and many provide extensions that employ the scheduling provided by the operating system. Typically they have a larger and different set of words from the stand-alone Forth's PAUSE word for task creation, suspension, destruction and modification of priority.
The basic data structure of Forth is the "dictionary" which maps "words" to executable code or named data structures. The dictionary is laid out in memory as a tree of linked lists with the links proceeding from the latest (most recently) defined word to the oldest, until a sentinel value, usually a NULL pointer, is found. A context switch causes a list search to start at a different leaf. A linked list search continues as the branch merges into the main trunk leading eventually back to the sentinel, the root.
There can be several dictionaries. In rare cases such as meta-compilation a dictionary might be isolated and stand-alone.
The effect resembles that of nesting namespaces and can overload keywords depending on the context.
In ANS  Forth, unnamed words can be defined with the word :NONAME which compiles the following words up to the next ; (semi-colon) and leaves an execution token on the data stack. The execution token provides an opaque handle for the compiled semantics, similar to the function pointers of the C programming language.
Data objects defined by these and similar words are global in scope. The function provided by local variables in other languages is provided by the data stack in Forth (although Forth also has real local variables). Forth programming style uses very few named data objects compared with other languages; typically such data objects are used to contain data which is used by a number of words or tasks (in a multitasked implementation).[38]
The word CR comes before the text to print. By convention, the Forth interpreter does not start output on a new line. Also by convention, the interpreter waits for input at the end of the previous line, after an ok prompt. There is no implied "flush-buffer" action in Forth's CR, as sometimes is in other programming languages.
Racket is a general-purpose, multi-paradigm programming language. The Racket language is a modern dialect of Lisp and a descendant of Scheme. It is designed as a platform for programming language design and implementation.[9] In addition to the core Racket language, Racket is also used to refer to the family of programming languages[10] and set of tools supporting development on and with Racket.[11] Racket is also used for scripting, computer science education, and research.
The Racket platform provides an implementation of the Racket language (including a runtime system,[12] libraries, and compiler supporting several compilation modes: machine code, machine-independent, interpreted, and JIT) along with the DrRacket integrated development environment (IDE) written in Racket.[13] Racket is used by the ProgramByDesign outreach program, which aims to turn computer science into "an indispensable part of the liberal arts curriculum".[14][15]
The platform distribution is free and open-source software distributed under the Apache 2.0 and MIT licenses.[20] Extensions and packages written by the community may be uploaded to Racket's package catalog.
Matthias Felleisen founded PLT Inc. in the mid 1990s, first as a research group, soon after as a project dedicated to producing pedagogic materials for novice programmers (lectures, exercises/projects, software).  In January 1995, the group decided to develop a pedagogic programming environment based on Scheme.  Matthew Flatt cobbled together MrEd, the original virtual machine for Racket, from libscheme,[21] wxWidgets, and a few other free systems.[22] In the years that followed, a team including Flatt, Robby Findler, Shriram Krishnamurthi, Cormac Flanagan, and many others produced DrScheme, a programming environment for novice Scheme programmers and a research environment for gradual typing.[13]  The main development language that DrScheme supported was named PLT Scheme.
In parallel, the team began conducting workshops for high school teachers, training them in program design and functional programming. Field tests with these teachers and their students provided essential clues for directing the development.
The first generation of PLT Scheme revisions introduced features for programming in the large with both modules and classes. Version 42 introduced units – a first-class module system – to complement classes for large scale development.[24] The class system gained features (e.g. Java-style interfaces) and also lost several features (e.g. multiple inheritance) throughout these versions.[16]  The language evolved throughout a number of successive versions, and gaining milestone popularity in Version 53, leading to extensive work and the following Version 100, which would be equivalent to a "1.0" release in current popular version systems.
On 7 June 2010, PLT Scheme was renamed Racket.[27] The renaming coincided with the release of Version 5.0. Subsequently, the graphical user interface (GUI) backend was rewritten in Racket from C++ in Version 5.1 using native UI toolkits on all platforms.[22] Version 5.2 included a background syntax checking tool, a new plotting library, a database library, and a new extended REPL.[28] Version 5.3 included a new submodule feature for optionally loaded modules,[29] new optimization tools, a JSON library, and other features.[30] Version 5.3.1 introduced major improvements to DrRacket: the background syntax checker was turned on by default and a new documentation preview tool was added.[31]
Racket's core language includes macros, modules, lexical closures, tail calls, delimited continuations,[38] parameters (fluid variables), software contracts,[39] green threads and OS threads,[40][41][42] and more. The language also comes with primitives, such as eventspaces and custodians, which control resource management and enables the language to act like an operating system for loading and managing other programs.[12] Further extensions to the language are created with the powerful macro system, which together with the module system and custom parsers can control all aspects of a language.[43] Most language constructs in Racket are implemented as macros in the base language. These include a mixin class system,[16] a component (or module) system as expressive as opaque ascription in the ML module system,[17] and pattern matching.
Further, the language features the first contract system for a higher-order programming language.[44]
Racket's contract system is inspired by the Design by Contract work for Eiffel and extends it to work for higher-order values such as first-class functions, objects, reference cells, and so on. For example, an object that is checked by a contract can be ensured to make contract checks when its methods are eventually invoked.
The module-level extensibility features are combined with a Scheme-like hygienic macro system, which provides more features than Lisp's s-expression manipulation system,[47][48] Scheme 84's hygienic extend-syntax macros, or R5RS's syntax-rules. Indeed, it is fair to say that the macro system is a carefully tuned application programming interface (API) for compiler extensions. Using this compiler API, programmers can add features and entire domain-specific languages in a manner that makes them completely indistinguishable from built-in language constructs.
The macro system in Racket has been used to construct entire language dialects. This includes Typed Racket, which is a gradually typed dialect of Racket that eases the migration from untyped to typed code,[49] Lazy Racket—a dialect with lazy evaluation,[50] and Hackett, which combines Haskell and Racket.[51] The pedagogical programming language Pyret was originally implemented in Racket.[52][53]
Other dialects include FrTime (functional reactive programming), Scribble (documentation language),[54] Slideshow (presentation language),[55] and several languages for education.[56][57]
Racket's core distribution provides libraries to aid the development of programming languages.[18] Such languages are not restricted to s-expression based syntax. In addition to conventional readtable-based syntax extensions, the directive #lang enables the invocation of arbitrary parsers, which can be implemented using the parser tools library.[58] See Racket logic programming for an example of such a language.
DrRacket (formerly DrScheme) is widely used among introductory computer science courses that teach Scheme or Racket and is lauded for its simplicity and appeal to beginner programmers. The IDE was originally built for use with the TeachScheme! project (now ProgramByDesign), an outreach effort by Northeastern University and a number of affiliated universities for attracting high school students to computer science courses at the college level.
The editor provides highlighting for syntax and run-time errors, parenthesis matching, a debugger and an algebraic stepper. Its student-friendly features include support for multiple "language levels" (Beginning Student, Intermediate Student and so on). It also has integrated library support, and sophisticated analysis tools for advanced programmers. Further, module-oriented programming is supported with the module browser, a contour view, integrated testing and coverage measurements, and refactoring support. It provides integrated, context-sensitive access to an extensive hyper-linked help system named "Help Desk".
Apart from having a basis in programming language theory, Racket was designed as a general-purpose language for production systems. Thus, the Racket distribution features an extensive library that covers systems and network programming, web development,[59] a uniform interface to the underlying operating system, a dynamic foreign function interface,[63] several flavours of regular expressions, lexer/parser generators,[58] logic programming, and a complete GUI framework.
Agent-oriented programming allows the developer to build, extend and use software agents, which are abstractions of objects that can message other agents.
Array programming (also termed vector or multidimensional) languages generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays.
Aspect-oriented programming enables developers to add new functionality to code, known as "advice", without modifying that code itself; rather, it uses a pointcut to implement the advice into code blocks.
An authoring language is a programming language designed for use by a non-computer expert to easily create tutorials, websites, and other interactive computer programs.
A concatenative programming language is a point-free computer programming language in which all expressions denote functions, and the juxtaposition of expressions denotes function composition.[4] Concatenative programming replaces function application, which is common in other programming styles, with function composition as the default way to build subroutines.
A constraint programming language is a declarative programming language where relationships between variables are expressed as constraints. Execution proceeds by attempting to find values for the variables which satisfy all declared constraints.
Dataflow programming languages rely on a (usually visual) representation of the flow of data to specify the program.  Frequently used for reacting to discrete events or for processing streams of data.  Examples of dataflow languages include:
Declarative languages express the logic of a computation without describing its control flow in detail. Declarative programming stands in contrast to imperative programming via imperative programming languages, where control flow is specified by serial orders (imperatives). (Pure) functional and logic-based programming languages are also declarative, and constitute the major subcategories of the declarative category. This section lists additional examples not in those subcategories.
An esoteric programming language is a programming language designed as a test of the boundaries of computer programming language design, as a proof of concept, or as a joke.
Extension programming languages are languages embedded into another program and used to harness its features in extension scripts.
Fourth-generation programming languages are high-level programming languages built around database systems. They are generally used in commercial environments.
Functional programming languages define programs and subroutines as mathematical functions and treat them as first-class. Many so-called functional languages are "impure", containing imperative features. Many functional languages are tied to mathematical calculation tools. Functional languages include:
Imperative programming languages may be multi-paradigm and appear in other classifications. Here is a list of programming languages that follow the imperative paradigm:
Interpreted languages are programming languages in which programs may be executed from source code form, by an interpreter. Theoretically, any language can be compiled or interpreted, so the term interpreted language generally refers to languages that are usually interpreted rather than compiled.
Some programming languages without the inherent ability to manually manage memory, like Cython,[27] Swift,[c] and Scala[28] (Scala Native only), are able to import or call functions like malloc and free from C through a foreign function interface.
List-based languages are a type of data-structured language that are based on the list data structure.
Metaprogramming is the writing of programs that write or manipulate other programs, including themselves, as their data or that do part of the work that is otherwise done at run time during compile time. In many cases, this allows programmers to get more done in the same amount of time as they would take to write all the code manually.
Multiparadigm languages support more than one programming paradigm. They allow a program to use more than one programming style. The goal is to allow programmers to use the best tool for a job, admitting that no one paradigm solves all problems in the easiest or most efficient way.
Several general-purpose programming languages, such as C and Python, are also used for technical computing, this list focuses on languages almost exclusively used for technical computing.
Class-based object-oriented programming languages support objects defined by their class. Class definitions include member data. Message passing is a key concept, if not the main concept, in object-oriented languages.
Procedural programming languages are based on the concept of the unit and scope (the data viewing range) of an executable code statement. A procedural program is composed of one or more units or modules, either user coded or provided in a code library; each module is composed of one or more procedures, also called a function, routine, subroutine, or method, depending on the language. Examples of procedural languages include:
Reflective programming languages let programs examine and possibly modify their high-level structure at runtime or compile-time. This is most common in high-level virtual machine programming languages like Smalltalk, and less common in lower-level programming languages like C. Languages and platforms supporting reflection:
Stack-based languages are a type of data-structured language that are based on the stack data structure.
Synchronous programming languages are optimized for programming reactive systems, systems that are often interrupted and must respond quickly. Many such systems are also called realtime systems, and are used often in embedded systems.
A shading language is a graphics programming language adapted to programming shader effects. Such language forms usually consist of special data types, like "color" and "normal". Due to the variety of target markets for 3D computer graphics.
They provide both higher hardware abstraction and a more flexible programming model than previous paradigms which hardcoded transformation and shading equations. This gives the programmer greater control over the rendering process and delivers richer content at lower overhead.
The system programming languages are for low-level tasks like memory management or task management. A system programming language usually refers to a programming language used for system programming; such languages are designed for writing system software, which usually requires different development approaches when compared with application software.
System software is computer software designed to operate and control the computer hardware, and to provide a platform for running application software. System software includes software categories such as operating systems, utility software, device drivers, compilers, and linkers. Examples of system languages include:
Visual programming languages let users specify programs in a two-(or more)-dimensional way, instead of as one-dimensional text strings, via graphic layouts of various types. Some dataflow programming languages are also visual languages.
In computer science, imperative programming is a programming paradigm of software that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates step by step (generally order of the steps being determined in source code by the placement of statements one below the other),[1] rather than on high-level descriptions of its expected results.
The term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying all the details of how the program should achieve the result.[2]
Procedural programming is a type of imperative programming in which the program is built from one or more procedures (also termed subroutines or functions). The terms are often used as synonyms, but the use of procedures has a dramatic effect on how imperative programs appear and how they are constructed. Heavy procedural programming, in which state changes are localized to procedures or restricted to explicit arguments and returns from procedures, is a form of structured programming. Since the 1960s, structured programming and modular programming in general have been promoted as techniques to improve the maintainability and overall quality of imperative programs. The concepts behind object-oriented programming attempt to extend this approach.
Procedural programming could be considered a step toward declarative programming. A programmer can often tell, simply by looking at the names, arguments, and return types of procedures (and related comments), what a particular procedure is supposed to do, without necessarily looking at the details of how it achieves its result. At the same time, a complete program is still imperative since it fixes the statements to be executed and their order of execution to a large extent.
The programming paradigm used to build programs for almost all computers typically follows an imperative model.[note 1] Digital computer hardware is designed to execute machine code, which is native to the computer and is usually written in the imperative style, although low-level compilers and interpreters using other paradigms exist for some architectures such as lisp machines.
From this low-level perspective, the program state is defined by the contents of memory, and the statements are instructions in the native machine language of the computer. Higher-level imperative languages use variables and more complex statements, but still follow the same paradigm. Recipes and process checklists, while not computer programs, are also familiar concepts that are similar in style to imperative programming; each step is an instruction, and the physical world holds the state. Since the basic ideas of imperative programming are both conceptually familiar and directly embodied in the hardware, most computer languages are in the imperative style.
Early in the development of high-level programming languages, the introduction of the block enabled the construction of programs in which a group of statements and declarations could be treated as if they were one statement. This, alongside the introduction of subroutines, enabled complex structures to be expressed by hierarchical decomposition into simpler procedural structures.
Many imperative programming languages (such as Fortran, BASIC, and C) are abstractions of assembly language.[3]
The earliest imperative languages were the machine languages of the original computers. In these languages, instructions were very simple, which made hardware implementation easier but hindered the creation of complex programs. FORTRAN, developed by John Backus at International Business Machines (IBM) starting in 1954, was the first major programming language to remove the obstacles presented by machine code in the creation of complex programs. FORTRAN was a compiled language that allowed named variables, complex expressions, subprograms, and many other features now common in imperative languages. The next two decades saw the development of many other major high-level imperative programming languages. In the late 1950s and 1960s, ALGOL was developed in order to allow mathematical algorithms to be more easily expressed and even served as the operating system's target language for some computers. MUMPS (1966) carried the imperative paradigm to a logical extreme, by not having any statements at all, relying purely on commands, even to the extent of making the IF and ELSE commands independent of each other, connected only by an intrinsic variable named $TEST. COBOL (1960) and BASIC (1964) were both attempts to make programming syntax look more like English. In the 1970s, Pascal was developed by Niklaus Wirth, and C was created by Dennis Ritchie while he was working at Bell Laboratories. Wirth went on to design Modula-2 and Oberon. For the needs of the United States Department of Defense, Jean Ichbiah and a team at Honeywell began designing Ada in 1978, after a 4-year project to define the requirements for the language. The specification was first published in 1983, with revisions in 1995, 2005, and 2012.
The 1980s saw a rapid growth in interest in object-oriented programming. These languages were imperative in style, but added features to support objects. The last two decades of the 20th century saw the development of many such languages. Smalltalk-80, originally conceived by Alan Kay in 1969, was released in 1980, by the Xerox Palo Alto Research Center (PARC). Drawing from concepts in another object-oriented language—Simula (which is considered the world's first object-oriented programming language, developed in the 1960s)—Bjarne Stroustrup designed C++, an object-oriented language based on C. Design of C++ began in 1979 and the first implementation was completed in 1983. In the late 1980s and 1990s, the notable imperative languages drawing on object-oriented concepts were Perl, released by Larry Wall in 1987; Python, released by Guido van Rossum in 1990; Visual Basic and Visual C++ (which included Microsoft Foundation Class Library (MFC) 2.0), released by Microsoft in 1991 and 1993 respectively; PHP, released by Rasmus Lerdorf in 1994; Java, by James Gosling (Sun Microsystems) in 1995, JavaScript, by Brendan Eich (Netscape), and Ruby, by Yukihiro "Matz" Matsumoto, both released in 1995. Microsoft's .NET Framework (2002) is imperative at its core, as are its main target languages, VB.NET and C# that run on it; however Microsoft's F#, a functional language, also runs on it.
COBOL's development was tightly controlled, so dialects did not emerge to require ANSI standards. As a consequence, it was not changed for 15 years until 1974. The 1990s version did make consequential changes, like object-oriented programming.[6]
ALGOL (1960) stands for "ALGOrithmic Language." It had a profound influence on programming language design.[7] Emerging from a committee of European and American programming language experts, it used standard mathematical notation and had a readable structured design. Algol was first to define its syntax using the Backus–Naur form.[7] This led to syntax-directed compilers. It added features like:
Basic pioneered the interactive session.[8] It offered operating system commands within its environment:
C programming language (1973) got its name because the language BCPL was replaced with B, and AT&T Bell Labs called the next version "C." Its purpose was to write the UNIX operating system.[10] C is a relatively small language -- making it easy to write compilers. Its growth mirrored the hardware growth in the 1980s.[10] Its growth also was because it has the facilities of assembly language, but uses a high-level syntax. It added advanced features like:
In the 1970s, software engineers needed language support to break large projects down into modules.[18] One obvious feature was to decompose large projects physically into separate files. A less obvious feature was to decompose large projects logically into abstract datatypes.[18] At the time, languages supported concrete (scalar) datatypes like integer numbers, floating-point numbers, and strings of characters. Concrete datatypes have their representation as part of their name.[19] Abstract datatypes are structures of concrete datatypes — with a new name assigned. For example, a list of integers could be called integer_list.
Object-oriented imperative languages developed by combining the need for classes and the need for safe functional programming.[21] A function, in an object-oriented language, is assigned to a class. An assigned function is then referred to as a method, member function, or operation. Object-oriented programming is executing operations on objects.[22]
Object-oriented languages support a syntax to model subset/superset relationships. In set theory, an element of a subset inherits all the attributes contained in the superset. For example, a student is a person. Therefore, the set of students is a subset of the set of persons. As a result, students inherit all the attributes common to all persons. Additionally, students have unique attributes that other persons don't have. Object-oriented languages model subset/superset relationships using inheritance.[23] Object-oriented programming became the dominant language paradigm by the late 1990s.[18]
This article compares a large number of programming languages by tabulating their data types, their expression, statement, and declaration syntax, and some common operating-system interfaces.
The tables are not sorted lexicographically ascending by programming language name by default, and that some languages have entries in some tables but not others.
Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.
An educational programming language (EPL) is a programming language used primarily as a learning tool, and a starting point before transitioning to more complex programming languages.
Initially, machine code was the sole method of programming computers. Assembly language (ASM), introduced mnemonics to replace low-level instructions, making it one of the oldest programming languages still used today. Numerous dialects and implementations exist, each tailored to a specific computer processor architecture. Assembly languages are low-level and more challenging to use, as they are untyped and rigid. For educational purposes, simplified dialects of assembly languages have been developed to make coding more accessible to beginners.
As part of the One Laptop per Child project, a sequence of Smalltalk-based languages has been developed, each designed to act as an introduction to the next. The structure is Scratch to Etoys to Squeak to any Smalltalk.[6] Each provides graphical environments that may be used to teach not only programming concepts to kids but also physics and mathematics simulations, story-telling exercises, etc., through the use of constructive learning. Smalltalk and Squeak have fully featured application development languages that have been around and well-respected for decades; Scratch is a children's learning tool.
An esoteric programming language (sometimes shortened to esolang) is a programming language designed to test the boundaries of computer programming language design, as a proof of concept, as software art, as a hacking interface to another language (particularly functional programming or procedural programming languages), or as a joke. The use of the word esoteric distinguishes them from languages that working developers use to write software. The creators of most esolangs do not intend them to be used for mainstream programming, although some esoteric features, such as visuospatial syntax,[1] have inspired practical applications in the arts. Such languages are often popular among hackers and hobbyists.[citation needed]
Usability is rarely a goal for designers of esoteric programming languages; often their design leads to quite the opposite. Their usual aim is to remove or replace conventional language features while still maintaining a language that is Turing-complete, or even one for which the computational class is unknown.
The earliest, and still the canonical example of an esoteric programming language, is INTERCAL,[2] designed in 1972 by Don Woods and James M. Lyon, who said that their intention was to create a programming language unlike any with which they were familiar.[3][4] It parodied elements of established programming languages of the day such as Fortran, COBOL and assembly language.
In 1993, Wouter van Oortmerssen created FALSE, a small stack-oriented programming language with syntax designed to make the code inherently obfuscated, confusing and unreadable. Its compiler is only 1024 bytes in size.[5] This inspired Urban Müller to create an even smaller language, the now-infamous Brainfuck, which consists of only eight recognized characters. Along with Chris Pressey's Befunge (like FALSE, but with a two-dimensional instruction pointer), Brainfuck is now one of the best-supported esoteric programming languages, with canonical examples of minimal Turing tarpits and needlessly obfuscated language features. Brainfuck is related to the P′′ family of Turing machines.
While esoteric programming languages differ in many ways, there are some common traits that characterize many languages, such as parody, minimalism, and the goal of making programming difficult.[2] Many esoteric programming languages, such as brainfuck, and similar, use single characters as commands, however, it is not uncommon for languages to read line by line like conventional programming languages.
Conventional imperative programming languages typically allow data to be stored in variables, but esoteric languages may utilize different methods of storing and accessing data. Languages like Brainfuck and Malbolge only permit data to be read through a single pointer, which must be moved to a location of interest before data is read. Others, like Befunge and Shakespeare, utilize one or more stacks to hold data, leading to a manner of execution akin to Reverse Polish notation. Finally, there are languages which explore alternative forms of number representation: the Brainfuck variant Boolfuck only permits operations on single bits, while Malbolge and INTERCAL variant TriINTERCAL replace bits altogether with a base 3 ternary system.[6]
Many esoteric programming languages are designed to produce code that is deeply obfuscated, making it difficult to read and to write.[9] The purpose of this may be to provide an interesting puzzle or challenge for program writers: Malbolge for instance was explicitly designed to be challenging, and so it has features like self-modifying code and highly counterintuitive operations.[9] On the other hand, some esoteric languages become difficult to write due to their other design choices. Brainfuck is committed to the idea of a minimalist instruction set, so even though its instructions are straightforward in principle, the code that arises is difficult for a human to read. INTERCAL's difficulty arises as a result of the choice to avoid operations used in any other programming language, which stems from its origin as a parody of other languages.[9]
One of the aims of esoteric programming languages is to parody or spoof existing languages and trends in the field of programming.[9] For instance, the first esoteric language INTERCAL began as a spoof of languages used in the 1960s, such as APL, Fortran, and COBOL. INTERCAL's rules appear to be the inverse of rules in these other languages.[10] However, the subject of parody is not always another established programming language. Shakespeare can be viewed as spoofing the structure of Shakespearean plays, for instance. The language Ook! is a parody of Brainfuck, where Brainfuck's eight commands are replaced by various orangutan sounds like "Ook. Ook?"[6]
Binary lambda calculus is designed from an algorithmic information theory perspective to allow for the densest possible code with the most minimal means, featuring a 29-byte self interpreter, a 21-byte prime number sieve, and a 112-byte Brainfuck interpreter.[12]
Chef is a stack-oriented programming language created by David Morgan-Mar, designed to make programs look like cooking recipes.[18] Programs consist of a title, a list of variables and their data values, and a list of stack manipulation instructions.[19] A joking design principle states that "program recipes should not only generate valid output, but be easy to prepare and delicious", and Morgan-Mar notes that an example "Hello, World!" program with 101 eggs and 111 cups (~26,640 mL) oil would produce "a lot of food for one person."[19][20]
INTERCAL, short for "Compiler Language With No Pronounceable Acronym", was created in 1972 as a parody to satirize aspects of the various programming languages at the time.[4]
JSFuck is an esoteric programming style of JavaScript, where code is written using only six characters: [, ], (, ), !, and +. 
Unlike Brainfuck, which requires its own compiler or interpreter, JSFuck is valid JavaScript code, meaning JSFuck programs can be run in any web browser or engine that interprets JavaScript.[23][24] It has been used in a number of cross-site scripting (XSS) attacks on websites such as eBay due to its ability to evade cross-site scripting detection filters.[25]
While the semantics of LOLCODE is not unusual, its syntax has been described as a linguistic phenomenon, representing an unusual example of informal speech and internet slang in programming.[26]
Malbolge (named after the 8th circle of Hell) was designed to be the most difficult and esoteric programming language. Among other features, code is self-modifying by design and the effect of an instruction depends on its address in memory.[27]
Shakespeare Programming Language (SPL) is designed to make programs look like Shakespearean plays. For example, the following statement declares a point in the program which can be reached via a GOTO-type statement:[citation needed]
Unlambda is a minimalist functional programming language based on SKI calculus, but combined with first-class continuations and imperative I/O (with input usually requiring the use of continuations).[31]
The cultural context of esolangs has been studied by Geoff Cox, who writes that esolangs "shift attention from command and control toward cultural expression and refusal",[32] seeing esolangs as similar to code art and code poetry, such as Mez Breeze's mezangelle, a belief shared by others in field.[33] Daniel Temkin describes Brainfuck as "refusing to ease the boundary between human expression and assembly code and thereby taking us on a ludicrous journey of logic,"[34] exposing the inherent conflict between human thinking and computer logic by deconstructing their relationship. He connects programming within an esolang to performing an event score such as those of the Fluxus movement, where playing out the irregular rules of the logic in code makes the point of view of the language clear.[35]
In computer science, extensible programming is a style of computer programming that focuses on mechanisms to extend the programming language, compiler, and runtime system (environment). Extensible programming languages, supporting this style of programming, were an active area of work in the 1960s, but the movement was marginalized in the 1970s.[1] Extensible programming has become a topic of renewed interest in the 21st century.[2]
The first paper usually[1][3] associated with the extensible programming language movement is M. Douglas McIlroy's 1960 paper on macros for high-level programming languages.[4] Another early description of the principle of extensibility occurs in Brooker and Morris's 1960 paper on the compiler-compiler.[5] The peak of the movement was marked by two academic symposia, in 1969 and 1971.[6][7] By 1975, a survey article on the movement by Thomas A. Standish[1] was essentially a post mortem. The Forth was an exception, but it went essentially unnoticed.
any programming language in which programs and data are essentially interchangeable can be regarded as an extendible [sic] language. ... this can be seen very easily from the fact that Lisp has been used as an extendible language for years.[8]
Standish attributed the failure of the extensibility movement to the difficulty of programming successive extensions. A programmer might build a first shell of macros around a base language. Then, if a second shell of macros is built around that, any subsequent programmer must be intimately familiar with both the base language, and the first shell. A third shell would require familiarity with the base and both the first and second shells, and so on. Shielding a programmer from lower-level details is the intent of the abstraction movement that supplanted the extensibility movement.
Despite the earlier presentation of Simula as extensible, by 1975, Standish's survey does not seem in practice to have included the newer abstraction-based technologies (though he used a very general definition of extensibility that technically could have included them). A 1978 history of programming abstraction from the invention of the computer until then, made no mention of macros, and gave no hint that the extensible languages movement had ever occurred.[9] Macros were tentatively admitted into the abstraction movement by the late 1980s (perhaps due to the advent of hygienic macros), by being granted the pseudonym syntactic abstractions.[10]
In the modern sense, a system that supports extensible programming will provide all of the features described below[citation needed].
In extensible programming, a compiler is not a monolithic program that converts source code input into binary executable output. The compiler itself must be extensible to the point that it is really a collection of plugins that assist with the translation of source language input into anything. For example, an extensible compiler will support the generation of object code, code documentation, re-formatted source code, or any other desired output. The architecture of the compiler must permit its users to "get inside" the compilation process and provide alternative processing tasks at every reasonable step in the compilation process.
At runtime, extensible programming systems must permit languages to extend the set of operations that it permits. For example, if the system uses a byte-code interpreter, it must allow new byte-code values to be defined. As with extensible syntax, it is acceptable for there to be some (smallish) set of fundamental or intrinsic operations that are immutable. However, it must be possible to overload or augment those intrinsic operations so that new or additional behavior can be supported.
Extensible programming systems should regard programs as data to be processed. Those programs should be completely devoid of any kind of formatting information. The visual display and editing of programs to users should be a translation function, supported by the extensible compiler, that translates the program data into forms more suitable for viewing or editing. Naturally, this should be a two-way translation. This is important because it must be possible to easily process extensible programs in a variety of ways. It is unacceptable for the only uses of source language input to be editing, viewing and translation to machine code. The arbitrary processing of programs is facilitated by de-coupling the source input from specifications of how it should be processed (formatted, stored, displayed, edited, etc.).
Extensible programming systems must support the debugging of programs using the constructs of the original source language regardless of the extensions or transformation the program has undergone in order to make it executable. Most notably, it cannot be assumed that the only way to display runtime data is in structures or arrays. The debugger, or more correctly 'program inspector', must permit the display of runtime data in forms suitable to the source language. For example, if the language supports a data structure for a business process or work flow, it must be possible for the debugger to display that data structure as a fishbone chart or other form provided by a plugin.
Invariant-based programming[1] is a programming methodology where specifications and invariants are written before the actual program statements. Writing down the invariants during the programming process has a number of advantages: it requires the programmer to make their intentions about the program behavior explicit before actually implementing it, and invariants can be evaluated dynamically during execution to catch common programming errors. Furthermore, if strong enough, invariants can be used to prove the correctness of the program based on the formal semantics of program statements. A combined programming and specification language, connected to a powerful formal proof system, will generally be required for full verification of non-trivial programs. In this case a high degree of automation of proofs is also possible.
In most existing programming languages the main organizing structures are control flow blocks such as for loops, while loops and if statements. Such languages may not be ideal for invariants-first programming, since they force the programmer to make decisions about control flow before writing the invariants. Furthermore, most programming languages do not have good support for writing specifications and invariants, since they lack quantifier operators and one can typically not express higher order properties.
The following is list of researchers of programming language theory, design, implementation, and related areas.
One thing the most visited websites have in common is that they are dynamic websites. Their development typically involves server-side coding, client-side coding and database technology. The programming languages applied to deliver such dynamic web content vary vastly between sites.
Language-oriented programming (LOP)[1] is a software-development paradigm where "language" is a software building block with the same status as objects, modules and components,[2] and rather than solving problems in general-purpose programming languages, the programmer creates one or more domain-specific languages (DSLs) for the problem first, and solves the problem in those languages. Language-oriented programming was first described in detail in Martin Ward's 1994 paper Language Oriented Programming.[1]
The concept of language-oriented programming takes the approach to capture requirements in the user's terms, and then to try to create an implementation language as isomorphic as possible to the user's descriptions, so that the mapping between requirements and implementation is as direct as possible. A measure of the closeness of this isomorphism is the "redundancy" of the language, defined as the number of editing operations needed to implement a stand-alone change in requirements. It is not assumed a-priori what is the best language for implementing the new language. Rather, the developer can choose among options created by analysis of the information flows — what information is acquired, what its structure is, when it is acquired, from whom, and what is done with it.[3]
The Racket programming language and RascalMPL were designed to support language-oriented programming from the ground up.[2] Other language workbench[4] tools such as JetBrains MPS, Kermeta, or Xtext provide the tools to design and implement DSLs and language-oriented programming.[5]
Literate programming is a programming paradigm introduced in 1984 by Donald Knuth in which a computer program is given as an explanation of how it works in a natural language, such as English, interspersed (embedded) with snippets of macros and traditional source code, from which compilable source code can be generated.[1] The approach is used in scientific computing and in data science routinely for reproducible research and open access purposes.[2] Literate programming tools are used by millions of programmers today.[3]
The literate programming paradigm, as conceived by Donald Knuth, represents a move away from writing computer programs in the manner and order imposed by the compiler, and instead gives programmers macros to develop programs in the order demanded by the logic and flow of their thoughts.[4] Literate programs are written as an exposition of logic in more natural language in which macros are used to hide abstractions and traditional source code, more like the text of an essay.
Literate programming (LP) tools are used to obtain two representations from a source file: one understandable by a compiler or interpreter, the "tangled" code, and another for viewing as formatted documentation, which is said to be "woven" from the literate source.[5] While the first generation of literate programming tools were computer language-specific, the later ones are language-agnostic and exist beyond the individual programming languages.
Literate programming was first introduced in 1984 by Donald Knuth, who intended it to create programs that were suitable literature for human beings. He implemented it at Stanford University as a part of his research on algorithms and digital typography. The implementation was called "WEB" since he believed that it was one of the few three-letter words of English that had not yet been applied to computing.[6] However, it resembles the complicated nature of software delicately pieced together from simple materials.[1] The practice of literate programming has seen an important resurgence in the 2010s with the use of computational notebooks, especially in data science.
Literate programming is writing out the program logic in a human language with included (separated by a primitive markup) code snippets and macros. Macros in a literate source file are simply title-like or explanatory phrases in a human language that describe human abstractions created while solving the programming problem, and hiding chunks of code or lower-level macros. These macros are similar to the algorithms in pseudocode typically used in teaching computer science. These arbitrary explanatory phrases become precise new operators, created on the fly by the programmer, forming a meta-language on top of the underlying programming language.
A preprocessor is used to substitute arbitrary hierarchies, or rather "interconnected 'webs' of macros",[7] to produce the compilable source code with one command ("tangle"), and documentation with another ("weave"). The preprocessor also provides an ability to write out the content of the macros and to add to already created macros in any place in the text of the literate program source file, thereby disposing of the need to keep in mind the restrictions imposed by traditional programming languages or to interrupt the flow of thought.
According to Knuth,[8][9]
literate programming provides higher-quality programs, since it forces programmers to explicitly state the thoughts behind the program, making poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one's thoughts during a program's creation.[10] The resulting documentation allows the author to restart their own thought processes at any later time, and allows other programmers to understand the construction of the program more easily. This differs from traditional documentation, in which a programmer is presented with source code that follows a compiler-imposed order, and must decipher the thought process behind the program from the code and its associated comments. The meta-language capabilities of literate programming are also claimed to facilitate thinking, giving a higher "bird's eye view" of the code and increasing the number of concepts the mind can successfully retain and process. Applicability of the concept to programming on a large scale, that of commercial-grade programs, is proven by an edition of TeX code as a literate program.[8]
Knuth also claims that literate programming can lead to easy porting of software to multiple environments, and even cites the implementation of TeX as an example.[11]
Literate programming is very often misunderstood[12] to refer only to formatted documentation produced from a common file with both source code and comments – which is properly called documentation generation – or to voluminous commentaries included with code. This is the converse of  literate programming: well-documented code or documentation extracted from code follows the structure of the code, with documentation embedded in the code; while in literate programming, code is embedded in documentation, with the code following the structure of the documentation.
This misconception has led to claims that comment-extraction tools, such as the Perl Plain Old Documentation or Java Javadoc systems, are "literate programming tools". However, because these tools do not implement the "web of abstract concepts" hiding behind the system of natural-language macros, or provide an ability to change the order of the source code from a machine-imposed sequence to one convenient to the human mind, they cannot properly be called literate programming tools in the sense intended by Knuth.[12][13]
A classic example of literate programming is the literate implementation of the standard Unix wc word counting program. Knuth presented a CWEB version of this example in Chapter 12 of his Literate Programming book. The same example was later rewritten for the noweb literate programming tool.[14] This example provides a good illustration of the basic elements of literate programming.
The following snippet of the wc literate program[14] shows how arbitrary descriptive phrases in a natural language are used in a literate program to create macros, which act as new "operators" in the literate programming language, and hide chunks of code or other macros. The mark-up notation consists of double angle brackets (<<...>>) that indicate macros. The @ symbol which, in a noweb file, indicates the beginning of a documentation chunk. The <<*>> symbol stands for the "root", topmost node the literate programming tool will start expanding the web of macros from. Actually, writing out the expanded source code can be done from any section or subsection (i.e. a piece of code designated as <<name of the chunk>>=, with the equal sign), so one literate program file can contain several files with machine source code.
Macros are not the same as "section names" in standard documentation. Literate programming macros hide the real code behind themselves, and be used inside any low-level machine language operators, often inside logical operators such as if, while or case. This can be seen in the following wc literate program.[14]
The first published literate programming environment was WEB, introduced by Knuth in 1981 for his TeX typesetting system; it uses Pascal as its underlying programming language and TeX for typesetting of the documentation. The complete commented TeX source code was published in Knuth's TeX: The program, volume B of his 5-volume Computers and Typesetting. Knuth had privately used a literate programming system called DOC as early as 1979. He was inspired by the ideas of Pierre-Arnoul de Marneffe.[16] The free CWEB, written by Knuth and Silvio Levy, is WEB adapted for C and C++, runs on most operating systems, and can produce TeX and PDF documentation.
There are various other implementations of the literate programming concept as given below. Many of the newer among these do not have macros and hence do not comply with the order of human logic principle, which makes them perhaps "semi-literate" tools. These, however, allow cellular execution of code which makes them more along the lines of exploratory programming tools.
I had the feeling that top-down and bottom-up were opposing methodologies: one more suitable for program exposition and the other more suitable for program creation. But after gaining experience with WEB, I have come to realize that there is no need to choose once and for all between top-down and bottom-up, because a program is best thought of as a web instead of a tree. A hierarchical structure is present, but the most important thing about a program is its structural relationships. A complex piece of software consists of simple parts and simple relations between those parts; the programmer's task is to state those parts and those relationships, in whatever order is best for human comprehension not in some rigidly determined order like top-down or bottom-up.
Another surprising thing that I learned while using WEB was that traditional programming languages had been causing me to write inferior programs, although I hadn't realized what I was doing. My original idea was that WEB would be merely a tool for documentation, but I actually found that my WEB programs were better than the programs I had been writing in other languages.
I chose the name WEB partly because it was one of the few three-letter words of English that hadn't already been applied to computers. But as time went on, I've become extremely pleased with the name, because I think that a complex piece of software is, indeed, best regarded as a web that has been delicately pieced together from simple materials. We understand a complicated system by understanding its simple parts, and by understanding the simple relations between those parts and their immediate neighbors. If we express a program as a web of ideas, we can emphasize its structural properties in a natural and satisfying way.
Metaprogramming is a computer programming technique in which computer programs have the ability to treat other programs as their data. It means that a program can be designed to read, generate, analyse, or transform other programs, and even modify itself, while running.[1][2] In some cases, this allows programmers to minimize the number of lines of code to express a solution, in turn reducing development time.[3] It also allows programs more flexibility to efficiently handle new situations with no recompiling.
Metaprogramming can be used to move computations from runtime to compile time, to generate code using compile time computations, and to enable self-modifying code. The ability of a programming language to be its own metalanguage allows reflective programming, and is termed reflection.[4] Reflection is a valuable language feature to facilitate metaprogramming.
Metaprogramming was popular in the 1970s and 1980s using list processing languages such as Lisp. Lisp machine hardware gained some notice in the 1980s, and enabled applications that could process code. They were often used for artificial intelligence applications.
Metaprogramming enables developers to write programs and develop code that falls under the generic programming paradigm. Having the programming language itself as a first-class data type (as in Lisp, Prolog, SNOBOL, or Rebol) is also very useful; this is known as homoiconicity. Generic programming invokes a metaprogramming facility within a language by allowing one to write code without the concern of specifying data types since they can be supplied as parameters when used.
Lisp is probably the quintessential language with metaprogramming facilities, both because of its historical precedence and because of the simplicity and power of its metaprogramming. In Lisp metaprogramming, the unquote operator (typically a comma) introduces code that is evaluated at program definition time rather than at run time. The metaprogramming language is thus identical to the host programming language, and existing Lisp routines can be directly reused for metaprogramming if desired. This approach has been implemented in other languages by incorporating an interpreter in the program, which works directly with the program's data. There are implementations of this kind for some common high-level languages, such as RemObjects’ Pascal Script for Object Pascal.
A simple example of a metaprogram is this POSIX Shell script, which is an example of generative programming:
Not all metaprogramming involves generative programming. If programs are modifiable at runtime, or if incremental compiling is available (such as in C#, Forth, Frink, Groovy, JavaScript, Lisp, Elixir, Lua, Nim, Perl, PHP, Python, Rebol, Ruby, Rust, R, SAS, Smalltalk, and Tcl), then techniques can be used to perform metaprogramming without generating source code.
One style of generative approach is to employ domain-specific languages (DSLs). A fairly common example of using DSLs involves generative metaprogramming: lex and yacc, two tools used to generate lexical analysers and parsers, let the user describe the language using regular expressions and context-free grammars, and embed the complex algorithms required to efficiently parse the language.
Some argue that there is a sharp learning curve to make complete use of metaprogramming features.[8] Since metaprogramming gives more flexibility and configurability at runtime, misuse or incorrect use of metaprogramming can result in unwarranted and unexpected errors that can be extremely difficult to debug to an average developer. It can introduce risks in the system and make it more vulnerable if not used with care. Some of the common problems, which can occur due to wrong use of metaprogramming are inability of the compiler to identify missing configuration parameters, invalid or incorrect data can result in unknown exception or different results.[9] Due to this, some believe[8] that only high-skilled developers should work on developing features which exercise metaprogramming in a language or platform and average developers must learn how to use these features as part of convention.
The IBM/360 and derivatives had powerful macro assembler facilities that were often used to generate complete assembly language programs [citation needed] or sections of programs (for different operating systems for instance). Macros provided with CICS transaction processing system had assembler macros that generated COBOL statements as a pre-processing step.
Use of dependent types allows proving that generated code is never invalid.[15] However, this approach is leading-edge and rarely found outside of research programming languages.
Ruby is an interpreted, high-level, general-purpose programming language. It was designed with an emphasis on programming productivity and simplicity. In Ruby, everything is an object, including primitive data types. It was developed in the mid-1990s by Yukihiro "Matz" Matsumoto in Japan.
Ruby is dynamically typed and uses garbage collection and just-in-time compilation. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. According to the creator, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, BASIC, and Lisp.[10][3]
By 2000, Ruby was more popular than Python in Japan.[20] In September 2000, the first English language book Programming Ruby was printed, which was later freely released to the public, further widening the adoption of Ruby amongst English speakers. In early 2002, the English-language ruby-talk mailing list was receiving more messages than the Japanese-language ruby-list, demonstrating Ruby's increasing popularity in the non-Japanese speaking world.
Ruby 3.2 was released on December 25, 2022.[44] It brings support for being run inside of a WebAssembly environment via a WASI interface. Regular expressions also receives some improvements, including a faster, memoized matching algorithm to protect against certain ReDoS attacks, and configurable timeouts for regular expression matching. Additional debugging and syntax features are also included in this release, which include syntax suggestion, as well as error highlighting. The MJIT compiler has been re-implemented as a standard library module, while the YJIT, a Rust-based JIT compiler now supports more architectures on Linux.
Matsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design.[45] At a Google Tech Talk in 2008 he said, "I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language."[46] He stresses that systems design needs to emphasize human, rather than computer, needs:[47]
Often people, especially computer engineers, focus on the machines. They think, "By doing this, the machine will run fast. By doing this, the machine will run more effectively. By doing this, the machine will something something something." They are focusing on machines. But in fact we need to focus on humans, on how humans care about doing programming or operating the application of the machines. We are the masters. They are the slaves.
Everyone has an individual background. Someone may come from Python, someone else may come from Perl, and they may be surprised by different aspects of the language. Then they come up to me and say, 'I was surprised by this feature of the language, so Ruby violates the principle of least surprise.' Wait. Wait. The principle of least surprise is not for you only. The principle of least surprise means principle of least my surprise. And it means the principle of least surprise after you learn Ruby very well. For example, I was a C++ programmer before I started designing Ruby. I programmed in C++ exclusively for two or three years. And after two years of C++ programming, it still surprises me.
Ruby has been described as a multi-paradigm programming language: it allows procedural programming (defining functions/variables outside classes makes them part of the root, 'self' Object), with object orientation (everything is an object) or functional programming (it has anonymous functions, closures, and continuations; statements all have values, and functions return the last evaluation). It has support for introspection, reflective programming, metaprogramming, and interpreter-based threads. Ruby features dynamic typing, and supports parametric polymorphism.
One of the differences from Python and Perl is that Ruby keeps all of its instance variables completely private to the class and only exposes them through accessor methods (attr_writer, attr_reader, etc.). Unlike the "getter" and "setter" methods of other languages like C++ or Java, accessor methods in Ruby can be created with a single line of code via metaprogramming; however, accessor methods can also be created in the traditional fashion of C++ and Java. As invocation of these methods does not require the use of parentheses, it is trivial to change an instance variable into a full function, without modifying a single line of calling code or having to do any refactoring achieving similar functionality to C# and VB.NET property members.
Matsumoto originally developed Ruby on the 4.3BSD-based Sony NEWS-OS 3.x, but later migrated his work to SunOS 4.x, and finally to Linux.[61][62] By 1999, Ruby was known to work across many different operating systems. Modern Ruby versions and implementations are available on all major desktop, mobile and server-based operating systems. Ruby is also supported across a number of cloud hosting platforms like Jelastic, Heroku, Google Cloud Platform and others.
The Ruby Application Archive, which hosted applications, documentation, and libraries for Ruby programming, was maintained until 2013, when its function was transferred to RubyGems.[65]
In computer science, reflective programming or reflection is the ability of a process to examine, introspect, and modify its own structure and behavior.[1]
The earliest computers were programmed in their native assembly languages, which were inherently reflective, as these original architectures could be programmed by defining instructions as data and using self-modifying code. As the bulk of programming moved to higher-level compiled languages such as Algol, Cobol, Fortran, Pascal, and C, this reflective ability largely disappeared until new programming languages with reflection built into their type systems appeared.[citation needed]
Brian Cantwell Smith's 1982 doctoral dissertation introduced the notion of computational reflection in procedural programming languages and the notion of the meta-circular interpreter as a component of 3-Lisp.[2][3]
Reflection helps programmers make generic software libraries to display data, process different formats of data, perform serialization and deserialization of data for communication, or do bundling and unbundling of data for containers or bursts of communication.
Effective use of reflection almost always requires a plan: A design framework, encoding description, object library, a map of a database or entity relations.
In object-oriented programming languages such as Java, reflection allows inspection of classes, interfaces, fields and methods at runtime without knowing the names of the interfaces, fields, methods at compile time. It also allows instantiation of new objects and invocation of methods.
Reflection is often used as part of software testing, such as for the runtime creation/instantiation of mock objects.
In some object-oriented programming languages such as C# and Java, reflection can be used to bypass member accessibility rules. For C#-properties this can be achieved by writing directly onto the (usually invisible) backing field of a non-public property. It is also possible to find non-public methods of classes and types and manually invoke them. This works for project-internal files as well as external libraries such as .NET's assemblies and Java's archives.
These features can be implemented in different ways. In MOO, reflection forms a natural part of everyday programming idiom. When verbs (methods) are called, various variables such as verb (the name of the verb being called) and this (the object on which the verb is called) are populated to give the context of the call. Security is typically managed by accessing the caller stack programmatically: Since callers() is a list of the methods by which the current verb was eventually called, performing tests on callers()[0] (the command invoked by the original user) allows the verb to protect itself against unauthorised use.
The following code snippets create an instance foo of class Foo and invoke its method PrintHello. For each programming language, normal and reflection-based call sequences are shown.
In computer programming, a scientific programming language can refer to two degrees of the same concept.
In a wide sense, a scientific programming language is a programming language that is used widely for computational science and computational mathematics. In this sense, C/C++ and Python can be considered scientific programming languages.
In a stronger sense, a scientific programming language is one that is designed and optimized for the use of mathematical formula and matrices.[1]
Such languages are characterized not only by the availability of libraries performing mathematical or scientific functions, but by the syntax of the language itself.[2] For example, neither C++ nor Python have built-in matrix types or functions for matrix arithmetic (addition, multiplication etc.); instead, this functionality is made available through standard libraries.  Scientific programming languages in the stronger sense include ALGOL, APL, Fortran, J, Julia, Maple, MATLAB, Octave, and R.[3][4]
Scientific programming languages should not be confused with scientific language in general, which refers loosely to the higher standards in precision, correctness and concision expected from practitioners of the scientific method.
Scientific programming languages provide facilities to work with linear algebra. For example, the following Julia program solves a system of linear equations:
In a scientific programming language, we can compute function optima with a syntax close to mathematical language. For instance, the following Julia code finds the minimum of the polynomial 



P
(
x
,
y
)
=

x

2


−
3
x
y
+
5

y

2


−
7
y
+
3


{\displaystyle P(x,y)=x^{2}-3xy+5y^{2}-7y+3}

.
In this example, Newton's method for minimizing is used. Modern scientific programming languages will use automatic differentiation to compute the gradients and Hessians of the function given as input; cf. differentiable programming. Here, automatic forward differentiation has been chosen for that task.
Older scientific programming languages such as the venerable Fortran would require the programmer to pass, next to the function to be optimized, a function that computes the gradient, and a function that computes the Hessian.
With more knowledge of the function to be minimized, more efficient algorithms can be used.  For instance, convex optimization provides faster computations when the function is convex, quadratic programming provides faster computations when the function is at most quadratic in its variables, and linear programming when the function is at most linear.
Software engineering – application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is the application of engineering to software.[1]
The ACM Computing Classification system is a poly-hierarchical ontology that organizes the topics of the field and can be used in semantic web applications and as a de facto standard classification system for the field.   The major section "Software and its Engineering" provides an outline and ontology for software engineering.
Software engineers build software (applications, operating systems, system software) that people use.
Applications influence software engineering by pressuring developers to solve problems in new ways. For example, consumer software emphasizes low cost, medical software emphasizes high quality, and Internet commerce software emphasizes rapid development.
A platform combines computer hardware and an operating system. As platforms grow more powerful and less costly, applications and tools grow more widely available.
Skilled software engineers know a lot of computer science including what is possible and impossible, and what is easy and hard for software.
Deliverables must be developed for many SE projects. Software engineers rarely make all of these deliverables themselves. They usually cooperate with the writers, trainers, installers, marketers, technical support people, and others who make many of these deliverables.
The history of programming languages spans from documentation of early mechanical computers to modern tools for software development. Early programming languages were highly specialized, relying on mathematical notation and similarly obscure syntax.[1] Throughout the 20th century, research in compiler theory led to the creation of high-level programming languages, which use a more accessible syntax to communicate instructions.
The first high-level programming language was Plankalkül, created by Konrad Zuse between 1942 and 1945.[2] The first high-level language to have an associated compiler was created by Corrado Böhm in 1951, for his PhD thesis.[3] The first commercially available language was FORTRAN (FORmula TRANslation), developed in 1956 (first manual appeared in 1956, but first developed in 1954) by a team led by John Backus at IBM.
In the 1940s, the first recognizably modern electrically powered computers were created. The limited speed and memory capacity forced programmers to write hand-tuned assembly language programs. It was eventually realized that programming in assembly language required a great deal of intellectual effort.[citation needed]
An early proposal for a high-level programming language was Plankalkül, developed by Konrad Zuse for his Z1 computer between 1942 and 1945 but not implemented at the time.[7]
The first functioning programming languages designed to communicate instructions to a computer were written in the early 1950s. John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer.[8] Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be interpreted into machine code every time it ran, making the process much slower than running the equivalent machine code.
In the early 1950s, Alick Glennie developed Autocode, possibly the first compiled programming language, at the University of Manchester. In 1954, a second iteration of the language, known as the "Mark 1 Autocode", was developed for the Mark 1 by R. A. Brooker. Brooker, with the University of Manchester, also developed an autocode for the Ferranti Mercury in the 1950s. The version for the EDSAC 2 was devised by Douglas Hartree of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances and was noted for its object code optimization and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.
Another early programming language was devised by Grace Hopper in the US, named FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English language programming language and implemented a prototype.[14] The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959.[15] Flow-Matic was a major influence in the design of COBOL, since only it and its direct descendant AIMACO were in use at the time.[16]
Other languages still in use today include LISP (1958), invented by John McCarthy and COBOL (1959), created by the Short Range Committee. Another milestone in the late 1950s was the publication, by a committee of American and European computer scientists, of "a new language for algorithms"; the ALGOL 60 Report (the "ALGOrithmic Language"). This report consolidated many ideas circulating at the time and featured three key language innovations:
The period from the late 1960s to the late 1970s brought a major flowering of programming languages. Most of the major language paradigms now in use were invented in this period:[original research?]
The 1960s and 1970s also saw considerable debate over the merits of "structured programming", which essentially meant programming without the use of goto. A significant fraction of programmers believed that, even in languages that provide goto, it is bad programming style to use it except in rare circumstances. This debate was closely related to language design: some languages had no goto, which forced the use of structured programming.
The 1980s were years of relative consolidation in imperative languages. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the prior decade. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating so-called fifth-generation programming languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Research in Miranda, a functional language with lazy evaluation, began to take hold in this decade.
One important new trend in language design was an increased focus on programming for large-scale systems through the use of modules, or large-scale organizational units of code. Modula, Ada, and ML all developed notable module systems in the 1980s. Module systems were often wedded to generic programming constructs: generics being, in essence, parametrized modules[citation needed] (see also Polymorphism (computer science)).
Although major new paradigms for imperative programming languages did not appear, many researchers expanded on the ideas of prior languages and adapted them to new contexts. For example, the languages of the Argus and Emerald systems adapted object-oriented programming to distributed computing systems.
The 1980s also brought advances in programming language implementation. The reduced instruction set computer (RISC) movement in computer architecture postulated that hardware should be designed for compilers rather than for human assembly programmers. Aided by central processing unit (CPU) speed improvements that enabled increasingly aggressive compiling methods, the RISC movement sparked greater interest in compiler technology for high-level languages.
The rapid growth of the Internet in the mid-1990s was the next major historic event in programming languages. By opening up a radically new platform for computer systems, the Internet created an opportunity for new languages to be adopted. In particular, the JavaScript programming language rose to popularity because of its early integration with the Netscape Navigator web browser. Various other scripting languages achieved widespread use in developing customized applications for web servers such as PHP. The 1990s saw no fundamental novelty in imperative languages, but much recombination and maturation of old ideas. This era began the spread of functional languages. A big driving philosophy was programmer productivity. Many rapid application development (RAD) languages emerged, which usually came with an integrated development environment (IDE), garbage collection, and were descendants of older languages. All such languages were object-oriented. These included Object Pascal, Objective Caml (renamed OCaml), Visual Basic, and Java. Java in particular received much attention.
Some programming languages included other languages in their distribution to save the development time. for example both of Python and Ruby included Tcl to support GUI programming through libraries like Tkinter.
Big Tech companies introduced multiple new programming languages that are designed to serve their needs. for example:
Many Big Tech companies continued introducing new programming languages that are designed to serve their needs and provides first-class support for their platforms. for example:
The development of new programming languages continues, and some new languages appears with focus on providing a replacement for current languages. These new languages try to provide the advantages of a known language like C++ (versatile and fast) while adding safety or reducing complexity. Other new languages try to bring ease of use as provided by Python while adding performance as a priority. Also, the growing of Machine Learning and AI tools still plays a big rule behind these languages' development, where some visual languages focus on integrating these AI tools while other textual languages focus on providing more suitable support for developing them. [22][23][24]
Essentials of Programming Languages (EOPL) is a textbook on programming languages by Daniel P. Friedman, Mitchell Wand, and Christopher T. Haynes.
EOPL surveys the principles of programming languages from an operational perspective. It starts with an interpreter in Scheme for a simple functional core language similar to the lambda calculus and then systematically adds constructs. For each addition, for example, variable assignment or thread-like control, the book illustrates an increase in expressive power of the programming language and a demand for new constructs for the formulation of a direct interpreter. The book also demonstrates that systematic transformations, say, store-passing style or continuation-passing style, can eliminate certain constructs from the language in which the interpreter is formulated.
Like SICP, EOPL represents a significant departure from the prevailing textbook approach in the 1980s. At the time, a book on the principles of programming languages presented four to six (or even more) programming languages and discussed their programming idioms and their implementation at a high level. The most successful books typically covered ALGOL 60 (and the so-called Algol family of programming languages), SNOBOL, Lisp, and Prolog. Even today, a fair number of textbooks on programming languages are just such surveys, though their scope has narrowed.
EOPL was started in 1983, when Indiana was one of the leading departments in programming languages research. Eugene Kohlbecker, one of Friedman's PhD students, transcribed and collected his "311 lectures". Other faculty members, including Mitch Wand and Christopher Haynes, started contributing and turned "The Hitchhiker's Guide to the Meta-Universe"—as Kohlbecker had called it—into the systematic, interpreter and transformation-based survey that it is now. Over the 25 years of its existence, the book has become a near-classic; it is now in its third edition, including additional topics such as types and modules. Its first part now incorporates ideas on programming from HtDP, another unconventional textbook,  which uses Scheme to teach the principles of program design. The authors, as well as Matthew Flatt, have recently provided DrRacket plug-ins and language levels for teaching with EOPL.
EOPL has spawned at least two other related texts: Queinnec's[2] Lisp in Small Pieces[3] and Krishnamurthi's Programming Languages: Application and Interpretation.
Types and Programming Languages, ISBN 0-262-16209-1, is a book by Benjamin C. Pierce on type systems published in 2002.
A review by Frank Pfenning called it "probably the single most important book in the area of programming languages in recent years."[1]
Concepts, Techniques, and Models of Computer Programming is a textbook published in 2004 about general computer programming concepts from MIT Press written by Université catholique de Louvain professor Peter Van Roy and Royal Institute of Technology, Sweden professor Seif Haridi.
Using a carefully selected progression of subsets of the Oz programming language, the book explains the most important programming concepts, techniques, and models (paradigms).
Delphi is a general-purpose programming language and a software product that uses the Delphi dialect of the Object Pascal programming language and provides an integrated development environment (IDE) for rapid application development of desktop, mobile, web, and console software,[3] currently developed and maintained by Embarcadero Technologies.
Delphi includes a code editor, a visual designer, an integrated debugger, a source code control component, and support for third-party plugins. The code editor features Code Insight (code completion), Error Insight (real-time error-checking), and refactoring. The visual forms designer has the option of using either the Visual Component Library (VCL) for pure Windows development or the FireMonkey (FMX) framework for cross-platform development. Database support is a key feature and is provided by FireDAC (Database Access Components). Delphi is known for its fast compilation speed, native code, and developer productivity.[citation needed]
Delphi was originally developed by Borland as a rapid application development tool for Windows as the successor of Turbo Pascal. Delphi added full object-oriented programming to the existing language, and the language has grown to support generics, anonymous methods, closures, and native Component Object Model (COM) support.
Delphi uses the Pascal-based programming language Object Pascal created by Anders Hejlsberg for Borland (now IDERA) as the successor to Turbo Pascal. It supports native cross-compilation to many platforms including Windows, Linux, iOS, and Android.
To better support development for Microsoft Windows and interoperate with code developed with other software development tools, Delphi supports independent interfaces of Component Object Model (COM) with reference counting class implementations, and support for many third-party components. Interface implementations can be delegated to fields or properties of classes. Message handlers are implemented by tagging a method of a class with the integer constant of the message to handle.[citation needed]
Database connectivity is extensively supported through VCL database-aware and database access components.
Delphi uses a strongly typed high-level programming language, intended to be easy to use and originally based on the earlier Object Pascal language. Pascal was originally developed as a general-purpose language "suitable for expressing the fundamental constructs known at the time in a concise and logical way", and "its implementation was to be efficient and competitive with existing FORTRAN compilers"[8] but without low-level programming facilities or access to hardware. Turbo Pascal and its descendants, including Delphi, support access to hardware and low-level programming, with the facility to incorporate code written in assembly language and other languages. Delphi's object-orientation features only class- and interface-based polymorphism.[9] Metaclasses are first class objects. Objects are references to the objects (as in Java), which Delphi implicitly de-references, so there is usually no need to manually allocate memory for pointers to objects or use similar techniques that some other languages need. There are dedicated reference-counted string types, and also null-terminated strings.
Delphi is the medium used to teach programming in South African schools as a subject of information technology (IT).[11]
The C Programming Language (sometimes termed K&R, after its authors' initials) is a computer programming book written by Brian Kernighan and Dennis Ritchie, the latter of whom originally designed and implemented the C programming language, as well as co-designed the Unix operating system with which development of the language was closely intertwined.  The book was central to the development and popularization of C and is still widely read and used today.  Because the book was co-authored by the original language designer, and because the first edition of the book served for many years as the de facto standard for the language, the book was regarded by many to be the authoritative reference on C.[1][2]
The first edition, published February 22, 1978, was the first widely available book on the C programming language. Its version of C is sometimes termed K&R C (after the book's authors), often to distinguish this early version from the later version of C standardized as ANSI C.[6]
C was first standardized in 1989 (as ANSI X3.159-1989) and has since undergone several revisions. However, no new edition of The C Programming Language has been issued to cover the more recent standards.
Byte magazine stated in August 1983, "[The C Programming Language] is the definitive work on the C language. Don't read any further until you have this book!"[1] Jerry Pournelle wrote in the magazine that year that the book "is still the standard ... a bit terse". He continued, "You can learn the C language without getting Kernighan and Ritchie, but that's doing it the hard way. You're also working too hard if you make it the only book on C that you buy."[9]
The C Programming Language has often been cited as a model for technical writing, with reviewers describing it as having clear presentation and concise treatment.  Examples generally consist of complete programs of the type one is likely to encounter in daily use of the language, with an emphasis on system programming.  Its authors wrote,
We have tried to retain the brevity of the first edition.  C is not a big language, and it is not well served by a big book.  We have improved the exposition of critical features, such as pointers, that are central to C programming.  We have refined the original examples, and have added new examples in several chapters.  For instance, the treatment of complicated declarations is augmented by programs that convert declarations into words and vice versa.  As before, all examples have been tested directly from the text, which is in machine-readable form.
The book introduced the "Hello, World!" program, which prints only the text "hello, world" as an illustration of a minimal working C program.  Since then, many texts have followed that convention for introducing a programming language.
The influence of The C Programming Language on programmers, a generation of whom first worked with C in universities and industry, has led many to accept the authors' programming style and conventions as recommended practice, if not normative practice.  For example, the coding and formatting style of the programs presented in both editions of the book is often referred to as "K&R style" or the "One True Brace Style"[ambiguous] and became the coding style used by convention in the source code for the Unix and Linux kernels.
Eiffel is an object-oriented programming language designed by Bertrand Meyer (an object-orientation proponent and author of Object-Oriented Software Construction) and Eiffel Software. Meyer conceived the language in 1985 with the goal of increasing the reliability of commercial software development.[4] The first version was released in 1986. In 2005, the International Organization for Standardization (ISO) released a technical standard for Eiffel.
The design of the language is closely connected with the Eiffel programming method. Both are based on a set of principles, including design by contract, command–query separation, the uniform-access principle, the single-choice principle, the open–closed principle, and option–operand separation.
Eiffel was originally developed by Eiffel Software, a company founded by Bertrand Meyer. Object-Oriented Software Construction contains a detailed treatment of the concepts and theory of the object technology that led to Eiffel's design.[9]
The design goal behind the Eiffel language, libraries, and programming methods is to enable programmers to create reliable, reusable software modules. Eiffel supports multiple inheritance, genericity, polymorphism, encapsulation, type-safe conversions, and parameter covariance. Eiffel's most important contribution to software engineering is design by contract (DbC), in which assertions, preconditions, postconditions, and class invariants are employed to help ensure program correctness without sacrificing efficiency.
Eiffel's design is based on object-oriented programming theory, with only minor influence of other programming paradigms or concern for support of legacy code. Eiffel formally supports abstract data types. Under Eiffel's design, a software text should be able to reproduce its design documentation from the text itself, using a formalized implementation of the "Abstract Data Type".
EiffelStudio is an integrated development environment available under either an open source or a commercial license. It offers an object-oriented environment for software engineering. EiffelEnvision is a plug-in for Microsoft Visual Studio that allows users to edit, compile, and debug Eiffel projects from within the Microsoft Visual Studio IDE. Five other open source implementations are available: "The Eiffel Compiler" tecomp; Gobo Eiffel; SmartEiffel, the GNU implementation, based on an older version of the language; LibertyEiffel, based on the SmartEiffel compiler; and Visual Eiffel.
Several other programming languages incorporate elements first introduced in Eiffel. Sather, for example, was originally based on Eiffel but has since diverged, and now includes several functional programming features. The interactive-teaching language Blue, forerunner of BlueJ, is also Eiffel-based. The Apple Media Tool includes an Eiffel-based Apple Media Language.
The Eiffel language definition is an international standard of the ISO. The standard was developed by ECMA International, which first approved the standard on 21 June 2005 as Standard ECMA-367, Eiffel: Analysis, Design and Programming Language. In June 2006, ECMA and ISO adopted the second version. In November 2006, ISO first published that version. The standard can be found and used free of charge on the ECMA site.[10] The ISO version[11] is identical in all respects except formatting.
Eiffel Software, "The Eiffel Compiler" tecomp and Eiffel-library-developer Gobo have committed to implementing the standard; Eiffel Software's EiffelStudio 6.1 and "The Eiffel Compiler" tecomp implement some of the major new mechanisms—in particular, inline agents, assigner commands, bracket notation, non-conforming inheritance, and attached types. The SmartEiffel team has turned away from this standard to create its own version of the language, which they believe to be closer to the original style of Eiffel. Object Tools has not disclosed whether future versions of its Eiffel compiler will comply with the standard. LibertyEiffel implements a dialect somewhere in between the SmartEiffel language and the standard.
A class contains features, which are similar to "routines", "members", "attributes" or "methods" in other object-oriented programming languages. A class also defines its invariants, and contains other properties, such as a "notes" section for documentation and metadata. Eiffel's standard data types, such as INTEGER, STRING and ARRAY, are all themselves classes.
Eiffel has five basic executable instructions: assignment, object creation, routine call, condition, and iteration. Eiffel's control structures are strict in enforcing structured programming: every block has exactly one entry and exactly one exit.
Unlike many object-oriented languages, but like Smalltalk, Eiffel does not permit any assignment into attributes of objects, except within the features of an object, which is the practical application of the principle of information hiding or data abstraction, requiring formal interfaces for data mutation. To put it in the language of other object-oriented programming languages, all Eiffel attributes are "protected", and "setters" are needed for client objects to modify values. An upshot of this is that "setters" can and normally do, implement the invariants for which Eiffel provides syntax.
A programming language's look and feel is often conveyed using a "Hello, world!" program. Such a program written in Eiffel might be:
Void-safe ability, like static typing, is another facility for improving software quality. Void-safe software is protected from runtime errors caused by calls to void references, and therefore will be more reliable than software in which calls to void targets can occur. The analogy to static typing is a useful one. Void-safe ability can be seen as an extension to the type system, or a step beyond static typing, because the mechanism for ensuring void safety is integrated into the type system.
Classes may be defined with deferred class rather than with class to indicate that the class may not be directly instantiated. Non-instantiatable classes are called abstract classes in some other object-oriented programming languages. In Eiffel parlance, only an "effective" class can be instantiated (it may be a descendant of a deferred class). A feature can also be deferred by using the deferred keyword in place of a do clause. If a class has any deferred features it must be declared as deferred; however, a class with no deferred features may nonetheless itself be deferred.
Deferred classes play some of the same role as interfaces in languages such as Java, though many object-oriented programming theorists believe interfaces are themselves largely an answer to Java's lack of multiple inheritance (which Eiffel has).[14][15]
Eiffel's "agent" mechanism wraps operations into objects. This mechanism can be used for iteration, event-driven programming, and other contexts in which it is useful to pass operations around the program structure. Other programming languages, especially ones that emphasize functional programming, allow a similar pattern using continuations, closures, or generators; Eiffel's agents emphasize the language's object-oriented paradigm, and use a syntax and semantics similar to code blocks in Smalltalk and Ruby.
Often "once routines" perform a required initialization: multiple calls to a library can include a call to the initialization procedure, but only the first such call will perform the required actions. Using this pattern initialization can be decentralized, avoiding the need for a special initialization module. "Once routines" are similar in purpose and effect to the singleton pattern in many programming languages, and to the Borg pattern used in Python.
The conversion mechanism simply generalizes the ad hoc conversion rules (such as indeed between INTEGER and REAL) that exist in most programming languages, making them applicable to any type as long as the above principle is observed. For example, a DATE class may be declared to convert to STRING; this makes it possible to create a string from a date simply through
A number of networking and threading libraries are available, such as EiffelNet and EiffelThreads. A concurrency model for Eiffel, based on the concepts of design by contract, is SCOOP, or Simple Concurrent Object-Oriented Programming, not yet part of the official language definition but available in EiffelStudio. CAMEO[17] is an (unimplemented) variation of SCOOP for Eiffel. Concurrency also interacts with exceptions. Asynchronous exceptions can be troublesome (where a routine raises an exception after its caller has itself finished).[18]
"Assigner commands" are a companion mechanism designed in the same spirit of allowing well-established, convenient notation reinterpreted in the framework of object-oriented programming. Assigner commands allow assignment-like syntax to call "setter" procedures. An assignment proper can never be of the form a.x := v as this violates information hiding; you have to go for a setter command (procedure). For example, the hash table class can have the function and the procedure
In contrast to most curly bracket programming languages, Eiffel makes a clear distinction between expressions and instructions. This is in line with the Command-Query Separation principle of the Eiffel method.
The specification of Eiffel includes guidelines for displaying software texts in typeset formats: keywords in bold, user-defined identifiers and constants are shown in italics, comments, operators, and punctuation marks in Roman, with program text in blue as in the present article to distinguish it from explanatory text. For example, the "Hello, world!" program given above would be rendered as below in Eiffel documentation:
Eiffel is a purely object-oriented language but provides an open architecture for interfacing with "external" software in any other programming language.
This is a "genealogy" of programming languages. Languages are categorized under the ancestor language with the strongest influence. Those ancestor languages are listed in alphabetic order. Any such categorization has a large arbitrary element, since programming languages often incorporate major ideas from multiple sources.
A very high-level programming language (VHLL) is a programming language with a very high level of abstraction, used primarily as a professional programmer productivity tool.[citation needed]
VHLLs are usually domain-specific languages, limited to a very specific application, purpose, or type of task, and they are often scripting languages (especially extension languages), controlling a specific environment. For this reason, very high-level programming languages are often referred to as goal-oriented programming languages.[citation needed]
The term VHLL was used in the 1990s for what are today more often called high-level programming languages (not "very") used for scripting, such as Perl, Python, PHP, Ruby, and Visual Basic.[1][2]
In software engineering and computer science, abstraction is the process of generalizing concrete details,[1] such as attributes, away from the study of objects and systems to focus attention on details of greater importance.[2] Abstraction is a fundamental concept in computer science and software engineering, especially within the object-oriented programming paradigm.[3] Examples of this include:
Computing mostly operates independently of the concrete world. The hardware implements a model of computation that is interchangeable with others.[6] The software is structured in architectures to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. Greenspun's tenth rule is an aphorism on how such an architecture is both inevitable and complex.
Language abstraction is a central form of abstraction in computing: new artificial languages are developed to express specific aspects of a system. Modeling languages help in planning. Computer languages can be processed with a computer. An example of this abstraction process is the generational development of programming language from the first-generation programming language (machine language) to the second-generation programming language (assembly language) and the third-generation programming language (high-level programming language). Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in scripting languages and domain-specific languages.
Within a programming language, some features let the programmer create new abstractions. These include subroutines, modules, polymorphism, and software components. Some other abstractions such as software design patterns and architectural styles remain invisible to a translator and operate only in the design of a system.
Some abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions they are built on. The software engineer and writer Joel Spolsky has criticized these efforts by claiming that all abstractions are leaky – that they can never completely hide the details below;[7] however, this does not negate the usefulness of abstraction.
Some abstractions are designed to inter-operate with other abstractions – for example, a programming language may contain a foreign function interface for making calls to the lower-level language.
Different programming languages provide different types of abstraction, depending on the intended applications for the language. For example:
Analysts have developed various methods to formally specify software systems. Some known methods include:
Programming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a Pascal-like fashion:
Structured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with a reduction of the complexity potential for side-effects.
These layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.
While much of data abstraction occurs through computer science and automation, there are times when this process is done manually and without programming intervention. One way this can be understood is through data abstraction within the process of conducting a systematic review of the literature. In this methodology, data is abstracted by one or several abstractors when conducting a meta-analysis, with errors reduced through dual data abstraction followed by independent checking, known as adjudication.[11]
In object-oriented programming theory, abstraction involves the facility to define objects that represent abstract "actors" that can perform work, report on and change their state, and "communicate" with other objects in the system. The term encapsulation refers to the hiding of state details, but extending the concept of data type from earlier programming languages to associate behavior most strongly with the data, and standardizing the way that different data types interact, is the beginning of abstraction. When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called polymorphism. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called delegation or inheritance.
Various object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of polymorphism in object-oriented programming, which includes the substitution of one data type for another in the same or similar role. Although not as generally supported, a configuration or image or package may predetermine a great many of these bindings at compile time, link time, or load time. This would leave only a minimum of such bindings to change at run-time.
Although these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code – all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.
In general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged—thus it is entirely under the control of the programmer, and it is called an abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.
When discussing formal semantics of programming languages, formal methods or abstract interpretation, abstraction refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a concrete (more precise) model of execution.
The level of abstraction included in a programming language can influence its overall usability. The Cognitive dimensions framework includes the concept of abstraction gradient in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.
Computer science commonly presents levels (or, less commonly, layers) of abstraction, wherein each level represents a different model of the same information and processes, but with varying amounts of detail. Each level uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.
[12]
Each relatively abstract, "higher" level builds on a relatively concrete, "lower" level, which tends to provide an increasingly "granular" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.
Since many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:
Physical level – The lowest level of abstraction describes how a system actually stores data. The physical level describes complex low-level data structures in detail.
Logical level – The next higher level of abstraction describes what data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This is referred to as physical data independence. Database administrators, who must decide what information to keep in a database, use the logical level of abstraction.
View level – The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many views for the same database.
Layered architecture partitions the concerns of the application into stacked groups (layers).
It is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.
A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term "domain-specific language" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.
The line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as AWK and shell scripts, but was mostly used as a general-purpose programming language later on. By contrast, PostScript is a Turing-complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.
The design and use of appropriate DSLs is a key part of domain engineering, by using a language suitable to the domain at hand – this may consist of using an existing DSL or GPL, or developing a new DSL. Language-oriented programming considers the creation of special-purpose languages for expressing problems as standard part of the problem-solving process. Creating a domain-specific language (with software to support it), rather than reusing an existing language, can be worthwhile if the language allows a particular type of problem or solution to be expressed more clearly than an existing language would allow and the type of problem in question reappears sufficiently often. Pragmatically, a DSL may be specialized to a particular problem domain, a particular problem representation technique, a particular solution technique, or other aspects of a domain.
A domain-specific language is somewhere between a tiny programming language and a scripting language, and is often used in a way analogous to a programming library. The boundaries between these concepts are quite blurry, much like the boundary between scripting languages and general-purpose languages.
Domain-specific languages are languages (or often, declared syntaxes or grammars) with very specific goals in design and implementation. A domain-specific language can be one of a visual diagramming language, such as those created by the Generic Eclipse Modeling System, programmatic abstractions, such as the Eclipse Modeling Framework, or textual languages. For instance, the command line utility grep has a regular expression syntax which matches patterns in lines of text. The sed utility defines a syntax for matching and replacing regular expressions.  Often, these tiny languages can be used together inside a shell to perform more complex programming tasks.
The line between domain-specific languages and scripting languages is somewhat blurred, but domain-specific languages often lack low-level functions for filesystem access, interprocess control, and other functions that characterize full-featured programming languages, scripting or otherwise. Many domain-specific languages do not compile to byte-code or executable code, but to various kinds of media objects: GraphViz exports to PostScript, GIF, JPEG, etc., where Csound compiles to audio files, and a ray-tracing domain-specific language like POV compiles to graphics files.
A data definition language like SQL presents an interesting case: it can be deemed a domain-specific language because it is specific to a specific domain (in SQL's case, accessing and managing relational databases), and is often called from another application, but SQL has more keywords and functions than many scripting languages, and is often thought of as a language in its own right, perhaps because of the prevalence of database manipulation in programming and the amount of mastery required to be an expert in the language.
Further blurring this line, many domain-specific languages have exposed APIs, and can be accessed from other programming languages without breaking the flow of execution or calling a separate process, and can thus operate as programming libraries.
Some domain-specific languages expand over time to include full-featured programming tools, which further complicates the question of whether a language is domain-specific or not. A good example is the functional language XSLT, specifically designed for transforming one XML graph into another, which has been extended since its inception to allow (particularly in its 2.0 version) for various forms of filesystem interaction, string and date manipulation, and data typing.
Adopting a domain-specific language approach to software engineering involves both risks and opportunities. The well-designed domain-specific language manages to find the proper balance between these.
General purpose programming languages rarely support such idioms, but domain-specific languages can describe them, e.g.:
Examples of domain-specific programming languages include HTML, Logo for pencil-like drawing, Verilog and VHDL hardware description languages, MATLAB and GNU Octave for matrix programming, Mathematica, Maple and Maxima for symbolic mathematics, Specification and Description Language for reactive and distributed systems, spreadsheet formulas and macros, SQL for relational database queries, YACC grammars for creating parsers, regular expressions for specifying lexers, the Generic Eclipse Modeling System for creating diagramming languages, Csound for sound and music synthesis, and the input languages of GraphViz and GrGen, software packages used for graph layout and graph rewriting, Hashicorp Configuration Language used for Terraform and other Hashicorp tools, Puppet also has its own configuration language.
The GML scripting language used by GameMaker Studio is a domain-specific language targeted at novice programmers to easily be able to learn programming. While the language serves as a blend of multiple languages including Delphi, C++, and BASIC. Most of functions in that language after compiling in fact calls runtime functions written in language specific for targeted platform, so their final implementation is not visible to user. The language primarily serves to make it easy for anyone to pick up the language and develop a game, and thanks to GM runtime which handles main game loop and keeps implementation of called functions, few lines of code is required for simplest game, instead of thousands.
FilterMeister is a programming environment, with a programming language that is based on C, for the specific purpose of creating Photoshop-compatible image processing filter plug-ins; FilterMeister runs as a Photoshop plug-in itself and it can load and execute scripts or compile and export them as independent plug-ins.
Although the FilterMeister language reproduces a significant portion of the C language and function library, it contains only those features which can be used within the context of Photoshop plug-ins and adds a number of specific features only useful in this specific domain.
There has been much interest in domain-specific languages to improve the productivity and quality of software engineering.  Domain-specific language could possibly provide a robust set of tools for efficient software engineering. Such tools are beginning to make their way into the development of critical software systems.
The Software Cost Reduction Toolkit[6] is an example of this. The toolkit is a suite of utilities including a specification editor to create a requirements specification, a dependency graph browser to display variable dependencies, a consistency checker to catch missing cases in well-formed formulas in the specification, a model checker and a theorem prover to check program properties against the specification, and an invariant generator that automatically constructs invariants based on the requirements.
A newer development is language-oriented programming, an integrated software engineering methodology based mainly on creating, optimizing, and using domain-specific languages.
Complementing language-oriented programming, as well as all other forms of domain-specific languages, are the class of compiler writing tools called metacompilers. A metacompiler is not only useful for generating parsers and code generators for domain-specific languages, but a metacompiler itself compiles a domain-specific metalanguage specifically designed for the domain of metaprogramming.
Besides parsing domain-specific languages, metacompilers are useful for generating a wide range of software engineering and analysis tools.  The meta-compiler methodology is often found in program transformation systems.
Metacompilers that played a significant role in both computer science and the computer industry include Meta-II,[7] and its descendant TreeMeta.[8]
The purpose of business rules engines is to define a representation of business logic in as human-readable fashion as possible.  This allows both subject-matter experts and developers to work with and understand the same representation of the business logic.  Most rules engines provide both an approach to simplifying the control structures for business logic (for example, using declarative rules or decision tables) coupled with alternatives to programming syntax in favor of DSLs.
Gherkin is a language designed to define test cases to check the behavior of software, without specifying how that behavior is implemented. It is meant to be read and used by non-technical users using a natural language syntax and a line-oriented design. The tests defined with Gherkin must then be implemented in a general programming language. Then, the steps in a Gherkin program acts as a syntax for method invocation accessible to non-developers.
The term programming domain is mostly used when referring to domain-specific programming languages. It refers to a set of programming languages or programming environments that were written specifically for a particular domain, where domain means a broad subject for end users such as accounting or  finance, or a category of program usage such as artificial intelligence or email. Languages and systems within a single programming domain would have functions common to the domain and may omit functions that are irrelevant to a domain.[1]
Software testability is the degree to which a software artifact (e.g. a software system, module, requirement, or design document) supports testing in a given test context. If the testability of an artifact is high, then finding faults in the system (if any) by means of testing is easier.
Many software systems are untestable, or not immediately testable. For example, Google's ReCAPTCHA, without having any metadata about the images is not a testable system. Recaptcha, however, can be immediately tested if for each image shown, there is a tag stored elsewhere. Given this meta information, one can test the system.
Therefore, testability is often thought of as an extrinsic property which results from interdependency of the software to be tested and the test goals, test methods used, and test resources (i.e., the test context). Even though testability can not be measured directly (such as software size) it should be considered an intrinsic property of a software artifact because it is highly correlated with other key software qualities such as encapsulation, coupling, cohesion, and redundancy.
A lower degree of testability results in increased test effort. In extreme cases a lack of testability may hinder testing parts of the software or software requirements at all.
Testability, a property applying to empirical hypothesis, involves two components.
The effort and effectiveness of software tests depends on numerous factors including:
Treating the requirement as axioms, testability can be treated via asserting existence of a function 




F

S




{\displaystyle F_{S}}

 (software)
such that input 




I

k




{\displaystyle I_{k}}

 generates output 




O

k




{\displaystyle O_{k}}

, therefore 




F

S


:
I
→
O


{\displaystyle F_{S}:I\to O}

. Therefore, the ideal software generates the tuple 



(

I

k


,

O

k


)


{\displaystyle (I_{k},O_{k})}

 which is the input-output set 



Σ


{\displaystyle \Sigma }

,
standing for specification.
This article discusses a set of tactics useful in software testing. It is intended as a comprehensive list of tactical approaches to software quality assurance (more widely colloquially known as quality assurance (traditionally called by the acronym "QA")) and general application of the test method (usually just called "testing" or sometimes "developer testing").
Software testing methods are traditionally divided into white- and black-box testing. These two approaches are used to describe the point of view that a test engineer takes when designing test cases.
White-box testing (also known as clear box testing, glass box testing, transparent box testing and structural testing, by seeing the source code) tests internal structures or workings of a program, as opposed to the functionality exposed to the end-user. In white-box testing an internal perspective of the system, as well as programming skills, are used to design test cases. The tester chooses inputs to exercise paths through the code and determine the appropriate outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT).
While white-box testing can be applied at the unit, integration and system levels of the software testing process, it is usually done at the unit level. It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it might not detect unimplemented parts of the specification or missing requirements.
Code coverage tools can evaluate the completeness of a test suite that was created with any method, including black-box testing. This allows the software team to examine parts of a system that are rarely tested and ensures that the most important function points have been tested.[1][unreliable source?] Code coverage as a software metric can be reported as a percentage for:
Black-box testing treats the software as a "black box", examining functionality without any knowledge of internal implementation, without seeing the source code. The testers are only aware of what the software is supposed to do, not how it does it.[2] Black-box testing methods include: equivalence partitioning, boundary value analysis, all-pairs testing, state transition tables, decision table testing, fuzz testing, model-based testing, use case testing, exploratory testing and specification-based testing.
Specification-based testing aims to test the functionality of software according to the applicable requirements.[3] This level of testing usually requires thorough test cases to be provided to the tester, who then can simply verify that for a given input, the output value (or behavior), either "is" or "is not" the same as the expected value specified in the test case.
Test cases are built around specifications and requirements, i.e., what the application is supposed to do. It uses external descriptions of the software, including specifications, requirements, and designs to derive test cases. These tests can be functional or non-functional, though usually functional.
One advantage of the black box technique is that no programming knowledge is required.  Whatever biases the programmers may have had, the tester likely has a different set and may emphasize different areas of functionality. On the other hand, black-box testing has been said to be "like a walk in a dark labyrinth without a flashlight."[5]  Because they do not examine the source code, there are situations when a tester writes many test cases to check something that could have been tested by only one test case, or leaves some parts of the program untested.
This method of test can be applied to all levels of software testing: unit, integration, system and acceptance. It typically comprises most if not all testing at higher levels, but can also dominate unit testing as well.
The aim of visual testing is to provide developers with the ability to examine what was happening at the point of software failure by presenting the data in such a way that the developer can easily find the information she or he requires, and the information is expressed clearly.[6][7]
Visual testing is particularly well-suited for environments that deploy agile methods in their development of software, since agile methods require greater communication between testers and developers and collaboration within small teams.[citation needed]
Ad hoc testing and exploratory testing are important methodologies for checking software integrity, because they require less preparation time to implement, while the important bugs can be found quickly. In ad hoc testing, where testing takes place in an improvised, impromptu way, the ability of a test tool to visually record everything that occurs on a system becomes very important in order to document the steps taken to uncover the bug.[clarification needed][citation needed]
Visual testing is gathering recognition in customer acceptance and usability testing, because the test can be used by many individuals involved in the development process.[citation needed] For the customer, it becomes easy to provide detailed bug reports and feedback, and for program users, visual testing can record user actions on screen, as well as their voice and image, to provide a complete picture at the time of software failure for the developers.
Grey-box testing (American spelling: gray-box testing) involves having knowledge of internal data structures and algorithms for purposes of designing tests, while executing those tests at the user, or black-box level. The tester is not required to have full access to the software's source code.[2] Manipulating input data and formatting output do not qualify as grey-box, because the input and output are clearly outside of the "black box" that we are calling the system under test. This distinction is particularly important when conducting integration testing between two modules of code written by two different developers, where only the interfaces are exposed for test.
However, tests that require modifying a back-end data repository such as a database or a log file does qualify as grey-box, as the user would not normally be able to change the data repository in normal production operations.[citation needed] Grey-box testing may also include reverse engineering to determine, for instance, boundary values or error messages.
By knowing the underlying concepts of how the software works, the tester makes better-informed testing choices while testing the software from outside. Typically, a grey-box tester will be permitted to set up an isolated testing environment with activities such as seeding a database. The tester can observe the state of the product being tested after performing certain actions such as executing SQL statements against the database and then executing queries to ensure that the expected changes have been reflected. Grey-box testing implements intelligent test scenarios, based on limited information. This will particularly apply to data type handling, exception handling, and so on.[8]
Many programming groups are relying more and more on automated testing, especially groups that use
test-driven development. There are many frameworks to write tests in, and continuous integration software will run tests automatically every time code is checked into a version control system.
There are generally four recognized levels of tests: unit testing, integration testing, component interface testing, and system testing. Tests are frequently grouped by where they are added in the software development process, or by the level of specificity of the test. The main levels during the development process as defined by the SWEBOK guide are unit-, integration-, and system testing that are distinguished by the test target without implying a specific process model.[9] Other test levels are classified by the testing objective.[9]
There are two different levels of tests from the perspective of customers: low-level testing (LLT) and high-level testing (HLT). LLT is a group of tests for different level components of software application or product. HLT is a group of tests for the whole software application or product.[citation needed]
These types of tests are usually written by developers as they work on code (white-box style), to ensure that the specific function is working as expected. One function might have multiple tests, to catch corner cases or other branches in the code. Unit testing alone cannot verify the functionality of a piece of software, but rather is used to ensure that the building blocks of the software work independently from each other.
Unit testing is a software development process that involves synchronized application of a broad spectrum of defect prevention and detection strategies in order to reduce software development risks, time, and costs.  It is performed by the software developer or engineer during the construction phase of the software development lifecycle. Rather than replace traditional QA focuses, it augments it. Unit testing aims to eliminate construction errors before code is promoted to QA; this strategy is intended to increase the quality of the resulting software as well as the efficiency of the overall development and QA process.
Depending on the organization's expectations for software development, unit testing might include static code analysis, data-flow analysis, metrics analysis, peer code reviews, code coverage analysis and other software verification practices.
Integration testing is any type of software testing that seeks to verify the interfaces between components against a software design. Software components may be integrated in an iterative way or all together ("big bang"). Normally the former is considered a better practice since it allows interface issues to be located more quickly and fixed.
Integration testing works to expose defects in the interfaces and interaction between integrated components (modules). Progressively larger groups of tested software components corresponding to elements of the architectural design are integrated and tested until the software works as a system.[11]
Operational acceptance is used to conduct operational readiness (pre-release) of a product, service or system as part of a quality management system.  OAT is a common type of non-functional software testing, used mainly in software development and software maintenance projects. This type of testing focuses on the operational readiness of the system to be supported, and/or to become part of the production environment. Hence, it is also known as operational readiness testing (ORT) or Operations readiness and assurance (OR&A) testing. Functional testing within OAT is limited to those tests which are required to verify the non-functional aspects of the system.
In addition, the software testing should ensure that the portability of the system, as well as working as expected, does not also damage or partially corrupt its operating environment or cause other processes within that environment to become inoperative.[15]
A common cause of software failure (real or perceived) is a lack of its compatibility with other application software, operating systems (or operating system versions, old or new), or target environments that differ greatly from the original (such as a terminal or GUI application intended to be run on the desktop now being required to become a web application, which must render in a web browser). For example, in the case of a lack of backward compatibility, this can occur because the programmers develop and test software only on the latest version of the target environment, which not all users may be running. This results in the unintended consequence that the latest work may not function on earlier versions of the target environment, or on older hardware that earlier versions of the target environment was capable of using. Sometimes such issues can be fixed by proactively abstracting operating system functionality into a separate program module or library.
Smoke testing consists of minimal attempts to operate the software, designed to determine whether there are any basic problems that will prevent it from working at all. Such tests can be used as build verification test.
Regression testing focuses on finding defects after a major code change has occurred. Specifically, it seeks to uncover software regressions, as degraded or lost features, including old bugs that have come back. Such regressions occur whenever software functionality that was previously working correctly, stops working as intended. Typically, regressions occur as an unintended consequence of program changes, when the newly developed part of the software collides with the previously existing code. Common methods of regression testing include re-running previous sets of test cases and checking whether previously fixed faults have re-emerged. The depth of testing depends on the phase in the release process and the risk of the added features. They can either be complete, for changes added late in the release or deemed to be risky, or be very shallow, consisting of positive tests on each feature, if the changes are early in the release or deemed to be of low risk. Regression testing is typically the largest test effort in commercial software development,[16] due to checking numerous details in prior software features, and even new software can be developed while using some old test cases to test parts of the new design to ensure prior functionality is still supported.
Alpha testing is simulated or actual operational testing by potential users/customers or an independent test team at the developers' site. Alpha testing is often employed for off-the-shelf software as a form of internal acceptance testing, before the software goes to beta testing.[17]
Beta testing comes after alpha testing and can be considered a form of external user acceptance testing. Versions of the software, known as beta versions, are released to a limited audience outside of the programming team known as beta testers. The software is released to groups of people so that further testing can ensure the product has few faults or bugs. Beta versions can be made available to the open public to increase the feedback field to a maximal number of future users and to deliver value earlier, for an extended or even indefinite period of time (perpetual beta).[citation needed]
Non-functional testing refers to aspects of the software that may not be related to a specific function or user action, such as scalability or other performance, behavior under certain constraints, or security. Testing will determine the breaking point, the point at which extremes of scalability or performance leads to unstable execution.  Non-functional requirements tend to be those that reflect the quality of the product, particularly in the context of the suitability perspective of its users.
Continuous testing is the process of executing automated tests as part of the software delivery pipeline to obtain immediate feedback on the business risks associated with a software release candidate.[18][19]  Continuous testing includes the validation of both functional requirements and non-functional requirements; the scope of testing extends from validating bottom-up requirements or user stories to assessing the system requirements associated with overarching business goals.[20][21][22]
Destructive testing attempts to cause the software or a sub-system to fail.  It verifies that the software functions properly even when it receives invalid or unexpected inputs, thereby establishing the robustness of input validation and error-management routines.[citation needed] Software fault injection, in the form of fuzzing, is an example of failure testing.  Various commercial non-functional testing tools are linked from the software fault injection page; there are also numerous open-source and free software tools available that perform destructive testing.
Load testing is primarily concerned with testing that the system can continue to operate under a specific load, whether that be large quantities of data or a large number of users. This is generally referred to as software scalability. The related load testing activity of when performed as a non-functional activity is often referred to as endurance testing. Volume testing is a way to test software functions even when certain components (for example a file or database) increase radically in size. Stress testing is a way to test reliability under unexpected or rare workloads. Stability testing (often referred to as load or endurance testing) checks to see if the software can continuously function well in or above an acceptable period.
Real-time software systems have strict timing constraints. To test if timing constraints are met, real-time testing is used.
Security testing is essential for software that processes confidential data to prevent system intrusion by hackers.
The general ability of software to be internationalized and localized can be automatically tested without actual translation, by using pseudolocalization. It will verify that the application still works, even after it has been translated into a new language or adapted for a new culture (such as different currencies or time zones).[24]
"Development testing" is a software development process that involves synchronized application of a broad spectrum of defect prevention and detection strategies in order to reduce software development risks, time, and costs.  It is performed by the software developer or engineer during the construction phase of the software development lifecycle. Rather than replace traditional QA focuses, it augments it. Development Testing aims to eliminate construction errors before code is promoted to QA; this strategy is intended to increase the quality of the resulting software as well as the efficiency of the overall development and QA process.
Depending on the organization's expectations for software development, Development Testing might include static code analysis, data flow analysis, metrics analysis, peer code reviews, unit testing, code coverage analysis, traceability, and other software verification practices.
In software testing, conformance testing verifies that a product performs according to its specified standards. Compilers, for instance, are extensively tested to determine whether they meet the recognized standard for that language.
A software system is a system of intercommunicating components based on software forming part of a computer system (a combination of hardware and software). It "consists of a number of separate programs, configuration files, which are used to set up these programs, system documentation, which describes the structure of the system, and user documentation, which explains how to use the system".[1]
A software system differs from a computer program or software. While a computer program is generally a set of instructions (source, or object code) that perform a specific task, a software system is more or an encompassing concept with many more components such as specification, test results, end-user documentation, maintenance records, etc.[2]
The use of the term software system is at times related to the application of systems theory approaches in the context of software engineering. A software system consists of several separate computer programs and associated configuration files, documentation, etc., that operate together.[1] The concept is used in the study of large and complex software, because it focuses on the major components of software and their interactions. It is also related to the field of software architecture.
Software systems are an active area of research for groups interested in software engineering in particular and systems engineering in general.[3] Academic journals like the Journal of Systems and Software (published by Elsevier) are dedicated to the subject.[4]
The ACM Software System Award is an annual award that honors people or an organization "for developing a system that has had a lasting influence, reflected in contributions to concepts, in commercial acceptance, or both".[5] It has been awarded by the Association for Computing Machinery (ACM) since 1983, with a cash prize sponsored by IBM.
Major categories of software systems include those based on application software development, programming software, and system software although the distinction can sometimes be difficult. Examples of software systems include operating systems, computer reservations systems, air traffic control systems, military command and control systems, telecommunication networks, content management systems, database management systems, expert systems, embedded systems, etc.
In software engineering, a walkthrough or walk-through is a form of software peer review "in which a designer or programmer leads members of the development team and other interested parties through a software product, and the participants ask questions and make comments about possible errors, violation of development standards, and other problems".[1] The reviews are also performed by assessors, specialists, etc. and are suggested or mandatory as required by norms and standards.[2]
"Software product" normally refers to some kind of technical document.  As indicated by the IEEE definition, this might be a software design document or program source code, but use cases, business process definitions, test case specifications, and a variety of other technical documentation may also be walked through.
A walkthrough differs from software technical reviews in its openness of structure and its objective of familiarization. It differs from software inspection in its ability to suggest direct alterations to the product reviewed. It lacks of direct focus on training and process improvement, process and product measurement.
A walkthrough may be quite informal, or may follow the process detailed in IEEE 1028 and outlined in the article on software reviews.
Inspection in software engineering, refers to peer review of any work product by trained individuals who look for defects using a well defined process. An inspection might also be referred to as a Fagan inspection after Michael Fagan, the creator of a very popular software inspection process.
An inspection is one of the most common sorts of review practices found in software projects.  The goal of the inspection is to identify defects. Commonly inspected work products include software requirements specifications and test plans. In an inspection, a work product is selected for review and a team is gathered for an inspection meeting to review the work product. A moderator is chosen to moderate the meeting. Each inspector prepares for the meeting by reading the work product and noting each defect. In an inspection, a defect is any part of the work product that will keep an inspector from approving it. For example, if the team is inspecting a software requirements specification, each defect will be text in the document which an inspector disagrees with.
The process is ended by the moderator when it satisfies some predefined exit criteria.
The term inspection refers to one of the most important elements of the entire process that surrounds the execution and successful completion of a software engineering project.
A code review can be done as a special kind of inspection in which the team examines a sample of code and fixes any defects in it. In a code review, a defect is a block of code which does not properly implement its requirements, which does not function as the programmer intended, or which is not incorrect but could be improved (for example, it could be made more readable or its performance could be improved). In addition to helping teams find and fix bugs, code reviews are useful both for cross-training programmers on the code being reviewed and for helping junior developers learn new programming techniques.
Peer reviews are considered an industry best-practice for detecting software defects early and learning about software artifacts. Peer Reviews are composed of software walkthroughs and software inspections and are integral to software product engineering activities. A collection of coordinated knowledge, skills, and behaviors facilitates the best possible practice of Peer Reviews. The elements of Peer Reviews include the structured review process, standard of excellence product checklists, defined roles of participants, and the forms and reports.
Software inspections are the most rigorous form of Peer Reviews and fully utilize these elements in detecting defects. Software walkthroughs draw selectively upon the elements in assisting the producer to obtain the deepest understanding of an artifact and reaching a consensus among participants. Measured results reveal that Peer Reviews produce an attractive return on investment obtained through accelerated learning and early defect detection. For best results, Peer Reviews are rolled out within an organization through a defined program of preparing a policy and procedure, training practitioners and managers, defining measurements and populating a database structure, and sustaining the roll out infrastructure.
Software verification is a discipline of software engineering, programming languages, and theory of computation whose goal is to assure that software satisfies the expected requirements.
A broad definition of verification makes it related to software testing. In that case, there are two fundamental approaches to verification:
Under the ACM Computing Classification System, software verification topics appear under "Software and its engineering", within "Software creation", whereas Program verification also appears under Theory of computation under Semantics and reasoning, Program reasoning.
Dynamic verification is performed during the execution of software, and dynamically checks its behavior; it is commonly known as the Test phase.
Verification is a Review Process.
Depending on the scope of tests, we can categorize them in three families:
The aim of software dynamic verification is to find the errors introduced by an activity (for example, having a medical software to analyze bio-chemical data); or by the repetitive performance of one or more activities (such as a stress test for a web server, i.e. check if the current product of the activity is as correct as it was at the beginning of the activity).
Static verification is the process of checking that software meets requirements by inspecting the code before it runs. For example:
When it is defined more strictly, verification is equivalent only to static testing and it is intended to be applied to artifacts. And, validation (of the whole software product) would be equivalent to dynamic testing and intended to be applied to the running software product (not its artifacts, except requirements). Notice that requirements validation can be performed statically and dynamically (See artifact validation).
Software verification is often confused with software validation.  The difference between verification and validation:
In software project management, software testing, and software engineering, verification and validation is the process of checking that a software engineer system meets specifications and requirements so that it fulfills its intended purpose. It may also be referred to as software quality control.  It is normally the responsibility of software testers as part of the software development lifecycle. In simple terms, software verification is: "Assuming we should build X, does our software achieve its goals without any bugs or gaps?" On the other hand, software validation is: "Was X what we should have built? Does X meet the high-level requirements?"
"Building the product right" checks that the specifications are correctly implemented by the system while "building the right product" refers back to the user's needs. In some contexts, it is required to have written requirements for both as well as formal procedures or protocols for determining compliance. Ideally, formal methods provide a mathematical guarantee that software meets its specifications.
Building the product right implies the use of the Requirements Specification as input for the next phase of the development process, the design process, the output of which is the Design Specification. Then, it also implies the use of the Design Specification to feed the construction process. Every time the output of a process correctly implements its input specification, the software product is one step closer to final verification. If the output of a process is incorrect, the developers have not correctly implemented some component of that process. This kind of verification is called "artifact or specification verification".
It would imply to verify if the specifications are met by running the software but this is not possible (e.g., how can anyone know if the architecture/design/etc. are correctly implemented by running the software?). Only by reviewing its associated artifacts, can someone conclude whether or not the specifications are met.
The output of each software development process stage can also be subject to verification when checked against its input specification (see the definition by CMMI below).
Software validation checks that the software product satisfies or fits the intended use (high-level checking), i.e., the software meets the user requirements, not as specification artifacts or as needs of those who will operate the software only; but, as the needs of all the stakeholders (such as users, operators, administrators, managers, investors, etc.). There are two ways to perform software validation: internal and external. During internal software validation, it is assumed that the goals of the stakeholders were correctly understood and that they were expressed in the requirement artifacts precisely and comprehensively. If the software meets the requirement specification, it has been internally validated. External validation happens when it is performed by asking the stakeholders if the software meets their needs. Different software development methodologies call for different levels of user and stakeholder involvement and feedback; so, external validation can be a discrete or a continuous event. Successful final external validation occurs when all the stakeholders accept the software product and express that it satisfies their needs. Such final external validation requires the use of an acceptance test which is a dynamic test.
However, it is also possible to perform internal static tests to find out if the software meets the requirements specification but that falls into the scope of static verification because the software is not running.
Requirements should be validated before the software product as a whole is ready (the waterfall development process requires them to be perfectly defined before design starts; but iterative development processes do not require this to be so and allow their continual improvement).
Validation during the software development process can be seen as a form of User Requirements Specification validation; and, that at the end of the development process is equivalent to Internal and/or External Software validation. Verification, from CMMI's point of view, is evidently of the artifact kind.
In other words, software verification ensures that the output of each phase of the software development process effectively carries out what its corresponding input artifact specifies (requirement -> design -> software product), while software validation ensures that the software product meets the needs of all the stakeholders (therefore, the requirement specification was correctly and accurately expressed in the first place). Software verification ensures that "you built it right" and confirms that the product, as provided, fulfills the plans of the developers. Software validation ensures that "you built the right thing" and confirms that the product, as provided, fulfills the intended use and goals of the stakeholders.
Both verification and validation are related to the concepts of quality and of software quality assurance. By themselves, verification and validation do not guarantee software quality; planning, traceability, configuration management and other aspects of software engineering are required.
The definition of M&S validation focuses on the accuracy with which the M&S represents the real-world intended use(s).  Determining the degree of M&S accuracy is required because all M&S are approximations of reality, and it is usually critical to determine if the degree of approximation is acceptable for the intended use(s).  This stands in contrast to software validation.
In mission-critical software systems, formal methods may be used to ensure the correct operation of a system. These formal methods can prove costly, however, representing as much as 80 percent of total software design cost.
Independent Software Verification and Validation (ISVV) is targeted at safety-critical software systems and aims to increase the quality of software products, thereby reducing risks and costs throughout the operational life of the software. The goal of ISVV is to provide assurance that software performs to the specified level of confidence and within its designed parameters and defined requirements.[4][5]
ISVV activities are performed by independent engineering teams, not involved in the software development process, to assess the processes and the resulting products. The ISVV team independency is performed at three different levels: financial, managerial and technical.
ISVV goes beyond "traditional" verification and validation techniques, applied by development teams. While the latter aims to ensure that the software performs well against the nominal requirements, ISVV is focused on non-functional requirements such as robustness and reliability, and on conditions that can lead the software to fail.
ISVV derives from the application of IV&V (Independent Verification and Validation) to the software. Early ISVV application (as known today) dates back to the early 1970s when the U.S. Army sponsored the first significant program related to IV&V for the Safeguard Anti-Ballistic Missile System.[6] Another example is NASA's IV&V Program, which was established in 1993.[7]
By the end of the 1970s IV&V was rapidly becoming popular. The constant increase in complexity, size and importance of the software led to an increasing demand on IV&V applied to software.
Meanwhile, IV&V (and ISVV for software systems) consolidated and is now widely used by organizations such as the DoD, FAA,[8] NASA[7] and ESA.[9] IV&V is mentioned in DO-178B, ISO/IEC 12207 and formalized in IEEE 1012.
Initially in 2004-2005, a European consortium led by the European Space Agency, and composed of DNV, Critical Software SA, Terma and CODA SciSys plc created the first version of a guide devoted to ISVV, called "ESA Guide for Independent Verification and Validation" with support from other organizations.[10] This guide covers the methodologies applicable to all the software engineering phases in what concerns ISVV.
Software often must meet the compliance requirements of legally regulated industries, which is often guided by government agencies[11][12] or industrial administrative authorities. For instance, the FDA requires software versions and patches to be validated.[13]
An application programming interface (API) is a connection between computers or between computer programs. It is a type of software interface, offering a service to other pieces of software.[1] A document or standard that describes how to build such a connection or interface is called an API specification. A computer system that meets this standard is said to implement or expose an API. The term API may refer either to the specification or to the implementation.
In contrast to a user interface, which connects a computer to a person, an application programming interface connects computers or pieces of software to each other. It is not intended to be used directly by a person (the end user) other than a computer programmer[1] who is incorporating it into software. An API is often made up of different parts which act as tools or services that are available to the programmer. A program or a programmer that uses one of these parts is said to call that portion of the API. The calls that make up the API are also known as subroutines, methods, requests, or endpoints. An API specification defines these calls, meaning that it explains how to use or implement them.
The term API is often used to refer to web APIs,[2] which allow communication between computers that are joined by the internet. There are also APIs for programming languages, software libraries, computer operating systems, and computer hardware. APIs originated in the 1940s, though the term did not emerge until the 1960s and 70s.
An API opens a software system to interactions from the outside. It allows two software systems to communicate across a boundary — an interface — using mutually agreed-upon signals.[3] In other words, an API connects software entities together. Unlike a user interface, an API is typically not visible to users. It is an "under the hood" portion of a software system, used for machine-to-machine communication.[4]
A well-designed API exposes only objects or actions needed by software or software developers. It hides details that have no use. This abstraction simplifies programming.[5]
Building software using APIs has been compared to using building-block toys, such as Lego bricks. Software services or software libraries are analogous to the bricks; they may be joined together via their APIs, composing a new software product.[6] The process of joining is called integration.[3]
An API is often compared to a contract. It represents an agreement between parties: a service provider who offers the API and the software developers who rely upon it. If the API remains stable, or if it changes only in predictable ways, developers' confidence in the API will increase. This may increase their use of the API.[8]
The term API initially described an interface only for end-user-facing programs, known as application programs. This origin is still reflected in the name "application programming interface." Today, the term is broader, including also utility software and even hardware interfaces.[10]
The idea of the API is much older than the term itself. British computer scientists Maurice Wilkes and David Wheeler worked on a modular software library in the 1940s for EDSAC, an early computer. The subroutines in this library were stored on punched paper tape organized in a filing cabinet. This cabinet also contained what Wilkes and Wheeler called a "library catalog" of notes about each subroutine and how to incorporate it into a program. Today, such a catalog would be called an API (or an API specification or API documentation) because it instructs a programmer on how to use (or "call") each subroutine that the programmer needs.[10]
The term "application program interface" (without an -ing suffix) is first recorded in a paper called Data structures and techniques for remote computer graphics presented at an AFIPS conference in 1968.[12][10] The authors of this paper use the term to describe the interaction of an application—a graphics program in this case—with the rest of the computer system. A consistent application interface (consisting of Fortran subroutine calls) was intended to free the programmer from dealing with idiosyncrasies of the graphics display device, and to provide hardware independence if the computer or the display were replaced.[11]
The term was introduced to the field of databases by C. J. Date[13] in a 1974 paper called The Relational and Network Approaches: Comparison of the Application Programming Interface.[14] An API became a part of the ANSI/SPARC framework for database management systems. This framework treated the application programming interface separately from other interfaces, such as the query interface. Database professionals in the 1970s observed these different interfaces could be combined; a sufficiently rich application interface could support the other interfaces as well.[9]
This observation led to APIs that supported all types of programming, not just application programming. By 1990, the API was defined simply as "a set of services available to a programmer for performing certain tasks" by technologist Carl Malamud.[15]
Roy Fielding's dissertation Architectural Styles and the Design of Network-based Software Architectures at UC Irvine in 2000 outlined Representational state transfer (REST) and described the idea of a "network-based Application Programming Interface" that Fielding contrasted with traditional "library-based" APIs.[17] XML and JSON web APIs saw widespread commercial adoption beginning in 2000 and continuing as of 2021. The web API is now the most common meaning of the term API.[2]
The Semantic Web proposed by Tim Berners-Lee in 2001 included "semantic APIs" that recast the API as an open, distributed data interface rather than a software behavior interface.[18] Proprietary interfaces and agents became more widespread than open ones, but the idea of the API as a data interface took hold. Because web APIs are widely used to exchange data of all kinds online, API has become a broad term describing much of the communication on the internet.[16] When used in this way, the term API has overlap in meaning with the term communication protocol.
The interface to a software library is one type of API. The API describes and prescribes the "expected behavior" (a specification) while the library is an "actual implementation" of this set of rules.
A single API can have multiple implementations (or none, being abstract) in the form of different libraries that share the same programming interface.
API use can vary depending on the type of programming language involved.
An API for a procedural language such as Lua could consist primarily of basic routines to execute code, manipulate data or handle errors while an API for an object-oriented language, such as Java, would provide a specification of classes and its class methods.[20][21] Hyrum's law states that "With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody."[22] Meanwhile, several studies show that most applications that use an API tend to use a small part of the API.[23]
An API can also be related to a software framework: a framework can be based on several libraries implementing several APIs, but unlike the normal use of an API, the access to the behavior built into the framework is mediated by extending its content with new classes plugged into the framework itself.
An API can specify the interface between an application and the operating system.[28] POSIX, for example, specifies a set of common APIs that aim to enable an application written for a POSIX conformant operating system to be compiled for another POSIX conformant operating system.
Linux and Berkeley Software Distribution are examples of operating systems that implement the POSIX APIs.[29]
Remote APIs allow developers to manipulate remote resources through protocols, specific standards for communication that allow different technologies to work together, regardless of language or platform.
For example, the Java Database Connectivity API allows developers to query many different types of databases with the same set of functions, while the Java remote method invocation API uses the Java Remote Method Protocol to allow invocation of functions that operate remotely, but appear local to the developer.[33][34]
Therefore, remote APIs are useful in maintaining the object abstraction in object-oriented programming; a method call, executed locally on a proxy object, invokes the corresponding method on the remote object, using the remoting protocol, and acquires the result to be used locally as a return value.
When used in the context of web development, an API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format. An example might be a shipping company API that can be added to an eCommerce-focused website to facilitate ordering shipping services and automatically include current shipping rates, without the site developer having to enter the shipper's rate table into a web database. While "web API" historically has been virtually synonymous with web service, the recent trend (so-called Web 2.0) has been moving away from Simple Object Access Protocol (SOAP) based web services and service-oriented architecture (SOA) towards more direct representational state transfer (REST) style web resources and resource-oriented architecture (ROA).[37] Part of this trend is related to the Semantic Web movement toward Resource Description Framework (RDF), a concept to promote web-based ontology engineering technologies. Web APIs allow the combination of multiple APIs into new applications known as mashups.[38]
In the social media space, web APIs have allowed web communities to facilitate sharing content and data between communities and applications. In this way, content that is created in one place dynamically can be posted and updated to multiple locations on the web.[39] For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data.[40]
The design of an API has significant impact on its usage.[5] The principle of information hiding describes the role of programming interfaces as enabling modular programming by hiding the implementation details of the modules so that users of modules need not understand the complexities inside the modules.[41] Thus, the design of an API attempts to provide only the tools a user would expect.[5] The design of programming interfaces represents an important part of software architecture, the organization of a complex piece of software.[42]
In 2010, Oracle Corporation sued Google for having distributed a new implementation of Java embedded in the Android operating system.[59] Google had not acquired any permission to reproduce the Java API, although permission had been given to the similar OpenJDK project. Judge William Alsup ruled in the Oracle v. Google case that APIs cannot be copyrighted in the U.S. and that a victory for Oracle would have widely expanded copyright protection to a "functional set of symbols" and allowed the copyrighting of simple software commands:
In software engineering and development, a software metric is a standard of measure of a degree to which a software system or process possesses some property.[1][2] Even if a metric is not a measurement (metrics are functions, while measurements are the numbers obtained by the application of metrics), often the two terms are used as synonyms. Since quantitative measurements are essential in all sciences, there is a continuous effort by computer science practitioners and theoreticians to bring similar approaches to software development. The goal is obtaining objective, reproducible and quantifiable measurements, which may have numerous valuable applications in schedule and budget planning, cost estimation, quality assurance, testing, software debugging, software performance optimization, and optimal personnel task assignments.
As software development is a complex process, with high variance on both methodologies and objectives, it is difficult to define or measure software qualities and quantities and to determine a valid and concurrent measurement metric, especially when making such a prediction prior to the detail design. Another source of difficulty and debate is in determining which metrics matter, and what they mean.[8][9]
The practical utility of software measurements has therefore been limited to the following domains:
A specific measurement may target one or more of the above aspects, or the balance between them, for example as an indicator of team motivation or project performance.
[10]
Additionally metrics vary between static and dynamic program code, as well as for object oriented software (systems).[11][12]
Some software development practitioners point out that simplistic measurements can cause more harm than good.[13] Others have noted that metrics have become an integral part of the software development process.[8]
Impact of measurement on programmer psychology have raised concerns for harmful effects to performance due to stress, performance anxiety, and attempts to cheat the metrics, while others find it to have positive impact on developers value towards their own work, and prevent them being undervalued. Some argue that the definition of many measurement methodologies are imprecise, and consequently it is often unclear how tools for computing them arrive at a particular result,[14] while others argue that imperfect quantification is better than none (“You can’t control what you can't measure.”).[15] Evidence shows that software metrics are being widely used by government agencies, the US military, NASA,[16] IT consultants, academic institutions,[17] and commercial and academic development estimation software.
Software versioning is the process of assigning either unique version names or unique version numbers to unique states of computer software. Within a given version number category (e.g., major or minor), these numbers are generally assigned in increasing order and correspond to new developments in the software. At a fine-grained level, revision control is used for keeping track of incrementally-different versions of information, whether or not this information is computer software, in order to be able to roll any changes back.
Modern computer software is often tracked using two different software versioning schemes: an internal version number that may be incremented many times in a single day, such as a revision control number, and a release version that typically changes far less often, such as semantic versioning[1] or a project code name.
Later lists of files including their versions were added, and dependencies amongst them. Linux distributions like Debian, with its dpkg, early on created package management software which could resolve dependencies between their packages. Debian's first try was that a package knew other packages which depended on it. From 1994 on this idea was inverted, so a package that knew the packages it needed. When installing a package, dependency resolution was used to automatically calculate the packages needed as well, and install them with the desired package. To facilitate upgrades, minimum package versions were introduced. Thus the numbering scheme needed to tell which version was newer than the required one.[3][4][5]
A variety of version numbering schemes have been created to keep track of different versions of a piece of software. The ubiquity of computers has also led to these schemes being used in contexts outside computing.
In sequence-based software versioning schemes, each software release is assigned a unique identifier that consists of one or more sequences of numbers or letters.[6] This is the extent of the commonality; schemes vary widely in areas such as the number of sequences, the attribution of meaning to individual sequences, and the means of incrementing the sequences.
Semantic versioning (aka SemVer)[1] is a widely-adopted version scheme[7] that encodes a version by a three-part version number (Major.Minor.Patch), an optional pre-release tag, and an optional build meta tag. In this scheme, risk and functionality are the measures of significance. Breaking changes are indicated by increasing the major number (high risk); new, non-breaking features increment the minor number (medium risk); and all other non-breaking changes increment the patch number (lowest risk). The presence of a pre-release tag (-alpha, -beta) indicates substantial risk, as does a major number of zero (0.y.z), which is used to indicate a work-in-progress that may contain any level of potentially breaking changes (highest risk). As an example of inferring compatibility from a SemVer version, software which relies on version 2.1.5 of an API is compatible with version 2.2.3, but not necessarily with 3.2.4.
Developers may choose to jump multiple minor versions at a time to indicate that significant features have been added, but are not enough to warrant incrementing a major version number; for example, Internet Explorer 5 from 5.1 to 5.5 or Adobe Photoshop 5 to 5.5. This may be done to emphasize the value of the upgrade to the software user or, as in Adobe's case, to represent a release halfway between major versions (although levels of sequence-based versioning are not necessarily limited to a single digit, as in Blender version 2.91 or Minecraft Java Edition starting from 1.7.10).
A different approach is to use the major and minor numbers along with an alphanumeric string denoting the release type, e.g. "alpha" (a), "beta" (b), or "release candidate" (rc). A software release train using this approach might look like 0.5, 0.6, 0.7, 0.8, 0.9 → 1.0b1, 1.0b2 (with some fixes), 1.0b3 (with more fixes) → 1.0rc1 (which, if it is stable enough), 1.0rc2 (if more bugs are found) → 1.0. It is a common practice in this scheme to lock out new features and breaking changes during the release candidate phases and, for some teams, even betas are locked down to bug fixes only, to ensure convergence on the target release.
In most proprietary software, the first released version of a software product has version 1.[citation needed]
Often programmers write new software to be backward compatible, i.e., the new software is designed to interact correctly with older versions of the software (using old protocols and file formats) and the most recent version (using the latest protocols and file formats). For example, IBM z/OS is designed to work properly with 3 consecutive major versions of the operating system running in the same sysplex.
This enables people who run a high availability computer cluster to keep most of the computers up and running while one machine at a time is shut down, upgraded, and restored to service.[12]
Often packet headers and file format include a version number – sometimes the same as the version number of the software that wrote it; other times a "protocol version number" independent of the software version number.
The code to handle old deprecated protocols and file formats is often seen as cruft.
Software in the experimental stage (alpha or beta) often uses a zero in the first ("major") position of the sequence to designate its status. However, this scheme is only useful for the early stages, not for upcoming releases with established software where the version number has already progressed past 0.[1]
There are two schools of thought regarding how numeric version numbers are incremented. Most free and open-source software packages, including MediaWiki, treat versions as a series of individual numbers, separated by periods, with a progression such as 1.7.0, 1.8.0, 1.8.1, 1.9.0, 1.10.0, 1.11.0, 1.11.1, 1.11.2, and so on.
On the other hand, some software packages identify releases by decimal numbers: 1.7, 1.8, 1.81, 1.82, 1.9, etc. Decimal versions were common in the 1980s, for example with NetWare, DOS, and Microsoft Windows, but even in the 2000s have been for example used by Opera[13] and Movable Type.[14] In the decimal scheme, 1.81 is the minor version following 1.8, while maintenance releases (i.e. bug fixes only) may be denoted with an alphabetic suffix, such as 1.81a or 1.81b.
There is sometimes a fourth, unpublished number which denotes the software build (as used by Microsoft). Adobe Flash is a notable case where a four-part version number is indicated publicly, as in 10.1.53.64. Some companies also include the build date. Version numbers may also include letters and other characters, such as Lotus 1-2-3 Release 1a.
The Python Software Foundation has published PEP 440 – Version Identification and Dependency Specification,[23] outlining their own flexible scheme, that defines an epoch segment, a release segment, pre-release and post-release segments and a development release segment.
The Microsoft Windows operating system was first labelled with standard version numbers for Windows 1.0 through Windows 3.11. After this Microsoft excluded the version number from the product name. For Windows 95 (version 4.0), Windows 98 (4.10) and Windows 2000 (5.0), year of the release was included in the product title. After Windows 2000, Microsoft created the Windows Server family which continued the year-based style with a difference: For minor releases, Microsoft suffixed "R2" to the title, e.g., Windows Server 2008 R2 (version 6.1). This style had remained consistent to this date. The client versions of Windows however did not adopt a consistent style. First, they received names with arbitrary alphanumeric suffixes as with Windows Me (4.90), Windows XP (5.1), and Windows Vista (6.0). Then, once again Microsoft adopted incremental numbers in the title, but this time, they were not versioning numbers; the version numbers of Windows 7, Windows 8 and Windows 8.1 are respectively 6.1, 6.2 and 6.3. In Windows 10, the version number leaped to 10.0[29] and subsequent updates to the OS only incremented build number and update build revision (UBR) number.
Some software producers use different schemes to denote releases of their software. The Debian project uses a major/minor versioning scheme for releases of its operating system but uses code names from the movie Toy Story during development to refer to stable, unstable, and testing releases.[31]
Urbit uses Kelvin versioning (named after the absolute Kelvin temperature scale): software versions start at a high number and count down to version 0, at which point the software is considered finished and no further modifications are made.[35][36]
Software may have an "internal" version number which differs from the version number shown in the product name (and which typically follows version numbering rules more consistently). Java SE 5.0, for example, has the internal version number of 1.5.0, and versions of Windows from NT 4 on have continued the standard numerical versions internally: Windows 2000 is NT 5.0, XP is Windows NT 5.1, Windows Server 2003 and Windows XP Professional x64 Edition are NT 5.2, Windows Server 2008 and Vista are NT 6.0, Windows Server 2008 R2 and Windows 7 are NT 6.1, Windows Server 2012 and Windows 8 are NT 6.2, and Windows Server 2012 R2 and Windows 8.1 are NT 6.3. Windows 10 was initially intended to be NT 6.4, as the earliest Technical Preview build shared to the public is numbered 6.4.9841. However, that did not last as the version of Windows 10 was quickly artificially increased to 10.0[37] to align with the commercial name, resulting in the first released version of the operating system being numbered 10.0.10240. Note, however, that Windows NT is only on its fifth major revision, as its first release was numbered 3.1 (to match the then-current Windows release number) and the Windows 10 launching made a version leap from 6.3 to 10.0.
In conjunction with the various versioning schemes listed above, a system for denoting pre-release versions is generally used, as the program makes its way through the stages of the software release life cycle.
Programs that are in an early stage are often called "alpha" software, after the first letter in the Greek alphabet. After they mature but are not yet ready for release, they may be called "beta" software, after the second letter in the Greek alphabet. Generally alpha software is tested by developers only, while beta software is distributed for community testing.
Some systems use numerical versions less than 1 (such as 0.9), to suggest their approach toward a final "1.0" release. This is a common convention in open source software.[38][39] However, if the pre-release version is for an existing software package (e.g. version 2.5), then an "a" or "alpha" may be appended to the version number. So the alpha version of the 2.5 release might be identified as 2.5a or 2.5.a.
An alternative is to refer to pre-release versions as "release candidates", so that software packages which are soon to be released as a particular version may carry that version tag followed by "rc-#", indicating the number of the release candidate; when the final version is released, the "rc" tag is removed.
A software release train is a form of software release schedule in which a number of distinct series of versioned software releases for multiple products are released as a number of different "trains" on a regular schedule. Generally, for each product line, a number of different release trains are running at a given time, with each train moving from initial release to eventual maturity and retirement on a planned schedule. Users may experiment with a newer release train before adopting it for production, allowing them to experiment with newer, "raw", releases early, while continuing to follow the previous train's point releases for their production systems prior to moving to the new release train as it becomes mature.
Cisco's IOS software platform used a release train schedule with many distinct trains for many years. More recently, a number of other platforms including Firefox and Fenix for Android,[40] Eclipse,[41] LibreOffice,[42] Ubuntu,[43] Fedora,[44] Python,[45] digiKam[46] and VMware[47] have adopted the release train model.
The same odd-even system is used by some other software with long release cycles, such as Node.js up to version 0.12 as well as WineHQ.[48]
Version numbers very quickly evolve from simple integers (1, 2, ...) to rational numbers (2.08, 2.09, 2.10)
and then to non-numeric "numbers" such as 4:3.4.3-2. These complex version numbers are therefore better treated as character strings. Operating systems that include package management facilities (such as all non-trivial Linux or BSD distributions) will use a distribution-specific algorithm for comparing version numbers of different software packages. For example, the ordering algorithms of Red Hat and derived distributions differ to those of the Debian-like distributions.
As an example of surprising version number ordering implementation behavior, in Debian, leading zeroes are ignored in chunks, so that 5.0005 and 5.5 are considered as equal, and 5.5 < 5.0006. This can confuse users; string-matching tools may fail to find a given version number; and this can cause subtle bugs in package management if the programmers use string-indexed data structures such as version-number indexed hash tables.
To ease sorting, some software packages represent each component of the major.minor.release scheme with a fixed width. Perl represents its version numbers as a floating-point number; for example, Perl's 5.8.7 release can also be represented as 5.008007. This allows a theoretical version of 5.8.10 to be represented as 5.008010. Other software packages pack each segment into a fixed bit width; for example, on Microsoft Windows, version number 6.3.9600.16384 would be represented as hexadecimal 0x0006000325804000. The floating-point scheme breaks down if any segment of the version number exceeds 999; a packed-binary scheme employing 16 bits apiece breaks down after 65535.
The free-software and open source communities tend to release software early and often. Initial versions are numbers less than 1, with these 0.x version used to convey that the software is incomplete and not reliable enough for general release or usable in its current state. Backward-incompatible changes are common with 0.x versions.
Version 1.0 is used as a major milestone, indicating that the software has at least all major features plus functions the developers wanted to get into that version, and is considered reliable enough for general release.[38][39] A good example of this is the Linux kernel, which was first released as version 0.01 in 1991,[50] and took until 1994 to reach version 1.0.0.[51]
Since the internet has become widespread, most commercial software vendors no longer follow the maxim that a major version should be "complete" and instead rely on patches with bugfixes to sort out the known issues which a solution has been found for and could be fixed.[citation needed]
A relatively common practice is to make major jumps in version numbers for marketing reasons. Sometimes software vendors just bypass the 1.0 release or quickly release a release with a subsequent version number because 1.0 software is considered by many customers too immature to trust with production deployments.[citation needed] For example, as in the case of dBase II, a product is launched with a version number that implies that it is more mature than it is.
Version numbers are used in practical terms by the consumer, or client, to identify or compare their copy of the software product against another copy, such as the newest version released by the developer. For the programmer or company, versioning is often used on a revision-by-revision basis, where individual parts of the software are compared and contrasted with newer or older revisions of those same parts, often in a collaborative version control system.
In the 21st century, more programmers started to use a formalized version policy, such as the semantic versioning policy.[1] The purpose of such policies is to make it easier for other programmers to know when code changes are likely to break things they have written. Such policies are especially important for software libraries and frameworks, but may also be very useful for command-line applications (which may be called from other applications) and for other applications (which may be scripted and/or extended by third parties).
Versioning is also a required practice to enable many schemes of patching and upgrading software, especially to automatically decide what and where to upgrade to.
Versioning amongst documents is relatively similar to the routine used with computers and software engineering, where with each small change in the structure, contents, or conditions, the version number is incremented by 1, or a smaller or larger value, again depending on the personal preference of the author and the size or importance of changes made.
Technical drawing and CAD software files may also use some kind of primitive versioning number to keep track of changes.
Modular programming is a software design technique that emphasizes separating the functionality of a program into independent, interchangeable modules, such that each contains everything necessary to execute only one aspect or "concern" of the desired functionality.
A module interface expresses the elements that are provided and required by the module. The elements defined in the interface are detectable by other modules. The implementation contains the working code that corresponds to the elements declared in the interface. Modular programming is closely related to structured programming and object-oriented programming, all having the same goal of facilitating construction of large software programs and systems by decomposition into smaller pieces, and all originating around the 1960s. While the historical usage of these terms has been inconsistent, "modular programming" now refers to the high-level decomposition of the code of an entire program into pieces: structured programming to the low-level code use of structured control flow, and object-oriented programming to the data use of objects, a kind of data structure.
In object-oriented programming, the use of interfaces as an architectural pattern to construct modules is known as interface-based programming.[citation needed]
Modular programming, in the form of subsystems (particularly for I/O) and software libraries, dates to early software systems, where it was used for code reuse. Modular programming per se, with a goal of modularity, developed in the late 1960s and 1970s, as a larger-scale analog of the concept of structured programming (1960s). The term "modular programming" dates at least to the National Symposium on Modular Programming, organized at the Information and Systems Institute in July 1968 by Larry Constantine; other key concepts were information hiding (1972) and separation of concerns (SoC, 1974).
Modules were not included in the original specification for ALGOL 68 (1968), but were included as extensions in early implementations, ALGOL 68-R (1970) and ALGOL 68C (1970), and later formalized.[1] One of the first languages designed from the start for modular programming was the short-lived Modula (1975), by Niklaus Wirth. Another early modular language was Mesa (1970s), by Xerox PARC, and Wirth drew on Mesa as well as the original Modula in its successor, Modula-2 (1978), which influenced later languages, particularly through its successor, Modula-3 (1980s). Modula's use of dot-qualified names, like M.a to refer to object a from module M, coincides with notation to access a field of a record (and similarly for attributes or methods of objects), and is now widespread, seen in C++, C#, Dart, Go, Java, OCaml, and Python, among others. Modular programming became widespread from the 1980s: the original Pascal language (1970) did not include modules, but later versions, notably UCSD Pascal (1978) and Turbo Pascal (1983) included them in the form of "units", as did the Pascal-influenced Ada (1980). The Extended Pascal ISO 10206:1990 standard kept closer to Modula2 in its modular support. Standard ML (1984)[2] has one of the most complete module systems, including functors (parameterized modules) to map between modules.
In the 1980s and 1990s, modular programming was overshadowed by and often conflated with object-oriented programming, particularly due to the popularity of C++ and Java. For example, the C family of languages had support for objects and classes in C++ (originally C with Classes, 1980) and Objective-C (1983), only supporting modules 30 years or more later. Java (1995) supports modules in the form of packages, though the primary unit of code organization is a class. However, Python (1991) prominently used both modules and objects from the start, using modules as the primary unit of code organization and "packages" as a larger-scale unit; and Perl 5 (1994) includes support for both modules and objects, with a vast array of modules being available from CPAN (1993). OCaml (1996) followed ML by supporting modules and functors.
Modular programming is now widespread, and found in virtually all major languages developed since the 1990s. The relative importance of modules varies between languages, and in class-based object-oriented languages there is still overlap and confusion with classes as a unit of organization and encapsulation, but these are both well-established as distinct concepts.
Furthermore, the term "package" has other uses in software (for example .NET NuGet packages). A component is a similar concept, but typically refers to a higher level; a component is a piece of a whole system, while a module is a piece of an individual program. The scale of the term "module" varies significantly between languages; in Python it is very small-scale and each file is a module, while in Java 9 it is planned to be large-scale, where a module is a collection of packages, which are in turn collections of files.
Modular programming can be performed even where the programming language lacks explicit syntactic features to support named modules, like, for example, in C. This is done by using existing language features, together with, for example, coding conventions, programming idioms and the physical code structure. IBM i also uses modules when programming in the Integrated Language Environment (ILE).
With modular programming, concerns are separated such that modules perform logically discrete functions, interacting through well-defined interfaces. Often modules form a directed acyclic graph (DAG); in this case a cyclic dependency between modules is seen as indicating that these should be a single module. In the case where modules do form a DAG they can be arranged as a hierarchy, where the lowest-level modules are independent, depending on no other modules, and higher-level modules depend on lower-level ones. A particular program or library is a top-level module of its own hierarchy, but can in turn be seen as a lower-level module of a higher-level program, library, or system.
This makes modular designed systems, if built correctly, far more reusable than a traditional monolithic design, since all (or many) of these modules may then be reused (without change) in other projects. This also facilitates the "breaking down" of projects into several smaller projects. Theoretically, a modularized software project will be more easily assembled by large teams, since no team members are creating the whole system, or even need to know about the system as a whole. They can focus just on the assigned smaller task.
In computer programming and software testing, smoke testing (also confidence testing, sanity testing,[1] build verification test (BVT)[2][3][4] and build acceptance test) is preliminary testing or sanity testing to reveal simple failures severe enough to, for example, reject a prospective software release. Smoke tests are a subset of test cases that cover the most important functionality of a component or system, used to aid assessment of whether main functions of the software appear to work correctly.[1][2] When used to determine if a computer program should be subjected to further, more fine-grained testing, a smoke test may be called a pretest[5] or an intake test.[1] Alternatively, it is a set of tests run on each new build of a product to verify that the build is testable before the build is released into the hands of the test team.[6] In the DevOps paradigm, use of a build verification test step is one hallmark of the continuous integration maturity stage.[7]
For example, a smoke test may address basic questions like "does the program run?", "does the user interface open?", or "does clicking the main button do anything?" The process of smoke testing aims to determine whether the application is so badly broken as to make further immediate testing unnecessary. As the book Lessons Learned in Software Testing[8] puts it, "smoke tests broadly cover product features in a limited time [...] if key features don't work or if key bugs haven't yet been fixed, your team won't waste further time installing or testing".[3]
Smoke testing is also done by testers before accepting a build for further testing. Microsoft claims that after code reviews, "smoke testing is the most cost-effective method for identifying and fixing defects in software".[10]
In software development, the term was metaphorically adopted to describe a preliminary round of testing that checks for basic functionality. Like its physical counterparts, a software smoke test aims to identify critical failures early, ensuring the system is stable and that all required components are functioning before proceeding to more comprehensive testing, such as end-to-end or load testing.
For example, Cem Kaner, James Bach, and Brett Pettichord explain in Lessons Learned in Software Testing:
A software regression is a type of software bug where a feature that has worked before stops working. This may happen after changes are applied to the software's source code, including the addition of new features and bug fixes.[1] They may also be introduced by changes to the environment in which the software is running, such as system upgrades, system patching or a change to daylight saving time.[2] A software performance regression is a situation where the software still functions correctly, but performs more slowly or uses more memory or resources than before.[3] Various types of software regressions have been identified in practice, including the following:[4]
Regressions are often caused by encompassed bug fixes included in software patches. One approach to avoiding this kind of problem is regression testing. A properly designed test plan aims at preventing this possibility before releasing any software.[5] Automated testing and well-written test cases can reduce the likelihood of a regression.
Techniques have been proposed that try to prevent regressions from being introduced into software at various stages of development, as outlined below.
In order to avoid regressions being seen by the end-user after release, developers regularly run regression tests after changes are introduced to the software. These tests can include unit tests to catch local regressions as well as integration tests to catch remote regressions.[6] Regression testing techniques often leverage existing test cases to minimize the effort involved in creating them.[7] However, due to the volume of these existing tests, it is often necessary to select a representative subset, using techniques such as test-case prioritization.
For detecting performance regressions, software performance tests are run on a regular basis, to monitor the response time and resource usage metrics of the software after subsequent changes.[8] Unlike functional regression tests, the results of performance tests are subject to variance - that is, results can differ between tests due to variance in performance measurements; as a result, a decision must be made on whether a change in performance numbers constitutes a regression, based on experience and end-user demands. Approaches such as statistical significance testing and change point detection are sometimes used to aid in this decision.[9]
Since debugging and localizing the root cause of a software regression can be expensive,[10][11] there also exist some methods that try to prevent regressions from being committed into the code repository in the first place. For example, Git Hooks enable developers to run test scripts before code changes are committed or pushed to the code repository.[12] In addition, change impact analysis has been applied to software to predict the impact of a code change on various components of the program, and to supplement test case selection and prioritization.[13][14] Software linters are also often added to commit hooks to ensure consistent coding style, thereby minimizing stylistic issues that can make the software prone to regressions.[15]
Many of the techniques used to find the root cause of non-regression software bugs can also be used to debug software regressions, including breakpoint debugging, print debugging, and program slicing. The techniques described below are often used specifically to debug software regressions.
Profiling measures the performance and resource usage of various components of a program, and is used to generate data useful in debugging performance issues. In the context of software performance regressions, developers often compare the call trees (also known as "timelines") generated by profilers for both the buggy version and the previously working version, and mechanisms exist to simplify this comparison.[22] Web development tools typically provide developers the ability to record these performance profiles.[23][24]
Logging also helps with performance regression localization, and similar to call trees, developers can compare systematically-placed performance logs of multiple versions of the same software.[25] A tradeoff exists when adding these performance logs, as adding many logs can help developers pinpoint which portions of the software are regressing at smaller granularities, while adding only a few logs will also reduce overhead when executing the program.[26]
The software release life cycle is the process of developing, testing, and distributing a software product (e.g., an operating system). It typically consists of several stages, such as pre-alpha, alpha, beta, and release candidate, before the final version, or "gold", is released to the public.
Pre-alpha refers to the early stages of development, when the software is still being designed and built. Alpha testing is the first phase of formal testing, during which the software is tested internally using white-box techniques. Beta testing is the next phase, in which the software is tested by a larger group of users, typically outside of the organization that developed it. The beta phase is focused on reducing impacts on users and may include usability testing.
After beta testing, the software may go through one or more release candidate phases, in which it is refined and tested further, before the final version is released.
Some software, particularly in the internet and technology industries, is released in a perpetual beta state, meaning that it is continuously being updated and improved, and is never considered to be a fully completed product. This approach allows for a more agile development process and enables the software to be released and used by users earlier in the development cycle.
Pre-alpha refers to all activities performed during the software project before formal testing. These activities can include requirements analysis, software design, software development, and unit testing. In typical open source development, there are several types of pre-alpha versions. Milestone versions include specific sets of functions and are released as soon as the feature is complete.[citation needed]
The alpha phase of the release life cycle is the first phase of software testing (alpha is the first letter of the Greek alphabet, used as the number 1). In this phase, developers generally test the software using white-box techniques. Additional validation is then performed using black-box or gray-box techniques, by another testing team. Moving to black-box testing inside the organization is known as alpha release.[1][2]
Alpha software is not thoroughly tested by the developer before it is released to customers. Alpha software may contain serious errors, and any resulting instability could cause crashes or data loss.[3] Alpha software may not contain all of the features that are planned for the final version.[4] In general, external availability of alpha software is uncommon for proprietary software, while open source software often has publicly available alpha versions. The alpha phase usually ends with a feature freeze, indicating that no more features will be added to the software. At this time, the software is said to be feature-complete. A beta test is carried out following acceptance testing at the supplier's site (the alpha test) and immediately before the general release of the software as a product.[5]
A feature-complete (FC) version of a piece of software has all of its planned or primary features implemented but is not yet final due to bugs, performance or stability issues.[6] This occurs at the end of alpha testing in development.
Usually, feature-complete software still has to undergo beta testing and bug fixing, as well as performance or stability enhancement before it can go to release candidate, and finally gold status.
Beta, named after the second letter of the Greek alphabet, is the software development phase following alpha. A beta phase generally begins when the software is feature-complete but likely to contain several known or unknown bugs.[7] Software in the beta phase will generally have many more bugs in it than completed software and speed or performance issues, and may still cause crashes or data loss. The focus of beta testing is reducing impacts on users, often incorporating usability testing. The process of delivering a beta version to the users is called beta release and is typically the first time that the software is available outside of the organization that developed it. Software beta releases can be either open or closed, depending on whether they are openly available or only available to a limited audience. Beta version software is often useful for demonstrations and previews within an organization and to prospective customers. Some developers refer to this stage as a preview, preview release, prototype, technical preview or technology preview (TP),[8] or early access.
Beta testers are people who actively report issues with beta software. They are usually customers or representatives of prospective customers of the organization that develops the software. Beta testers tend to volunteer their services free of charge but often receive versions of the product they test, discounts on the release version, or other incentives.[9][10]
Some software is kept in so-called perpetual beta, where new features are continually added to the software without establishing a final "stable" release. As the Internet has facilitated the rapid and inexpensive distribution of software, companies have begun to take a looser approach to the use of the word beta.[11]
Developers may release either a closed beta, or an open beta; closed beta versions are released to a restricted group of individuals for a user test by invitation, while open beta testers are from a larger group, or anyone interested. Private beta could be suitable for the software that is capable of delivering value but is not ready to be used by everyone either due to scaling issues, lack of documentation or still missing vital features. The testers report any bugs that they find, and sometimes suggest additional features they think should be available in the final version.
Some software products (e.g. Linux distributions like Debian) also have long-term support (LTS) releases which are based on full releases that have already been tried and tested and receive only security updates.[citation needed]
Once released, the software is generally known as a "stable release". The formal term often depends on the method of release: physical media, online release, or a web application.[12]
The term "release to manufacturing" (RTM), also known as "going gold", is a term used when a software product is ready to be delivered. This build may be digitally signed, allowing the end user to verify the integrity and authenticity of the software purchase. The RTM build is known as the "gold master" or GM[13] is sent for mass duplication or disc replication if applicable. The terminology is taken from the audio record-making industry, specifically the process of mastering. RTM precedes general availability (GA) when the product is released to the public. A golden master build (GM) is typically the final build of a piece of software in the beta stages for developers. Typically, for iOS, it is the final build before a major release, however, there have been a few exceptions.
RTM is typically used in certain retail mass-production software contexts—as opposed to a specialized software production or project in a commercial or government production and distribution—where the software is sold as part of a bundle in a related computer hardware sale and typically where the software and related hardware is ultimately to be available and sold on mass/public basis at retail stores to indicate that the software has met a defined quality level and is ready for mass retail distribution. RTM could also mean in other contexts that the software has been delivered or released to a client or customer for installation or distribution to the related hardware end user computers or machines. The term does not define the delivery mechanism or volume; it only states that the quality is sufficient for mass distribution. The deliverable from the engineering organization is frequently in the form of a golden master media used for duplication or to produce the image for the web.
General availability (GA) is the marketing stage at which all necessary commercialization activities have been completed and a software product is available for purchase, depending, however, on language, region, and electronic vs. media availability.[14] Commercialization activities could include security and compliance tests, as well as localization and worldwide availability. The time between RTM and GA can take from days to months before a generally available release can be declared, due to the time needed to complete all commercialization activities required by GA. At this stage, the software has "gone live".
Release to the Web (RTW) or Web release is a means of software delivery that utilizes the Internet for distribution. No physical media are produced in this type of release mechanism by the manufacturer. Web releases have become more common as Internet usage has grown.[citation needed]
During its supported lifetime, the software is sometimes subjected to service releases, patches or service packs, sometimes also called "interim releases" or "maintenance releases" (MR). For example, Microsoft released three major service packs for the 32-bit editions of Windows XP and two service packs for the 64-bit editions.[15] Such service releases contain a collection of updates, fixes, and enhancements, delivered in the form of a single installable package. They may also implement new features. Some software is released with the expectation of regular support. Classes of software that generally involve protracted support as the norm include anti-virus suites and massively multiplayer online games. Continuing with this Windows XP example, Microsoft did offer paid updates for five more years after the end of extended support. This means that support ended on April 8, 2019.[16]
When software is no longer sold or supported, the product is said to have reached end-of-life, to be discontinued, retired, deprecated, abandoned, or obsolete, but user loyalty may continue its existence for some time, even long after its platform is obsolete—e.g., the Common Desktop Environment[17] and Sinclair ZX Spectrum.[18]
Usage of the "alpha/beta" test terminology originated at IBM.[citation needed] Similar terminologies for IBM's software development were used by people involved with IBM from at least the 1950s (and probably earlier). "A" test was the verification of a new product before the public announcement. The "B" test was the verification before releasing the product to be manufactured. The "C" test was the final test before the general availability of the product. As software became a significant part of IBM's offerings, the alpha test terminology was used to denote the pre-announcement test and the beta test was used to show product readiness for general availability. Martin Belsky, a manager on some of IBM's earlier software projects claimed to have invented the terminology. IBM dropped the alpha/beta terminology during the 1960s, but by then it had received fairly wide notice. The usage of "beta test" to refer to testing done by customers was not done in IBM. Rather, IBM used the term "field test".
Major public betas developed afterward, with early customers having purchased a "pioneer edition" of the WordVision word processor for the IBM PC for $49.95. In 1984, Stephen Manes wrote that "in a brilliant marketing coup, Bruce and James Program Publishers managed to get people to pay for the privilege of testing the product."[19] In September 2000, a boxed version of Apple's Mac OS X Public Beta operating system was released.[20] Between September 2005 and May 2006, Microsoft released community technology previews (CTPs) for Windows Vista.[21] From 2009 to 2011, Minecraft was in public beta.
In February 2005, ZDNet published an article about the phenomenon of a beta version often staying for years and being used as if it were at the production level.[22] It noted that Gmail and Google News, for example, had been in beta for a long time although widely used; Google News left beta in January 2006, followed by Google Apps (now named Google Workspace), including Gmail, in July 2009.[12] Since the introduction of Windows 8, Microsoft has called pre-release software a preview rather than beta. All pre-release builds released through the Windows Insider Program launched in 2014 are termed "Insider Preview builds". "Beta" may also indicate something more like a release candidate, or as a form of time-limited demo, or marketing technique.[23]
In computer science, fault injection is a testing technique for understanding how computing systems behave when stressed in unusual ways. This can be achieved using physical- or software-based means, or using a hybrid approach.[1] Widely studied physical fault injections include the application of high voltages, extreme temperatures and electromagnetic pulses on electronic components, such as computer memory and central processing units.[2][3] By exposing components to conditions beyond their intended operating limits, computing systems can be coerced into mis-executing instructions and corrupting critical data.
In software testing, fault injection is a technique for improving the coverage of a test by introducing faults to test code paths; in particular error handling code paths, that might otherwise rarely be followed. It is often used with stress testing and is widely considered to be an important part of developing robust software.[4] Robustness testing[5] (also known as syntax testing, fuzzing or fuzz testing) is a type of fault injection commonly used to test for vulnerabilities in communication interfaces such as protocols, command line parameters, or APIs.
The technique of fault injection dates back to the 1970s[7] when it was first used to induce faults at a hardware level. This type of fault injection is called Hardware Implemented Fault Injection (HWIFI) and attempts to simulate hardware failures within a system. The first experiments in hardware fault involved nothing more than shorting connections on circuit boards and observing the effect on the system (bridging faults). It was used primarily as a test of the dependability of the hardware system. Later specialised hardware was developed to extend this technique, such as devices to bombard specific areas of a circuit board with heavy radiation. It was soon found that faults could be induced by software techniques and that aspects of this technique could be useful for assessing software systems. Collectively these techniques are known as Software Implemented Fault Injection (SWIFI).
Runtime Injection techniques use a software trigger to inject a fault into a running software system. Faults can be injected via a number of physical methods and triggers can be implemented in a number of ways, such as: Time Based triggers (When the timer reaches a specified time an interrupt is generated and the interrupt handler associated with the timer can inject the fault. ); Interrupt Based Triggers (Hardware exceptions and software trap mechanisms are used to generate an interrupt at a specific place in the system code or on a particular event within the system, for instance, access to a specific memory location).
Complex software systems, especially multi-vendor distributed systems based on open standards, perform input/output operations to exchange data via stateful, structured exchanges known as "protocols." One kind of fault injection that is particularly useful to test protocol implementations (a type of software code that has the unusual characteristic in that it cannot predict or control its input) is fuzzing. Fuzzing is an especially useful form of Black-box testing since the various invalid inputs that are submitted to the software system do not depend on, and are not created based on knowledge of, the details of the code running inside the system.
These parameters create the fault space realm. The fault space realm will increase exponentially by increasing system complexity. Therefore, the traditional fault injection method will not be applicable to use in the modern cyber-physical systems, because they will be so slow, and they will find a small number of faults (less fault coverage). Hence, the testers need an efficient algorithm to choose critical faults that have a higher impact on system behavior. Thus, the main research question is how to find critical faults in the fault space realm which have catastrophic effects on system behavior. Here are some methods that can aid fault injection to efficiently explore the fault space to reach higher fault coverage in less simulation time.
Fault injection can take many forms. In the testing of operating systems for example, fault injection is often performed by a driver (kernel-mode software) that intercepts system calls (calls into the kernel) and randomly returning a failure for some of the calls.  This type of fault injection is useful for testing low-level user-mode software.  For higher level software, various methods inject faults.  In managed code, it is common to use instrumentation. Although fault injection can be undertaken by hand, a number of fault injection tools exist to automate the process of fault injection.[30]
Depending on the complexity of the API for the level where faults are injected, fault injection tests often must be carefully designed to minimize the number of false positives.  Even a well designed fault injection test can sometimes produce situations that are impossible in the normal operation of the software.  For example, imagine there are two API functions, Commit and PrepareForCommit, such that alone, each of these functions can possibly fail, but if PrepareForCommit is called and succeeds, a subsequent call to Commit is guaranteed to succeed.  Now consider the following code:
In software quality assurance, performance testing is in general a testing practice performed to determine how a system performs in terms of responsiveness and stability under a particular workload.[1] It can also serve to investigate, measure, validate or verify other quality attributes of the system, such as scalability, reliability and resource usage.
Performance testing, a subset of performance engineering, is a computer science practice which strives to build performance standards into the implementation, design and architecture of a system.
Load testing is the simplest form of performance testing. A load test is usually conducted to understand the behavior of the system under a specific expected load. This load can be the expected concurrent number of users on the application performing a specific number of transactions within the set duration. This test will give out the response times of all the important business critical transactions. The database, application server, etc. are also monitored during the test, this will assist in identifying bottlenecks in the application software and the hardware that the software is installed on
However, performance testing is frequently not performed against a specification; e.g., no one will have expressed what the maximum acceptable response time for a given population of users should be. Performance testing is frequently used as part of the process of performance profile tuning. The idea is to identify the "weakest link" – there is inevitably a part of the system which, if it is made to respond faster, will result in the overall system running faster. It is sometimes a difficult task to identify which part of the system represents this critical path, and some test tools include (or can have add-ons that provide) instrumentation that runs on the server (agents) and reports transaction times, database access times, network overhead, and other server monitors, which can be analyzed together with the raw performance statistics. Without such instrumentation one might have to have someone crouched over Windows Task Manager at the server to see how much CPU load the performance tests are generating (assuming a Windows system is under test).
As a first step, the patterns generated by these 4 parameters provide a good indication on where the bottleneck lies. To determine the exact root cause of the issue, software engineers use tools such as profilers to measure what parts of a device or software contribute most to the poor performance, or to establish throughput levels (and thresholds) for maintained acceptable response time.
Performance testing technology employs one or more PCs or Unix servers to act as injectors, each emulating the presence of numbers of users and each running an automated sequence of interactions (recorded as a script, or as a series of scripts to emulate different types of user interaction) with the host whose performance is being tested. Usually, a separate PC acts as a test conductor, coordinating and gathering metrics from each of the injectors and collating performance data for reporting purposes. The usual sequence is to ramp up the load: to start with a few virtual users and increase the number over time to a predetermined maximum. The test result shows how the performance varies with the load, given as number of users vs. response time. Various tools are available to perform such tests. Tools in this category usually execute a suite of tests which emulate real users against the system. Sometimes the results can reveal oddities, e.g., that while the average response time might be acceptable, there are outliers of a few key transactions that take considerably longer to complete – something that might be caused by inefficient database queries, pictures, etc.
The term load testing[1] or stress testing is used in different ways in the professional software testing community. Load testing generally refers to the practice of modeling the expected usage of a software program by simulating multiple users accessing the program concurrently.[2] As such, this testing is most relevant for multi-user systems; often one built using a client/server model, such as web servers. However, other types of software systems can also be load tested. For example, a word processor or graphics editor can be forced to read an extremely large document; or a financial package can be forced to generate a report based on several years' worth of data. The most accurate load testing simulates actual use, as opposed to testing using theoretical or analytical modeling.
Load testing lets you measure your website's quality of service (QOS) performance based on actual customer behavior. Nearly all the load testing tools and frameworks follow the classical load testing paradigm: when customers visit your website, a script recorder records the communication and then creates related interaction scripts. A load generator tries to replay the recorded scripts, which could possibly be modified with different test parameters before replay. In the replay procedure, both the hardware and software statistics will be monitored and collected by the conductor, these statistics include the CPU, memory, disk IO of the physical servers and the response time, the throughput of the system under test (SUT), etc. And at last, all these statistics will be analyzed and a load testing report will be generated.
Load and performance testing analyzes software intended for a multi-user audience by subjecting the software to different numbers of virtual and live users while monitoring performance measurements under these different loads. Load and performance testing is usually conducted in a test environment identical to the production environment before the software system is permitted to go live.
A common misconception is that load testing software provides record and playback capabilities like regression testing tools. Load testing tools analyze the entire OSI protocol stack whereas most regression testing tools focus on GUI performance. For example, a regression testing tool will record and playback a mouse click on a button on a web browser, but a load testing tool will send out hypertext the web browser sends after the user clicks the button. In a multiple-user environment, load testing tools can send out hypertext for multiple users with each user having a unique login ID, password, etc.
Real-time computing (RTC) is the computer science term for hardware and software systems subject to a "real-time constraint", for example from event to system response.[1] Real-time programs must guarantee response within specified time constraints, often referred to as "deadlines".[2]
Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems (RTOSes), and real-time networks. Each of these provide essential frameworks on which to build a real-time software application.
Minicomputers, particularly in the 1970s onwards, when built into dedicated embedded systems such as DOG (Digital on-screen graphic) scanners, increased the need for low-latency priority-driven responses to important interactions with incoming data. Operating systems such as Data General's RDOS (Real-Time Disk Operating System) and RTOS with background and foreground scheduling as well as Digital Equipment Corporation's RT-11 date from this era. Background-foreground scheduling allowed low priority tasks CPU time when no foreground task needed to execute, and gave absolute priority within the foreground to threads/tasks with the highest priority. Real-time operating systems would also be used for time-sharing multiuser duties. For example, Data General Business Basic could run in the foreground or background of RDOS and would introduce additional elements to the scheduling algorithm to make it more appropriate for people interacting via dumb terminals.
Early personal computers were sometimes used for real-time computing. The possibility of deactivating other interrupts allowed for hard-coded loops with defined timing, and the low interrupt latency allowed the implementation of a real-time operating system, giving the user interface and the disk drives lower priority than the real-time thread. Compared to these the programmable interrupt controller of the Intel CPUs (8086..80586) generates a very large latency and the Windows operating system is neither a real-time operating system nor does it allow a program to take over the CPU completely and use its own scheduler, without using native machine language and thus bypassing all interrupting Windows code. However, several coding libraries exist which offer real time capabilities in a high level language on a variety of operating systems, for example Java Real Time. Later microprocessors such as the Motorola 68000 and subsequent family members (68010, 68020, ColdFire etc.) also became popular with manufacturers of industrial control systems. This application area is one where real-time control offers genuine advantages in terms of process performance and safety.[citation needed]
In the context of multitasking systems the scheduling policy is normally priority driven (pre-emptive schedulers). In some situations, these can guarantee hard real-time performance (for instance if the set of tasks and their priorities is known in advance). There are other hard real-time schedulers such as rate-monotonic which is not common in general-purpose systems, as it requires additional information in order to schedule a task: namely a bound or worst-case estimate for how long the task must execute. Specific algorithms for scheduling such hard real-time tasks exist, like earliest deadline first, which, ignoring the overhead of context switching, is sufficient for system loads of less than 100%.[7] New overlay scheduling systems, such as an adaptive partition scheduler assist in managing large systems with a mixture of hard real-time and non real-time applications.
A signal processing algorithm that cannot keep up with the flow of input data with output falling further and further behind the input, is not real-time. If the delay of the output (relative to the input) is bounded regarding a process which operates over an unlimited time, then that signal processing algorithm is real-time, even if the throughput delay may be very long.
Real-time computing is sometimes misunderstood to be high-performance computing, but this is not an accurate classification.[11] For example, a massive supercomputer executing a scientific simulation may offer impressive performance, yet it is not executing a real-time computation. Conversely, once the hardware and software for an anti-lock braking system have been designed to meet its required deadlines, no further performance gains are obligatory or even useful. Furthermore, if a network server is highly loaded with network traffic, its response time may be slower, but will (in most cases) still succeed before it times out (hits its deadline). Hence, such a network server would not be considered a real-time system: temporal failures (delays, time-outs, etc.) are typically small and compartmentalized (limited in effect), but are not catastrophic failures. In a real-time system, such as the FTSE 100 Index, a slow-down beyond limits would often be considered catastrophic in its application context. The most important requirement of a real-time system is consistent output, not high throughput.
Some kinds of software, such as many chess-playing programs, can fall into either category. For instance, a chess program designed to play in a tournament with a clock will need to decide on a move before a certain deadline or lose the game, and is therefore a real-time computation, but a chess program that is allowed to run indefinitely before moving is not. In both of these cases, however, high performance is desirable: the more work a tournament chess program can do in the allotted time, the better its moves will be, and the faster an unconstrained chess program runs, the sooner it will be able to move. This example also illustrates the essential difference between real-time computations and other computations: if the tournament chess program does not make a decision about its next move in its allotted time it loses the game—i.e., it fails as a real-time computation—while in the other scenario, meeting the deadline is assumed not to be necessary. High-performance is indicative of the amount of processing that is performed in a given amount of time, whereas real-time is the ability to get done with the processing to yield a useful output in the available time.
Near real-time also refers to delayed real-time transmission of voice and video. It allows playing video images, in approximately real-time, without having to wait for an entire large video file to download. Incompatible databases can export/import to common flat files that the other database can import/export on a scheduled basis so they can sync/share common data in "near real-time" with each other.
Property testing is a field of theoretical computer science, concerned with the design of super-fast algorithms for approximate decision making, where the decision refers to properties or parameters of huge objects.[1]
A property testing algorithm for a decision problem is an algorithm whose query complexity (the number of queries made to its input) is much smaller than the instance size of the problem.  Typically, property testing algorithms are used to determine whether some combinatorial structure S (such as a graph or a boolean function) satisfies some property P, or is "far" from having this property (meaning that an ε-fraction of the representation of S must be modified to make S satisfy P), using only a small number of "local" queries to the object. [2][3]
For example, the following promise problem admits an algorithm whose query complexity is independent of the instance size (for an arbitrary constant ε > 0):
Property testing algorithms are central to the definition of probabilistically checkable proofs, as a probabilistically checkable proof is essentially a proof that can be verified by a property testing algorithm.
Formally, a property testing algorithm with query complexity q(n) and proximity parameter ε for a decision problem L is a randomized algorithm that, on input x (an instance of L) makes at most q(|x|) queries to x and behaves as follows:
A property testing algorithm is said to have one-sided error if it satisfies the stronger condition that the accepting probability for instances x ∈ L is 1 instead of 2/3.
A property testing algorithm is said be non-adaptive if it performs all its queries before it "observes" any answers to previous queries. Such an algorithm can be viewed as operating in the following manner.  First the algorithm receives its input.  Before looking at the input, using its internal randomness, the algorithm decides which symbols of the input are to be queried.  Next, the algorithm observes these symbols.  Finally, without making any additional queries (but possibly using its randomness), the algorithm decides whether to accept or reject the input. [2]
The main efficiency parameter of a property testing algorithm is its query complexity, which is the maximum number of input symbols inspected over all inputs of a given length (and all random choices made by the algorithm).  Computer scientists are interested in designing algorithms whose query complexity is as small as possible. In many cases, the running time of property testing algorithms is sublinear in the instance length.  Typically, the goal is first to make the query complexity as small as possible as a function of the instance size n, and then study the dependency on the proximity parameter ε.
Unlike other complexity-theoretic settings, the asymptotic query complexity of property testing algorithms is affected dramatically by the representation of instances.  For example, when ε = 0.01, the problem of testing bipartiteness of dense graphs (which are represented by their adjacency matrix) admits an algorithm of constant query complexity. In contrast, sparse graphs on n vertices (which are represented by their adjacency list) require property testing algorithms of query complexity Ω(n1/2).
The query complexity of property testing algorithms grows as the proximity parameter ε becomes smaller for all non-trivial properties.  This dependence on ε is necessary, as a change of fewer than ε symbols in the input cannot be detected with constant probability using fewer than O(1/ε) queries.  Many interesting properties of dense graphs can be tested using query complexity that depends only on ε and not on the graph size n.  However, the query complexity can grow enormously fast as a function of ε.  For example, for a long time, the best known algorithm for testing whether a graph does not contain any triangle had a query complexity which is a tower function of poly(1/ε), and only in 2010 was this improved to a tower function of log(1/ε).  One of the reasons for this enormous growth in bounds is that many of the positive results for property testing of graphs are established using the Szemerédi regularity lemma, which also has tower-type bounds in its conclusions. The connection of property testing to the Szemerédi regularity lemma and related graph removal lemmas is elaborated on below.
The field of graph property testing was first introduced by Goldreich, Goldwasser, and Ron. In their seminal paper published in 1998, an abstract graph partition problem is analyzed and some testers provided. These include as special cases several important graph properties like bipartiteness, k-colorability, having a large clique, and having a large cut. [4] In particular, the natural algorithms that sample a subgraph and check whether it satisfy the property are all correct, albeit with perhaps-suboptimal query complexities.
Informally, an oblivious tester is oblivious to the size of the input. For a graph property P, it is an algorithm that takes as input a parameter ε and graph G, and then runs as a property testing algorithm on G for the property P with proximity parameter ε that makes exactly q(ε) queries to G.
Crucially, the number of queries an oblivious tester makes is a constant dependent only on ε and not the size of the input graph G. In complete analogy with property testing algorithms, we can talk about oblivious testers with one-sided error.
In this section, we will give some natural oblivious testing algorithms with one-sided error for triangle-freeness, bipartiteness, and k-colorability. They are natural in the sense that we follow the natural idea of randomly sampling some subset X of vertices of G and checking whether the graph property holds on the subgraph spanned by X by brute-force search. We have one-sided error since these properties are actually hereditary: if G satisfy the property, so must the induced subgraph spanned by X, so our tester always accepts.
For bipartiteness and k-colorability, let δ be the desired upper bound on error probability for the following testers. Note that query complexity should not be confused with running time. The latter is often exponential (as is the case of both) due to a lack of polynomial time decision algorithm to test the property on the induced subgraph. We instead check by brute-force search. [4]
TDD is related to the test-first programming concepts of extreme programming, begun in 1999,[2] but more recently has created more general interest in its own right.[3]
Software engineer Kent Beck, who is credited with having developed or "rediscovered"[5] the technique, stated in 2003 that TDD encourages simple designs and inspires confidence.[6]
The original description of TDD was in an ancient book about programming. It said you take the input tape, manually type in the output tape you expect, then program until the actual output tape matches the expected output. After I'd written the first xUnit framework in Smalltalk I remembered reading this and tried it out. That was the origin of TDD for me. When describing TDD to older programmers, I often hear, "Of course. How else could you program?" Therefore I refer to my role as "rediscovering" TDD.
When using external libraries, it is important not to write tests that are so small as to effectively test merely the library itself,[3] unless there is some reason to believe that the library is buggy or not feature-rich enough to serve all the needs of the software under development.
TDD has been adopted outside of software development, in both product and service teams, as test-driven work.[9] For testing to be successful, it needs to be practiced at the micro and macro levels. Every method in a class, every input data value, log message, and error code, amongst other data points, need to be tested.[10] Similar to TDD, non-software teams develop quality control (QC) checks (usually manual tests rather than automated tests) for each aspect of the work prior to commencing. These QC checks are then used to inform the design and validate the associated outcomes. The six steps of the TDD sequence are applied with minor semantic changes:
Writing the tests first: The tests should be written before the functionality that is to be tested. This has been claimed to have many benefits. It helps ensure that the application is written for testability, as the developers must consider how to test the application from the outset rather than adding it later. It also ensures that tests for every feature gets written. Additionally, writing the tests first leads to a deeper and earlier understanding of the product requirements, ensures the effectiveness of the test code, and maintains a continual focus on software quality.[11] When writing feature-first code, there is a tendency by developers and organizations to push the developer on to the next feature, even neglecting testing entirely. The first TDD test might not even compile at first, because the classes and methods it requires may not yet exist. Nevertheless, that first test functions as the beginning of an executable specification.[12]
In object oriented design this still does not provide access to private data and methods. Therefore, extra work may be necessary for unit tests. In Java and other languages, a developer can use reflection to access private fields and methods.[13] Alternatively, an inner class can be used to hold the unit tests so they have visibility of the enclosing class's members and attributes. In the .NET Framework and some other programming languages, partial classes may be used to expose private methods and data for the tests to access.
When code under development relies on a database, a web service, or any other external process or service, enforcing a unit-testable separation is also an opportunity and a driving force to design more modular, more testable and more reusable code.[17] Two steps are necessary:
A corollary of such dependency injection is that the actual database or other external-access code is never tested by the TDD process itself. To avoid errors that may arise from this, other tests are needed that instantiate the test-driven code with the "real" implementations of the interfaces discussed above. These are integration tests and are quite separate from the TDD unit tests. There are fewer of them, and they must be run less often than the unit tests. They can nonetheless be implemented using the same testing framework.
Integration tests that alter any persistent store or database should always be designed carefully with consideration of the initial and final state of the files or database, even if any test fails. This is often achieved using some combination of the following techniques:
Advanced practices of test-driven development can lead to acceptance test–driven development (ATDD) and specification by example where the criteria specified by the customer are automated into acceptance tests, which then drive the traditional unit test-driven development (UTDD) process.[18] This process ensures the customer has an automated mechanism to decide whether the software meets their requirements. With ATDD, the development team now has a specific target to satisfy – the acceptance tests – which keeps them continuously focused on what the customer really wants from each user story.
Some best practices that an individual could follow would be to separate common set-up and tear-down logic into test support services utilized by the appropriate test cases, to keep each test oracle focused on only the results necessary to validate its test, and to design time-related tests to allow tolerance for execution in non-real time operating systems. The common practice of allowing a 5-10 percent margin for late execution reduces the potential number of false negatives in test execution. It is also suggested to treat test code with the same respect as production code. Test code must work correctly for both positive and negative cases, last a long time, and be readable and maintainable. Teams can get together and review tests and test practices to share effective techniques and catch bad habits.[19]
In a larger system, the impact of poor component quality is magnified by the complexity of interactions. This magnification makes the benefits of TDD accrue even faster in the context of larger projects. However, the complexity of the total population of tests can become a problem in itself, eroding potential gains. It sounds simple, but a key initial step is to recognize that test code is also important software and should be produced and maintained with the same rigor as the production code.
Creating and managing the architecture of test software within a complex system is just as important as the core product architecture. Test drivers interact with the UUT, test doubles and the unit test framework.[11]
Test Driven Development (TDD) is a software development approach where tests are written before the actual code. It offers several advantages:
Test-driven development offers the ability to take small steps when required. It allows a programmer to focus on the task at hand as the first goal is to make the test pass. Exceptional cases and error handling are not considered initially, and tests to create these extraneous circumstances are implemented separately. Test-driven development ensures in this way that all written code is covered by at least one test. This gives the programming team, and subsequent users, a greater level of confidence in the code.
TDD can lead to more modularized, flexible, and extensible code. This effect often comes about because the methodology requires that the developers think of the software in terms of small units that can be written and tested independently and integrated together later. This leads to smaller, more focused classes, looser coupling, and cleaner interfaces. The use of the mock object design pattern also contributes to the overall modularization of the code because this pattern requires that the code be written so that modules can be switched easily between mock versions for unit testing and "real" versions for deployment.
Madeyski[30] provided empirical evidence (via a series of laboratory experiments with over 200 developers) regarding the superiority of the TDD practice over the traditional Test-Last approach or testing for correctness approach, with respect to the lower coupling between objects (CBO). The mean effect size represents a medium (but close to large) effect on the basis of meta-analysis of the performed experiments which is a substantial finding. It suggests a better modularization (i.e., a more modular design), easier reuse and testing of the developed software products due to the TDD programming practice.[30] Madeyski also measured the effect of the TDD practice on unit tests using branch coverage (BC) and mutation score indicator (MSI),[31][32][33] which are indicators of the thoroughness and the fault detection effectiveness of unit tests, respectively. The effect size of TDD on branch coverage was medium in size and therefore is considered substantive effect.[30] These findings have been subsequently confirmed by further, smaller experimental evaluations of TDD.[34][35][36][37]
Test-driven development does not perform sufficient testing in situations where full functional tests are required to determine success or failure, due to extensive use of unit tests.[38] Examples of these are user interfaces, programs that work with databases, and some that depend on specific network configurations. TDD encourages developers to put the minimum amount of code into such modules and to maximize the logic that is in testable library code, using fakes and mocks to represent the outside world.[39]
A high number of passing unit tests may bring a false sense of security, resulting in fewer additional software testing activities, such as integration testing and compliance testing.
In software engineering, a software development process or software development life cycle (SDLC) is a process of planning and managing software development. It typically involves dividing software development work into smaller, parallel, or sequential steps or sub-processes to improve design and/or product management. The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.[1]
Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.
A life-cycle "model" is sometimes considered a more general term for a category of methodologies and a software development "process" is a particular instance as adopted by a specific organization.[citation needed] For example, many specific software development processes fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.
The software development methodology framework did not emerge until the 1960s. According to Elliott (2004), the systems development life cycle can be considered to be the oldest formalized methodology framework for building information systems. The main idea of the software development life cycle has been "to pursue the development of information systems in a very deliberate, structured and methodical way, requiring each stage of the life cycle––from the inception of the idea to delivery of the final system––to be carried out rigidly and sequentially"[2] within the context of the framework being applied. The main target of this methodology framework in the 1960s was "to develop large scale functional business systems in an age of large scale business conglomerates. Information systems activities revolved around heavy data processing and number crunching routines."[2]
Requirements gathering and analysis:
The first phase of the custom software development process involves understanding the client's requirements and objectives. This stage typically involves engaging in thorough discussions and conducting interviews with stakeholders to identify the desired features, functionalities, and overall scope of the software. The development team works closely with the client to analyze existing systems and workflows, determine technical feasibility, and define project milestones.
Planning and design:
Once the requirements are understood, the custom software development team proceeds to create a comprehensive project plan. This plan outlines the development roadmap, including timelines, resource allocation, and deliverables. The software architecture and design are also established during this phase. User interface (UI) and user experience (UX) design elements are considered to ensure the software's usability, intuitiveness, and visual appeal.
Development:
With the planning and design in place, the development team begins the coding process. This phase involves writing, testing, and debugging the software code. Agile methodologies, such as scrum or kanban, are often employed to promote flexibility, collaboration, and iterative development. Regular communication between the development team and the client ensures transparency and enables quick feedback and adjustments.
Testing and quality assurance:
To ensure the software's reliability, performance, and security, rigorous testing and quality assurance (QA) processes are carried out. Different testing techniques, including unit testing, integration testing, system testing, and user acceptance testing, are employed to identify and rectify any issues or bugs. QA activities aim to validate the software against the predefined requirements, ensuring that it functions as intended.
Deployment and implementation:
Once the software passes the testing phase, it is ready for deployment and implementation. The development team assists the client in setting up the software environment, migrating data if necessary, and configuring the system. User training and documentation are also provided to ensure a smooth transition and enable users to maximize the software's potential.
Maintenance and support:
After the software is deployed, ongoing maintenance and support become crucial to address any issues, enhance performance, and incorporate future enhancements. Regular updates, bug fixes, and security patches are released to keep the software up-to-date and secure. This phase also involves providing technical support to end users and addressing their queries or concerns.
Methodologies, processes, and frameworks range from specific prescriptive steps that can be used directly by an organization in day-to-day work, to flexible frameworks that an organization uses to generate a custom set of steps tailored to the needs of a specific project or group. In some cases, a "sponsor" or "maintenance" organization distributes an official set of documents that describe the process. Specific examples include:
Since DSDM in 1994, all of the methodologies on the above list except RUP have been agile methodologies - yet many organizations, especially governments, still use pre-agile processes (often waterfall or similar). Software process and software quality are closely interrelated; some unexpected facets and effects have been observed in practice.[3]
Among these, another software development process has been established in open source. The adoption of these best practices known and established processes within the confines of a company is called inner source.
Software prototyping is about creating prototypes, i.e. incomplete versions of the software program being developed.
A basic understanding of the fundamental business problem is necessary to avoid solving the wrong problems, but this is true for all software methodologies.
"Agile software development" refers to a group of software development frameworks based on iterative development, where requirements and solutions evolve via collaboration between self-organizing cross-functional teams. The term was coined in the year 2001 when the Agile Manifesto was formulated.
Agile software development uses iterative development as a basis but advocates a lighter and more people-centric viewpoint than traditional approaches. Agile processes fundamentally incorporate iteration and the continuous feedback that it provides to successively refine and deliver a software system.
Continuous integration is the practice of merging all developer working copies to a shared mainline several times a day.[4]
Grady Booch first named and proposed CI in his 1991 method,[5] although he did not advocate integrating several times a day. Extreme programming (XP) adopted the concept of CI and did advocate integrating more than once per day – perhaps as many as tens of times per day.
Rapid application development (RAD) is a software development methodology, which favors iterative development and the rapid construction of prototypes instead of large amounts of up-front planning. The "planning" of software developed using RAD is interleaved with writing the software itself. The lack of extensive pre-planning generally allows software to be written much faster and makes it easier to change requirements.
The term was first used to describe a software development process introduced by James Martin in 1991. According to Whitten (2003), it is a merger of various structured techniques, especially data-driven information technology engineering, with prototyping techniques to accelerate software systems development.[6]
The waterfall model is a traditional engineering approach applied to software engineering. A strict waterfall approach discourages revisiting and revising any prior phase once it is complete. [according to whom?] This "inflexibility" in a pure waterfall model has been a source of criticism by supporters of other more "flexible" models. It has been widely blamed for several large-scale government projects running over budget, over time and sometimes failing to deliver on requirements due to the big design up front approach.[according to whom?] Except when contractually required, the waterfall model has been largely superseded by more flexible and versatile methodologies developed specifically for software development.[according to whom?] See Criticism of waterfall model.
In 1988, Barry Boehm published a formal software system development "spiral model," which combines some key aspects of the waterfall model and rapid prototyping methodologies, in an effort to combine advantages of top-down and bottom-up concepts. It provided emphasis on a key area many felt had been neglected by other methodologies: deliberate iterative risk analysis, particularly suited to large-scale complex systems.
Shape Up is a software development approach introduced by Basecamp in 2018. It is a set of principles and techniques that Basecamp developed internally to overcome the problem of projects dragging on with no clear end. Its primary target audience is remote teams. Shape Up has no estimation and velocity tracking, backlogs, or sprints, unlike waterfall, agile, or scrum. Instead, those concepts are replaced with appetite, betting, and cycles. As of 2022, besides Basecamp, notable organizations that have adopted Shape Up include UserVoice and Block.[12][13]
In software project management, software testing, and software engineering, verification and validation is the process of checking that a software engineer system meets specifications and requirements so that it fulfills its intended purpose. It may also be referred to as software quality control.  It is normally the responsibility of software testers as part of the software development lifecycle. In simple terms, software verification is: "Assuming we should build X, does our software achieve its goals without any bugs or gaps?" On the other hand, software validation is: "Was X what we should have built? Does X meet the high-level requirements?"
"Building the product right" checks that the specifications are correctly implemented by the system while "building the right product" refers back to the user's needs. In some contexts, it is required to have written requirements for both as well as formal procedures or protocols for determining compliance. Ideally, formal methods provide a mathematical guarantee that software meets its specifications.
Building the product right implies the use of the Requirements Specification as input for the next phase of the development process, the design process, the output of which is the Design Specification. Then, it also implies the use of the Design Specification to feed the construction process. Every time the output of a process correctly implements its input specification, the software product is one step closer to final verification. If the output of a process is incorrect, the developers have not correctly implemented some component of that process. This kind of verification is called "artifact or specification verification".
It would imply to verify if the specifications are met by running the software but this is not possible (e.g., how can anyone know if the architecture/design/etc. are correctly implemented by running the software?). Only by reviewing its associated artifacts, can someone conclude whether or not the specifications are met.
The output of each software development process stage can also be subject to verification when checked against its input specification (see the definition by CMMI below).
Software validation checks that the software product satisfies or fits the intended use (high-level checking), i.e., the software meets the user requirements, not as specification artifacts or as needs of those who will operate the software only; but, as the needs of all the stakeholders (such as users, operators, administrators, managers, investors, etc.). There are two ways to perform software validation: internal and external. During internal software validation, it is assumed that the goals of the stakeholders were correctly understood and that they were expressed in the requirement artifacts precisely and comprehensively. If the software meets the requirement specification, it has been internally validated. External validation happens when it is performed by asking the stakeholders if the software meets their needs. Different software development methodologies call for different levels of user and stakeholder involvement and feedback; so, external validation can be a discrete or a continuous event. Successful final external validation occurs when all the stakeholders accept the software product and express that it satisfies their needs. Such final external validation requires the use of an acceptance test which is a dynamic test.
However, it is also possible to perform internal static tests to find out if the software meets the requirements specification but that falls into the scope of static verification because the software is not running.
Requirements should be validated before the software product as a whole is ready (the waterfall development process requires them to be perfectly defined before design starts; but iterative development processes do not require this to be so and allow their continual improvement).
Validation during the software development process can be seen as a form of User Requirements Specification validation; and, that at the end of the development process is equivalent to Internal and/or External Software validation. Verification, from CMMI's point of view, is evidently of the artifact kind.
In other words, software verification ensures that the output of each phase of the software development process effectively carries out what its corresponding input artifact specifies (requirement -> design -> software product), while software validation ensures that the software product meets the needs of all the stakeholders (therefore, the requirement specification was correctly and accurately expressed in the first place). Software verification ensures that "you built it right" and confirms that the product, as provided, fulfills the plans of the developers. Software validation ensures that "you built the right thing" and confirms that the product, as provided, fulfills the intended use and goals of the stakeholders.
Both verification and validation are related to the concepts of quality and of software quality assurance. By themselves, verification and validation do not guarantee software quality; planning, traceability, configuration management and other aspects of software engineering are required.
The definition of M&S validation focuses on the accuracy with which the M&S represents the real-world intended use(s).  Determining the degree of M&S accuracy is required because all M&S are approximations of reality, and it is usually critical to determine if the degree of approximation is acceptable for the intended use(s).  This stands in contrast to software validation.
In mission-critical software systems, formal methods may be used to ensure the correct operation of a system. These formal methods can prove costly, however, representing as much as 80 percent of total software design cost.
Independent Software Verification and Validation (ISVV) is targeted at safety-critical software systems and aims to increase the quality of software products, thereby reducing risks and costs throughout the operational life of the software. The goal of ISVV is to provide assurance that software performs to the specified level of confidence and within its designed parameters and defined requirements.[4][5]
ISVV activities are performed by independent engineering teams, not involved in the software development process, to assess the processes and the resulting products. The ISVV team independency is performed at three different levels: financial, managerial and technical.
ISVV goes beyond "traditional" verification and validation techniques, applied by development teams. While the latter aims to ensure that the software performs well against the nominal requirements, ISVV is focused on non-functional requirements such as robustness and reliability, and on conditions that can lead the software to fail.
ISVV derives from the application of IV&V (Independent Verification and Validation) to the software. Early ISVV application (as known today) dates back to the early 1970s when the U.S. Army sponsored the first significant program related to IV&V for the Safeguard Anti-Ballistic Missile System.[6] Another example is NASA's IV&V Program, which was established in 1993.[7]
By the end of the 1970s IV&V was rapidly becoming popular. The constant increase in complexity, size and importance of the software led to an increasing demand on IV&V applied to software.
Meanwhile, IV&V (and ISVV for software systems) consolidated and is now widely used by organizations such as the DoD, FAA,[8] NASA[7] and ESA.[9] IV&V is mentioned in DO-178B, ISO/IEC 12207 and formalized in IEEE 1012.
Initially in 2004-2005, a European consortium led by the European Space Agency, and composed of DNV, Critical Software SA, Terma and CODA SciSys plc created the first version of a guide devoted to ISVV, called "ESA Guide for Independent Verification and Validation" with support from other organizations.[10] This guide covers the methodologies applicable to all the software engineering phases in what concerns ISVV.
Software often must meet the compliance requirements of legally regulated industries, which is often guided by government agencies[11][12] or industrial administrative authorities. For instance, the FDA requires software versions and patches to be validated.[13]
Software quality control  is the set of procedures used by organizations[1] to ensure that a software product will meet its quality goals at the best value to the customer,[2] and to continually improve the organization’s ability to produce software products in the future.[1]
Software quality control refers to specified functional requirements as well as non-functional requirements such as supportability, performance and usability.[2] It also refers to the ability for software to perform well in unforeseeable scenarios and to keep a relatively low defect rate.
These specified procedures and outlined requirements lead to the idea of Verification and Validation and software testing.
It is distinct from software quality assurance which encompasses processes and standards for ongoing maintenance of high quality of products, e.g. software deliverables, documentation and processes - avoiding defects. Whereas software quality control is a validation of artifacts compliance against established criteria - finding defects.
Software quality control is a function that checks whether a software component, or supporting artifact meets requirements, or is "fit for use". Software Quality Control is commonly referred to as Testing.
Validation: "Are we building the right product?" The software should do what the user really requires.
Software quality assurance (SQA) is a means and practice of monitoring all software engineering processes, methods, and work products to ensure compliance against defined standards.[1] It may include ensuring conformance to standards or models, such as ISO/IEC 9126 (now superseded by ISO 25010), SPICE or CMMI.[2]
It includes standards and procedures that managers, administrators or developers may use to review and audit software products and activities to verify that the software meets quality criteria which link to standards.
SQA encompasses the entire software development process, including requirements engineering, software design, coding, code reviews, source code control, software configuration management, testing, release management and software integration. It is organized into goals, commitments, abilities, activities, measurements, verification and validation.[3]
Guidelines for the application of ISO 9001:2015 to computer software are described in ISO/IEC/IEEE 90003:2018.[4] External entities can be contracted as part of process assessments to verify that projects are standard-compliant. More specifically in case of software, ISO/IEC 9126 (now superseded by ISO 25010) should be considered and applied for software quality.
Quality assurance activities take place at each phase of development. Analysts use application technology and techniques to achieve high-quality specifications and designs, such as model-driven design. Engineers and technicians find bugs and problems with related software quality through testing activities. Standards and process deviations are identified and addressed throughout development by project managers or quality managers, who also ensure that changes to functionality, performance, features, architecture and component (in general: changes to product or service scope) are made only after appropriate review, e.g. as part of change control boards.[5]
Certifications can differ within a profession by the level or specific area of expertise to which they refer. For example, in the IT Industry there are different certifications available for software tester, project manager, and developer. Also, the Joint Commission on Allied Health Personnel in Ophthalmology offers three certifications in the same profession, but with increasing complexity.
For software testing, the certifications can be grouped into exam-based and education-based. For exam-based certifications the candidate must pass an exam, which can also be learned by self-study. For example, for International Software Testing Qualifications Board Certified Tester by the International Software Testing Qualifications Board[16] or Certified Software Tester by QAI or Certified Software Quality Engineer by American Society for Quality. Education-based certifications are the instructor-led sessions, where each course has to be passed. The Certified Software Test Professional (CSTP), the Certified Agile Software Test Professional (CASTP), the Certified Test Manager (CTM) and other certifications by Institute for Software Testing[17][16] are such examples. It is common for people who require certifications to undergo paid structured learning to help them achieve their goals.
There is considerable variety among software testing writers and consultants about what constitutes responsible software testing. Proponents of a context-driven approach[1] consider much of the writing about software testing to be doctrine, while others believe this contradicts the IEEE 829 documentation standard.[2]
Around 1990, a new style of writing about testing began to challenge previous approaches. The seminal work in this area is often regarded as Testing Computer Software by Cem Kaner.[5] Instead of the assumption that testers have full access to source code and complete specifications, these writers, including Kaner and James Bach, argued that testers must learn to work under conditions of uncertainty and constant change. Meanwhile, an opposing trend toward process "maturity" also gained ground, in the form of the Capability Maturity Model. The agile testing movement, which includes but is not limited to testing methods practiced on agile development projects, is popular mainly in commercial circles, whereas CMM was embraced by government and military software providers.
Exploratory testing means simultaneous test design and test execution with an emphasis on learning. Scripted testing means that learning and test design happen prior to test execution, and quite often the learning has to be done again during test execution. Exploratory testing is very common, but in most writing and training about testing it is barely mentioned and generally misunderstood. Some writers consider it a primary and essential practice. Structured exploratory testing is a compromise when the testers are familiar with the software. A vague test plan, known as a test charter, is written up, describing what functionalities need to be tested but not how, allowing the individual testers to choose the method and steps of testing.
Some writers believe that test automation is so expensive relative to its value that it should be used sparingly.[6] Others, such as advocates of agile development, recommend automating 100% of all tests. A challenge with automation is that automated testing requires automated test oracles (an oracle is a mechanism or principle by which a problem in the software can be recognized). Such tools have value in load testing software (by signing on to an application with hundreds or thousands of instances simultaneously), or in checking for intermittent errors in software.
The success of automated software testing depends on complete and comprehensive test planning. Software development strategies such as test-driven development are highly compatible with the idea of devoting a large part of an organization's testing resources to automated testing. Many large software organizations perform automated testing. Some have developed their own automated testing environments specifically for internal development, and not for resale.
if software testing is gathering information on a product or program for stakeholders then it doesn't follow that there is a choice between testing implementation or design - i.e. this is a faulty assumption.[clarification needed]
Ideally, software testers should not be limited only to testing software implementation, but also to testing software design. With this assumption, the role and involvement of testers will change dramatically. In such an environment, the test cycle will change too. To test software design, testers would review requirement and design specifications together with designer and programmer, potentially helping to identify bugs earlier in software development.
One principle in software testing is summed up by the classical Latin question posed by Juvenal:
Quis Custodiet Ipsos Custodes (Who watches the watchmen?), or is alternatively referred 
informally, as the "Heisenbug" concept (a common misconception that confuses Heisenberg's uncertainty principle with observer effect). The idea is that any form of observation is also an interaction, that the act of testing can also affect that which is being tested.[7]
In practical terms, the test engineer is testing software (and sometimes hardware or firmware) with other software (and hardware and firmware). The process can fail in ways that are not the result of defects in the target but rather result from defects in (or indeed intended features of) the testing tool.
Database testing usually consists of a layered process, including the user interface (UI) layer, the business layer, the data access layer and the database itself. The UI layer deals with the interface design of the database, while the business layer includes databases supporting business strategies.
Databases, the collection of interconnected files on a server, storing information, may not deal with the same type of data, i.e. databases may be heterogeneous. As a result, many kinds of implementation and integration errors may occur in large database systems, which negatively affect the system's performance, reliability, consistency and security. Thus, it is important to test in order to obtain a database system which satisfies the ACID properties (Atomicity, Consistency, Isolation, and Durability) of a database management system.[1]
One of the most critical layers is the data access layer, which deals with databases directly during the communication process. Database testing mainly takes place at this layer and involves testing strategies such as quality control and quality assurance of the product databases.[2] Testing at these different layers is frequently used to maintain the consistency of database systems, most commonly    seen in the following examples:
The figure indicates the areas of testing involved during different database testing methods, such as black-box testing and white-box testing.
Pros and Cons of black box testing include: Test case generation in black box testing is fairly simple. Their generation is completely independent of software development and can be done in an early stage of development. As a consequence, the programmer has better knowledge of how to design the database application and uses less time for debugging. Cost for development of black box test cases is lower than development of white box test cases. The major drawback of black box testing is that it is unknown how much of the program is being tested. Also, certain errors cannot be detected.[3]
White-box testing mainly deals with the internal structure of the database. The specification details are hidden from the user.
The main advantage of white box testing in database testing is that coding errors are detected, so internal bugs in the database can be eliminated. The limitation of white box testing is that SQL statements are not covered.
While generating test cases for database testing, the semantics of SQL statement need to be reflected in the test cases. For that purpose, a technique called WHite bOx Database Application Technique "(WHODATE)" is used. As shown in the figure, SQL statements are independently converted into GPL statements, followed by traditional white box testing to generate test cases which include SQL semantics.[4]
A set fixture describes the initial state of the database before entering the testing. After setting fixtures, database behavior is tested for defined test cases. Depending on the outcome, test cases are either modified or kept as is. The "tear down" stage either results in terminating testing or continuing with other test cases.[5]
For successful database testing the following workflow executed by each single test is commonly executed:
The International Software Testing Qualifications Board (ISTQB) is a software testing certification board that operates internationally.[1] Founded in Edinburgh in November 2002, the ISTQB is a non-profit association legally registered in Belgium.
ISTQB Certified Tester is a standardized qualification for software testers and the certification is offered by the ISTQB. The qualifications are based on a syllabus, and there is a hierarchy of qualifications and guidelines for accreditation and examination. More than 1 million ISTQB exams have been delivered and over 721,000 certifications issued; the ISTQB consists of 67 member boards worldwide representing more than 100 countries as of April 2021.[2]
The current ISTQB Foundation Level certification is based on the 2018 syllabus. The Foundation Level qualification is suitable for anyone who needs to demonstrate practical knowledge of the fundamental concepts of software testing including people in roles such as testers, test analysts, test engineers, test consultants, test managers, user acceptance testers and software developers.[8]
It is also appropriate for individuals who need a basic understanding of software testing including project managers, quality managers, software development managers, business analysts, IT directors and management consultants.[8]
IEEE 829-2008, also known as the 829 Standard for Software and System Test Documentation, was an IEEE standard that specified the form of a set of documents for use in eight defined stages of software testing and system testing, each stage potentially producing its own separate type of document. The standard specified the format of these documents, but did not stipulate whether they must all be produced, nor did it include any criteria regarding adequate content for these documents. These were a matter of judgment outside the purview of the standard.
The standard formed part of the training syllabus of the ISEB Foundation and Practitioner Certificates in Software Testing promoted by the British Computer Society. ISTQB, following the formation of its own syllabus based on ISEB's and Germany's ASQF syllabi, also adopted IEEE 829 as the reference standard for software and system test documentation.
Dr. David Gelperin and Dr. William C. Hetzel developed the Systematic Test and Evaluation Process (STEP) methodology in order to implement the original IEEE-829 Standard for Software Test Documentation.[2]
Software documentation is written text or illustration that accompanies computer software or is embedded in the source code.  The documentation either explains how the software operates or how to use it, and may mean different things to people in different roles.
Requirements documentation is the description of what a particular software does or should do. It is used throughout development to communicate how the software functions or how it is intended to operate. It is also used as an agreement or as the foundation for agreement on what the software will do. Requirements are produced and consumed by everyone involved in the production of software, including: end users, customers, project managers, sales, marketing, software architects, usability engineers, interaction designers, developers, and testers.
The variation and complexity of requirement documentation make it a proven challenge. Requirements may be implicit and hard to uncover. It is difficult to know exactly how much and what kind of documentation is needed and how much can be left to the architecture and design documentation, and it is difficult to know how to document requirements considering the variety of people who shall read and use the documentation. Thus, requirements documentation is often incomplete (or non-existent). Without proper requirements documentation, software changes become more difficult — and therefore more error prone (decreased software quality) and time-consuming (expensive).
The need for requirements documentation is typically related to the complexity of the product, the impact of the product, and the life expectancy of the software. If the software is very complex or developed by many people (e.g., mobile phone software), requirements can help better communicate what to achieve. If the software is safety-critical and can have a negative impact on human life (e.g., nuclear power systems, medical equipment, mechanical equipment), more formal requirements documentation is often required. If the software is expected to live for only a month or two (e.g., very small mobile phone applications developed specifically for a certain campaign) very little requirements documentation may be needed. If the software is a first release that is later built upon, requirements documentation is very helpful when managing the change of the software and verifying that nothing has been broken in the software when it is modified.
Traditionally, requirements are specified in requirements documents (e.g. using word processing applications and spreadsheet applications). To manage the increased complexity and changing nature of requirements documentation (and software documentation in general), database-centric systems and special-purpose requirements management tools are advocated.
In Agile software development, requirements are often expressed as user stories with accompanying acceptance criteria. User stories are typically part of a feature, or an epic, which is a broader functionality or set of related functionalities that deliver a specific value to the user based on the business requirements.
Architecture documentation (also known as software architecture description) is a special type of design document. In a way, architecture documents are third derivative from the code (design document being second derivative, and code documents being first). Very little in the architecture documents is specific to the code itself. These documents do not describe how to program a particular routine, or even why that particular routine exists in the form that it does, but instead merely lays out the general requirements that would motivate the existence of such a routine. A good architecture document is short on details but thick on explanation. It may suggest approaches for lower level design, but leave the actual exploration trade studies to other documents.
A very important part of the design document in enterprise software development is the Database Design Document (DDD). It contains Conceptual, Logical, and Physical Design Elements. The DDD includes the formal information that the people who interact with the database need. The purpose of preparing it is to create a common source to be used by all players within the scene. The potential users are:
It is very important to include all information that is to be used by all actors in the scene. It is also very important to update the documents as any change occurs in the database as well.
It is important for the code documents associated with the source code (which may include README files and API documentation) to be thorough, but not so verbose that it becomes overly time-consuming or difficult to maintain them. Various how-to and overview documentation guides are commonly found specific to the software application or software product being documented by API writers. This documentation may be used by developers, testers, and also end-users. Today, a lot of high-end applications are seen in the fields of power, energy, transportation, networks, aerospace, safety, security, industry automation, and a variety of other domains. Technical documentation has become important within such organizations as the basic and advanced level of information may change over a period of time with architecture changes.  There is evidence that the existence of good code documentation actually reduces maintenance costs for software.[1]
Often, tools such as Doxygen, NDoc, Visual Expert, Javadoc, JSDoc, EiffelStudio, Sandcastle, ROBODoc, POD, TwinText, or Universal Report can be used to auto-generate the code documents—that is, they extract the comments and software contracts, where available, from the source code and create reference manuals in such forms as text or HTML files.
Respected computer scientist Donald Knuth has noted that documentation can be a very difficult afterthought process and has advocated literate programming, written at the same time and location as the source code and extracted by automatic means. The programming languages Haskell and CoffeeScript have built-in support for a simple form of literate programming, but this support is not widely used.
Elucidative Programming is the result of practical applications of Literate Programming in real programming contexts. The Elucidative paradigm proposes that source code and documentation be stored separately.
Often, software developers need to be able to create and access information that is not going to be part of the source file itself. Such annotations are usually part of several software development activities, such as code walks and porting, where third-party source code is analysed in a functional way. Annotations can therefore help the developer during any stage of software development where a formal documentation system would hinder progress.
In the case of a software library, the code documents and user documents could in some cases be effectively equivalent and worth conjoining, but for a general application this is not often true.
Typically, the user documentation describes each feature of the program, and assists the user in realizing these features. It is very important for user documents to not be confusing, and for them to be up to date.  User documents do not need to be organized in any particular way, but it is very important for them to have a thorough index. Consistency and simplicity are also very valuable. User documentation is considered to constitute a contract specifying what the software will do. API Writers are very well accomplished towards writing good user documents as they would be well aware of the software architecture and programming techniques used. See also technical writing.
A common complaint among users regarding software documentation is that only one of these three approaches was taken to the near-exclusion of the other two. It is common to limit provided software documentation for personal computers to online help that gives only reference information on commands or menu items.  The job of tutoring new users or helping more experienced users get the most out of a program is left to private publishers, who are often given significant assistance by the software developer.
"The resistance to documentation among developers is well known and needs no emphasis."[10] 
This situation is particularly prevalent in agile software development because these methodologies try to avoid any unnecessary activities that do not directly bring value.
Specifically, the Agile Manifesto advocates valuing "working software over comprehensive documentation", which could be interpreted cynically as "We want to spend all our time coding. Remember, real programmers don't write documentation."[11]
A survey among software engineering experts revealed, however, that documentation is by no means considered unnecessary in agile development.
Yet it is acknowledged that there are motivational problems in development, and that documentation methods tailored to agile development (e.g. through Reputation systems and Gamification) may be needed.[12][13]
Docs as Code is an approach to documentation that treats it with the same rigor and processes as software code. This includes:
Software requirements[1] for a system are the description of what the system should do, the service or services that it provides and the constraints on its operation.  The IEEE Standard Glossary of Software Engineering Terminology defines a requirement as:[2]
The activities related to working with software requirements can broadly be broken down into elicitation, analysis, specification, and management.[3]
Note that the wording Software requirements is additionally used in software release notes to explain, which depending on software packages are required for a certain software to be built/installed/used.[1]
Requirements Triage or prioritization of requirements is another activity which often follows analysis.[4] This relates to Agile software development in the planning phase, e.g. by Planning poker, however it might not be the same depending on the context and nature of the project and requirements or product/service that is being built.
Requirements change during projects and there are often many of them. Management of this change becomes paramount to ensuring that the correct software is built for the stakeholders.
Taking into account that these activities may involve some artifacts such as observation reports (user observation), questionnaires (interviews, surveys and polls), use cases, user stories; activities such as requirement workshops (charrettes), brainstorming, mind mapping, role-playing; and even, prototyping;[5] software products providing some or all of these capabilities can be used to help achieve these tasks.
A Software requirements specification (SRS) document might be created using general-purpose software like a word processor or one of several specialized tools. Some of these tools can import, edit, export and publish SRS documents. It may help to make SRS documents while following a standardised structure and methodology, such as ISO/IEC/IEEE 29148:2018. Likewise, software may or not use some standard to import or export requirements (such as ReqIF) or not allow these exchanges at all.
Software design is the process of conceptualizing how a software system will work before it is implemented or modified.[1]
Software design also refers to the direct result of the design process – the concepts of how the software will work which consists of both design documentation and undocumented concepts.
Software design usually is directed by goals for the resulting system and involves problem-solving and planning – including both 
high-level software architecture and low-level component and algorithm design.
In terms of the waterfall development process, software design is the activity of following requirements specification and before coding.[2]
The design process enables a designer to model various aspects of a software system before it exists.
Creativity, past experience, a sense of what makes "good" software, and a commitment to quality are success factors for a competent design. However, the design process is not always a straightforward procedure.
The software design model can be compared to an architected plan for a house. High-level plans represent the totality of the house (e.g., a three-dimensional rendering of the house). Lower-level plans provide guidance for constructing each detail (e.g., the plumbing lay). Similarly, the software design model provides a variety of views of the proposed software solution.
Software systems inherently deal with uncertainties, and the size of software components can significantly influence a system's outcomes, both positively and negatively. Neal Ford and Mark Richards propose an iterative approach to address the challenge of identifying and right-sizing components. This method emphasizes continuous refinement as teams develop a more nuanced understanding of system behavior and requirements.[3]
Software design documentation may be reviewed or presented to allow constraints, specifications and even requirements to be adjusted prior to coding. Redesign may occur after a review of a programmed simulation or prototype. It is possible to design software in the process of coding, without a plan or requirement analysis,[4] but for more complex projects this is less feasible. A separate design prior to coding allows for multidisciplinary designers and subject-matter experts (SMEs) to collaborate with programmers in order to produce software that is useful and technically sound.
One component of software design is software requirements analysis (SRA). SRA is a part of the software development process that lists specifications used in software engineering.
A design process may include the production of artifacts such as flow chart, use case, Pseudocode, Unified Modeling Language model and other Fundamental modeling concepts. For user centered software, design may involve user experience design yielding a storyboard to help determine those specifications.
Basic design principles enable a software engineer to navigate the design process. Davis[5] suggests a set of principles for software design, which have been adapted and extended in the following list:
In his object model, Grady Booch mentions Abstraction, Encapsulation, Modularisation, and Hierarchy as fundamental software design principles.[6] The acronym PHAME (Principles of Hierarchy, Abstraction, Modularisation, and Encapsulation) is sometimes used to refer to these four fundamental principles.[7]
There are many aspects to consider in the design of a piece of software. The importance of each consideration should reflect the goals and expectations that the software is being created to meet. Some of these aspects are:
A modeling language can be used to express information, knowledge or systems in a structure that is defined by a consistent set of rules. These rules are used for interpretation of the components within the structure. A modeling language can be graphical or textual. Examples of graphical modeling languages for software design include:
A software designer may identify a design aspect which has been visited and perhaps even solved by others in the past. A template or pattern describing a solution to a common problem is known as a design pattern. The reuse of such patterns can increase software development velocity.[11]
The difficulty of using the term "design" in relation to software is that in some senses, the source code of a program is the design for the program that it produces. To the extent that this is true, "software design" refers to the design of the design.  Edsger W. Dijkstra referred to this layering of semantic levels as the "radical novelty" of computer programming,[12] and Donald Knuth used his experience writing TeX to describe the futility of attempting to design a program prior to implementing it:
^Roger S. Pressman (2001). Software engineering: a practitioner's approach. McGraw-Hill. ISBN 0-07-365578-3.
Software project management is the process of planning and leading software projects.[1] It is a sub-discipline of project management in which software projects are planned, implemented, monitored and controlled.
In the 1970s and 1980s, the software industry grew very quickly, as computer companies quickly recognized the relatively low cost of software production compared to hardware production and circuitry. To manage new development efforts, companies applied the established project management methods, but project schedules slipped during test runs, especially when confusion occurred in the gray zone between the user specifications and the delivered software. To be able to avoid these problems, software project management methods focused on matching user requirements to delivered products, in a method known now as the waterfall model.
As the industry has matured, analysis of software project management failures has shown that the following are the most common causes:[2][3][4]
The first five items in the list above show the difficulties articulating the needs of the client in such a way that proper resources can deliver the proper project goals. Specific software project management tools are useful and often necessary, but the true art in software project management is applying the correct method and then using tools to support the method. Without a method, tools are worthless. Since the 1960s, several proprietary software project management methods have been developed by software manufacturers for their own use, while computer consulting firms have also developed similar methods for their clients. Today software project management methods are still evolving, but the current trend leads away from the waterfall model to a more cyclic project delivery model that imitates a software development process.
A software development process is concerned primarily with the production aspect of software development, as opposed to the technical aspect, such as software tools. These processes exist primarily for supporting the management of software development, and are generally skewed toward addressing business concerns. Many software development processes can be run in a similar way to general project management processes. Examples are:
The purpose of project planning is to identify the scope of the project, estimate the work involved, and create a project schedule. Project planning begins with requirements that define the software to be developed. The project plan is then developed to describe the tasks that will lead to completion. The project execution is the process of completing the tasks defined in the project plan.
In some implementations of software development processes, issues are investigated by quality assurance analysts a system is verified for correctness, and then assigned back to a member of the development team to resolve the identified issue.  They can also be identified by system users during the User Acceptance Testing (UAT) phase.
As a subdiscipline of project management, some regard the management of software development akin to the management of manufacturing, which can be performed by someone with management skills, but no programming skills. John C. Reynolds rebuts this view, and argues that software development is entirely design work, and compares a manager who cannot program to the managing editor of a newspaper who cannot write.[7]
A programmer, computer programmer or coder is an author of computer source code – someone with skill in computer programming.
The professional titles software developer and software engineer are used for jobs that require a programmer.
The job titles that include programming tasks have differing connotations across the computer industry and to different individuals. The following are notable descriptions.
A software developer primarily implements software based on specifications and fixes bugs. Other duties may include reviewing code changes and testing. 
To achieve the required skills for the job, they might obtain a computer science or associate degree, attend a programming boot camp or be self-taught.
A software engineer usually is responsible for the same tasks as a developer
plus broader responsibilities of software engineering including architecting and designing new features and applications, targeting new platforms, managing the software development lifecycle (design, implementation, testing, and deployment), leading a team of programmers, communicating with customers, managers and other engineers, considering system stability and quality, and exploring software development methodologies.
Sometimes, a software engineer is required to have a degree in software engineering, computer engineering, or computer science. Some countries legally require an engineering degree to be called engineer. 
[1][2][3]
British countess and mathematician Ada Lovelace is often considered to be the first computer programmer.
She authored an algorithm, which was published in October 1842, for calculating Bernoulli numbers on the Charles Babbage analytical engine.[4] 
Because the machine was not completed in her lifetime, she never experienced the algorithm in action.
In 1941, German civil engineer Konrad Zuse was the first person to execute a program on a working, program-controlled, electronic computer.[5] From 1943 to 1945, per computer scientist Wolfgang K. Giloi and AI professor Raúl Rojas et al., Zuse created the first, high-level programming language, Plankalkül.[6][7]
Members of the 1945 ENIAC programming team of Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman have since been credited as the first professional computer programmers.[8][9]
The first company founded specifically to provide software products and services was the Computer Usage Company in 1955. Before that time, computers were programmed either by customers or the few commercial computer manufacturers of the time, such as Sperry Rand and IBM.[10]
The software industry expanded in the early 1960s, almost immediately after computers were first sold in mass-produced quantities. Universities, governments, and businesses created a demand for software. Many of these programs were written in-house by full-time staff programmers; some were distributed between users of a particular machine for no charge, while others were sold on a commercial basis. Other firms, such as Computer Sciences Corporation (founded in 1959), also started to grow. Computer manufacturers soon started bundling operating systems, system software and programming environments with their machines; the IBM 1620 came with the 1620 Symbolic Programming System and FORTRAN.[11]
The industry expanded greatly with the rise of the personal computer (PC) in the mid-1970s, which brought computing to the average office worker. In the following years, the PC also helped create a constantly growing market for games, applications and utility software. This resulted in increased demand for software developers for that period of time.[12]
Computer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming — advanced computing technologies and sophisticated new languages and programming tools — have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.[13]
Programmers work in many settings, including corporate information technology (IT) departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers.[13] Programming is considered a profession.[14][15][16]
Programming editors, also known as source code editors, are text editors that are specifically designed for programmers or developers to write the source code of an application or a program. Most of these editors include features useful for programmers, which may include color syntax highlighting, auto indentation, auto-complete, bracket matching, syntax check, and allows plug-ins. These features aid the users during coding, debugging and testing.[18]
According to BBC News, 17% of computer science students could not find work in their field six months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey.[19]
After the crash of the dot-com bubble (1999–2001) and the Great Recession (2008), many U.S. programmers were left without work or with lower wages.[20][21] In addition, enrollment in computer-related degrees and other STEM degrees (STEM attrition)[22] in the US has been dropping for years, especially for women,[23][24] which, according to Beaubouef and Mason,[25] could be attributed to a lack of general interest in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers. For programmers, the U.S. Bureau of Labor Statistics (BLS) Occupational Outlook originally predicted a growth for programmers of 12 percent from 2010 to 2020[26] and thereafter a decline of -7 percent from 2016 to 2026, a further decline of -9 percent from 2019 to 2029, a decline of -10 percent from 2021 to 2031.[13] and then a decline of -11 percent from 2022 to 2032.[27] Since computer programming can be done from anywhere in the world, companies sometimes hire programmers in countries where wages are lower.[13] If domestic demand has increased, it is mainly to replace existing programmers who have retired or switched occupations.[28] However, for software developers BLS projects current growth at 17%[29] that is down from their prediction from 2019 to 2029 of a 22% increase in employment, from 1,469,200 to 1,785,200 jobs with a median base salary of $110,000 per year. This prediction is lower than the earlier 2010 to 2020 predicted increase of 30% for software developers.[30][31][21] Though the distinction is somewhat ambiguous, software developers engage in a wider array of aspects of application development and are generally higher skilled than programmers, making outsourcing less of a risk.[32][33] Another reason for the decline for programmers is their skills are being merged with other professions, such as developers, as employers increase the requirements for a position over time (i.e. Full-stack developer). Then there is the additional concern that recent advances in artificial intelligence might impact the demand for future generations of Software professions.[34][35][36][37][38][39][40]
Software testing can provide objective, independent information about the quality of software and the risk of its failure to a user or sponsor.[1]
Software testing can determine the correctness of software for specific scenarios but cannot determine correctness for all scenarios.[2][3] It cannot find all bugs.
Based on the criteria for measuring correctness from an oracle, software testing employs principles and mechanisms that might recognize a problem. Examples of oracles include specifications, contracts,[4] comparable products, past versions of the same product, inferences about intended or expected purpose, user or customer expectations, relevant standards, and applicable laws.
Software testing is often dynamic in nature; running the software to verify actual output matches expected. It can also be static in nature; reviewing code and its associated documentation.
Software testing is often used to answer the question: Does the software do what it is supposed to do and what it needs to do?
Information learned from software testing may be used to improve the process by which software is developed.[5]: 41–43
Software testing should follow a "pyramid" approach wherein most of your tests should be unit tests, followed by integration tests and finally end-to-end (e2e) tests should have the lowest proportion.[6][7][8]
A study conducted by NIST in 2002 reported that software bugs cost the U.S. economy $59.5 billion annually. More than a third of this cost could be avoided if better software testing was performed.[9][dubious – discuss]
Outsourcing software testing because of costs is very common, with China, the Philippines, and India being preferred destinations.[citation needed]
Glenford J. Myers initially introduced the separation of debugging from testing in 1979.[10] Although his attention was on breakage testing ("A successful test case is one that detects an as-yet undiscovered error."[10]: 16 ), it illustrated the desire of the software engineering community to separate fundamental development activities, such as debugging, from that of verification.
Software testing typically includes handling software bugs – a defect in the code that causes an undesirable result.[11]: 31  Bugs generally slow testing progress and involve programmer assistance to debug and fix.
A defect that does not cause failure at one point in time may later occur due to environmental changes. Examples of environment change include running on new computer hardware, changes in data, and interacting with different software.[12]
Software testing may involve a Requirements gap – omission from the design for a requirement.[5]: 426  Requirement gaps can often be non-functional requirements such as testability, scalability, maintainability, performance, and security.
A fundamental limitation of software testing is that testing under all combinations of inputs and preconditions (initial state) is not feasible, even with a simple product.[3]: 17–18 [13] 
Defects that manifest in unusual conditions are difficult to find in testing. Also, non-functional dimensions of quality (how it is supposed to be versus what it is supposed to do) – usability, scalability, performance, compatibility, and reliability – can be subjective; something that constitutes sufficient value to one person may not to another.
Software testing can be categorized into levels based on how much of the software system is the focus of a test.[18][19][20][21]
There are many approaches to software testing. Reviews, walkthroughs, or inspections are referred to as static testing, whereas executing programmed code with a given set of test cases is referred to as dynamic testing.[23][24]
Static testing is often implicit, like proofreading, plus when programming tools/text editors check source code structure or compilers (pre-compilers) check syntax and data flow as static program analysis. Dynamic testing takes place when the program itself is run. Dynamic testing may begin before the program is 100% complete in order to test particular sections of code and are applied to discrete functions or modules.[23][24] Typical techniques for these are either using stubs/drivers or execution from a debugger environment.[24]
Passive testing means verifying the system's behavior without any interaction with the software product. Contrary to active testing, testers do not provide any test data but look at system logs and traces. They mine for patterns and specific behavior in order to make some kind of decisions.[25] This is related to offline runtime verification and log analysis.
Software testing can often be divided into white-box and black-box. These two approaches are used to describe the point of view that the tester takes when designing test cases. A hybrid approach called grey-box that includes aspects of both boxes may also be applied to software testing methodology.[31][32]
White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) verifies the internal structures or workings of a program, as opposed to the functionality exposed to the end-user. In white-box testing, an internal perspective of the system (the source code), as well as programming skills are used to design test cases. The tester chooses inputs to exercise paths through the code and determines the appropriate outputs.[31][32] This is analogous to testing nodes in a circuit, e.g., in-circuit testing (ICT).
While white-box testing can be applied at the unit, integration, and system levels of the software testing process, it is usually done at the unit level.[33] It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it might not detect unimplemented parts of the specification or missing requirements.
Code coverage tools can evaluate the completeness of a test suite that was created with any method, including black-box testing. This allows the software team to examine parts of a system that are rarely tested and ensures that the most important function points have been tested.[35] Code coverage as a software metric can be reported as a percentage for:[31][35][36]
Black-box testing (also known as functional testing) describes designing test cases without knowledge of the implementation, without reading the source code. The testers are only aware of what the software is supposed to do, not how it does it.[38] Black-box testing methods include: equivalence partitioning, boundary value analysis, all-pairs testing, state transition tables, decision table testing, fuzz testing, model-based testing, use case testing, exploratory testing, and specification-based testing.[31][32][36]
Specification-based testing aims to test the functionality of software according to the applicable requirements.[39] This level of testing usually requires thorough test cases to be provided to the tester, who then can simply verify that for a given input, the output value (or behavior), either "is" or "is not" the same as the expected value specified in the test case. Test cases are built around specifications and requirements, i.e., what the application is supposed to do. It uses external descriptions of the software, including specifications, requirements, and designs, to derive test cases. These tests can be functional or non-functional, though usually functional. Specification-based testing may be necessary to assure correct functionality, but it is insufficient to guard against complex or high-risk situations.[40]
The aim of visual testing is to provide developers with the ability to examine what was happening at the point of software failure by presenting the data in such a way that the developer can easily find the information he or she requires, and the information is expressed clearly.[44][45]
Ad hoc testing and exploratory testing are important methodologies for checking software integrity because they require less preparation time to implement, while the important bugs can be found quickly.[46] In ad hoc testing, where testing takes place in an improvised impromptu way, the ability of the tester(s) to base testing off documented methods and then improvise variations of those tests can result in a more rigorous examination of defect fixes.[46] However, unless strict documentation of the procedures is maintained, one of the limits of ad hoc testing is lack of repeatability.[46]
Grey-box testing (American spelling: gray-box testing) involves using knowledge of internal data structures and algorithms for purposes of designing tests while executing those tests at the user, or black-box level. The tester will often have access to both "the source code and the executable binary."[47] Grey-box testing may also include reverse engineering (using dynamic code analysis) to determine, for instance, boundary values or error messages.[47] Manipulating input data and formatting output do not qualify as grey-box, as the input and output are clearly outside of the "black box" that we are calling the system under test. This distinction is particularly important when conducting integration testing between two modules of code written by two different developers, where only the interfaces are exposed for the test.
By knowing the underlying concepts of how the software works, the tester makes better-informed testing choices while testing the software from outside. Typically, a grey-box tester will be permitted to set up an isolated testing environment with activities, such as seeding a database. The tester can observe the state of the product being tested after performing certain actions such as executing SQL statements against the database and then executing queries to ensure that the expected changes have been reflected. Grey-box testing implements intelligent test scenarios based on limited information. This will particularly apply to data type handling, exception handling, and so on.[48]
Most software systems have installation procedures that are needed before they can be used for their main purpose. Testing these procedures to achieve an installed software system that may be used is known as installation testing.[49]: 139  These procedures may involve full or partial upgrades, and install/uninstall processes.
A common cause of software failure (real or perceived) is a lack of its compatibility with other application software, operating systems (or operating system versions, old or new), or target environments that differ greatly from the original (such as a terminal or GUI application intended to be run on the desktop now being required to become a Web application, which must render in a Web browser). For example, in the case of a lack of backward compatibility, this can occur because the programmers develop and test software only on the latest version of the target environment, which not all users may be running. This results in the unintended consequence that the latest work may not function on earlier versions of the target environment, or on older hardware that earlier versions of the target environment were capable of using. Sometimes such issues can be fixed by proactively abstracting operating system functionality into a separate program module or library.
Smoke testing consists of minimal attempts to operate the software, designed to determine whether there are any basic problems that will prevent it from working at all. Such tests can be used as build verification test.
Regression testing focuses on finding defects after a major code change has occurred. Specifically, it seeks to uncover software regressions, as degraded or lost features, including old bugs that have come back. Such regressions occur whenever software functionality that was previously working correctly, stops working as intended. Typically, regressions occur as an unintended consequence of program changes, when the newly developed part of the software collides with the previously existing code. Regression testing is typically the largest test effort in commercial software development,[50] due to checking numerous details in prior software features, and even new software can be developed while using some old test cases to test parts of the new design to ensure prior functionality is still supported.
Acceptance testing is system-level testing to ensure the software meets customer expectations.[51][52][53][54]
Tests are frequently grouped into these levels by where they are performed in the software development process, or by the level of specificity of the test.[54]
OAT is used to conduct operational readiness (pre-release) of a product, service or system as part of a quality management system. OAT is a common type of non-functional software testing, used mainly in software development and software maintenance projects. This type of testing focuses on the operational readiness of the system to be supported, or to become part of the production environment. Hence, it is also known as operational readiness testing (ORT) or operations readiness and assurance (OR&A) testing. Functional testing within OAT is limited to those tests that are required to verify the non-functional aspects of the system.
In addition, the software testing should ensure that the portability of the system, as well as working as expected, does not also damage or partially corrupt its operating environment or cause other processes within that environment to become inoperative.[55]
Contractual acceptance testing is performed based on the contract's acceptance criteria defined during the agreement of the contract, while regulatory acceptance testing is performed based on the relevant regulations to the software product. Both of these two tests can be performed by users or independent testers. Regulation acceptance testing sometimes involves the regulatory agencies auditing the test results.[54]
Alpha testing is simulated or actual operational testing by potential users/customers or an independent test team at the developers' site. Alpha testing is often employed for off-the-shelf software as a form of internal acceptance testing before the software goes to beta testing.[56]
Beta testing comes after alpha testing and can be considered a form of external user acceptance testing. Versions of the software, known as beta versions, are released to a limited audience outside of the programming team known as beta testers. The software is released to groups of people so that further testing can ensure the product has few faults or bugs. Beta versions can be made available to the open public to increase the feedback field to a maximal number of future users and to deliver value earlier, for an extended or even indefinite period of time (perpetual beta).[57]
Non-functional testing refers to aspects of the software that may not be related to a specific function or user action, such as scalability or other performance, behavior under certain constraints, or security. Testing will determine the breaking point, the point at which extremes of scalability or performance leads to unstable execution. Non-functional requirements tend to be those that reflect the quality of the product, particularly in the context of the suitability perspective of its users.
Continuous testing is the process of executing automated tests as part of the software delivery pipeline to obtain immediate feedback on the business risks associated with a software release candidate.[58][59] Continuous testing includes the validation of both functional requirements and non-functional requirements; the scope of testing extends from validating bottom-up requirements or user stories to assessing the system requirements associated with overarching business goals.[60][61]
Destructive testing attempts to cause the software or a sub-system to fail. It verifies that the software functions properly even when it receives invalid or unexpected inputs, thereby establishing the robustness of input validation and error-management routines.[citation needed] Software fault injection, in the form of fuzzing, is an example of failure testing. Various commercial non-functional testing tools are linked from the software fault injection page; there are also numerous open-source and free software tools available that perform destructive testing.
Load testing is primarily concerned with testing that the system can continue to operate under a specific load, whether that be large quantities of data or a large number of users. This is generally referred to as software scalability. The related load testing activity of when performed as a non-functional activity is often referred to as endurance testing. Volume testing is a way to test software functions even when certain components (for example a file or database) increase radically in size. Stress testing is a way to test reliability under unexpected or rare workloads. Stability testing (often referred to as load or endurance testing) checks to see if the software can continuously function well in or above an acceptable period.
Real-time software systems have strict timing constraints. To test if timing constraints are met, real-time testing is used.
Accessibility testing is done to ensure that the software is accessible to persons with disabilities. Some of the common web accessibility tests are
Security testing is essential for software that processes confidential data to prevent system intrusion by hackers.
Testing for internationalization and localization validates that the software can be used with different languages and geographic regions. The process of pseudolocalization is used to test the ability of an application to be translated to another language, and make it easier to identify when the localization process may introduce new bugs into the product.
Globalization testing verifies that the software is adapted for a new culture, such as different currencies or time zones.[64]
Development testing is a software development process that involves the synchronized application of a broad spectrum of defect prevention and detection strategies in order to reduce software development risks, time, and costs. It is performed by the software developer or engineer during the construction phase of the software development lifecycle. Development testing aims to eliminate construction errors before code is promoted to other testing; this strategy is intended to increase the quality of the resulting software as well as the efficiency of the overall development process.
Depending on the organization's expectations for software development, development testing might include static code analysis, data flow analysis, metrics analysis, peer code reviews, unit testing, code coverage analysis, traceability, and other software testing practices.
Concurrent or concurrency testing assesses the behaviour and performance of software and systems that use concurrent computing, generally under normal usage conditions. Typical problems this type of testing will expose are deadlocks, race conditions and problems with shared memory/resource handling.
In software testing, conformance testing verifies that a product performs according to its specified standards. Compilers, for instance, are extensively tested to determine whether they meet the recognized standard for that language.
Metamorphic testing (MT) is a property-based software testing technique, which can be an effective approach for addressing the test oracle problem and test case generation problem. The test oracle problem is the difficulty of determining the expected outcomes of selected test cases or to determine whether the actual outputs agree with the expected outcomes.
In an organization, testers may be in a separate team from the rest of the software development team or they may be integrated into one team. Software testing can also be performed by non-dedicated software testers.
Notable software testing roles and titles include:[66] test manager, test lead, test analyst, test designer, tester, automation developer, and test administrator.[67]
Agile software development commonly involves testing while the code is being written and organizing teams with both programmers and testers and with team members performing both programming and testing.
One agile practice, test-driven software development (TDD), is a way of unit testing such that unit-level testing is performed while writing the product code.[70] Test code is updated as new features are added and failure conditions are discovered (bugs fixed). Commonly, the unit test code is maintained with the project code, integrated in the build process, and run on each build and as part of regression testing. Goals of this continuous integration is to support development and reduce defects.[71][70]
Even in organizations that separate teams by programming and testing functions, many often have the programmers perform unit testing.[72]
The terms verification and validation are commonly used interchangeably in the industry; it is also common to see these two terms defined with contradictory definitions. According to the IEEE Standard Glossary of Software Engineering Terminology:[11]: 80–81
In the case of IEEE standards, the specified requirements, mentioned in the definition of validation, are the set of problems, needs and wants of the stakeholders that the software must solve and satisfy. Such requirements are documented in a Software Requirements Specification (SRS). And, the products mentioned in the definition of verification, are the output artifacts of every phase of the software development process. These products are, in fact, specifications such as Architectural Design Specification, Detailed Design Specification, etc. The SRS is also a specification, but it cannot be verified (at least not in the sense used here, more on this subject below).
But, for the ISO 9000, the specified requirements are the set of specifications, as just mentioned above, that must be verified. A specification, as previously explained, is the product of a software development process phase that receives another specification as input. A specification is verified successfully when it correctly implements its input specification. All the specifications can be verified except the SRS because it is the first one (it can be validated, though). Examples: The Design Specification must implement the SRS; and, the Construction phase artifacts must implement the Design Specification.
Both the SRS and the software must be validated. The SRS can be validated statically by consulting with the stakeholders. Nevertheless, running some partial implementation of the software or a prototype of any kind (dynamic testing) and obtaining positive feedback from them, can further increase the certainty that the SRS is correctly formulated. On the other hand, the software, as a final and running product (not its artifacts and documents, including the source code) must be validated dynamically with the stakeholders by executing the software and having them to try it.
In some organizations, software testing is part of a software quality assurance (SQA) process.[3]: 347  In SQA, software process specialists and auditors are concerned with the software development process rather than just the artifacts such as documentation, code and systems. They examine and change the software engineering process itself to reduce the number of faults that end up in the delivered software: the so-called defect rate. What constitutes an acceptable defect rate depends on the nature of the software; a flight simulator video game would have much higher defect tolerance than software for an actual airplane. Although there are close links with SQA, testing departments often exist independently, and there may be no SQA function in some companies.[citation needed]
Software testing is an activity to investigate software under test in order to provide quality-related information to stakeholders. By contrast, QA (quality assurance) is the implementation of policies and procedures intended to prevent defects from reaching customers.
There are a number of frequently used software metrics, or measures, which are used to assist in determining the state of the software or the adequacy of the testing.
A software testing process can produce several artifacts. The actual artifacts produced are a factor of the software development model used, stakeholder and organisational needs.
A test case normally consists of a unique identifier, requirement references from a design specification, preconditions, events, a series of steps (also known as actions) to follow, input, output, expected result, and the actual result. Clinically defined, a test case is an input and an expected result.[75] This can be as terse as "for condition x your derived result is y", although normally test cases describe in more detail the input scenario and what results might be expected. It can occasionally be a series of steps (but often steps are contained in a separate test procedure that can be exercised against multiple test cases, as a matter of economy) but with one expected result or expected outcome. The optional fields are a test case ID, test step, or order of execution number, related requirement(s), depth, test category, author, and check boxes for whether the test is automatable and has been automated. Larger test cases may also contain prerequisite states or steps, and descriptions. A test case should also contain a place for the actual result. These steps can be stored in a word processor document, spreadsheet, database, or other common repositories. In a database system, you may also be able to see past test results, who generated the results, and what system configuration was used to generate those results. These past results would usually be stored in a separate table.
A test script is a procedure or programming code that replicates user actions. Initially, the term was derived from the product of work created by automated regression test tools. A test case will be a baseline to create test scripts using a tool or a program.
The software, tools, samples of data input and output, and configurations are all referred to collectively as a test harness.
Several certification programs exist to support the professional aspirations of software testers and quality assurance specialists. A few practitioners argue that the testing field is not ready for certification, as mentioned in the controversy section.
Boehm doesn't even cite a paper for the TRW data, except when writing for "Making Software" in 2010, and there he cited the original 1976 article. There exists a large study conducted at TRW at the right time for Boehm to cite it, but that paper doesn't contain the sort of data that would support Boehm's claims.[86]
A programming tool or software development tool is a computer program that is used to develop another computer program, usually by helping the developer manage computer files. For example, a programmer may use a tool called a source code editor to edit source code files, and then a compiler to convert the source code into machine code files. They may also use build tools that automatically package executable program and data files into shareable packages or install kits.
Whether a program is considered a development tool can be subjective. Some programs, such as the GNU compiler collection, are used exclusively for software development while others, such as Notepad, are not meant specifically for development but are nevertheless often used for programming.
Computer-aided software engineering (CASE) is a domain of software tools used to design and implement applications. CASE tools are similar to and are partly inspired by computer-aided design (CAD) tools used for designing hardware products.  CASE tools are intended to help develop high-quality, defect-free, and maintainable software.[1] CASE software was often associated with methods for the development of information systems together with automated tools that could be used in the software development process.[2]
Another major thread emerged as a logical extension to the data dictionary of a database. By extending the range of metadata held, the attributes of an application could be held within a dictionary and used at runtime. This "active dictionary" became the precursor to the more modern model-driven engineering capability. However, the active dictionary did not provide a graphical representation of any of the metadata. It was the linking of the concept of a dictionary holding analysts' metadata, as derived from the use of an integrated set of techniques, together with the graphical representation of such data that gave rise to the earlier versions of CASE.[4]
The next entrant into the market was Excelerator from Index Technology in Cambridge, Mass.  While DesignAid ran on Convergent Technologies and later Burroughs Ngen networked microcomputers, Index launched Excelerator on the IBM PC/AT platform. While, at the time of launch, and for several years, the IBM platform did not support networking or a centralized database as did the Convergent Technologies or Burroughs machines, the allure of IBM was strong, and Excelerator came to prominence. Hot on the heels of Excelerator were a rash of offerings from companies such as Knowledgeware (James Martin, Fran Tarkenton and Don Addington), Texas Instrument's CA Gen and Andersen Consulting's FOUNDATION toolset (DESIGN/1, INSTALL/1, FCP).[5]
CASE tools were at their peak in the early 1990s.[6]  According to the PC Magazine of January 1990, over 100 companies were offering nearly 200 different CASE tools.[5]  At the time IBM had proposed AD/Cycle, which was an alliance of software vendors centered on IBM's Software repository using IBM DB2 in mainframe and OS/2:
CASE tools support specific tasks in the software development life-cycle. They can be divided into the following categories:
Workbenches integrate two or more CASE tools and support specific software-process activities. Hence they achieve:
An example workbench is Microsoft's Visual Basic programming environment. It incorporates several development tools: a GUI builder, a smart code editor, debugger, etc. Most commercial CASE products tended to be such workbenches that seamlessly integrated two or more tools. Workbenches also can be classified in the same manner as tools; as focusing on Analysis, Development, Verification, etc. as well as being focused on the upper case, lower case, or processes such as configuration management that span the complete life-cycle.
An environment is a collection of CASE tools or workbenches that attempts to support the complete software process. This contrasts with tools that focus on one specific task or a specific part of the life-cycle. CASE environments are classified by Fuggetta as follows:[9]
In practice, the distinction between workbenches and environments was flexible. Visual Basic for example was a programming workbench but was also considered a 4GL environment by many. The features that distinguished workbenches from environments were deep integration via a shared repository or common language and some kind of methodology (integrated and process-centered environments) or domain (4GL) specificity.[9]
A software standard is a standard, protocol, or other common format of a document, file, or data transfer accepted and used by one or more software developers while working on one or more than one computer programs.  Software standards enable interoperability between different programs created by different developers.
Software standards consist of certain terms, concepts, data formats, document styles and techniques agreed upon by software creators so that their software can understand the files and data created by a different computer program. To be considered a standard, a certain protocol needs to be accepted and incorporated by a group of developers who contribute to the definition and maintenance of the standard.
Some developers prefer using standards for software development because of the efficiencies it provides for code development[1] and wider user acceptance and use of the resulting application.[2]
The protocols HTML, TCP/IP, SMTP, POP and FTP are examples of software standards that application designers must understand and follow if their software expects to interface with these standards. For instance, in order for an email sent using Microsoft Outlook to be read by someone using Yahoo! Mail, the email must be sent using the SMTP so that the recipient's software can understand and correctly parse and display the email. Without such a standardized protocol, two different software applications would be unable to accurately share and display the information delivered between each other.
Some other widely used data formats, while understood and used by a variety of computer programs, are not considered a software standard. Microsoft Office file formats, such as .doc and .xls, are commonly converted by other computer programs to use, but are still owned and controlled by Microsoft, unlike text files (TXT or RTF.[3])
Representatives from standards organizations, like W3C[4] and ISOC,[5] collaborate on how to make a unified software standard to ensure seamless communication between software applications. These organisations consist of groups of larger software companies like Microsoft and Apple Inc.
A standard can be a closed standard or an open standard. The documentation for an open standard is open to the public and anyone can create a software that implements and uses the standard. The documentation and specification for closed standards are not available to the public, enabling its developer to sell and license the code to manage their data format to other interested software developers.  While this process increases the revenue potential for a useful file format, it may limit acceptance and drive the adoption of a similar, open standard instead.[6]
In software engineering, a software development process or software development life cycle (SDLC) is a process of planning and managing software development. It typically involves dividing software development work into smaller, parallel, or sequential steps or sub-processes to improve design and/or product management. The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.[1]
Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.
A life-cycle "model" is sometimes considered a more general term for a category of methodologies and a software development "process" is a particular instance as adopted by a specific organization.[citation needed] For example, many specific software development processes fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.
The software development methodology framework did not emerge until the 1960s. According to Elliott (2004), the systems development life cycle can be considered to be the oldest formalized methodology framework for building information systems. The main idea of the software development life cycle has been "to pursue the development of information systems in a very deliberate, structured and methodical way, requiring each stage of the life cycle––from the inception of the idea to delivery of the final system––to be carried out rigidly and sequentially"[2] within the context of the framework being applied. The main target of this methodology framework in the 1960s was "to develop large scale functional business systems in an age of large scale business conglomerates. Information systems activities revolved around heavy data processing and number crunching routines."[2]
Requirements gathering and analysis:
The first phase of the custom software development process involves understanding the client's requirements and objectives. This stage typically involves engaging in thorough discussions and conducting interviews with stakeholders to identify the desired features, functionalities, and overall scope of the software. The development team works closely with the client to analyze existing systems and workflows, determine technical feasibility, and define project milestones.
Planning and design:
Once the requirements are understood, the custom software development team proceeds to create a comprehensive project plan. This plan outlines the development roadmap, including timelines, resource allocation, and deliverables. The software architecture and design are also established during this phase. User interface (UI) and user experience (UX) design elements are considered to ensure the software's usability, intuitiveness, and visual appeal.
Development:
With the planning and design in place, the development team begins the coding process. This phase involves writing, testing, and debugging the software code. Agile methodologies, such as scrum or kanban, are often employed to promote flexibility, collaboration, and iterative development. Regular communication between the development team and the client ensures transparency and enables quick feedback and adjustments.
Testing and quality assurance:
To ensure the software's reliability, performance, and security, rigorous testing and quality assurance (QA) processes are carried out. Different testing techniques, including unit testing, integration testing, system testing, and user acceptance testing, are employed to identify and rectify any issues or bugs. QA activities aim to validate the software against the predefined requirements, ensuring that it functions as intended.
Deployment and implementation:
Once the software passes the testing phase, it is ready for deployment and implementation. The development team assists the client in setting up the software environment, migrating data if necessary, and configuring the system. User training and documentation are also provided to ensure a smooth transition and enable users to maximize the software's potential.
Maintenance and support:
After the software is deployed, ongoing maintenance and support become crucial to address any issues, enhance performance, and incorporate future enhancements. Regular updates, bug fixes, and security patches are released to keep the software up-to-date and secure. This phase also involves providing technical support to end users and addressing their queries or concerns.
Methodologies, processes, and frameworks range from specific prescriptive steps that can be used directly by an organization in day-to-day work, to flexible frameworks that an organization uses to generate a custom set of steps tailored to the needs of a specific project or group. In some cases, a "sponsor" or "maintenance" organization distributes an official set of documents that describe the process. Specific examples include:
Since DSDM in 1994, all of the methodologies on the above list except RUP have been agile methodologies - yet many organizations, especially governments, still use pre-agile processes (often waterfall or similar). Software process and software quality are closely interrelated; some unexpected facets and effects have been observed in practice.[3]
Among these, another software development process has been established in open source. The adoption of these best practices known and established processes within the confines of a company is called inner source.
Software prototyping is about creating prototypes, i.e. incomplete versions of the software program being developed.
A basic understanding of the fundamental business problem is necessary to avoid solving the wrong problems, but this is true for all software methodologies.
"Agile software development" refers to a group of software development frameworks based on iterative development, where requirements and solutions evolve via collaboration between self-organizing cross-functional teams. The term was coined in the year 2001 when the Agile Manifesto was formulated.
Agile software development uses iterative development as a basis but advocates a lighter and more people-centric viewpoint than traditional approaches. Agile processes fundamentally incorporate iteration and the continuous feedback that it provides to successively refine and deliver a software system.
Continuous integration is the practice of merging all developer working copies to a shared mainline several times a day.[4]
Grady Booch first named and proposed CI in his 1991 method,[5] although he did not advocate integrating several times a day. Extreme programming (XP) adopted the concept of CI and did advocate integrating more than once per day – perhaps as many as tens of times per day.
Rapid application development (RAD) is a software development methodology, which favors iterative development and the rapid construction of prototypes instead of large amounts of up-front planning. The "planning" of software developed using RAD is interleaved with writing the software itself. The lack of extensive pre-planning generally allows software to be written much faster and makes it easier to change requirements.
The term was first used to describe a software development process introduced by James Martin in 1991. According to Whitten (2003), it is a merger of various structured techniques, especially data-driven information technology engineering, with prototyping techniques to accelerate software systems development.[6]
The waterfall model is a traditional engineering approach applied to software engineering. A strict waterfall approach discourages revisiting and revising any prior phase once it is complete. [according to whom?] This "inflexibility" in a pure waterfall model has been a source of criticism by supporters of other more "flexible" models. It has been widely blamed for several large-scale government projects running over budget, over time and sometimes failing to deliver on requirements due to the big design up front approach.[according to whom?] Except when contractually required, the waterfall model has been largely superseded by more flexible and versatile methodologies developed specifically for software development.[according to whom?] See Criticism of waterfall model.
In 1988, Barry Boehm published a formal software system development "spiral model," which combines some key aspects of the waterfall model and rapid prototyping methodologies, in an effort to combine advantages of top-down and bottom-up concepts. It provided emphasis on a key area many felt had been neglected by other methodologies: deliberate iterative risk analysis, particularly suited to large-scale complex systems.
Shape Up is a software development approach introduced by Basecamp in 2018. It is a set of principles and techniques that Basecamp developed internally to overcome the problem of projects dragging on with no clear end. Its primary target audience is remote teams. Shape Up has no estimation and velocity tracking, backlogs, or sprints, unlike waterfall, agile, or scrum. Instead, those concepts are replaced with appetite, betting, and cycles. As of 2022, besides Basecamp, notable organizations that have adopted Shape Up include UserVoice and Block.[12][13]
Extreme programming (XP) is a software development methodology intended to improve software quality and responsiveness to changing customer requirements. As a type of agile software development,[1][2][3] it advocates frequent releases in short development cycles, intended to improve productivity and introduce checkpoints at which new customer requirements can be adopted.
Other elements of extreme programming include programming in pairs or doing extensive code review, unit testing of all code, not programming features until they are actually needed, a flat management structure, code simplicity and clarity, expecting changes in the customer's requirements as time passes and the problem is better understood, and frequent communication with the customer and among programmers.[2][3][4] The methodology takes its name from the idea that the beneficial elements of traditional software engineering practices are taken to "extreme" levels. As an example, code reviews are considered a beneficial practice; taken to the extreme, code can be reviewed continuously (i.e. the practice of pair programming).
Kent Beck developed extreme programming during his work on the Chrysler Comprehensive Compensation System (C3) payroll project.[5] Beck became the C3 project leader in March 1996. He began to refine the development methodology used in the project and wrote a book on the methodology (Extreme Programming Explained, published in October 1999).[5] Chrysler cancelled the C3 project in February 2000, after seven years, when Daimler-Benz acquired the company.[6] Ward Cunningham was another major influence on XP.
Many extreme-programming practices have been around for some time; the methodology takes "best practices" to extreme levels. For example, the "practice of test-first development, planning and writing tests before each micro-increment" was used as early as NASA's Project Mercury, in the early 1960s.[7] To shorten the total development time, some formal test documents (such as for acceptance testing) have been developed in parallel with (or shortly before) the software being ready for testing. A NASA independent test group can write the test procedures, based on formal requirements and logical limits, before programmers write the software and integrate it with the hardware. XP takes this concept to the extreme level, writing automated tests (sometimes inside software modules) which validate the operation of even small sections of software coding, rather than only testing the larger features.
Rapidly changing requirements demanded shorter product life-cycles, and often clashed with traditional methods of software development.
Information about the principles and practices behind XP disseminated to the wider world through discussions on the original wiki, Cunningham's WikiWikiWeb. Various contributors discussed and expanded upon the ideas, and some spin-off methodologies resulted (see agile software development). Also, XP concepts have been explained[by whom?], for several years, using a hypertext system map on the XP website at http://www.extremeprogramming.org c. 1999.
Beck edited a series of books on XP, beginning with his own Extreme Programming Explained (1999, ISBN 0-201-61641-6), spreading his ideas to a much larger audience. Authors in the series went through various aspects attending XP and its practices. The series included a book critical of the practices.
XP generated significant interest among software communities in the late 1990s and early 2000s, seeing adoption in a number of environments radically different from its origins.
Meanwhile, other agile-development practices have not stood still, and as of 2019[update] XP continues to evolve, assimilating more lessons from experiences in the field, to use other practices. In the second edition of Extreme Programming Explained (November 2004), five years after the first edition, Beck added more values and practices and differentiated between primary and corollary practices.
Extreme Programming Explained describes extreme programming as a software-development discipline that organizes people to produce higher-quality software more productively.
XP attempts to reduce the cost of changes in requirements by having multiple short development cycles, rather than a long one.
In this doctrine, changes are a natural, inescapable and desirable aspect of software-development projects, and should be planned for, instead of attempting to define a stable set of requirements.
Extreme programming also introduces a number of basic values, principles and practices on top of the agile methodology.
XP describes four basic activities that are performed within the software development process: coding, testing, listening, and designing. Each of those activities is described below.
The advocates of XP argue that the only truly important product of the system development process is code – software instructions that a computer can interpret. Without code, there is no working product.
Coding can be used to figure out the most suitable solution. Coding can also help to communicate thoughts about programming problems. A programmer dealing with a complex programming problem, or finding it hard to explain the solution to fellow programmers, might code it in a simplified manner and use the code to demonstrate what they mean. Code, say the proponents of this position, is always clear and concise and cannot be interpreted in more than one way. Other programmers can give feedback on this code by also coding their thoughts.
Testing is central to extreme programming.[9] Extreme programming's approach is that if a little testing can eliminate a few flaws, a lot of testing can eliminate many more flaws.
Extreme programming initially recognized four values in 1999: communication, simplicity, feedback, and courage. A new value, respect, was added in the second edition of Extreme Programming Explained. Those five values are described below.
Building software systems requires communicating system requirements to the developers of the system. In formal software development methodologies, this task is accomplished through documentation. Extreme programming techniques can be viewed as methods for rapidly building and disseminating institutional knowledge among members of a development team. The goal is to give all developers a shared view of the system which matches the view held by the users of the system. To this end, extreme programming favors simple designs, common metaphors, collaboration of users and programmers, frequent verbal communication, and feedback.
Extreme programming encourages starting with the simplest solution. Extra functionality can then be added later. The difference between this approach and more conventional system development methods is the focus on designing and coding for the needs of today instead of those of tomorrow, next week, or next month. This is sometimes summed up as the "You aren't gonna need it" (YAGNI) approach.[10] Proponents of XP acknowledge the disadvantage that this can sometimes entail more effort tomorrow to change the system; their claim is that this is more than compensated for by the advantage of not investing in possible future requirements that might change before they become relevant. Coding and designing for uncertain future requirements implies the risk of spending resources on something that might not be needed, while perhaps delaying crucial features. Related to the "communication" value, simplicity in design and coding should improve the quality of communication. A simple design with very simple code could be easily understood by most programmers in the team.
Feedback is closely related to communication and simplicity. Flaws in the system are easily communicated by writing a unit test that proves a certain piece of code will break. The direct feedback from the system tells programmers to recode this part. A customer is able to test the system periodically according to the functional requirements, known as user stories.[5] To quote Kent Beck, "Optimism is an occupational hazard of programming. Feedback is the treatment."[11]
Another version of XP rules was proposed by Ken Auer[13] in XP/Agile Universe 2003. He felt XP was defined by its rules, not its practices (which are subject to more variation and ambiguity). He defined two categories: "Rules of Engagement" which dictate the environment in which software development can take place effectively, and "Rules of Play" which define the minute-by-minute activities and rules within the framework of the Rules of Engagement.
Extreme programming sees feedback as most useful if it is done frequently and promptly. It stresses that minimal delay between an action and its feedback is critical to learning and making changes. Unlike traditional system development methods, contact with the customer occurs in more frequent iterations. The customer has clear insight into the system that is being developed, and can give feedback and steer the development as needed. With frequent feedback from the customer, a mistaken design decision made by the developer will be noticed and corrected quickly, before the developer spends much time implementing it.
Unit tests contribute to the rapid feedback principle. When writing code, running the unit test provides direct feedback as to how the system reacts to the changes made. This includes running not only the unit tests that test the developer's code, but running in addition all unit tests against all the software, using an automated process that can be initiated by a single command. That way, if the developer's changes cause a failure in some other portion of the system that the developer knows little or nothing about, the automated all-unit-test suite will reveal the failure immediately, alerting the developer of the incompatibility of their change with other parts of the system, and the necessity of removing or modifying their change. Under traditional development practices, the absence of an automated, comprehensive unit-test suite meant that such a code change, assumed harmless by the developer, would have been left in place, appearing only during integration testing – or worse, only in production; and determining which code change caused the problem, among all the changes made by all the developers during the weeks or even months previous to integration testing, was a formidable task.
This is about treating every problem as if its solution were "extremely simple". Traditional system development methods say to plan for the future and to code for reusability. Extreme programming rejects these ideas.
The advocates of extreme programming say that making big changes all at once does not work. Extreme programming applies incremental changes: for example, a system might have small releases every three weeks. When many little steps are made, the customer has more control over the development process and the system that is being developed.
The practices in XP have been heavily debated.[5] Proponents of extreme programming claim that by having the on-site customer[5] request changes informally, the process becomes flexible, and saves the cost of formal overhead. Critics of XP claim this can lead to costly rework and project scope creep beyond what was previously agreed or funded.[citation needed]
Change-control boards are a sign that there are potential conflicts in project objectives and constraints between multiple users. XP's expedited methods are somewhat dependent on programmers being able to assume a unified client viewpoint so the programmer can concentrate on coding, rather than documentation of compromise objectives and constraints.[14] This also applies when multiple programming organizations are involved, particularly organizations which compete for shares of projects.[citation needed]
In 2004, industrial extreme programming (IXP)[15] was introduced as an evolution of XP. It is intended to bring the ability to work in large and distributed teams. It now has 23 practices and flexible values.
In 2003, Matt Stephens and Doug Rosenberg published Extreme Programming Refactored: The Case Against XP, which questioned the value of the XP process and suggested ways in which it could be improved.[6] This triggered a lengthy debate in articles, Internet newsgroups, and web-site chat areas. The core argument of the book is that XP's practices are interdependent but that few practical organizations are willing/able to adopt all the practices; therefore the entire process fails. The book also makes other criticisms, and it draws a likeness of XP's "collective ownership" model to socialism in a negative manner.
Certain aspects of XP have changed since the publication of Extreme Programming Refactored; in particular, XP now accommodates modifications to the practices as long as the required objectives are still met. XP also uses increasingly generic terms for processes. Some argue that these changes invalidate previous criticisms; others claim that this is simply watering the process down.
Other authors have tried to reconcile XP with the older methodologies in order to form a unified methodology. Some of these XP sought to replace, such as the waterfall methodology; example: Project Lifecycles: Waterfall, Rapid Application Development (RAD), and All That. JPMorgan Chase & Co. tried combining XP with the computer programming methods of capability maturity model integration (CMMI), and Six Sigma. They found that the three systems reinforced each other well, leading to better development, and did not mutually contradict.[16]
Extreme programming's initial buzz and controversial tenets, such as pair programming and continuous design, have attracted particular criticisms, such as the ones coming from McBreen,[17] Boehm and Turner,[18] Matt Stephens and Doug Rosenberg.[19] Many of the criticisms, however, are believed by Agile practitioners to be misunderstandings of agile development.[20]
In particular, extreme programming has been reviewed and critiqued by Matt Stephens's and Doug Rosenberg's Extreme Programming Refactored.[6]
Scrum is an agile team collaboration framework commonly used in software development and other industries.
The use of the term scrum in software development came from a 1986 Harvard Business Review paper titled "The New New Product Development Game" by Hirotaka Takeuchi and Ikujiro Nonaka. Based on case studies from manufacturing firms in the automotive, photocopier, and printer industries, the authors outlined a new approach to product development for increased speed and flexibility. They called this the rugby approach, as the process involves a single cross-functional team operating across multiple overlapping phases in which the team "tries to go the distance as a unit, passing the ball back and forth".[6] The authors later developed scrum in their book, The Knowledge Creating Company.[7]
In the early 1990s, Ken Schwaber used what would become scrum at his company, Advanced Development Methods. Jeff Sutherland, John Scumniotales, and Jeff McKenna developed a similar approach at Easel Corporation, referring to the approach with the term scrum.[8] Sutherland and Schwaber later worked together to integrate their ideas into a single framework, formally known as scrum. Schwaber and Sutherland tested scrum and continually improved it, leading to the publication of a research paper in 1995,[9] and the Manifesto for Agile Software Development in 2001.[10] Schwaber also collaborated with Babatunde Ogunnaike at DuPont Research Station and the University of Delaware to develop Scrum. Ogunnaike believed that software development projects could often fail when initial conditions change if product management was not rooted in empirical practice.[3]
Scrum is frequently tailored or adapted in different contexts to achieve varying aims.[41] A common approach to adapting scrum is the combination of scrum with other software development methodologies, as scrum does not cover the whole product development lifecycle.[42] Various scrum practitioners have also suggested more detailed techniques for how to apply or adapt scrum to particular problems or organizations. Many refer to these techniques as 'patterns', an analogous use to design patterns in architecture and software.[43][44]
Scrumban is a software production model based on scrum and kanban. To illustrate each stage of work, teams working in the same space often use post-it notes or a large whiteboard.[45] Kanban models allow a team to visualize work stages and limitations.[46]
A systematic review found "that Distributed Scrum has no impact, positive or negative on overall project success" in distributed software development.[52]
Martin Fowler, one of the authors of the Manifesto for Agile Software Development, has criticized what he calls "faux-agile" practices that are "disregarding Agile's values and principles",[53] and "the Agile Industrial Complex imposing methods upon people" contrary to the Agile principle of valuing "individuals and interactions over processes and tools"[10] and allowing the individuals doing the work to decide how the work is done, changing processes to suit their needs.
Open-source software development (OSSD) is the process by which open-source software, or similar software whose source code is publicly available, is developed by an open-source software project. These are software products available with its source code under an open-source license to study, change, and improve its design. Examples of some popular open-source software products are Mozilla Firefox, Google Chromium, Android, LibreOffice and the VLC media player.
In 1997, Eric S. Raymond wrote The Cathedral and the Bazaar.[1] In this book, Raymond makes the distinction between two kinds of software development. The first is the conventional closed-source development. This kind of development method is, according to Raymond, like the building of a cathedral; central planning, tight organization and one process from start to finish. The second is the progressive open-source development, which is more like "a great babbling bazaar of differing agendas and approaches out of which a coherent and stable system could seemingly emerge only by a succession of miracles." The latter analogy points to the discussion involved in an open-source development process.
Differences between the two styles of development, according to Bar and Fogel, are in general the handling (and creation) of bug reports and feature requests, and the constraints under which the programmers are working.[2] In closed-source software development, the programmers are often spending a lot of time dealing with and creating bug reports, as well as handling feature requests. This time is spent on creating and prioritizing further development plans. This leads to part of the development team spending a lot of time on these issues, and not on the actual development. Also, in closed-source projects, the development teams must often work under management-related constraints (such as deadlines, budgets, etc.) that interfere with technical issues of the software. In open-source software development, these issues are solved by integrating the users of the software in the development process, or even letting these users build the system themselves.[citation needed]
Open-source software development can be divided into several phases. The phases specified here are derived from Sharma et al.[3] A diagram displaying the process-data structure of open-source software development is shown on the right. In this picture, the phases of open-source software development are displayed, along with the corresponding data elements. This diagram is made using the meta-modeling and meta-process modeling techniques.
Several types of open-source projects exist. First, there is the garden variety of software programs and libraries, which consist of standalone pieces of code. Some might even be dependent on other open-source projects. These projects serve a specified purpose and fill a definite need. Examples of this type of project include the Linux kernel, the Firefox web browser and the LibreOffice office suite of tools.
Distributions are another type of open-source project. Distributions are collections of software that are published from the same source with a common purpose. The most prominent example of a "distribution" is an operating system. There are many Linux distributions (such as Debian, Fedora Core, Mandriva, Slackware, Ubuntu etc.) which ship the Linux kernel along with many user-land components. There are other distributions, like ActivePerl, the Perl programming language for various operating systems, and Cygwin distributions of open-source programs for Microsoft Windows.
Other open-source projects, like the BSD derivatives, maintain the source code of an entire operating system, the kernel and all of its core components, in one revision control system; developing the entire system together as a single team. These operating system development projects closely integrate their tools, more so than in the other distribution-based systems.
Finally, there is the book or standalone document project. These items usually do not ship as part of an open-source software package. Linux Documentation Project hosts many such projects that document various aspects of the Linux operating system. There are many other examples of this type of open-source project.
It is hard to run an open-source project following a more traditional software development method like the waterfall model, because in these traditional methods it is not allowed to go back to a previous phase. In open-source software development, requirements are rarely gathered before the start of the project; instead they are based on early releases of the software product, as Robbins describes.[4] Besides requirements, often volunteer staff is attracted to help develop the software product based on the early releases of the software. This networking effect is essential according to Abrahamsson et al.: “if the introduced prototype gathers enough attention, it will gradually start to attract more and more developers”. However, Abrahamsson et al. also point out that the community is very harsh, much like the business world of closed-source software: “if you find the customers you survive, but without customers you die”.[5]
Fuggetta[6] argues that “rapid prototyping, incremental and evolutionary development, spiral lifecycle, rapid application development, and, recently, extreme programming and the agile software process can be equally applied to proprietary and open source software”. He also pinpoints Extreme Programming as an extremely useful method for open source software development. More generally, all Agile programming methods are applicable to open-source software development, because of their iterative and incremental character. Other Agile methods are equally useful for both open and closed source software development: Internet-Speed Development, for example is suitable for open-source software development because of the distributed development principle it adopts. Internet-Speed Development uses geographically distributed teams to ‘work around the clock’. This method, mostly adopted by large closed-source firms, (because they're the only ones which afford development centers in different time zones), works equally well in open source projects because a software developed by a large group of volunteers shall naturally tend to have developers spread across all time zones.
Many open-source projects are now using distributed revision control systems, which scale better than centralized repositories such as SVN and CVS.  Popular examples are git, used by the Linux kernel,[8] and Mercurial, used by the Python programming language.[citation needed]
A debugger is a computer program that is used to debug (and sometimes test or optimize) other programs. GNU Debugger (GDB) is an example of a debugger used in open-source software development. This debugger offers remote debugging, what makes it especially applicable to open-source software development.[citation needed]
A memory leak tool or memory debugger is a programming tool for finding memory leaks and buffer overflows. A memory leak is a particular kind of unnecessary memory consumption by a computer program, where the program fails to release memory that is no longer needed. Examples of memory leak detection tools used by Mozilla are the XPCOM Memory Leak tools.
Validation tools are used to check if pieces of code conform to the specified syntax. An example of a validation tool is Splint.[citation needed]
A package management system is a collection of tools to automate the process of installing, upgrading, configuring, and removing software packages from a computer. The Red Hat Package Manager (RPM) for .rpm and Advanced Packaging Tool (APT) for .deb file format, are package management systems used by a number of Linux distributions.[citation needed]
A software bug is a design defect (bug) in computer software. A computer program with many or serious bugs may be described as buggy.
The effects of a software bug range from minor (such as a misspelled word in the user interface) to severe (such as frequent crashing).
In 2002, a study commissioned by the US Department of Commerce's National Institute of Standards and Technology concluded that "software bugs, or errors, are so prevalent and so detrimental that they cost the US economy an estimated $59 billion annually, or about 0.6 percent of the gross domestic product".[1]
Since the 1950s, some computer systems have been designed to detect or auto-correct various software errors during operations.
Mistake metamorphism (from Greek meta = "change", morph = "form") refers to the evolution of a defect in the final stage of software deployment. Transformation of a mistake committed by an analyst in the early stages of the software development lifecycle, which leads to a defect in the final stage of the cycle has been called mistake metamorphism.[2]
Sometimes the use of bug to describe the behavior of software is contentious due to perception. Some suggest that the term should be abandoned; contending that bug implies that the defect arose on its own and push to use defect instead since it more clearly indicates they are caused by a human.[8]
Preventing bugs as early as possible in the software development process is a target of investment and innovation.[11][12]
Newer programming languages tend to be designed to prevent common bugs based on vulnerabilities of existing languages. Lessons learned from older languages such as BASIC and C are used to inform the design of later languages such as C# and Rust.
A compiled language allows for detecting some typos (such as a misspelled identifier) before runtime which is earlier in the software development process than for an interpreted language.
Languages may include features such as a static type system, restricted namespaces and modular programming. For example, for a typed, compiled language (like C):
Programming techniques such as programming style and defensive programming are intended to prevent typos.
Agile software development may involve frequent software releases with relatively small changes. Defects are revealed by user feedback.
Tools for static code analysis help developers by inspecting the program text beyond the compiler's capabilities to spot potential problems. Although in general the problem of finding all programming errors given a specification is not solvable (see halting problem), these tools exploit the fact that human programmers tend to make certain kinds of simple mistakes often when writing software.
Tools to monitor the performance of the software as it is running, either specifically to find problems such as bottlenecks or to give assurance as to correct working, may be embedded in the code explicitly (perhaps as simple as a statement saying PRINT "I AM HERE"), or provided as tools. It is often a surprise to find where most of the time is taken by a piece of code, and this removal of assumptions might cause the code to be rewritten.
Open source development allows anyone to examine source code. A school of thought popularized by Eric S. Raymond as Linus's law says that popular open-source software has more chance of having few or no bugs than other software, because "given enough eyeballs, all bugs are shallow".[13] This assertion has been disputed, however: computer security specialist Elias Levy wrote that "it is easy to hide vulnerabilities in complex, little understood and undocumented source code," because, "even if people are reviewing the code, that doesn't mean they're qualified to do so."[14] An example of an open-source software bug was the 2008 OpenSSL vulnerability in Debian.
Debugging can be a significant part of the software development lifecycle. Maurice Wilkes, an early computing pioneer, described his realization in the late 1940s that
“a good part of the remainder of my life was going to be spent in finding errors in my own programs”.[15]
In an embedded system, the software is often modified to work around a hardware bug since it's cheaper than modifying the hardware.
Tools are often used to track bugs and other issues with software. Typically, different tools
are used by the software development team to track their workload than by customer service to track user feedback.[17]
A tracked item is often called bug, defect, ticket, issue, feature, or for agile software development, story or epic. Items are often categorized by aspects such as severity, priority and version number.
A software release that emphasizes bug fixes may be called a maintenance release – to differentiate it from a release that emphasizes new features or other changes.
It is common practice to release software with known, low-priority bugs or other issues.  Possible reasons include but are not limited to:
The amount and type of damage a software bug may cause affects decision-making, processes and policy regarding software quality. In applications such as human spaceflight, aviation, nuclear power, health care, public transport or automotive safety, since software flaws have the potential to cause human injury or even death, such software will have far more scrutiny and quality control than, for example, an online shopping website. In applications such as banking, where software flaws have the potential to cause serious financial damage to a bank or its customers, quality control is also more important than, say, a photo editing application.
Another study in 1990 reported that exceptionally good software development processes can achieve deployment failure rates as low as 0.1 per 1000 SLOC.[29] This figure is iterated in literature such as Code Complete by Steve McConnell,[30] and the NASA study on Flight Software Complexity.[31]
Some projects even attained zero defects: the firmware in the IBM Wheelwriter typewriter which consists of 63,000 SLOC, and the Space Shuttle software with 500,000 SLOC.[29]
Numerical operations can result in unexpected output, slow processing, or crashing.[34]
Such a bug can be from a lack of awareness of the qualities of the data storage such as a loss of precision due to rounding, numerically unstable algorithms, arithmetic overflow and underflow, or from lack of awareness of how calculations are handled by different software coding languages such as division by zero which in some languages may throw an exception, and in others may return a special value such as NaN or infinity.
The Open Technology Institute, run by the group, New America,[39] released a report "Bugs in the System" in August 2016 stating that U.S. policymakers should make reforms to help researchers identify and address software bugs. The report "highlights the need for reform in the field of software vulnerability discovery and disclosure."[40] One of the report's authors said that Congress has not done enough to address cyber software vulnerability, even though Congress has passed a number of bills to combat the larger issue of cyber security.[40]
Government researchers, companies, and cyber security experts are the people who typically discover software flaws. The report calls for reforming computer crime and copyright laws.[40]
A feature is "a prominent or distinctive user-visible aspect, quality, or characteristic of a software system or systems", as defined by Kang et al.[1] At the implementation level, "it is a structure that extends and modifies the structure of a given software in order to satisfy a stakeholder’s requirement, to implement and encapsulate a design decision, and to offer a configuration option", as defined by Apel et al.[2]
The term feature means the same for software as it does for any kind of system. For example, the British Royal Navy's HMS Dreadnought (1906) was considered an important milestone in naval technology because of its advanced features that did not exist in pre-dreadnought battleships.[3]
Feature also applies to concepts such as a programming language. 
The Python programming language is well-known for its feature of using whitespace characters (spaces and tabs) instead of curly braces to indicate different blocks of code.[6]
Another similar high-level, object oriented programming language, Ruby, is noteworthy for using the symbols "@" and "$" to highlight different variable scopes, which the developers claim improves code readability. Its developers also claim that one of its important features is a high amount of flexibility.[7]
The Institute of Electrical and Electronics Engineers (IEEE) defines feature in the (obsolete) standard for software test documentation IEEE 829 as a "distinguishing characteristic of a software item (e.g., performance, portability, or functionality)".[8]
Although feature is typically used for a positive aspect of a software system, a software bug is also a feature but with negative value.
The terminal emulator xterm has many notable features, including compatibility with the X Window System, the ability to emulate a VT220 and VT320[9] terminal with ANSI color, and the ability to input escape sequences using a computer mouse or other similar device, and the ability to run on multiple different Unix-like operating systems (e.g. Linux, AIX, BSD, and HP-UX).[10]
One mechanism for introducing feature-rich software to the user is the concept of progressive disclosure, a technique where features are introduced gradually as they become required, to reduce the potential confusion caused by displaying a wealth of features at once.[11]
Sometimes, feature-rich is considered a negative attribute. The terms feature creep, software bloat, and featuritis refer to software that is overly feature-rich.[12] This type of excessive inclusion of features is in some cases a result of design by committee.[13]
To counteract the tendency of software developers to add additional, unnecessary features, the Unix philosophy was developed in the 1970s by Bell Labs employees working on the Unix operating system such as Ken Thompson and Dennis Ritchie. The philosophy can be summarized as:  software programs should generally only complete one primary task and that "small is beautiful".[14][15]
Legacy code is old computer source code that is no longer supported on standard hardware and environments, and is a codebase that is in some respect obsolete or supporting something obsolete. Legacy code may be written in programming languages, use frameworks and external libraries, or use architecture and patterns that are no longer considered modern, increasing the mental burden and ramp-up time for software engineers who work on the codebase. Legacy code may have zero or insufficient automated tests, making refactoring dangerous and likely to introduce bugs.[2] Long-lived code is susceptible to software rot, where changes to the runtime environment, or surrounding software or hardware may require maintenance or emulation of some kind to keep working. Legacy code may be present to support legacy hardware, a separate legacy system, or a legacy customer using an old feature or software version.
An example of legacy hardware are legacy ports like PS/2 and VGA ports, and CPUs with older, incompatible instruction sets (with e.g. newer operating systems). Examples in legacy software include legacy file formats like .swf for Adobe Flash or .123 for Lotus 1-2-3, and text files encoded with legacy character encodings like EBCDIC.
The first use of the term legacy to describe computer systems probably occurred in the 1960s.[3]  By the 1980s it was commonly used to refer to existing computer systems to distinguish them from the design and implementation of new systems. Legacy was often heard during a conversion process, for example, when moving data from the legacy system to a new database.
While this term may indicate that some engineers may feel that a system is out of date, a legacy system can continue to be used for a variety of reasons. It may simply be that the system still provides for the users' needs. In addition, the decision to keep an old system may be influenced by economic reasons such as return on investment challenges or vendor lock-in, the inherent challenges of change management, or a variety of other reasons other than functionality. Backward compatibility (such as the ability of newer systems to handle legacy file formats and character encodings) is a goal that software developers often include in their work.
Legacy systems are considered to be potentially problematic by some software engineers for several reasons.[4]
Printing improvements are problematic because legacy software systems often add no formatting instructions, or they use protocols that are not usable in modern PC/Windows printers. A print server can be used to intercept the data and translate it to a more modern code. Rich Text Format (RTF) or PostScript documents may be created in the legacy application and then interpreted at a PC before being printed.
The change being undertaken in some organizations is to switch to automated business process (ABP) software which generates complete systems. These systems can then interface to the organizations' legacy systems and use them as data repositories. This approach can provide a number of significant benefits: the users are insulated from the inefficiencies of their legacy systems, and the changes can be incorporated quickly and easily in the ABP software.
Model-driven reverse and forward engineering approaches can be also used for the improvement of legacy software.[11]
Andreas M. Hein researched the use of legacy systems in space exploration at the Technical University of Munich.  According to Hein, legacy systems are attractive for reuse if an organization has the capabilities for verification, validation, testing, and operational history.[12][13] These capabilities must be integrated into various software life cycle phases such as development, implementation, usage, or maintenance. For software systems, the capability to use and maintain the system are crucial. Otherwise the system will become less and less understandable and maintainable.
Some in the software engineering prefer to describe "legacy code" without the connotation of being obsolete. Among the most prevalent neutral conceptions are source code inherited from someone else and source code inherited from an older version of the software. Eli Lopian, CEO of Typemock, has defined it as "code that developers are afraid to change".[15] Michael Feathers[16] introduced a definition of legacy code as code without tests, which reflects the perspective of legacy code being difficult to work with in part due to a lack of automated regression tests. He also defined characterization tests to start putting legacy code under test.
The term legacy support is often used in conjunction with legacy systems. The term may refer to a feature of modern software.  For example, Operating systems with "legacy support" can detect and use older hardware. The term may also be used to refer to a business function; e.g. a software or hardware vendor that is supporting, or providing software maintenance, for older products.
The term "legacy mode" often refers specifically to backward compatibility. A software product that is capable of performing as though it were a previous version of itself, is said to be "running in legacy mode".  This kind of feature is common in operating systems and web browsers, where many applications depend on these underlying components.
Virtualization technology is a recent innovation allowing legacy systems to continue to operate on modern hardware by running older operating systems and browsers on a software system that emulates legacy hardware.
Database design is the organization of data according to a database model. The designer determines what data must be stored and how the data elements interrelate. With this information, they can begin to fit the data to the database model.[1] A database management system manages the data accordingly.
The first step of database design involves classifying data and identifying interrelationships. The theoretical representation of data is called an ontology or a conceptual data model.
In a majority of cases, the person designing a database is a person with expertise in database design, rather than expertise in the domain from which the data to be stored is drawn e.g. financial information, biological information etc. Therefore, the data to be stored in a particular database must be determined in cooperation with a person who does have expertise in that domain, and who is aware of the meaning of the data to be stored within the system.
This process is one which is generally considered part of requirements analysis, and requires skill on the part of the database designer to elicit the needed information from those with the domain knowledge. This is because those with the necessary domain knowledge often cannot clearly express the system requirements for the database as they are unaccustomed to thinking in terms of the discrete data elements which must be stored. Data to be stored can be determined by Requirement Specification.[2]
Once a database designer is aware of the data which is to be stored within the database, they must then determine where dependency is within the data. Sometimes when data is changed you can be changing other data that is not visible. For example, in a list of names and addresses, assuming a situation where multiple people can have the same address, but one person cannot have more than one address, the address is dependent upon the name. When provided a name and the list the address can be uniquely determined; however, the inverse does not hold – when given an address and the list, a name cannot be uniquely determined because multiple people can reside at an address.  Because an address is determined by a name, an address is considered dependent on a name.
ER models are commonly used in information system design; for example, they are used to describe information requirements and / or the types of information to be stored in the database during the conceptual structure design phase.[3]
Once the relationships and dependencies amongst the various pieces of information have been determined, it is possible to arrange the data into a logical structure which can then be mapped into the storage objects supported by the database management system. In the case of relational databases the storage objects are tables which store data in rows and columns. In an Object database the storage objects correspond directly to the objects used by the Object-oriented programming language used to write the applications that will manage and access the data. The relationships may be defined as attributes of the object classes involved or as methods that operate on the object classes.
In the field of relational database design, normalization is a systematic way of ensuring that a database structure is suitable for general-purpose querying and free of certain undesirable characteristics—insertion, update, and deletion anomalies that could lead to  loss of data integrity.
A standard piece of database design guidance is that the designer should create a fully normalized design; selective denormalization can subsequently be performed, but only for performance reasons. The trade-off is storage space vs performance. The more normalized the design is, the less data redundancy there is (and therefore, it takes up less space to store), however, common data retrieval patterns may now need complex joins, merges, and sorts to occur – which takes up more data read, and compute cycles. Some modeling disciplines, such as the dimensional modeling approach to data warehouse design, explicitly recommend non-normalized designs, i.e. designs that in large part do not adhere to 3NF. Normalization consists of normal forms that are 1NF, 2NF, 3NF, Boyce-Codd NF (3.5NF), 4NF, 5NF and 6NF.
Document databases take a different approach. A document that is stored in such a database, typically would contain more than one normalized data unit and often the relationships between the units as well. If all the data units and the relationships in question are often retrieved together, then this approach optimizes the number of retrieves. It also simplifies how data gets replicated, because now there is a clearly identifiable unit of data whose consistency is self-contained. Another consideration is that reading and writing a single document in such databases will require a single transaction – which can be an important consideration in a Microservices architecture. In such situations, often, portions of the document are retrieved from other services via an API and stored locally for efficiency reasons. If the data units were to be split out across the services, then a read (or write) to support a service consumer might require more than one service calls, and this could result in management of multiple transactions, which may not be preferred.
The physical design of the database specifies the physical configuration of the database on the storage media. This includes detailed specification of data elements and data types.
This step involves specifying the indexing options and other parameters residing in the DBMS data dictionary. It is the detailed design of a system that includes modules & the database's hardware & software specifications of the system. Some aspects that are addressed at the physical layer:
In software engineering, a software design pattern or design pattern is a general, reusable solution to a commonly occurring problem in many contexts in software design.[1] A design pattern is not a rigid structure to be transplanted directly into source code. Rather, it is a description or a template for solving a particular type of problem that can be deployed in many different situations.[2] Design patterns can be viewed as formalized best practices that the programmer may use to solve common problems when designing a software application or system.
Object-oriented design patterns typically show relationships and interactions between classes or objects, without specifying the final application classes or objects that are involved.[citation needed] Patterns that imply mutable state may be unsuited for functional programming languages. Some patterns can be rendered unnecessary in languages that have built-in support for solving the problem they are trying to solve, and object-oriented patterns are not necessarily suitable for non-object-oriented languages.[citation needed]
Design patterns may be viewed as a structured approach to computer programming intermediate between the levels of a programming paradigm and a concrete algorithm.[citation needed]
Patterns originated as an architectural concept by Christopher Alexander as early as 1977 in A Pattern Language (c.f. his article, "The Pattern of Streets," JOURNAL OF THE AIP, September, 1966, Vol. 32, No. 5, pp. 273–278). In 1987, Kent Beck and Ward Cunningham began experimenting with the idea of applying patterns to programming – specifically pattern languages – and presented their results at the OOPSLA conference that year.[3][4] In the following years, Beck, Cunningham and others followed up on this work.
Design patterns gained popularity in computer science after the book Design Patterns: Elements of Reusable Object-Oriented Software was published in 1994  by the so-called "Gang of Four" (Erich Gamma, Richard Helm, Ralph Johnson and John Vlissides), which is frequently abbreviated as "GoF". That same year, the first Pattern Languages of Programming Conference was held, and the following year the Portland Pattern Repository was set up for documentation of design patterns. The scope of the term remains a matter of dispute. Notable books in the design pattern genre include:
Design patterns can speed up the development process by providing proven development paradigms.[6] Effective software design requires considering issues that may not become apparent until later in the implementation. Freshly written code can often have hidden, subtle issues that take time to be detected; issues that sometimes can cause major problems down the road. Reusing design patterns can help to prevent such issues,[7] and enhance code readability for those familiar with the patterns.
Software design techniques are difficult to apply to a broader range of problems.[citation needed] Design patterns provide general solutions, documented in a format that does not require specifics tied to a particular problem.
In 1996, Christopher Alexander was invited to give a Keynote Speech to the 1996 OOPSLA Convention.  Here he reflected on how his work on Patterns in Architecture had developed and his hopes for how the Software Design community could help Architecture extend Patterns to create living structures that use generative schemes that are more like computer code.
The annual Pattern Languages of Programming Conference proceedings [14] include many examples of domain-specific patterns.
Object-oriented design patterns typically show relationships and interactions between classes or objects, without specifying the final application classes or objects that are involved. Patterns that imply mutable state may be unsuited for functional programming languages. Some patterns can be rendered unnecessary in languages that have built-in support for solving the problem they are trying to solve, and object-oriented patterns are not necessarily suitable for non-object-oriented languages.
Some suggest that design patterns may be a sign that features are missing in a given programming language (Java or C++ for instance). Peter Norvig demonstrates that 16 out of the 23 patterns in the Design Patterns book (which is primarily focused on C++) are simplified or eliminated (via direct language support) in Lisp or Dylan.[29] Related observations were made by Hannemann and Kiczales who implemented several of the 23 design patterns using an aspect-oriented programming language (AspectJ) and showed that code-level dependencies were removed from the implementations of 17 of the 23 design patterns and that aspect-oriented programming could simplify the implementations of design patterns.[30]
See also Paul Graham's essay "Revenge of the Nerds".[31]
By definition, a pattern must be programmed anew into each application that uses it. Since some authors see this as a step backward from software reuse as provided by components, researchers have worked to turn patterns into components. Meyer and Arnout were able to provide full or partial componentization of two-thirds of the patterns they attempted.[34]
Software design patterns offer finer granularity compared to software architecture patterns and software architecture styles, as design patterns focus on solving detailed, low-level design problems within individual components or subsystems. Examples include Singleton, Factory Method, and Observer. [35][36][37]
Software Architecture Pattern refers to a reusable, proven solution to a recurring problem at the system level, addressing concerns related to the overall structure, component interactions, and quality attributes of the system.[citation needed] Software architecture patterns operate at a higher level of abstraction than design patterns, solving broader system-level challenges. While these patterns typically affect system-level concerns, the distinction between architectural patterns and architectural styles can sometimes be blurry. Examples include Circuit Breaker. [35][36][37]
Software Architecture Style refers to a high-level structural organization that defines the overall system organization, specifying how components are organized, how they interact, and the constraints on those interactions.[citation needed] Architecture styles typically include a vocabulary of component and connector types, as well as semantic models for interpreting the system's properties. These styles represent the most coarse-grained level of system organization. Examples include Layered Architecture, Microservices, and Event-Driven Architecture. [35][36][37]
Open-source software (OSS) is computer software that is released under a license in which the copyright holder grants users the rights to use, study, change, and distribute the software and its source code to anyone and for any purpose.[1][2] Open-source software may be developed in a collaborative, public manner. Open-source software is a prominent example of open collaboration, meaning any capable user is able to participate online in development, making the number of possible contributors indefinite. The ability to examine the code facilitates public trust in the software.[3]
Open-source software development can bring in diverse perspectives beyond those of a single company. A 2024 estimate of the value of open-source software to firms is $8.8 trillion, as firms would need to spend 3.5 times the amount they currently do without the use of open source software.[4]
Open-source code can be used for studying and allows capable end users to adapt software to their personal needs in a similar way user scripts and custom style sheets allow for web sites, and eventually publish the modification as a fork for users with similar preferences, and directly submit possible improvements as pull requests.
The Open Source Initiative's (OSI) definition is recognized by several governments internationally[5] as the standard or de facto definition. OSI uses The Open Source Definition to determine whether it considers a software license open source. The definition was based on the Debian Free Software Guidelines, written and adapted primarily by Perens.[6][7][8] Perens did not base his writing on the "four freedoms" from the Free Software Foundation (FSF), which were only widely available later.[9]
Under Perens' definition, open source is a broad software license that makes source code available to the general public with relaxed or non-existent restrictions on the use and modification of the code. It is an explicit "feature" of open source that it puts very few restrictions on the use or distribution by any organization or user, in order to enable the rapid evolution of the software.[10]
According to Feller et al. (2005), the terms "free software" and "open-source software" should be applied to any "software products distributed under terms that allow users" to use, modify, and redistribute the software "in any manner they see fit, without requiring that they pay the author(s) of the software a royalty or fee for engaging in the listed activities."[11]
Despite initially accepting it,[12] Richard Stallman of the FSF now flatly opposes the term "Open Source" being applied to what they refer to as "free software". Although he agrees that the two terms describe "almost the same category of software", Stallman considers equating the terms incorrect and misleading.[13] Stallman also opposes the professed pragmatism of the Open Source Initiative, as he fears that the free software ideals of freedom and community are threatened by compromising on the FSF's idealistic standards for software freedom.[14] The FSF considers free software to be a subset of open-source software, and Richard Stallman explained that DRM software, for example, can be developed as open source, despite that it does not give its users freedom (it restricts them), and thus does not qualify as free software.[13]
In his 1997 essay The Cathedral and the Bazaar, open-source influential contributor Eric S. Raymond suggests a model for developing OSS known as the bazaar model.[15] Raymond likens the development of software by traditional methodologies to building a cathedral, with careful isolated work by individuals or small groups.[15] He suggests that all software should be developed using the bazaar style, with differing agendas and approaches.[15]
In the traditional model of development, which he called the cathedral model, development takes place in a centralized way.[15] Roles are clearly defined.[15] Roles include people dedicated to designing (the architects), people responsible for managing the project, and people responsible for implementation.[15] Traditional software engineering follows the cathedral model.[15]
The bazaar model, however, is different.[15] In this model, roles are not clearly defined.[15] Some proposed characteristics of software developed using the bazaar model should exhibit the following patterns:[16]
Users should be treated as co-developers: The users are treated like co-developers and so they should have access to the source code of the software.[16] Furthermore, users are encouraged to submit additions to the software, code fixes for the software, bug reports, documentation, etc. Having more co-developers increases the rate at which the software evolves.[16] Linus's law states that given enough eyeballs all bugs are shallow.[16] This means that if many users view the source code, they will eventually find all bugs and suggest how to fix them.[16] Some users have advanced programming skills, and furthermore, each user's machine provides an additional testing environment.[16] This new testing environment offers the ability to find and fix a new bug.[16]
Early releases: The first version of the software should be released as early as possible so as to increase one's chances of finding co-developers early.[16]
Several versions: There should be at least two versions of the software.[16] There should be a buggier version with more features and a more stable version with fewer features.[16] The buggy version (also called the development version) is for users who want the immediate use of the latest features and are willing to accept the risk of using code that is not yet thoroughly tested.[16] The users can then act as co-developers, reporting bugs and providing bug fixes.[16][18]
High modularization: The general structure of the software should be modular allowing for parallel development on independent components.[16]
Dynamic decision-making structure: There is a need for a decision-making structure, whether formal or informal, that makes strategic decisions depending on changing user requirements and other factors.[16] Compare with extreme programming.[16]
Moreover, lower costs of marketing and logistical services are needed for OSS.[21] OSS can be a tool to promote a company's image, including its commercial products.[22] The OSS development approach has helped produce reliable, high quality software quickly and inexpensively.[21]
Open source development offers the potential to quicken innovation and create of social value.[23] In France for instance, a policy that incentivized government to favor free open-source software increased to nearly 600,000 OSS contributions per year, generating social value by increasing the quantity and quality of open-source software.[23] This policy also led to an estimated increase of up to 18% of tech startups and a 14% increase in the number of people employed in the IT sector.[23]
OSS can be highly reliable when it has thousands of independent programmers testing and fixing bugs of the software.[16] Open source is not dependent on the company or author that originally created it.[24] Even if the company fails, the code continues to exist and be developed by its users.[24]
Moreover, free software can be developed in accordance with purely technical requirements.[26] It does not require thinking about commercial pressure that often degrades the quality of the software.[26] Commercial pressures make traditional software developers pay more attention to customers' requirements than to security requirements, since such features are somewhat invisible to the customer.[26]
In open-source software development, tools are used to support the development of the product and the development process itself.[18]
Version control systems such as Centralized Version control system (CVCS) and the distributed version control system (DVCS)  are examples of tools, often open source, that help manage the source code files and the changes to those files for a software project in order to foster collaboration.[27] CVCS are centralized with a central repository while DVCS are decentralized and have a local repository for every user.[27] concurrent versions system (CVS) and later Subversion (SVN) and Git are examples of CVCS.[27] The repositories are hosted and published on source-code-hosting facilities such as GitHub.[27]
Open-source projects use utilities such as issue trackers to organize open-source software development. Commonly used bug trackers include Bugzilla and Redmine.[18]
Some possible ways of contributing to open-source software include such roles as programming, user interface design and testing, web design, bug triage, accessibility design and testing, UX design, code testing, and security review and testing.[28] However, there are several ways of contributing to OSS projects even without coding skills.[28] For example, some less technical ways of participating are documentation writing and editing, translation, project management, event organization and coordination, marketing, release management, community management, and public relations and outreach.[28]
The adoption of open-source software by industry is increasing over time.[32] OSS is popular in several industries such as telecommunications, aerospace, healthcare, and media & entertainment due to the benefits it provides.[33] Adoption of OSS is more likely in larger organizations and is dependent on the company's IT usage, operating efficiencies, and the productivity of employees.[32]
Industries are likely to use OSS due to back-office functionality, sales support, research and development, software features, quick deployment, portability across platforms and avoidance of commercial license management.[32] Additionally, lower cost for hardware and ownership are also important benefits.[32]
Organizations that contribute to the development and expansions of free and open-source software movements exist all over the world.[28] These organizations are dedicated to goals such as teaching and spreading technology.[28] As listed by a former vice president of the Open Source Initiative, some American organizations include the Free Software Foundation, Software Freedom Conservancy, the Open Source Initiative and Software in the Public Interest.[28] Within Europe some notable organizations are Free Software Foundation Europe, open-source projects EU (OSP) and OpenForum Europe (OFE).[28] One Australian organization is Linux Australia while Asia has Open source Asia and FOSSAsia.[28] Free and open source software for Africa (FOSSFA) and OpenAfrica are African organizations and Central and South Asia has such organizations as FLISOL and GRUP de usuarios de software libre Peru.[28] Outside of these, many more organizations dedicated to the advancement of open-source software exist.[28]
FOSS products are generally licensed under two types of licenses: permissive licensing and copyleft licensing.[34] Both of these types of licenses are different than proprietary licensing in that they can allow more users access to the software and allow for the creation of derivative works as specified by the terms of the specific license, as each license has its own rules.[34] Permissive licenses allow recipients of the software to implement the author's copyright rights without having to use the same license for distribution.[34] Examples of this type of license include the BSD, MIT, and Apache licenses.[34] Copyleft licenses are different in that they require recipients to use the same license for at least some parts of the distribution of their works.[34] Strong copyleft licenses require all derivative works to use the same license while weak copyleft licenses require the use of the same license only under certain conditions.[34] Examples of this type of license include the GNU family of licenses, and the MPL and EPL licenses.[34] The similarities between these two categories of licensing include that they provide a broad grant of copyright rights, require that recipients preserve copyright notices, and that a copy of the license is provided to recipients with the code.[34]
One important legal precedent for open-source software was created in 2008, when the Jacobson v Katzer case enforced terms of the Artistic license, including attribution and identification of modifications.[34] The ruling of this case cemented enforcement under copyright law when the conditions of the license were not followed.[34] Because of the similarity of the Artistic license to other open-source software licenses, the ruling created a precedent that applied widely.[34]
Examples of free-software license / open-source licenses include Apache licenses, BSD licenses, GNU General Public Licenses, GNU Lesser General Public License, MIT License, Eclipse Public License and Mozilla Public License.[34]
Several gray areas exist within software regulation that have great impact on open-source software, such as if software is a good or service, what can be considered a modification, governance through contract vs license, ownership and right of use.[34] While there have been developments on these issues, they often lead to even more questions.[34] The existence of these uncertainties in regulation has a negative impact on industries involved in technologies as a whole.[34]
Within the legal history of software as a whole, there was much debate on whether to protect it as intellectual property under patent law, copyright law or establishing a unique regulation.[34] Ultimately, copyright law became the standard with computer programs being considered a form of literary work, with some tweaks of unique regulation.[34]
Software is generally considered source code and object code, with both being protectable, though there is legal variety in this definition.[34] Some jurisdictions attempt to expand or reduce this conceptualization for their own purposes.[34] For example, The European Court of Justice defines a computer program as not including the functionality of a program, the programing language, or the format of data files.[34] By limiting protections of the different aspects of software, the law favors an open-source approach to software use.[34] The US especially has an open approach to software, with most open-source licenses originating there.[34] However, this has increased the focus on patent rights within these licenses, which has seen backlash from the OSS community, who prefer other forms of IP protection.[34]
Another issue includes technological protection measures (TPM) and digital rights management (DRM) techniques which were internationally legally recognized and protected in the 1996 World Intellectual Property Organization (WIPO) Treaty.[34] Open source software proponents disliked these technologies as they constrained end-users potentially beyond copyright law.[34] Europe responded to such complaints by putting TPM under legal controls, representing a victory for OSS supporters.[34]
In open-source communities, instead of owning the software produced, the producer owns the development of the evolving software.[35] In this way, the future of the software is open, making ownership or intellectual property difficult within OSS.[35] Licensing and branding can prevent others from stealing it, preserving its status as a public good.[35] Open source software can be considered a public good as it is available to everyone and does not decrease in value for others when downloaded by one person.[35] Open source software is unique in that it becomes more valuable as it is used and contributed to, instead of diminishing the resource. This is explained by concepts such as investment in reputation and network effects.[35]
The economic model of open-source software can be explained as developers contribute work to projects, creating public benefits.[35] Developers choose projects based on the perceived benefits or costs, such as improved reputation or value of the project.[35] The motivations of developers can come from many different places and reasons, but the important takeaway is that money is not the only or even most important incentivization.[35]
Throughout the mid 2000s, more and more tech companies have begun to use OSS.[24] For example, Dell's move of selling computers with GNU/Linux already installed.[24] Microsoft itself has launched a Linux-based operating system despite previous animosity with the OSS movement.[24] Despite these developments, these companies tend to only use OSS for certain purposes, leading to worries that OSS is being taken advantage of by corporations and not given anything in return.[24]
While many governments are interested in implementing and promoting open-source software due to the many benefits provided, a huge issue to be considered is cybersecurity.[36] While accidental vulnerabilities are possible, so are attacks by outside agents.[36] Because of these fears, governmental interest in contributing to the governance of software has become more prominent.[36] However, these are the broad strokes of the issue, with each country having their own specific politicized interactions with open-source software and their goals for its implementation.[36] For example, the United States has focused on national security in regard to open-source software implementation due to the perceived threat of the increase of open-source software activity in countries like China and Russia, with the Department of Defense considering multiple criteria for using OSS.[36] These criteria include: if it comes from and is maintained by trusted sources, whether it will continue to be maintained, if there are dependencies on sub-components in the software, component security and integrity, and foreign governmental influence.[36]
Another issue for governments in regard to open source is their investments in technologies such as operating systems, semiconductors, cloud, and artificial intelligence.[36] These technologies all have implications for global cooperation, again opening up security issues and political consequences.[36] Many countries have to balance technological innovation with technological dependence in these partnerships.[36] For example, after China's open-source dependent company Huawei was prevented from using Google's Android system in 2019, they began to create their own alternative operating system: Harmony OS.[36]
Germany recently established a Sovereign Tech Fund, to help support the governance and maintenance of the software that they use.
In the early days of computing, such as the 1950s and into the 1960s, programmers and developers shared software to learn from each other and evolve the field of computing.[37] For example, Unix included the operating system source code for users.[37] Eventually, the commercialization of software in the years 1970–1980 began to prevent this practice.[37] However, academics still often developed software collaboratively.[37]
In response, the open-source movement was born out of the work of skilled programmer enthusiasts, widely referred to as hackers or hacker culture.[38] One of these enthusiasts, Richard Stallman, was a driving force behind the free software movement, which would later allow for the open-source movement.[17] In 1984, he resigned from MIT to create a free operating system, GNU, after the programmer culture in his lab was stifled by proprietary software preventing source code from being shared and improved upon.[17] GNU was UNIX compatible, meaning that the programmer enthusiasts would still be familiar with how it worked.[17] However, it quickly became apparent that there was some confusion with the label Stallman had chosen of free software, which he described as free as in free speech, not free beer, referring to the meaning of free as freedom rather than price.[17] He later expanded this concept of freedom to the four essential freedoms.[17] Through GNU, open-source norms of incorporating others' source code, community bug fixes and suggestions of code for new features appeared.[17] In 1985, Stallman founded the Free Software Foundation (FSF) to promote changes in software and to help write GNU.[17] In order to prevent his work from being used in proprietary software, Stallman created the concept of copyleft, which allowed the use of his work by anyone, but under specific terms.[17] To do this, he created the GNU General Public License (GNU GPL) in 1989, which was updated in 1991.[17]  In 1991, GNU was combined with the Linux kernel written by Linus Torvalds, as a kernel was missing in GNU.[39] The operating system is now usually referred to as Linux.[17] Throughout this whole period, there were many other free software projects and licenses around at the time, all with different ideas of what the concept of free software was and should be, as well as the morality of proprietary software, such as Berkeley Software Distribution, TeX, and the X Window System.[40]
As free software developed, the Free Software Foundation began to look how to bring free software ideas and perceived benefits to the commercial software industry.[40] It was concluded that FSF's social activism was not appealing to companies and they needed a way to rebrand the free software movement to emphasize the business potential of sharing and collaborating on software source code.[40] The term open source was suggested by Christine Peterson in 1998 at a meeting of supporters of free software.[17] Many in the group felt the name free software was confusing to newcomers and holding back industry interest and they readily accepted the new designation of open source, creating the Open Source Initiative (OSI) and the OSI definition of what open source software is.[17] The Open Source Initiative's (OSI) definition is now recognized by several governments internationally as the standard or de facto definition.[39] The definition was based on the Debian Free Software Guidelines, written and adapted primarily by Bruce Perens.[41] The OSI definition differed from the free software definition in that it allows the inclusion of proprietary software and allows more liberties in its licensing.[17] Some, such as Stallman, agree more with the original concept of free software as a result because it takes a strong moral stance against proprietary software, though there is much overlap between the two movements in terms of the operation of the software.[17]
While the Open Source Initiative sought to encourage the use of the new term and evangelize the principles it adhered to, commercial software vendors found themselves increasingly threatened by the concept of freely distributed software and universal access to an application's source code, with an executive of Microsoft calling open source an intellectual property destroyer in 2001.[42] However, while free and open-source software (FOSS) has historically played a role outside of mainstream private software development, companies as large as Microsoft have begun to develop official open source presences on the Internet.[42] IBM, Oracle, and State Farm are just a few of the companies with a serious public stake in today's competitive open source market, marking a significant shift in the corporate philosophy concerning the development of FOSS.[42]
The future of the open source software community, and the free software community by extension, has become successful if not confused about what it stands for.[24] For example, Android and Ubuntu are examples milestones of success in the open source software rise to prominence from the sidelines of technological innovation as it existed in the early 2000s.[24] However, some in the community consider them failures in their representation of OSS due to issues such as the downplaying of the OSS center of Android by Google and its partners, the use of an Apache license that allowed forking and resulted in a loss of opportunities for collaboration within Android, the prioritization of convenience over freedom in Ubuntu, and features within Ubuntu that track users for marketing purposes.[24]
Open source software differs from proprietary software in that it is publicly available, the license requires no fees, modifications and distributions are allowed under license specifications.[43] All of this works to prevent a monopoly on any OSS product, which is a goal of proprietary software.[43] Proprietary software limits their customers' choices to either committing to using that software, upgrading it or switching to other software, forcing customers to have their software preferences impacted by their monetary cost.[43] The ideal case scenario for the proprietary software vendor would be a lock-in, where the customer does not or cannot switch software due to these costs and continues to buy products from that vendor.[43]
Within proprietary software, bug fixes can only be provided by the vendor, moving platforms requires another purchase and the existence of the product relies on the vendor, who can discontinue it at any point.[38] Additionally, proprietary software does not provide its source code and cannot be altered by users.[17] For businesses, this can pose a security risk and source of frustration, as they cannot specialize the product to their needs, and there may be hidden threats or information leaks within the software that they cannot access or change.[17]
Under OSI's definition, open source is a broad software license that makes source code available to the general public with relaxed or non-existent restrictions on the use and modification of the code.[44] It is an explicit feature of open source that it puts very few restrictions on the use or distribution by any organization or user, in order to enable the rapid evolution of the software.[44]
Richard Stallman, leader of the Free software movement and member of the free software foundation opposes the term open source being applied to what they refer to as free software.[13] Although he agrees that the two terms describe almost the same category of software, Stallman considers equating the terms incorrect and misleading.[13]  He believes that the main difference is that by choosing one term over the other lets others know about what one's goals are: development (open source) or a social stance (free software).[45] Nevertheless, there is significant overlap between open source software and free software.[13] Stallman also opposes the professed pragmatism of the Open Source Initiative, as he fears that the free software ideals of freedom and community are threatened by compromising on the FSF's idealistic standards for software freedom.[45] The FSF considers free software to be a subset of open-source software, and Richard Stallman explained that DRM software, for example, can be developed as open source, despite how it restricts its users, and thus does not qualify as free software.[13]
The FSF said that the term open source fosters an ambiguity of a different kind such that it confuses the mere availability of the source with the freedom to use, modify, and redistribute it.[13] On the other hand, the term free software was criticized for the ambiguity of the word free, which was seen as discouraging for business adoption, and for the historical ambiguous usage of the term.[45]
Developers have used the alternative terms Free and Open Source Software (FOSS), or Free/Libre and Open Source Software (FLOSS), consequently, to describe open-source software that is also free software.[28]
Software can be distributed with source code, which is a code that is readable.[46] Software is source available when this source code is available to be seen.[46] However to be source available or FOSS, the source code does not need to be accessible to all, just the users of that software.[46] While all FOSS software is source available because this is a requirement made by the Open Source Definition, not all source available software is FOSS.[46] For example, if the software does not meet other aspects of the Open Source Definition such as permitted modification or redistribution, even if the source code is available, the software is not FOSS.[46]
A recent trend within software companies is open sourcing, or transitioning their previous proprietary software into open source software through releasing it under an open-source license.[47][48] Examples of companies who have done this are Google, Microsoft and Apple.[47] Additionally, open sourcing can refer to programming open source software or installing open source software.[48] Open sourcing can be beneficial in multiple ways, such as attracting more external contributors who bring new perspectives and problem solving capabilities.[47] The downsides of open sourcing include the work that has to be done to maintaining the new community, such as making the base code easily understandable, setting up communication channels for new developers and creating documentation to allow new developers to easily join.[47] However, a review of several open sourced projects found that although a newly open sourced project attracts many newcomers, a great amount are likely to soon leave the project and their forks are also likely to not be impactful.[47]
Other concepts that may share some similarities to open source are shareware, public domain software, freeware, and software viewers/readers that are freely available but do not provide source code.[17] However, these differ from open source software in access to source code, licensing, copyright and fees.[17]
Despite being able to collaborate internationally, open source software contributors were found to mostly be located in large clusters such as Silicon Valley that largely collaborate within themselves.[49] Possible reasons for this phenomenon may be that the OSS contributor demographic largely works in software, meaning that the OSS geographic location is closely related to that dispersion and collaborations could be encouraged through work and social networks.[49] Code acceptance can be impacted by status within these social network clusters, creating unfair predispositions in code acceptance based on location.[50] Barriers to international collaboration also include linguistic or cultural differences.[51] Furthermore, each country has been shown to have a higher acceptance rate for code from contributors within their country except India, indicating a bias for culturally similar collaborators.[51]
In 2021, the countries with the highest open source software contributions included the United States, China, Germany, India, and the UK, in that order.[49] The counties with the highest OSS developers per capita from a study in 2021 include, in order, Iceland, Switzerland, Norway, Sweden, and Finland, while in 2008 the countries with top amount of estimated contributors in SourceForge were the United States, Germany, United Kingdom, Canada and France.[49][51] Though there have been several studies done on the distribution and contributions of OSS developers, this is still an open field that can be measured in several different ways.[51] For instance, Information and communication technology participation, population, wealth and proportion of access to the internet have been shown to be correlated with OSS contributions.[51]
Although gender diversity has been found to enhance team productivity, women still face biases while contributing to open source software projects when their gender is identifiable.[52] In 2002, only 1.5% of international open-source software developers were women, while women made up 28% of tech industry roles, demonstrating their low representation in the software field.[53] Despite OSS contributions having no prerequisites, this gender bias may continue to exist due to the common belief of contributors that gender should not matter, and the quality of code should be the only consideration for code acceptance, preventing the community from addressing the systemic disparities in female representation.[38] However, a more recent figure of female OSS participation internationally calculated across 2005 to 2021 is 9.8%, with most being recent contributors, indicating that female participation may be growing.[54]
Even though programming was originally seen as a female profession, there remains a large gap in computing.[55] Social identity tends to be a large concern as women in the tech industry face insecurity about attracting unwanted male attention and harassment or being unfeminine in their technology knowledge, having a large impact on confidence.[38] Some male tech participants make clear that they believe women fitting in within the culture is impossible, furthering the insecurity for women and their place in the tech industry.[52] Additionally, even in a voluntary contribution environment like open source software, women tend to end up doing the less technical aspects of projects, such as manual testing or documentation despite women and men showing the same productivity in OSS contributions.[52] Explicit biases include longer feedback time, more scrutinization of code and lower acceptance rate of code.[52] Specifically in the open-source software community, women report that sexually offensive language is common and the women's identity as female is given more attention that as an OSS contributor[38] Bias is hard to address due to the belief that gender should not matter, with most contributors feeling that women getting special treatment is unfair and success should be dependent on skill, preventing any changes to be more inclusive.[38]
Open source software projects are built and maintained by a network of programmers, who may often be volunteers, and are widely used in free as well as commercial products.[56]
Unix: Unix is an operating system created by AT&T that began as a precursor to open source software in that the free and open-source software revolution began when developers began trying to create operating systems without Unix code.[24] Unix was created in the 1960s, before the commercialization of software and before the concept of open source software was necessary, therefore it was not considered a true open source software project.[24] It started as a research project before being commercialized in the mid 1980s.[24] Before its commercialization, it represented many of the ideals held by the Free and Open source software revolution, including the decentralized collaboration of global users, rolling releases and a community culture of distaste towards proprietary software.[24]
BSD: Berkeley Software Distribution (BSD) is an operating system that began as a variant of Unix in 1978 that mixed Unix code with code from Berkeley labs to increase functionality.[24] As BSD was focused on increasing functionality, it would publicly share its greatest innovations with the main Unix operating system.[24] This is an example of the free public code sharing that is a central characteristic of FOSS today.[24] As Unix became commercialized in the 1980s, developers or members of the community who did not support proprietary software began to focus on BSD and turning it into an operating system that did not include any of Unix's code.[24] The final version of BSD was released in 1995.[24]
GNU: GNU is a free operating system created by Richard Stallman in 1984 with its name meaning Gnu's Not Unix.[24] The idea was to create a Unix alternative operating system that would be available for anyone to use and allow programmers to share code freely between them.[24] However, the goal of GNU was not to only replace Unix, but to make a superior version that had more technological capabilities.[24] It was released before the philosophical beliefs of the Free and Open source software revolution were truly defined.[24] Because of its creation by prominent FOSS programmer Richard Stallman, GNU was heavily involved in FOSS activism, with one of the greatest achievements of GNU being the creation of the GNU General Public License or GPL, which allowed developers to release software that could be legally shared and modified.[24]
Linux: Linux is an operating system kernel that was introduced in 1991 by Linus Torvalds.[24] Linux was inspired by making a better version of the for profit operating service Minux.[24] It was radically different than what other hackers were producing at the time due to it being totally free of cost and being decentralized.[24] Later, Linux was put under the GPL license, allowing people to make money with Linux and bringing Linux into the FOSS community.[24]
While the term open source applied originally only to the source code of software, it is now being applied to many other areas such as open-source ecology, a movement to decentralize technologies so that any human can use them.[13][57] However, it is often misapplied to other areas that have different and competing principles, which overlap only partially.[38]
The same principles that underlie open-source software can be found in many other ventures, such as open source, open content, and open collaboration.[58][3]
More than 90 percent of companies use open-source software as a component of their proprietary software.[59] The decision to use open-source software, or even engage with open-source projects to improve existing open-source software, is typically a pragmatic business decision.[60][61] When proprietary software is in direct competition with an open-source alternative, research has found conflicting results on the effect of the competition on the proprietary product's price and quality.[62]
For decades, some companies have made servicing of an open-source software product for enterprise users their business model. These companies control an open-source software product, and instead of charging for licensing or use, charge for improvements, integration, and other servicing.[63] Software as a service (SaaS) products based on open-source components are increasingly common.[64]
Open-source software is preferred for scientific applications, because it increases transparency and aids in the validation and acceptance of scientific results.[65]
In object-oriented programming, a class defines the shared aspects of objects created from the class. The capabilities of a class differ between programming languages, but generally the shared aspects consist of state (variables) and behavior (methods) that are each either associated with a particular object or with all objects of that class.[1][2]
An object expresses data type as an interface – the type of each member variable and the signature of each member function (method). A class defines an implementation of an interface, and instantiating the class results in an object that exposes the implementation via the interface.[3] In the terms of type theory, a class is an implementation‍—‌a concrete data structure and collection of subroutines‍—‌while a type is an interface. Different (concrete) classes can produce objects of the same (abstract) type (depending on type system). For example, the type (interface) Stack might be implemented by SmallStack that is fast for small stacks but scales poorly and ScalableStack that scales well but has high overhead for small stacks.
Some programming languages such as Eiffel support specification of invariants as part of the definition of the class, and enforce them through the type system. Encapsulation of state is necessary for being able to enforce the invariants of the class.
Every class implements (or realizes) an interface by providing structure and behavior. Structure consists of data and state, and behavior consists of code that specifies how methods are implemented.[8] There is a distinction between the definition of an interface and the implementation of that interface; however, this line is blurred in many programming languages because class declarations both define and implement an interface. Some languages, however, provide features that separate interface and implementation. For example, an abstract class can define an interface without providing an implementation.
In languages that support access specifiers, the interface of a class is considered to be the set of public members of the class, including both methods and attributes (via implicit getter and setter methods); any private members or internal data structures are not intended to be depended on by external code and thus are not part of the interface.
Object-oriented programming methodology dictates that the operations of any interface of a class are to be independent of each other. It results in a layered design where clients of an interface use the methods declared in the interface. An interface places no requirements for clients to invoke the operations of one interface in any particular order. This approach has the benefit that client code can assume that the operations of an interface are available for use whenever the client has access to the object.[9]
The various object-oriented programming languages enforce member accessibility and visibility to various degrees, and depending on the language's type system and compilation policies, enforced at either compile time or runtime. For example, the Java language does not allow client code that accesses the private data of a class to compile.[12] In the C++ language, private methods are visible, but not accessible in the interface; however, they may be made invisible by explicitly declaring fully abstract classes that represent the interfaces of the class.[13]
Most object-oriented programming languages allow the programmer to specify which classes are considered abstract and will not allow these to be instantiated. For example, in Java, C# and PHP, the keyword abstract is used.[26][27] In C++, an abstract class is a class having at least one abstract method given by the appropriate syntax in that language (a pure virtual function in C++ parlance).[25]
Software reuse is also a major benefit of using Object classes. Classes facilitate re-use via inheritance and interfaces. When a new behavior is required it can often be achieved by creating a new class and having that class inherit the default behaviors and data of its superclass and then tailoring some aspect of the behavior or data accordingly. Re-use via interfaces (also known as methods) occurs when another object wants to invoke (rather than create a new kind of) some object class. This method for re-use removes many of the common errors that can make their way into software when one program re-uses code from another.[52]
As a data type, a class is usually considered as a compile time construct.[53] A language or library may also support prototype or factory metaobjects that represent runtime information about classes, or even represent metadata that provides access to reflective programming (reflection) facilities and ability to manipulate data structure formats at runtime. Many languages distinguish this kind of run-time type information about classes from a class on the basis that the information is not needed at runtime. Some dynamic languages do not make strict distinctions between runtime and compile time constructs, and therefore may not distinguish between metaobjects and classes.
In contrast to creating an object from a class, some programming contexts support object creation by copying (cloning) a prototype object.[54]
A method in object-oriented programming (OOP) is a procedure associated with an object, and generally also a message. An object consists of state data and behavior; these compose an interface, which specifies how the object may be used. A method is a behavior of an object parametrized by a user.
In class-based programming, methods are defined within a class, and objects are instances of a given class. One of the most important capabilities that a method provides is method overriding - the same name (e.g., area) can be used for multiple different kinds of classes. This allows the sending objects to invoke behaviors and to delegate the implementation of those behaviors to the receiving object. A method in Java programming sets the behavior of a class object. For example, an object can send an area message to another object and the appropriate formula is invoked whether the receiving object is a rectangle, circle, triangle, etc.
These methods provide an abstraction layer that facilitates encapsulation and modularity. For example, if a bank-account class provides a getBalance() accessor method to retrieve the current balance (rather than directly accessing the balance data fields), then later revisions of the same code can implement a more complex mechanism for balance retrieval (e.g., a database fetch), without the dependent code needing to be changed. The concepts of encapsulation and modularity are not unique to object-oriented programming. Indeed, in many ways the object-oriented approach is simply the logical extension of previous paradigms such as abstract data types and structured programming.[3]
An abstract method is one with only a signature and no implementation body. It is often used to specify that a subclass must provide an implementation of the method, as in an abstract class. Abstract methods are used to specify interfaces in some programming languages.[6]
Some procedural languages were extended with object-oriented capabilities to leverage the large skill sets and legacy code for those languages but still provide the benefits of object-oriented development. Perhaps the most well-known example is C++, an object-oriented extension of the C programming language. Due to the design requirements to add the object-oriented paradigm on to an existing procedural language, message passing in C++ has some unique capabilities and terminologies. For example, in C++ a method is known as a member function. C++ also has the concept of virtual functions which are member functions that can be overridden in derived classes and allow for dynamic dispatch.
An application programming interface (API) is a connection between computers or between computer programs. It is a type of software interface, offering a service to other pieces of software.[1] A document or standard that describes how to build such a connection or interface is called an API specification. A computer system that meets this standard is said to implement or expose an API. The term API may refer either to the specification or to the implementation.
In contrast to a user interface, which connects a computer to a person, an application programming interface connects computers or pieces of software to each other. It is not intended to be used directly by a person (the end user) other than a computer programmer[1] who is incorporating it into software. An API is often made up of different parts which act as tools or services that are available to the programmer. A program or a programmer that uses one of these parts is said to call that portion of the API. The calls that make up the API are also known as subroutines, methods, requests, or endpoints. An API specification defines these calls, meaning that it explains how to use or implement them.
The term API is often used to refer to web APIs,[2] which allow communication between computers that are joined by the internet. There are also APIs for programming languages, software libraries, computer operating systems, and computer hardware. APIs originated in the 1940s, though the term did not emerge until the 1960s and 70s.
An API opens a software system to interactions from the outside. It allows two software systems to communicate across a boundary — an interface — using mutually agreed-upon signals.[3] In other words, an API connects software entities together. Unlike a user interface, an API is typically not visible to users. It is an "under the hood" portion of a software system, used for machine-to-machine communication.[4]
A well-designed API exposes only objects or actions needed by software or software developers. It hides details that have no use. This abstraction simplifies programming.[5]
Building software using APIs has been compared to using building-block toys, such as Lego bricks. Software services or software libraries are analogous to the bricks; they may be joined together via their APIs, composing a new software product.[6] The process of joining is called integration.[3]
An API is often compared to a contract. It represents an agreement between parties: a service provider who offers the API and the software developers who rely upon it. If the API remains stable, or if it changes only in predictable ways, developers' confidence in the API will increase. This may increase their use of the API.[8]
The term API initially described an interface only for end-user-facing programs, known as application programs. This origin is still reflected in the name "application programming interface." Today, the term is broader, including also utility software and even hardware interfaces.[10]
The idea of the API is much older than the term itself. British computer scientists Maurice Wilkes and David Wheeler worked on a modular software library in the 1940s for EDSAC, an early computer. The subroutines in this library were stored on punched paper tape organized in a filing cabinet. This cabinet also contained what Wilkes and Wheeler called a "library catalog" of notes about each subroutine and how to incorporate it into a program. Today, such a catalog would be called an API (or an API specification or API documentation) because it instructs a programmer on how to use (or "call") each subroutine that the programmer needs.[10]
The term "application program interface" (without an -ing suffix) is first recorded in a paper called Data structures and techniques for remote computer graphics presented at an AFIPS conference in 1968.[12][10] The authors of this paper use the term to describe the interaction of an application—a graphics program in this case—with the rest of the computer system. A consistent application interface (consisting of Fortran subroutine calls) was intended to free the programmer from dealing with idiosyncrasies of the graphics display device, and to provide hardware independence if the computer or the display were replaced.[11]
The term was introduced to the field of databases by C. J. Date[13] in a 1974 paper called The Relational and Network Approaches: Comparison of the Application Programming Interface.[14] An API became a part of the ANSI/SPARC framework for database management systems. This framework treated the application programming interface separately from other interfaces, such as the query interface. Database professionals in the 1970s observed these different interfaces could be combined; a sufficiently rich application interface could support the other interfaces as well.[9]
This observation led to APIs that supported all types of programming, not just application programming. By 1990, the API was defined simply as "a set of services available to a programmer for performing certain tasks" by technologist Carl Malamud.[15]
Roy Fielding's dissertation Architectural Styles and the Design of Network-based Software Architectures at UC Irvine in 2000 outlined Representational state transfer (REST) and described the idea of a "network-based Application Programming Interface" that Fielding contrasted with traditional "library-based" APIs.[17] XML and JSON web APIs saw widespread commercial adoption beginning in 2000 and continuing as of 2021. The web API is now the most common meaning of the term API.[2]
The Semantic Web proposed by Tim Berners-Lee in 2001 included "semantic APIs" that recast the API as an open, distributed data interface rather than a software behavior interface.[18] Proprietary interfaces and agents became more widespread than open ones, but the idea of the API as a data interface took hold. Because web APIs are widely used to exchange data of all kinds online, API has become a broad term describing much of the communication on the internet.[16] When used in this way, the term API has overlap in meaning with the term communication protocol.
The interface to a software library is one type of API. The API describes and prescribes the "expected behavior" (a specification) while the library is an "actual implementation" of this set of rules.
A single API can have multiple implementations (or none, being abstract) in the form of different libraries that share the same programming interface.
API use can vary depending on the type of programming language involved.
An API for a procedural language such as Lua could consist primarily of basic routines to execute code, manipulate data or handle errors while an API for an object-oriented language, such as Java, would provide a specification of classes and its class methods.[20][21] Hyrum's law states that "With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody."[22] Meanwhile, several studies show that most applications that use an API tend to use a small part of the API.[23]
An API can also be related to a software framework: a framework can be based on several libraries implementing several APIs, but unlike the normal use of an API, the access to the behavior built into the framework is mediated by extending its content with new classes plugged into the framework itself.
An API can specify the interface between an application and the operating system.[28] POSIX, for example, specifies a set of common APIs that aim to enable an application written for a POSIX conformant operating system to be compiled for another POSIX conformant operating system.
Linux and Berkeley Software Distribution are examples of operating systems that implement the POSIX APIs.[29]
Remote APIs allow developers to manipulate remote resources through protocols, specific standards for communication that allow different technologies to work together, regardless of language or platform.
For example, the Java Database Connectivity API allows developers to query many different types of databases with the same set of functions, while the Java remote method invocation API uses the Java Remote Method Protocol to allow invocation of functions that operate remotely, but appear local to the developer.[33][34]
Therefore, remote APIs are useful in maintaining the object abstraction in object-oriented programming; a method call, executed locally on a proxy object, invokes the corresponding method on the remote object, using the remoting protocol, and acquires the result to be used locally as a return value.
When used in the context of web development, an API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format. An example might be a shipping company API that can be added to an eCommerce-focused website to facilitate ordering shipping services and automatically include current shipping rates, without the site developer having to enter the shipper's rate table into a web database. While "web API" historically has been virtually synonymous with web service, the recent trend (so-called Web 2.0) has been moving away from Simple Object Access Protocol (SOAP) based web services and service-oriented architecture (SOA) towards more direct representational state transfer (REST) style web resources and resource-oriented architecture (ROA).[37] Part of this trend is related to the Semantic Web movement toward Resource Description Framework (RDF), a concept to promote web-based ontology engineering technologies. Web APIs allow the combination of multiple APIs into new applications known as mashups.[38]
In the social media space, web APIs have allowed web communities to facilitate sharing content and data between communities and applications. In this way, content that is created in one place dynamically can be posted and updated to multiple locations on the web.[39] For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data.[40]
The design of an API has significant impact on its usage.[5] The principle of information hiding describes the role of programming interfaces as enabling modular programming by hiding the implementation details of the modules so that users of modules need not understand the complexities inside the modules.[41] Thus, the design of an API attempts to provide only the tools a user would expect.[5] The design of programming interfaces represents an important part of software architecture, the organization of a complex piece of software.[42]
In 2010, Oracle Corporation sued Google for having distributed a new implementation of Java embedded in the Android operating system.[59] Google had not acquired any permission to reproduce the Java API, although permission had been given to the similar OpenJDK project. Judge William Alsup ruled in the Oracle v. Google case that APIs cannot be copyrighted in the U.S. and that a victory for Oracle would have widely expanded copyright protection to a "functional set of symbols" and allowed the copyrighting of simple software commands:
In software development, effort estimation is the process of predicting the most realistic amount of effort (expressed in terms of person-hours or money) required to develop or maintain software based on incomplete, uncertain and noisy input. Effort estimates may be used as input to project plans, iteration plans, budgets, investment analyses, pricing processes and bidding rounds.[1][2]
Published surveys on estimation practice suggest that expert estimation is the dominant strategy when estimating software development effort.[3]
Typically, effort estimates are over-optimistic and there is a strong over-confidence in their accuracy. The mean effort overrun seems to be about 30% and not decreasing over time. For a review of effort estimation error surveys, see.[4] However, the measurement of estimation error is problematic, see Assessing the accuracy of estimates.
The strong overconfidence in the accuracy of the effort estimates is illustrated by the finding that, on average, if a software professional is 90% confident or "almost sure" to include the actual effort in a minimum-maximum interval, the observed frequency of including the actual effort is only 60-70%.[5]
Software researchers and practitioners have been addressing the problems of effort estimation for software development projects since at least the 1960s; see, e.g., work by Farr[8][9] and Nelson.[10]
Most of the research has focused on the construction of formal software effort estimation models. The early models were typically based on regression analysis or mathematically derived from theories from other domains. Since then a high number of model building approaches have been evaluated, such as approaches founded on case-based reasoning, classification and regression trees, simulation, neural networks, Bayesian statistics, lexical analysis of requirement specifications, genetic programming, linear programming, economic production models, soft computing, fuzzy logic modeling, statistical bootstrapping, and combinations of two or more of these models. The perhaps most common estimation methods today are the parametric estimation models COCOMO, SEER-SEM and SLIM. They have their basis in estimation research conducted in the 1970s and 1980s and are since then updated with new calibration data, with the last major release being COCOMO II in the year 2000.  The estimation approaches based on functionality-based size measures, e.g., function points, is also based on research conducted in the 1970s and 1980s, but are re-calibrated with modified size measures and different counting approaches, such as the use case points[11] or object points  and COSMIC Function Points in the 1990s.
It is important to be aware of the limitations of each traditional approach to measuring software development productivity.[22]
The chronic underestimation of development effort has led to the coinage and popularity of numerous humorous adages, such as ironically referring to a task as a "small matter of programming" (when much effort is likely required), and citing laws about underestimation:
In software engineering, a software development process or software development life cycle (SDLC) is a process of planning and managing software development. It typically involves dividing software development work into smaller, parallel, or sequential steps or sub-processes to improve design and/or product management. The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.[1]
Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.
A life-cycle "model" is sometimes considered a more general term for a category of methodologies and a software development "process" is a particular instance as adopted by a specific organization.[citation needed] For example, many specific software development processes fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.
The software development methodology framework did not emerge until the 1960s. According to Elliott (2004), the systems development life cycle can be considered to be the oldest formalized methodology framework for building information systems. The main idea of the software development life cycle has been "to pursue the development of information systems in a very deliberate, structured and methodical way, requiring each stage of the life cycle––from the inception of the idea to delivery of the final system––to be carried out rigidly and sequentially"[2] within the context of the framework being applied. The main target of this methodology framework in the 1960s was "to develop large scale functional business systems in an age of large scale business conglomerates. Information systems activities revolved around heavy data processing and number crunching routines."[2]
Requirements gathering and analysis:
The first phase of the custom software development process involves understanding the client's requirements and objectives. This stage typically involves engaging in thorough discussions and conducting interviews with stakeholders to identify the desired features, functionalities, and overall scope of the software. The development team works closely with the client to analyze existing systems and workflows, determine technical feasibility, and define project milestones.
Planning and design:
Once the requirements are understood, the custom software development team proceeds to create a comprehensive project plan. This plan outlines the development roadmap, including timelines, resource allocation, and deliverables. The software architecture and design are also established during this phase. User interface (UI) and user experience (UX) design elements are considered to ensure the software's usability, intuitiveness, and visual appeal.
Development:
With the planning and design in place, the development team begins the coding process. This phase involves writing, testing, and debugging the software code. Agile methodologies, such as scrum or kanban, are often employed to promote flexibility, collaboration, and iterative development. Regular communication between the development team and the client ensures transparency and enables quick feedback and adjustments.
Testing and quality assurance:
To ensure the software's reliability, performance, and security, rigorous testing and quality assurance (QA) processes are carried out. Different testing techniques, including unit testing, integration testing, system testing, and user acceptance testing, are employed to identify and rectify any issues or bugs. QA activities aim to validate the software against the predefined requirements, ensuring that it functions as intended.
Deployment and implementation:
Once the software passes the testing phase, it is ready for deployment and implementation. The development team assists the client in setting up the software environment, migrating data if necessary, and configuring the system. User training and documentation are also provided to ensure a smooth transition and enable users to maximize the software's potential.
Maintenance and support:
After the software is deployed, ongoing maintenance and support become crucial to address any issues, enhance performance, and incorporate future enhancements. Regular updates, bug fixes, and security patches are released to keep the software up-to-date and secure. This phase also involves providing technical support to end users and addressing their queries or concerns.
Methodologies, processes, and frameworks range from specific prescriptive steps that can be used directly by an organization in day-to-day work, to flexible frameworks that an organization uses to generate a custom set of steps tailored to the needs of a specific project or group. In some cases, a "sponsor" or "maintenance" organization distributes an official set of documents that describe the process. Specific examples include:
Since DSDM in 1994, all of the methodologies on the above list except RUP have been agile methodologies - yet many organizations, especially governments, still use pre-agile processes (often waterfall or similar). Software process and software quality are closely interrelated; some unexpected facets and effects have been observed in practice.[3]
Among these, another software development process has been established in open source. The adoption of these best practices known and established processes within the confines of a company is called inner source.
Software prototyping is about creating prototypes, i.e. incomplete versions of the software program being developed.
A basic understanding of the fundamental business problem is necessary to avoid solving the wrong problems, but this is true for all software methodologies.
"Agile software development" refers to a group of software development frameworks based on iterative development, where requirements and solutions evolve via collaboration between self-organizing cross-functional teams. The term was coined in the year 2001 when the Agile Manifesto was formulated.
Agile software development uses iterative development as a basis but advocates a lighter and more people-centric viewpoint than traditional approaches. Agile processes fundamentally incorporate iteration and the continuous feedback that it provides to successively refine and deliver a software system.
Continuous integration is the practice of merging all developer working copies to a shared mainline several times a day.[4]
Grady Booch first named and proposed CI in his 1991 method,[5] although he did not advocate integrating several times a day. Extreme programming (XP) adopted the concept of CI and did advocate integrating more than once per day – perhaps as many as tens of times per day.
Rapid application development (RAD) is a software development methodology, which favors iterative development and the rapid construction of prototypes instead of large amounts of up-front planning. The "planning" of software developed using RAD is interleaved with writing the software itself. The lack of extensive pre-planning generally allows software to be written much faster and makes it easier to change requirements.
The term was first used to describe a software development process introduced by James Martin in 1991. According to Whitten (2003), it is a merger of various structured techniques, especially data-driven information technology engineering, with prototyping techniques to accelerate software systems development.[6]
The waterfall model is a traditional engineering approach applied to software engineering. A strict waterfall approach discourages revisiting and revising any prior phase once it is complete. [according to whom?] This "inflexibility" in a pure waterfall model has been a source of criticism by supporters of other more "flexible" models. It has been widely blamed for several large-scale government projects running over budget, over time and sometimes failing to deliver on requirements due to the big design up front approach.[according to whom?] Except when contractually required, the waterfall model has been largely superseded by more flexible and versatile methodologies developed specifically for software development.[according to whom?] See Criticism of waterfall model.
In 1988, Barry Boehm published a formal software system development "spiral model," which combines some key aspects of the waterfall model and rapid prototyping methodologies, in an effort to combine advantages of top-down and bottom-up concepts. It provided emphasis on a key area many felt had been neglected by other methodologies: deliberate iterative risk analysis, particularly suited to large-scale complex systems.
Shape Up is a software development approach introduced by Basecamp in 2018. It is a set of principles and techniques that Basecamp developed internally to overcome the problem of projects dragging on with no clear end. Its primary target audience is remote teams. Shape Up has no estimation and velocity tracking, backlogs, or sprints, unlike waterfall, agile, or scrum. Instead, those concepts are replaced with appetite, betting, and cycles. As of 2022, besides Basecamp, notable organizations that have adopted Shape Up include UserVoice and Block.[12][13]
The history of software engineering begins around the 1960s. Writing software has evolved into a profession concerned with how best to maximize the quality of software and of how to create it. Quality can refer to how maintainable software is, to its stability, speed, usability, testability, readability, size, cost, security, and number of flaws or "bugs", as well as to less measurable qualities like elegance, conciseness, and customer satisfaction, among many other attributes. How best to create high quality software is a separate and controversial problem covering software design principles, so-called "best practices" for writing code, as well as broader management issues such as optimal team size, process, how best to deliver software on time and as quickly as possible, work-place "culture", hiring practices, and so forth. All this falls under the broad rubric of software engineering.[1]
Early usages for the term software engineering include a 1965 letter from ACM president Anthony Oettinger,[6][7] lectures by Douglas T. Ross at MIT in the 1950s.[8] Margaret H. Hamilton is the person who came up with the idea of naming the discipline, software engineering, as a way of giving it legitimacy during the development of the Apollo Guidance Computer.[9][10]
I fought to bring the software legitimacy so that it—and those building it—would be given its due respect and thus I began to use the term 'software engineering' to distinguish it from hardware and other kinds of engineering, yet treat each type of engineering as part of the overall systems engineering process. When I first started using this phrase, it was considered to be quite amusing. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. Software eventually and necessarily gained the same respect as any other discipline
The NATO Science Committee sponsored two conferences[12] on software engineering in 1968 (Garmisch, Germany) and 1969, which gave the field its initial boost. Many believe these conferences marked the official start of the profession of software engineering.[6][13]
Software engineering was spurred by the so-called software crisis of the 1960s, 1970s, and 1980s, which identified many of the problems of software development. Many projects ran over budget and schedule. Some projects caused property damage. Through negligence in publishing software with critical bugs, some lost their lives due to software failures. One of the most striking examples of harm through software bugs was the Therac-25 race condition bug. The bug caused a radiation therapy machine to administer overdoses of radiation in cases where low doses should have been used. The software crisis was originally defined in terms of productivity, but evolved to emphasize quality. Some used the term software crisis to refer to their inability to hire enough qualified programmers.[citation needed] During this time, Silicon Valley cemented itself as the best location for software engineers to work.[14]
Peter G. Neumann has kept a contemporary list of software problems and disasters.[16] The software crisis has been fading from view, because it is psychologically extremely difficult to remain in crisis mode for a protracted period (more than 20 years). Nevertheless, software – especially real-time embedded software – remains risky and is pervasive, and it is crucial not to give in to complacency. Over the last 10–15 years Michael A. Jackson has written extensively about the nature of software engineering, has identified the main source of its difficulties as lack of specialization, and has suggested that his problem frames provide the basis for a "normal practice" of software engineering, a prerequisite if software engineering is to become an engineering science.[17]
One of the largest projects undertaken by software engineers during this time period was the development of modern operating systems. Starting in Bell Labs and then moving to UC Berkeley, Ken Thompson and Dennis Ritchie, among other software engineers, worked to create Unix V6 in 1975. Unix V6 was a landmark operating system that set standards for future operating systems and is used today to educate students about proper operating system principles.[18] Moreover, future operating systems built on Unix V6's methods, and its descendants can be grouped into five types of operating system paradigms: Grassroots Systems, Large-Scale Systems, Hybrid Systems, Experimental Systems, and Minor Systems.[19] In contrast with Unix, software engineers at MIT in 1983 built GNU (literally "GNU's Not Unix") as an open source alternative to Unix. As an early open source software, GNU was beloved by a small group of developers and its work grew the open source software development community in the 80s.[20]
For decades, solving the software crisis was paramount to researchers and companies producing software tools.
The cost of owning and maintaining software in the 1980s was twice as expensive as developing the software.[citation needed]
Seemingly, every new technology and practice from the 1970s through the 1990s was trumpeted as a silver bullet to solve the software crisis. Tools, discipline, formal methods, process, and professionalism were touted as silver bullets:[citation needed]
Some[who?] interpret[why?] no silver bullet to mean that software engineering failed.[clarification needed] However, with further reading, Brooks goes on to say: "We will surely make substantial progress over the next 40 years; an order of magnitude over 40 years is hardly magical ..."[citation needed]
The search for a single key to success never worked. All known technologies and practices have only made incremental improvements to productivity and quality. Yet, there are no silver bullets for any other profession, either. Others interpret no silver bullet as proof that software engineering has finally matured and recognized that projects succeed due to hard work.[citation needed]
However, it could also be said that there are, in fact, a range of silver bullets today, including lightweight methodologies (see "Project management"), spreadsheet calculators, customized browsers, in-site search engines, database report generators, integrated design-test coding-editors with memory/differences/undo, and specialty shops that generate niche software, such as information web sites, at a fraction of the cost of totally customized web site development.  Nevertheless, the field of software engineering appears too complex and diverse for a single "silver bullet" to improve most issues, and each issue accounts for only a small portion of all software problems.[citation needed]
The growth of browser usage, running on the HyperText Markup Language (HTML), changed the way in which information-display and retrieval was organized.  The widespread network connections led to the growth and prevention of international computer viruses on MS Windows computers, and the vast proliferation of spam e-mail became a major design issue in e-mail systems, flooding communication channels and requiring semi-automated pre-screening. Keyword-search systems evolved into web-based search engines, and many software systems had to be re-designed, for international searching, depending on search engine optimization (SEO) .  Human natural-language translation systems were needed to attempt to translate the information flow in multiple foreign languages, with many software systems being designed for multi-language usage, based on design concepts from human translators.  Typical computer-user bases went from hundreds, or thousands of users, to, often, many-millions of international users.[citation needed]
With the expanding demand for software in many smaller organizations, the need for inexpensive software solutions led to the growth of simpler, faster methodologies that developed running software, from requirements to deployment, quicker & easier.  The use of rapid-prototyping evolved to entire lightweight methodologies, such as Extreme Programming (XP), which attempted to simplify many areas of software engineering, including requirements gathering and reliability testing for the growing, vast number of small software systems.  Very large software systems still used heavily documented methodologies, with many volumes in the documentation set; however, smaller systems had a simpler, faster alternative approach to managing the development and maintenance of software calculations and algorithms, information storage/retrieval and display.[citation needed]
Software engineering is a young discipline, and is still developing. The directions in which software engineering is developing include:[citation needed]
Aspects help software engineers deal with quality attributes by providing tools to add or remove boilerplate code from many areas in the source code. Aspects describe how all objects or functions should behave in particular circumstances. For example, aspects can add debugging, logging, or locking control into all objects of particular types. Researchers are currently working to understand how to use aspects to design general-purpose code. Related concepts include generative programming and templates.
Experimental software engineering is a branch of software engineering interested in devising experiments on software, in collecting data from the experiments, and in devising laws and theories from this data.
Software product lines, aka product family engineering, is a systematic way to produce families of software systems, instead of creating a succession of completely individual products. This method emphasizes extensive, systematic, formal code reuse, to try to industrialize the software development process.
The Future of Software Engineering conference (FOSE), held at ICSE 2000, documented the state of the art of SE in 2000 and listed many problems to be solved over the next decade. The FOSE tracks at the  ICSE 2000 [22] and the ICSE 2007[23] conferences also help identify the state of the art in software engineering.[citation needed]
The profession is trying to define its boundary and content. The Software Engineering Body of Knowledge SWEBOK has been tabled as an ISO standard during 2006 (ISO/IEC TR 19759).[citation needed]
In 2006, Money Magazine and Salary.com rated software engineering as the best job in America in terms of growth, pay, stress levels, flexibility in hours and working environment, creativity, and how easy it is to enter and advance in the field.[24]
A wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems such as Cyc to deep learning to robot platforms such as the Roomba with open interface.[25] Recent advances in deep artificial neural networks and distributed computing have led to a proliferation of software libraries, including Deeplearning4j, TensorFlow, Theano and Torch.
Early symbolic AI inspired Lisp and Prolog, which dominated early AI programming. Modern AI development often uses mainstream languages such as Python or C++,[28] or niche languages such as Wolfram Language.[29]
Software crisis is a term used in the early days of computing science for the difficulty of writing useful and efficient computer programs in the required time. The software crisis was due to the rapid increases in computer power and the complexity of the problems that could be tackled. With the increase in the complexity of the software, many software problems arose because existing methods were inadequate.
The term "software crisis" was coined by some attendees at the first NATO Software Engineering Conference in 1968 at Garmisch, Germany.[1][2] Edsger Dijkstra's 1972 Turing Award Lecture makes reference to this same problem:[3]
The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem.
The causes of the software crisis were linked to the overall complexity of hardware and the software development process. The crisis manifested itself in several ways:
The main cause is that improvements in computing power had outpaced the ability of programmers to effectively use those capabilities. Various processes and methodologies have been developed over the last few decades to improve software quality management such as procedural programming and object-oriented programming. However, software projects that are large, complicated, poorly specified, or involve unfamiliar aspects, are still vulnerable to large, unanticipated problems.
Software Engineering Institute (SEI) is a federally funded research and development center in Pittsburgh, Pennsylvania, United States. Founded in 1984, the institute is now sponsored by the United States Department of Defense and the Office of the Under Secretary of Defense for Research and Engineering, and administrated by Carnegie Mellon University.[1][2]
The activities of the institute cover cybersecurity, software assurance, software engineering and acquisition, and component capabilities critical to the United States Department of Defense.
The Carnegie Mellon Software Engineering Institute is a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania,  United States. The SEI also has offices in  Washington, DC; Arlington County, Virginia; and Los Angeles, California. The SEI operates with major funding from the U.S. Department of Defense. The SEI also works with industry and academia through research collaborations.[1]
On November 14, 1984, the U.S. Department of Defense elected Carnegie Mellon University as the host site of the Software Engineering Institute.[1]
The institute was founded with an initial allocation of $6 million, with another $97 million to be allocated in the subsequent five years. The SEI's contract with the Department of Defense is subject to review and renewal every five years.
SEI focuses on several technical directions: software engineering, cybersecurity, assurance, and DoD critical component capabilities.[3]
The SEI defines specific initiatives aimed at improving organizations' software engineering capabilities.
Organizations need to effectively manage the acquisition, development, and evolution (ADE) of software-intensive systems. Success in software engineering management practices helps organizations predict and control quality, schedule, cost, cycle time, and productivity. The best-known example of SEI  in management practices is the SEI's Capability Maturity Model (CMM) for Software (now Capability Maturity Model Integration (CMMI)). The CMMI approach consists of models, appraisal methods, and training courses that have been proven to improve process performance. In 2006, Version 1.2 of the CMMI Product Suite included the release of CMMI for Development. CMMI for Development was the first of three constellations defined in Version 1.2: the others include CMMI for Acquisition and CMMI for Services. The CMMI for Services constellation was released in February 2009. Another management practice developed by CERT, which is part of the SEI, is the Resilience Management Model (CERT-RMM). The CERT-RMM is a capability model for operational resilience management. Version 1.0 of the Resilience Management Model was released in May 2010.
SEI work in engineering practices increases the ability of software engineers to analyze, predict, and control selected
functional and non-functional properties of software systems. Key SEI tools and methods include the SEI Architecture Tradeoff Analysis Method (ATAM) method, the SEI Framework for Software Product Line Practice, and the SEI Service Migration and Reuse Technique (SMART).
The SEI is also the home of the CERT/CC (CERT Coordination Center), a federally funded computer security organization. The SEI CERT Program's primary goals are to ensure that appropriate technology and systems-management practices are used to resist attacks on networked systems and to limit damage and ensure continuity of critical services in spite of successful attacks, accidents, or failures. The SEI CERT program is working with US-CERT to produce the Build Security In (BSI) website, which provides guidelines for building security into every phase of the software development lifecycle. The SEI has also conducted research on insider threats and computer forensics. Results of this research and other information now populate the CERT Virtual Training Environment.
The SEI Partner Network helps the SEI disseminate software engineering best practices. Organizations and individuals in the SEI Partner Network are selected, trained, and licensed by the SEI to deliver authentic SEI services, which include courses, consulting methods, and management processes.  The network currently consists of nearly 250 partner organizations worldwide.
The SEI sponsors national and international conferences, workshops, and user-group meetings. Other events cover subjects including the acquisition of software-intensive systems, commercial off-the-shelf (COTS)-based systems, network security and survivability, software process research, software product lines, CMMI, and the SEI Team Software Process.
The SEI Membership Program helps the software engineering community to network. SEI Members include small business owners, software and systems programmers, CEOs, directors, and managers from both Fortune 500 companies and government organizations.[4]
Through the SEI Affiliate Program, organizations place technical experts with the SEI for periods ranging from 12 months to four years. Affiliates currently are working on projects with the SEI to identify, develop, and demonstrate improved software engineering practices.
In order to recognize outstanding achievements in improving an organization's ability to create and evolve software-dependent systems, the SEI and IEEE Computer Society created the Software Process Achievement Award program.[5] In addition to rewarding excellence, the purpose of this award is to foster continuous advancement in the practice of software engineering and to disseminate insights, experiences, and proven practices throughout the relevant research and practitioner communities.
The SEI publishes reports that offer new technical information about software engineering topics, whether theoretical or applied. The SEI also publishes books on software engineering for industry, government and military applications and practices.
In addition, the SEI offers public courses, workshops, and conferences in process improvement, software architecture and product lines, and security.
On November 11, 2015, Roger Dingledine from the Tor Project accused[6] the Software Engineering Institute of aiding the Federal Bureau of Investigation in uncovering the identities of users of the Tor network.  Later prosecution showed the hack was paid for by the Department of Defense and subpoenaed by the FBI.[7]
The Software Engineering Body of Knowledge (SWEBOK (/ˈswiːˌbɒk/ SWEE-bok)) refers to the collective knowledge, skills, techniques, methodologies, best practices, and experiences accumulated within the field of software engineering over time. A baseline for this body of knowledge is presented in the Guide to the Software Engineering Body of Knowledge,[1] also known as the SWEBOK Guide, an ISO/IEC standard originally recognized as ISO/IEC TR 19759:2005[2] and later revised by ISO/IEC TR 19759:2015.[3] The SWEBOK Guide serves as a compendium and guide to the body of knowledge that has been developing and evolving over the past decades.
The published version of SWEBOK V3 has the following 15 knowledge areas (KAs) within the field of software engineering:
The 2004 edition of the SWEBOK Guide, known as SWEBOK 2004, defined ten knowledge areas (KAs) within the field of software engineering:
A similar effort to define a body of knowledge for software engineering is the "Computing Curriculum Software Engineering (CCSE)," officially named Software Engineering 2004 (SE2004). The curriculum largely overlaps with SWEBOK 2004 since the latter has been used as one of its sources, although it is more directed towards academia. Whereas the SWEBOK Guide defines the software engineering knowledge that practitioners should have after four years of practice, SE2004 defines the knowledge that an undergraduate software engineering student should possess upon graduation (including knowledge of mathematics, general engineering principles, and other related areas). SWEBOK V3 aims to address these intersections.
Software engineering – application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is the application of engineering to software.[1]
The ACM Computing Classification system is a poly-hierarchical ontology that organizes the topics of the field and can be used in semantic web applications and as a de facto standard classification system for the field.   The major section "Software and its Engineering" provides an outline and ontology for software engineering.
Software engineers build software (applications, operating systems, system software) that people use.
Applications influence software engineering by pressuring developers to solve problems in new ways. For example, consumer software emphasizes low cost, medical software emphasizes high quality, and Internet commerce software emphasizes rapid development.
A platform combines computer hardware and an operating system. As platforms grow more powerful and less costly, applications and tools grow more widely available.
Skilled software engineers know a lot of computer science including what is possible and impossible, and what is easy and hard for software.
Deliverables must be developed for many SE projects. Software engineers rarely make all of these deliverables themselves. They usually cooperate with the writers, trainers, installers, marketers, technical support people, and others who make many of these deliverables.
Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.[1]
Anticipating change helps software engineers build extensible software, which means they can enhance a software product without disrupting the underlying structure.[2]  
Research over 25 years showed that the cost of rework can be 10 to 100 times (5 to 10 times for smaller projects) more expensive than getting the requirements right the first time.  Given that 25% of the requirements change during development on average project, the need to reduce the cost of rework elucidates the need for anticipating change. [3]
Constructing for verification means building software in such a way that faults can be ferreted out readily by the software engineers writing the software, as well as during independent testing and operational activities. Specific techniques that support constructing for verification include following coding standards to support code reviews, unit testing, organizing code to support automated testing, and restricted use of complex or hard-to-understand language structures, among others.[2]
Systematic reuse can enable significant software productivity, quality, and cost improvements.  Reuse has two closely related facets:[2]
Numerous models have been created to develop software, some of which emphasize construction more than others. Some models are more linear from the construction point of view, such as the Waterfall and staged-delivery life cycle models. These models treat construction as an activity which occurs only after significant prerequisite work has been completed—including detailed requirements work, extensive design work, and detailed planning. Other models are more iterative, such as evolutionary prototyping, Extreme Programming, and Scrum. These approaches tend to treat construction as an activity that occurs concurrently with other software development activities, including requirements, design, and planning, or overlaps them.[1]
The choice of construction method is a key aspect of the construction planning activity. The choice of construction method affects the extent to which construction prerequisites (e.g. Requirements analysis, Software design, .. etc.) are performed, the order in which they are performed, and the degree to which they are expected to be completed before construction work begins. Construction planning also defines the order in which components are created and integrated, the software quality management processes, the allocation of task assignments to specific software engineers, and the other tasks, according to the chosen method.[1]
In order to account for the unanticipated gaps in the software design, during software construction some design modifications must be made on a smaller or larger scale to flesh out details of the software design.[4]
Construction languages include all forms of communication by which a human can specify an executable problem solution to a computer. They include configuration languages, toolkit languages, and programming languages:[6]
The purpose of construction testing is to reduce the gap between the time at which faults are inserted into the code and the time those faults are detected. In some cases, construction testing is performed after code has been written. In test-first programming, test cases are created before code is written. Construction involves two forms of testing, which are often performed by the software engineer who wrote the code:[1]
Implementing software reuse entails more than creating and using libraries of assets. It requires formalizing the practice of reuse by integrating reuse processes and activities into the software life cycle. The tasks related to reuse in software construction during coding and testing are:[1]
Studies have shown that a combination of these techniques needs to be used to achieve a high defect detection rate.  Other studies showed that different people tend to find different defects.  One study found that the extreme programming practices of pair programming, desk checking, unit testing, integration testing, and regression testing can achieve a 90% defect detection rate.[16]  An experiment involving experienced programmers found that on average they were able to find 5 errors (9 at best) out of 15 errors by testing.[18]
A key activity during construction is the integration of separately constructed routines, classes, components, and subsystems. In addition, a particular software system may need to be integrated with other software or hardware systems. Concerns related to construction integration include planning the sequence in which components will be integrated, creating scaffolding to support interim versions of the software, determining the degree of testing and quality work performed on components before they are integrated, and determining points in the project at which interim versions of the software are tested.[1]
Assertions are executable predicates which are placed in a program that allow runtime checks of the program.[19] Design by contract is a development approach in which preconditions and postconditions are included for each routine. Defensive programming is the protection a routine from being broken by invalid inputs.[22]
Error-handling refers to the programming practice of anticipating and coding for error conditions that may arise when the program runs. Exception-handling is a programming-language construct or hardware mechanism designed to handle the occurrence of exceptions, special conditions that change the normal flow of program execution.[23] Fault tolerance is a collection of techniques that increase software reliability by detecting errors and then recovering from them if possible or containing their effects if recovery is not possible.[22]
State-based programming is a programming technology using finite state machines to describe program behaviors.[22]  A table-driven method is a schema that uses tables to look up information rather than using logic statements (such as if and case).[24]
Runtime configuration is a technique that binds variable values and program settings when the program is running, usually by updating and reading configuration files in a just-in-time mode.  Internationalization is the technical activity of preparing a program, usually interactive software, to support multiple locales.  The corresponding activity, localization, is the activity of modifying a program to support a specific local language.[24]
Software bloat is a process whereby successive versions of a computer program become perceptibly slower, use more memory, disk space or processing power, or have higher hardware requirements than the previous version, while making only dubious user-perceptible improvements or suffering from feature creep. The term is not applied consistently; it is often used as a pejorative by end users, including to describe undesired user interface changes even if those changes had little or no effect on the hardware requirements. In long-lived software, perceived bloat can occur from the software servicing a large, diverse marketplace with many differing requirements. Most end users will feel they only need some limited subset of the available functions, and will regard the others as unnecessary bloat, even if end users with different requirements require those functions.
Actual (measurable) bloat can occur due to de-emphasising algorithmic efficiency in favour of other concerns like developer productivity, or possibly through the introduction of new layers of abstraction like a virtual machine or other scripting engine for the purposes of convenience when developer constraints are reduced. The perception of improved developer productivity, in the case of practising development within virtual machine environments, comes from the developers no longer taking resource constraints and usage into consideration during design and development; this allows the product to be completed faster but it results in increases to the end user's hardware requirements and/or compromised performance as a result.
The term "bloatware" is also used to describe unwanted pre-installed software or bundled programs.[1]
In computer programming, code bloat refers to the presence of program code (source code or machine code) that is perceived as unnecessarily long, slow, or otherwise wasteful of resources.
Software developers involved in the industry during the 1970s had severe limitations on processing power, disk space and memory. Every byte and clock cycle was taken into account, and much work went into fitting the programs into available resources. Achieving this efficiency was one of the highest values of computer programmers, and the best programs were often called "elegant", a term used by mathematicians to describe a proof which is tidy, parsimonious and powerful.
By the 21st century, the situation had reversed. Resources were perceived as cheap, and rapidity of coding and headline features for marketing seen as priorities.[2] In part, this is because technological advances have since increased processing capacity and storage density by orders of magnitude, while reducing the relative costs by similar orders of magnitude (see Moore's law). Additionally, the spread of computers through all levels of business and home life has produced a software industry many times larger than it was in the 1970s.  Programs are now usually churned out by teams, directed by committees in software development studios (also known as software houses or software factories) where each programmer works on only a part of the whole, on one or more subroutines.[citation needed]
Finally, software development tools and approaches often result in changes throughout a program to accommodate each feature, leading to a large-scale inclusion of code which affects the main operation of the software, and is required in order to support functions that themselves may be only rarely used. In particular, the advances in resources available have led to tools which allow easier development of code, again with less priority given to end efficiency.
Another cause of bloat is independently competing standards and products, which can create a demand for integration. There are now more operating systems, browsers, protocols, and storage formats than there were before, causing bloat in programs due to interoperability issues. For example, a program that once could only save in text format is now required to save in HTML, XML, XLS, CSV, PDF, DOC, and other formats.
Niklaus Wirth has summed up the situation in Wirth's law, which states that software speed is decreasing more quickly than hardware speed is increasing.
In his 2001 essay Strategy Letter IV: Bloatware and the 80/20 Myth,[3] Joel Spolsky argues that while 80% of the users only use 20% of the features (a variant on the Pareto principle), each one uses different features. Thus, "lite" software editions turn out to be useless for most, as they miss the one or two special features that are present in the "bloated" version. Spolsky sums the article with a quote by Jamie Zawinski referring to the Mozilla Application Suite (ultimately debloated as separate apps, of which the Firefox web browser is the only significant survivor):
Software bloat may also be a symptom of the second-system effect, described by Fred Brooks in The Mythical Man-Month.
“Bloatware" is software that has become bloated through inefficiency or accretion of features as outlined above.[3] The term is also commonly used for preinstalled software bundled on a device, usually by the hardware manufacturer, that is mostly unwanted by the purchaser.
The term may also be applied to the accumulation of unwanted and unused software elements that remain after partial and incomplete uninstallation. These elements may include whole programs, libraries, associated configuration information, or other data. Performance may deteriorate overall as a result of such remnants, as the unwanted software or software components can occupy both hard disk memory and RAM, waste processing time, add disk I/O, and cause delays at system startup and shutdown. In the worst cases, the leftover software may interfere with the correct operation of wanted software.[5]
Microsoft Windows has also been criticized as being bloated – with reference to Windows Vista and discussing the new, greatly slimmed down Windows 7 core components, Microsoft engineer Eric Traut commented that  "This is the core of Windows 7. This is a collection of components that we've taken out. A lot of people think of Windows as this really large, bloated operating system, and that may be a fair characterization, I have to admit. It is large. It contains a lot of stuff in it. But at its core, the kernel and the components that make up the very core of the operating system actually is pretty streamlined."[22][23] Ed Bott also expressed skepticism, noting that nearly every operating system that Microsoft has ever sold has been criticized as "bloated" on first release, even those now regarded as the exact opposite, such as MS-DOS.[24] Quoting Paul Thurrott, Bott agreed that the bloat stems from numerous enterprise-level features included in the operating system that were largely irrelevant to the average home user.
A number of technology blogs have also covered the issue of increased bloatware on cell phones. However, they refer to a different issue, specifically that of wireless carriers loading phones with software that, in many cases, cannot be easily, if at all, deleted. This has been most frequently cited with respect to Android devices, although this phenomenon exists on phones running many other operating systems.[26][27]
Some applications, such as GIMP, and software with additional functionality from plug-ins, use extensions or add-ons which are downloaded separately from the main application. These can be created by either the software developer or by third-party developers. Plug-ins, extensions, and add-ons add extra functionality which might have otherwise been packaged in the main program.
Open source software may use a similar technique using preprocessor directives to include features at compile time selectively. This is easier to implement and more secure than a plugin system, but has the disadvantage that a user who wants a specific set of features must compile the program from source.
Sometimes software becomes bloated because of "creeping featurism"[35] (Zawinski's law of software envelopment). One way to reduce that kind of bloat is described by the Unix philosophy of "writing programs that do one thing and do it well," and breaking what would be a single, complicated piece of software into numerous simpler components which can be chained together using pipes, shell scripts, or other forms of interapplication communication.
Software bloat may induce more vulnerabilities due to raise of difficulty in managing a large number of code and dependencies.  Furthermore, it may make software developer difficulty to understand the code they ship, increasing the difficulty for spot and fix vulnerabilities.[36][37]
A Bachelor of Software Engineering Honours BSEHons is an undergraduate academic degree (Bachelor's Degree) awarded for completing a program of study in the field of software development for computers in information technology.
"Software Engineering is the systematic development and application of techniques which lead to the creation of correct and reliable computer software."[1]
In following years a software engineering student will often have a much stronger focus on Software systems and data management. The inclusion of human factors in a software engineering degree has been heavily debated, arguing that products of software engineers often are too difficult to use by consumers.[2]
Sample B.S. in Software Engineering Degree Information from the University of Virginia - Wise
Core SWE Requirements:[3]
"A large difference exists between the software engineering skills taught at a typical
university or college and the skills that are desired of a software engineer by a typical
software development organization. At the heart of this difference seems to be the way
software engineering is typically introduced to students: general theory is presented in a
series of lectures and put into (limited) practice in an associated class project."[4]
Graduate prospects are projected to be excellent with the amount of software engineers in the industry estimated to rise by roughly 38% from 2006 to 2016, with total real wage in the industry increasing by an estimated 38.2%.[5] The U.S. Bureau of Labor Statistics Occupational Outlook Job Outlook for Software engineers and Developers 2019-29 predicts only a 22% growth.[6] After the crash of the dot-com bubble (1999–2001) and the Great Recession (2008), many U.S. software professions were left without work or with lower wages.[7] In addition, enrollment in computer-related degrees and other STEM degrees (STEM attrition)[8][9] in the US has been dropping for years, especially for women,[10] which, according to Beaubouef and Mason[11]  could be attributed to a lack of general interest in science and mathematics and also out of an apparent fear that software will be subject to the same pressures as manufacturing and agriculture careers. The U.S. Bureau of Labor Statistics Occupational Outlook 2014-24 predicts a decline for Computer Programmers of -8 percent, then for 2016-26 predicts a decline of -7 percent, then a decline of -9 percent from 2019 to 2029,[12] and finally predicts a decline of -10 percent from 2021 to 2031.[12]
In  Pakistan and Nepal, Bachelor of Engineering in Software Engineering (BE Software) is an 8-semester course of study.[13] This degree is provided by University of Engineering and Technology, Taxila, 
Virtual University of Pakistan,Superior university and many others and  Pokhara University Nepal.
In Bangladesh, this degree is named Bachelor of Science in Software Engineering (BS SE) which is also an 8-semester course of study.[14] University of Dhaka is the pioneer of Software Engineering education in Bangladesh offering Bachelor of Science in Software Engineering (BSSE) degree since 2009 with 6 months industry internship program.[15]
The Bachelor of Software Engineering degree is awarded to those who successfully complete an eight-semester program.
Software engineering professionalism is a movement to make software engineering a profession, with aspects such as degree and certification programs, professional associations, professional ethics, and government licensing.  The field is a licensed discipline in Texas in the United States[1] (Texas Board of Professional Engineers, since 2013), Engineers Australia[2](Course Accreditation since 2001, not Licensing), and many provinces in Davao.
In 1993 the IEEE and ACM began a joint effort called JCESEP, which evolved into SWECC in 1998 to explore making software engineering into a profession. The ACM pulled out of SWECC in May 1999, objecting to its support for the Texas professionalization efforts, of having state licenses for software engineers.  ACM determined that the state of knowledge and practice in software engineering was too immature to warrant licensing,
and that licensing would give false assurances of competence even if the body of knowledge were mature.[3]
The IEEE continued to support making software engineering a branch of traditional engineering.
In Canada the Canadian Information Processing Society established the Information Systems Professional certification process.  Also, by the late 1990s (1999 in British Columbia) the discipline of software engineering as a professional engineering discipline was officially created.  This has caused some disputes between the provincial engineering associations and companies who call their developers software engineers, even though these developers have not been licensed by any engineering association.[4]
In 1999, the Panel of Software Engineering was formed as part of the settlement between Engineering Canada and the Memorial University of Newfoundland over the school's use of the term "software engineering" in the name of a computer science program. Concerns were raised over the inappropriate use of the name "software engineering" to describe non-engineering programs could lead to student and public confusion, and ultimately threaten public safety.[5] The Panel issued recommendations to create a Software Engineering Accreditation Board, but the task force created to carry out the recommendations was unable to get the various stakeholders to agree to concrete proposals, resulting in separate accreditation boards.[6][7]
Software engineering ethics is a large field. In some ways it began as an unrealistic attempt to define bugs as unethical.[citation needed]  More recently it has been defined as the application of both computer science and engineering philosophy, principles, and practices to the design and development of software systems.  Due to this engineering focus and the increased use of software in mission critical and human critical systems, where failure can result in large losses of capital but more importantly lives such as the Therac-25 system, many ethical codes have been developed by a number of societies, associations and organizations. These entities, such as the ACM, IEEE, EGBC[8] and Institute for Certification of Computing Professionals (ICCP)[9] have formal codes of ethics. Adherence to the code of ethics is required as a condition of membership or certification. According to the ICCP, violation of the code can result in revocation of the certificate.  Also, all engineering societies require conformance to their ethical codes; violation of the code results in the revocation of the license to practice engineering in the society's jurisdiction.
A Software Engineering Code of Ethics[10][11] has been approved by the ACM and the IEEE-CS as the standard for teaching and practicing software engineering.
The following are examples of codes of conduct for Professional Engineers.  These 2 have been chosen because both jurisdictions have a designation for Professional Software Engineers.
Bill Joy argued that "better software" can only enable its privileged end users, make reality more power-pointy as opposed to more humane, and ultimately run away with itself so that "the future doesn't need us." He openly questioned the goals of software engineering in this respect, asking why it isn't trying to be more ethical rather than more efficient.[citation needed]  In his book Code and Other Laws of Cyberspace, Lawrence Lessig argues that computer code can regulate conduct in much the same way as the legal code.  Lessig and Joy urge people to think about the consequences of the software being developed, not only in a functional way, but also in how it affects the public and society as a whole.
Overall, due to the youth of software engineering, many of the ethical codes and values have been borrowed from other fields, such as mechanical and civil engineering.  However, there are many ethical questions that even these, much older, disciplines have not encountered.  Questions about the ethical impact of internet applications, which have a global reach, have never been encountered until recently and other ethical questions are still to be encountered.  This means the ethical codes for software engineering are a work in progress, that will change and update as more questions arise.[citation needed]
Since 2002, the IEEE Computer Society offered the Certified Software Development Professional (CSDP) certification exam (in 2015 this was replaced by several similar certifications). A group of experts from industry and academia developed the exam and maintained it. Donald Bagert, and at a later period Stephen Tockey headed the certification committee. Contents of the exam centered around the SWEBOK (Software Engineering Body of Knowledge) guide, with an additional emphasis on Professional Practices and Software Engineering Economics knowledge areas (KAs). The motivation was to produce a structure at an international level for software engineering's knowledge areas.[14][15]
The Bureau of Labor Statistics (BLS) classifies computer software engineers as a subcategory of "computer specialists", along with occupations such as computer scientist, Programmer, Database administrator and Network administrator.[16] The BLS classifies all other engineering disciplines, including computer hardware engineers, as engineers.[17]
In April 2013 the National Council of Examiners for Engineering and Surveying (NCEES) began offering a Professional Engineer (PE) exam for Software Engineering. The exam was developed in association with the IEEE Computer Society.[30] NCEES ended the exam in April 2019 due to lack of participation.[31]
In Texas Donald Bagert of Texas became the first professional software engineer in the U.S. on September 4, 1998 or October 9, 1998. As of May 2002, Texas had issued 44 professional engineering licenses for software engineers. Rochester Institute of Technology granted the first Software Engineering bachelor's degrees in 2001. Other universities have followed.
IT professionals with degrees in other fields (such as computer science or information systems) are restricted from using the title Software Engineer, or wording Software Engineer in a title, depending on their province or territory of residence.[citation needed]
Most Canadians who earn professional software engineering licenses study software engineering, computer engineering or electrical engineering.  Many times these people are already qualified to become professional engineers in their own fields but choose to be licensed as software engineers to differentiate themselves from computer scientists.
A person must be granted the “professional engineer” licence to have the right to practise professional software engineering as a Professional Engineer in Ontario.
To become licensed by Professional Engineers Ontario (PEO), one must:
Many graduates of Software Engineering programs are unable to obtain the PEO licence since the work they qualify for after graduation as entry-level is not related to engineering i.e. working in a software company writing code or testing code would not qualify them as their work experience does not fulfill the work experience guidelines the PEO sets. Also Software Engineering programs in Ontario and other provinces involve a series of courses in electrical, electronics, and computers engineering qualifying the graduates to even work in those fields.
A person must be granted the “engineer” licence to have the right to practise professional software engineering in Quebec. To become licensed by the Quebec order of engineers (in French : Ordre des ingénieurs du Québec - OIQ), a candidate must:
The term "engineer" in Canada is restricted to those who have graduated from a qualifying engineering programme. Some universities’ "software engineering" programmes are under the engineering faculty and therefore qualify, for example the University of Waterloo. Others, such as the University of Toronto have "software engineering" in the computer science faculty which does not qualify.  This distinction has to do with the way the profession is regulated.  Degrees in "Engineering" must be accredited by a national panel and have certain specific requirements to allow the graduate to pursue a career as a professional engineer. "Computer Science" degrees, even those with specialties in software engineering, do not have to meet these requirements so the computer science departments can generally teach a wider variety of topics and students can graduate without specific courses required to pursue a career as a professional engineer.[37]
The use of the title tölvunarfræðingur (computer scientist) is protected by law in Iceland.[38] Software engineering is taught in Computer Science departments in Icelandic universities. Icelandic law state that a permission must be obtained from the Minister of Industry when the degree was awarded abroad, prior to use of the title. The title is awarded to those who have obtained a BSc degree in Computer Science from a recognized higher educational institution.[39]
In New Zealand, the Institution of Professional Engineers New Zealand (IPENZ), which licenses and regulates the country's chartered engineers (CPEng), recognizes software engineering as a legitimate branch of professional engineering and accepts application of software engineers to obtain chartered status provided they have a tertiary degree of approved subjects. Software Engineering is included whereas Computer Science is normally not.[40]
Software engineers make up a significant portion of the global workforce. As of 2022, there are an estimated 26.9 million professional software engineers worldwide, up from 21 million in 2016.[1][2]
In 2023, there were an estimated 1.6 million [3] professional software developers in North America. There are 166 million people [4] employed in the US workforce, making software developers 0.96% of the total workforce.
The following two tables compare the number of software engineers (611,900 in 2002) versus the number of traditional engineers (1,157,020 in 2002).
There are another 1,500,000 people in system analysis, system administration, and computer support, many of whom might be called software engineers. Many systems analysts manage software development teams, and as analysis is an important software engineering role, many of them may be considered software engineers in the near future. This means that the number of software engineers may actually be much higher.
It is important to note that the number of software engineers declined by 5 to 10 percent from 2000 to 2002.
Computer and information system managers (264,790) manage software projects, as well as computer operations. Similarly, Construction and engineering managers (413,750) oversee engineering projects, manufacturing plants, and construction sites. Computer management is 64% the size of construction and engineering management.
Most people working in the field of computer science, whether making software systems (software engineering) or studying the theoretical and mathematical facts of software systems (computer science), acquire degrees in computer science. The data shows that the combined number of chemistry and physics educators (29,610) nearly equals the number of engineering educators (29,310). It is estimated that roughly half of computer science educators emphasize the practical (software engineering), and the other half emphasize the theoretical (computer science).[citation needed] This means that software engineering education is 56% the size of traditional engineering education. There are more computer science educators than chemistry and physics educators combined, or engineering educators.
Software engineers are part of the much larger software, hardware, application, and operations community. In 2000 in the U.S., there were about 680,000 software engineers and about 10,000,000 IT workers.
There has been a healthy growth in the number of India's IT professionals over the past few years. From a base of 6,800 knowledge workers in 1985–86, the number increased to 522,000 software and services professionals by the end of 2001–02. It is estimated that out of these 528,000 knowledge workers, almost 170,000 are working in the IT software and services export industry; nearly 106,000 are working in the IT enabled services and over 230,000 in user organizations.[26]
In May 2024, the Australian government reported that 169,300 Australians are employed as software and applications programmers, 17% of who are women. The role grew annually by 8,300 workers.[27]
Software development is the process of designing and implementing a software solution to satisfy a user. The process is more encompassing than programming, writing code, in that it includes conceiving the goal, evaluating feasibility, analyzing requirements, design, testing and release. The process is part of software engineering which also includes organizational management, project management, configuration management and other aspects.[1]
Software development involves many skills and job specializations including programming, testing, documentation, graphic design, user support, marketing, and fundraising.
Software development involves many tools including: compiler, integrated development environment (IDE), version control, computer-aided software engineering, and word processor.
Another focus in many programming methodologies is the idea of trying to catch issues such as security vulnerabilities and bugs as early as possible (shift-left testing) to reduce the cost of tracking and fixing them.[13]
In 2009, it was estimated that 32 percent of software projects were delivered on time and budget, and with the full functionality. An additional 44 percent were delivered, but missing at least one of these features. The remaining 24 percent were cancelled prior to release.[14]
The sources of ideas for software products are plentiful. These ideas can come from market research including the demographics of potential new customers, existing customers, sales prospects who rejected the product, other internal software development staff, or a creative third party. Ideas for software products are usually first evaluated by marketing personnel for economic feasibility, fit with existing channels of distribution, possible effects on existing product lines, required features, and fit with the company's marketing objectives. In the marketing evaluation phase, the cost and time assumptions become evaluated.[16] The feasibility analysis estimates the project's return on investment, its development cost and timeframe. Based on this analysis, the company can make a business decision to invest in further development.[17] After deciding to develop the software, the company is focused on delivering the product at or below the estimated cost and time, and with a high standard of quality (i.e., lack of bugs) and the desired functionality. Nevertheless, most software projects run late and sometimes compromises are made in features or quality to meet a deadline.[18]
Software analysis begins with a requirements analysis to capture the business needs of the software.[19] Challenges for the identification of needs are that current or potential users may have different and incompatible needs, may not understand their own needs, and change their needs during the process of software development.[20] Ultimately, the result of analysis is a detailed specification for the product that developers can work from. Software analysts often decompose the project into smaller objects, components that can be reused for increased cost-effectiveness, efficiency, and reliability.[19] Decomposing the project may enable a multi-threaded implementation that runs significantly faster on multiprocessor computers.[21]
During the analysis and design phases of software development, structured analysis is often used to break down the customer's requirements into pieces that can be implemented by software programmers.[22] The underlying logic of the program may be represented in data-flow diagrams, data dictionaries, pseudocode, state transition diagrams, and/or entity relationship diagrams.[23] If the project incorporates a piece of legacy software that has not been modeled, this software may be modeled to help ensure it is correctly incorporated with the newer software.[24]
Design involves choices about the implementation of the software, such as which programming languages and database software to use, or how the hardware and network communications will be organized. Design may be iterative with users consulted about their needs in a process of trial and error. Design often involves people expert in aspect such as database design, screen architecture, and the performance of servers and other hardware.[19] Designers often attempt to find patterns in the software's functionality to spin off distinct modules that can be reused with object-oriented programming. An example of this is the model–view–controller, an interface between a graphical user interface and the backend.[25]
The central feature of software development is creating and understanding the software that implements the desired functionality.[26] There are various strategies for writing the code. Cohesive software has various components that are independent from each other.[19] Coupling is the interrelation of different software components, which is viewed as undesirable because it increases the difficulty of maintenance.[27] Often, software programmers do not follow industry best practices, resulting in code that is inefficient, difficult to understand, or lacking documentation on its functionality.[28] These standards are especially likely to break down in the presence of deadlines.[29] As a result, testing, debugging, and revising the code becomes much more difficult. Code refactoring, for example adding more comments to the code, is a solution to improve the understandability of code.[30]
Testing is the process of ensuring that the code executes correctly and without errors. Debugging is performed by each software developer on their own code to confirm that the code does what it is intended to. In particular, it is crucial that the software executes on all inputs, even if the result is incorrect.[31] Code reviews by other developers are often used to scrutinize new code added to the project, and according to some estimates dramatically reduce the number of bugs persisting after testing is complete.[32] Once the code has been submitted, quality assurance—a separate department of non-programmers for most large companies—test the accuracy of the entire software product. Acceptance tests derived from the original software requirements are a popular tool for this.[31] Quality testing also often includes stress and load checking (whether the software is robust to heavy levels of input or usage), integration testing (to ensure that the software is adequately integrated with other software), and compatibility testing (measuring the software's performance across different operating systems or browsers).[31] When tests are written before the code, this is called test-driven development.[33]
Production is the phase in which software is deployed to the end user.[34] During production, the developer may create technical support resources for users[35][34] or a process for fixing bugs and errors that were not caught earlier. There might also be a return to earlier development phases if user needs changed or were misunderstood.[34]
Software development is performed by software developers, usually working on a team. Efficient communications between team members is essential to success. This is more easily achieved if the team is small, used to working together, and located near each other.[36] Communications also help identify problems at an earlier state of development and avoid duplicated effort. Many development projects avoid the risk of losing essential knowledge held by only one employee by ensuring that multiple workers are familiar with each component.[37] Software development involves professionals from various fields, not just software programmers but also individuals specialized in testing, documentation writing, graphic design, user support, marketing, and fundraising. Although workers for proprietary software are paid, most contributors to open-source software are volunteers.[38] Alternately, they may be paid by companies whose business model does not involve selling the software, but something else—such as services and modifications to open source software.[39]
Computer-aided software engineering (CASE) is tools for the partial automation of software development.[40] CASE enables designers to sketch out the logic of a program, whether one to be written, or an already existing one to help integrate it with new code or reverse engineer it (for example, to change the programming language).[41]
Documentation comes in two forms that are usually kept separate—that intended for software developers, and that made available to the end user to help them use the software.[42][43] Most developer documentation is in the form of code comments for each file, class, and method that cover the application programming interface (API)—how the piece of software can be accessed by another—and often implementation details.[44] This documentation is helpful for new developers to understand the project when they begin working on it.[45] In agile development, the documentation is often written at the same time as the code.[46] User documentation is more frequently written by technical writers.[47]
Accurate estimation is crucial at the feasibility stage and in delivering the product on time and within budget. The process of generating estimations is often delegated by the project manager.[48] Because the effort estimation is directly related to the size of the complete application, it is strongly influenced by addition of features in the requirements—the more requirements, the higher the development cost. Aspects not related to functionality, such as the experience of the software developers and code reusability, are also essential to consider in estimation.[49] As of 2019[update], most of the tools for estimating the amount of time and resources for software development were designed for conventional applications and are not applicable to web applications or mobile applications.[50]
An integrated development environment (IDE) supports software development with enhanced features compared to a simple text editor.[51] IDEs often include automated compiling, syntax highlighting of errors,[52] debugging assistance,[53] integration with version control, and semi-automation of tests.[51]
Version control is a popular way of managing changes made to the software. Whenever a new version is checked in, the software saves a backup of all modified files. If multiple programmers are working on the software simultaneously, it manages the merging of their code changes. The software highlights cases where there is a conflict between two sets of changes and allows programmers to fix the conflict.[54]
A view model is a framework that provides the viewpoints on the system and its environment, to be used in the software development process. It is a graphical representation of the underlying semantics of a view.
Intellectual property can be an issue when developers integrate open-source code or libraries into a proprietary product, because most open-source licenses used for software require that modifications be released under the same license. As an alternative, developers may choose a proprietary alternative or write their own software module.[57]
Software product management (sometimes referred to as digital product management or just product management depending on the context) is the discipline of building, implementing and managing digital products, taking into account life cycle, user interface and user experience design, use cases, and user audience. It governs the development cycle of a product from its inception to the market or customer delivery and service in order to maximize revenue. This is in contrast to software that is delivered in an ad hoc manner, typically to a limited clientele, e.g. service.[
To develop, sell and support a successful software product a business needs to solve its market problem, understand its market, identify the opportunity as well as develop and market an appropriate piece of software. Hence the need for product management as a core business function in software companies.
Hardware and physical product companies may also need software product management, since software and digital systems are often part of the delivery, such as when providing operating systems, or supporting the physical product or software embedded in a device.
A software product manager leads and manages one or several products from inception to phase-out in order to create customer value and deliver measurable business benefits. They collaborate with cross-functional teams (such as marketing, sales, engineering, finance, quality assurance/testing, customer service, manufacturing and installation) in order to fulfill a product vision and generate business success. The software product manager determines what products, enhancements and features to build and is accountable for the business success of an entire product portfolio. They develop the product roadmap and are responsible for the value chain of a product throughout its life cycle.
Software product management roles can be subdivided depending on their focus: product owner, product marketing manager, technical product manager and strategic product manager. Software program manager focuses on the project delivery of engineering processes, design, documentation, planning, execution, operations and feedback.
Software product management covers all steps from the inception of a product to its end of life. It consists of five major stages in the product life-cycle, namely:[citation needed]
Within these five phases it deals with the following aspects of a software product within a software and/or hardware company:
The above tasks are not sequential but can co-exist. For Product Managers to be efficient in the above tasks, they have to have both engineering and marketing skills. Hence, frequently, Silicon Valley firms prefer engineers who are also MBAs to do software product management.
Industry and academia established a standard for software product management education. According to this consensus, a software product manager is educated in the following areas:[2]
A software product manager also has knowledge in the software development domain.
The International Software Product Management Association (ISPMA) maintains the public body of knowledge and syllabi for international certification.
Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.
Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture,[12] and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]
Many of these algorithms are insufficient for solving large reasoning problems because they experience a "combinatorial explosion": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.
Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining "interesting" and actionable inferences from large databases),[21] and other areas.[22]
Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]
There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]
In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as "good".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]
State space search searches through a tree of possible states to try to find a goal state.[69] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[70]
Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[75] through the backpropagation algorithm.
Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[77]
Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[84]
Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.
Bayesian networks[92] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][94] learning (using the expectation–maximization algorithm),[h][96] planning (using decision networks)[97] and perception (using dynamic Bayesian networks).[90]
Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[90]
There are many kinds of classifiers in use.[99] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[100] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[101]
The naive Bayes classifier is reportedly the "most widely used learner"[102] at Google, due in part to its scalability.[103]
Neural networks are also used as classifiers.[104]
Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[105] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[106]
Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[113] and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[114] The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]
In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[126] Specialized programming languages such as Prolog were used in early AI research,[127] but general-purpose programming languages like Python have become predominant.[128]
AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).
For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[134] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[134][135] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[136] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[137] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[138][139]
Large language models, such as GPT-4, Gemini, Claude, LLaMa or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning[149] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[150] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[151] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.[152] The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[153] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.[154]
Generative artificial intelligence (Generative AI, GenAI,[166] or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data.[167][168][169] These models learn the underlying patterns and structures of their training data and use them to produce new data[170][171] based on the input, which often comes in the form of natural language prompts.[172][173]
Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora.[174][175][176][177] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[172][178][179]
Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[188][189][190]
Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for "classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights." For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.
AI has potential benefits and potential risks.[203] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to "solve intelligence, and then use that to solve everything else".[204] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[205] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[206]
Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.
Sensitive user data collected may include online activity records, geolocation data, video, or audio.[207] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[208] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[209]
In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[223] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[224]
Machine learning applications will be biased[k] if they learn from biased data.[240] The developers may not be aware that the bias exists.[241] Bias can be introduced by the way training data is selected and by the way a model is deployed.[242][240] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[243] The field of fairness studies how to prevent harms from algorithmic biases.
Criticism of COMPAS highlighted that machine learning models are designed to make "predictions" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these "recommendations" will likely be racist.[252] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]
It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as "cancerous", because pictures of malignancies typically include a ruler to show the scale.[257] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at "low risk" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[258]
People who have been harmed by an algorithm's decision have a right to an explanation.[259] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[260]
DARPA established the XAI ("Explainable Artificial Intelligence") program in 2014 to try to solve these problems.[261]
Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.
AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[271] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.[272][273]
In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI.[276] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[277] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classified only 9% of U.S. jobs as "high risk".[p][279] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[275] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[280][281]
Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously".[282] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[283]
From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[284]
Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:[316][317]
The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[322] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[323] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[324][325] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[326] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[326] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[326] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[327] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[328] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.[329] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the "Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[330]
In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[334] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[335][336] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[337][338]
The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as "astonishing":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[341]
Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[348] In 1965 Herbert Simon predicted, "machines will be capable, within twenty years, of doing any work a man can do".[349] In 1967 Marvin Minsky agreed, writing that "within a generation ... the problem of creating 'artificial intelligence' will substantially be solved".[350] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[352] and ongoing pressure from the U.S. Congress to fund more productive projects.[353] Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[354] The "AI winter", a period when obtaining funding for AI projects was difficult, followed.[9]
AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This "narrow" and "formal" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[365] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as "artificial intelligence" (a tendency known as the AI effect).[366]
However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or "AGI"), which had several well-funded institutions by the 2010s.[4]
Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]
For many specific tasks, other methods were abandoned.[y]
Deep learning's success was based on both hardware improvements (faster computers,[368] graphics processing units, cloud computing[369]) and access to large amounts of data[370] (including curated datasets,[369] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[326]
In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[303]
In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[371] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[372] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[373] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about $50 billion annually was invested in "AI" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in "AI".[374] About 800,000 "AI"-related U.S. job openings existed in 2022.[375] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[376]
Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. "Aeronautical engineering texts", they wrote, "do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"[383] AI founder John McCarthy agreed, writing that "Artificial intelligence is not, by definition, simulation of human intelligence".[384]
Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI,[388] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did "not actually use AI in a material way".[389]
No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term "artificial intelligence" to mean "machine learning with neural networks"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.
The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[396][397] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.
Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.
The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that "[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on."[401] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.
Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[404]
A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[400] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an "intelligence explosion" and Vernor Vinge called a "singularity".[416]
Edward Fredkin argues that "artificial intelligence is the next step in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[419]
Arguments for decomputing have been raised by Dan McQuillan (Resisting AI: An Anti-fascist Approach to Artificial Intelligence, 2022), meaning an opposition to the sweeping application and expansion of artificial intelligence. Similar to degrowth the approach criticizes AI as an outgrowth of the systemic issues and capitalist world we live in. Arguing that a different future is possible, in which distance between people is reduced and not increased to AI intermediaries.[420]
Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the "Multivac" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[424] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[425]
Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[426]
Certified Software Development Professional (CSDP) is a vendor-neutral professional certification in software engineering developed by the IEEE Computer Society for experienced software engineering professionals. This certification was offered globally since 2001 through Dec. 2014.
The certification program constituted an element of the Computer Society's major efforts in the area of Software engineering professionalism, along with the IEEE-CS and ACM Software Engineering 2004 (SE2004) Undergraduate Curricula Recommendations, and The Guide to the Software Engineering Body of Knowledge (SWEBOK Guide 2004), completed two years later.
As a further development of these elements, to facilitate the global portability of the software engineering certification, since 2005 through 2008 the International Standard ISO/IEC 24773:2008 "Software engineering -- Certification of software engineering professionals -- Comparison framework"
[2]
has been developed. (Please, see an overview of this ISO/IEC JTC 1 and IEEE standardization effort in the article published by Stephen B. Seidman, CSDP.
[3]
) The standard was formulated in such a way, that it allowed to recognize the CSDP certification scheme as basically aligned with it, soon after the standard's release date, 2008-09-01. Several later revisions of the CSDP certification were undertaken with the aim of making the alignment more complete. In 2019, ISO/IEC 24773:2008 has been withdrawn and revised (by ISO/IEC 24773-1:2019 [4]).
The certification was initially offered by the IEEE Computer Society to experienced software engineering and software development practitioners globally in 2001 in the course of the certification examination beta-testing. The CSDP certification program has been officially approved in 2002.
[1]
After December 2014 this certification program has been discontinued, all issued certificates are recognized as valid forever.
[5]
[6] 
A number of new similar certifications were introduced by the IEEE Computer Society, including the Professional Software Engineering Master (PSEM) and Professional Software Engineering Process Master (PSEPM) Certifications (the later soon discontinued).
To become a Certified Software Development Professional (CSDP) candidates had to have four years (initially six years) of professional software engineering experience, pass a three-and-half-hour, 180-question examination on various knowledge areas of software engineering, and possess at least a bachelor's degree in Computer Science or Software Engineering. The CSDP examination tested candidates' proficiency in internationally accepted, industry-standard[7] software engineering principles and practices.[8] CSDP credential holders are also obligated to adhere to the IEEE/ACM's Software Engineering Code of Ethics and Professional Practice.[9] [10]
As of 2021, the IEEE-CS offer which is a successor to CSDP is the Professional Software Engineering Master (PSEM) certification. The exam is three hours, is proctored remotely, and consists of 160 questions over the 11 SWEBOK knowledge areas: Software Requirements, Software Design, Software Construction, Software Testing, Software Maintenance, Software Configuration Management, Software Engineering Management, Software Engineering Process, Software Engineering Models and Methods, Software Quality, Software Engineering Economics.[11]
(There is also the Professional Software Developer (PSD) certification, which covers only 4 knowledge areas: software requirements, software design, software construction, and software testing.[12] The similarity of the name of this certification to the CSDP is confusing, it is a reputable credential but NOT an equivalent of CSDP.)
The IEEE Computer Society introduced the CSDP in 2002, and on October 27, 2008, it became the first certification to conform to ISO/IEC 24773 standard for software engineering certification.[13])
The Certified Software Development Associate (CSDA) certification was available to graduating students and early-career software professionals who did not meet the eligibility requirements for the CSDP.
[14]
[15]
The CSDP examination content was based on the Guide To The Software Engineering Body of Knowledge.  The examination covered content from all primary knowledge areas in the SWEBOK Guide Version 3. Below is a list of the topics tested in terms of their proportion of the total examination.[16]
The Software and Information Industry Association (SIIA) is a trade association dedicated to the entertainment, consumer and business software industries. Established in 1984 as the Software Publishers Association (SPA),[1] the SIIA took its new name when it merged with the related Information Industry Association on January 1, 1999. The joint enterprise was headed by Software Publishers Association founder Ken Wasch and operated out of the SPA's existing offices.[2]
The SPA was active in lobbying, industry research and anti-piracy efforts,[2] and was behind the 1992 Don't Copy That Floppy campaign.[3] The organization's head of research, Ann Stephens, went on to found PC Data in 1991.[4] By 1995, the SPA had over 1,100 software companies in its membership[5] and according to Wired was among "the most powerful computer-related trade groups" before its merger with the Information Industry Association.[6] While Microsoft became a member of the SPA in 1986, it split with the SIIA in 2000 after the group sided against Microsoft in United States v. Microsoft Corp.[7] The Wall Street Journal described Microsoft as the SIIA's "largest member" before the departure.[8]
Until 1999, the Software Publishers Association hosted the SPA Annual Conference for software companies. It was renamed the InfoSoft Essentials conference in 1999.[9]
Public Policy ~ legal and public policy
IP Protection ~  protecting software content
Connectiv ~ business information
ETIN ~ Education Technology
FISD ~ Financial & Information
SIPA ~ Specialized Information Publishers
SSD ~ Software & Services
Beginning in 1986,[13] the Software Publishers Association hosted the "Excellence in Software Awards" ceremony, an annual black-tie event that The Washington Post and Los Angeles Times compared to the Academy Awards.[14][15] The Excellence in Software Awards were later renamed the "CODiE Awards", and are now presented by the Software and Information Industry Association.[13]
The CODiE are awards to two broad categories: business technology and education technology. There are awards in more than 75 categories, advertised with the statement, "With a grand total of more than 75 different categories, you're sure to find several to meet your marketing/PR objectives!".[16] Notable past winners include companies such as Adobe, BrainPOP, Google, Knewton, McGraw-Hill Education, Jigsaw, Netsuite, Red Hat, Rosetta Stone, Salesforce.com, Digimind, Scribe Software, Vocus, WSJ.com, codemantra,[17] IXL Learning, itslearning, and more.[18]
Software craftsmanship is an approach to software development that emphasizes the coding skills of the software developers. It is a response by software developers to the perceived ills of the mainstream software industry, including the prioritization of financial concerns over developer accountability.
The Agile Manifesto, with its emphasis on "individuals and interactions over processes and tools" questioned some of these assumptions. The Software Craftsmanship Manifesto extends and challenges further the assumptions of the Agile Manifesto, drawing a metaphor between modern software development and the apprenticeship model of medieval Europe.
The movement traces its roots to the ideas expressed in written works. The Pragmatic Programmer by Andy Hunt and Dave Thomas and Software Craftsmanship by Pete McBreen explicitly position software development as heir to the guild traditions of medieval Europe. The philosopher Richard Sennett wrote about software as a modern craft in his book The Craftsman. Freeman Dyson, in his essay "Science as a Craft Industry", expands software crafts to include mastery of using software as a driver for economic benefit:
As aspiring Software Craftsmen we are raising the bar of professional software development by practicing it and helping others learn the craft. Through this work we have come to value:
The origins of software craftsmanship came from the agile software development movement which aimed to reform software project management in the 1990s.
In 1992, Jack W. Reeves' essay "What Is Software Design?"[4] suggested that software development is both a craft and an engineering discipline. Seven years later, in 1999, The Pragmatic Programmer was published. Its sub-title, "From Journeyman to Master", suggested that programmers go through stages in their professional development akin to the medieval guild traditions of Europe.
In 2001, Pete McBreen's book Software Craftsmanship was published. It suggested that software developers need not see themselves as part of the engineering tradition and that a different metaphor would be more suitable.
In December 2008, a number of aspiring software craftsmen met in Libertyville, Illinois, with the intent of establishing a set of principles for software craftsmanship. Three months later, a summary of the general conclusions was decided upon. It was presented publicly, for both viewing and signing, in the form of a Manifesto for Software Craftsmanship.[6]
In April 2009, two of the companies in the software craftsmanship movement, 8th Light and Obtiva, experimented with a "Craftsman Swap."[7] The Chicago Tribune covered this event on 15 June 2009.[8] In January 2010, a second Craftsman Swap was held between Obtiva and Relevance.[9]
The 'London Software Craftsmanship Community' (LSCC) was founded in 2010 and is today the largest and most active Software Craftsmanship community in the world, with more than 5000 craftspeople. In 2014, Sandro Mancuso, one of its co-founders, published a book The Software Craftsman: Professionalism, Pragmatism, Pride. It brought the software craftsmanship movement additional visibility, reinforcing the efforts to achieve higher technical excellence and customer satisfaction.
This is a list of academic conferences in computer science. Only conferences with separate articles are included; within each field, the conferences are listed alphabetically by their short names.
Conferences accepting a broad range of topics from theoretical computer science, including algorithms, data structures, computability, computational complexity, automata theory and formal languages:
Conferences whose topic is algorithms and data structures considered broadly, but that do not include other areas of theoretical computer science such as computational complexity theory:
Conferences on formal methods in software engineering, including formal specification, formal verification, and static code analysis:
Conferences on databases, information systems, information retrieval, data mining and the World Wide Web:
The ACM SIGSOFT Software Engineering Notes (SEN) is published by the Association for Computing Machinery (ACM) for the Special Interest Group on Software Engineering (SIGSOFT).[1] It was established in 1976, and the first issue appeared in May 1976.[2] It provides a forum for informal articles and other information on software engineering. The headquarters is in New York City.[2] Since 1990, it has been published five times a year.[2]
Software consists of computer programs that instruct the execution of a computer.[1] Software also includes design documents and specifications.
The history of software is closely tied to the development of digital computers in the mid-20th century. Early programs were written in the machine language specific to the hardware. The introduction of high-level programming languages in 1958 allowed for more human-readable instructions, making software development easier and more portable across different computer architectures. Software in a programming language is run through a compiler or interpreter to execute on the architecture's hardware. Over time, software has become complex, owing to developments in networking, operating systems, and databases.
The rise of cloud computing has introduced the new software delivery model Software as a Service (SaaS). In SaaS, applications are hosted by a provider and accessed over the Internet.
The process of developing software involves several stages. The stages include software design, programming, testing, release, and maintenance. Software quality assurance and security are critical aspects of software development, as bugs and security vulnerabilities can lead to system failures and security breaches. Additionally, legal issues such as software licenses and intellectual property rights play a significant role in the distribution of software products.
The first use of the word software to describe computer programs is credited to mathematician John Wilder Tukey in 1958.[3]
The first programmable computers, which appeared at the end of the 1940s,[4] were programmed in machine language. Machine language is difficult to debug and not portable across different computers.[5] Initially, hardware resources were more expensive than human resources.[6] As programs became complex, programmer productivity became the bottleneck. The introduction of high-level programming languages in 1958 hid the details of the hardware and expressed the underlying algorithms into the code .[7][8] Early languages include Fortran, Lisp, and COBOL.[8]
Software can also be categorized by how it is deployed. Traditional applications are purchased with a perpetual license for a specific version of the software, downloaded, and run on hardware belonging to the purchaser.[17]  The rise of the Internet and cloud computing enabled a new model, software as a service (SaaS),[18] in which the provider hosts the software (usually built on top of rented infrastructure or platforms)[19] and provides the use of the software to customers, often in exchange for a subscription fee.[17] By 2023, SaaS products—which are usually delivered via a web application—had become the primary method that companies deliver applications.[20]
Software companies aim to deliver a high-quality product on time and under budget. A challenge is that software development effort estimation is often inaccurate.[21] Software development begins by conceiving the project, evaluating its feasibility, analyzing the business requirements, and making a software design.[22][23] Most software projects speed up their development by reusing or incorporating existing software, either in the form of commercial off-the-shelf (COTS) or open-source software.[24][25] Software quality assurance is typically a combination of manual code review by other engineers[26] and automated software testing. Due to time constraints, testing cannot cover all aspects of the software's intended functionality, so developers often focus on the most critical functionality.[27] Formal methods are used in some safety-critical systems to prove the correctness of code,[28] while user acceptance testing helps to ensure that the product meets customer expectations.[29] There are a variety of software development methodologies, which vary from completing all steps in order to concurrent and iterative models.[30] Software development is driven by requirements taken from prospective users, as opposed to maintenance, which is driven by events such as a change request.[31]
Frequently, software is released in an incomplete state when the development team runs out of time or funding.[32] Despite testing and quality assurance, virtually all software contains bugs where the system does not work as intended. Post-release software maintenance is necessary to remediate these bugs when they are found and keep the software working as the environment changes over time.[33] New features are often added after the release. Over time, the level of maintenance becomes increasingly restricted before being cut off entirely when the product is withdrawn from the market.[34] As software ages, it becomes known as legacy software and can remain in use for decades, even if there is no one left who knows how to fix it.[35] Over the lifetime of the product, software maintenance is estimated to comprise 75 percent or more of the total development cost.[36][37]
Completing a software project involves various forms of expertise, not just in software programmers but also testing, documentation writing, project management, graphic design, user experience, user support, marketing, and fundraising.[38][39][23]
Software quality is defined as meeting the stated requirements as well as customer expectations.[40] Quality is an overarching term that can refer to a code's correct and efficient behavior, its reusability and portability, or the ease of modification.[41] It is usually more cost-effective to build quality into the product from the beginning rather than try to add it later in the development process.[42] Higher quality code will reduce lifetime cost to both suppliers and customers as it is more reliable and easier to maintain.[43][44] Software failures in safety-critical systems can be very serious including death.[43] By some estimates, the cost of poor quality software can be as high as 20 to 40 percent of sales.[45]  Despite developers' goal of delivering a product that works entirely as intended, virtually all software contains bugs.[46]
The rise of the Internet also greatly increased the need for computer security as it enabled malicious actors to conduct cyberattacks remotely.[47][48] If a bug creates a security risk, it is called a vulnerability.[49][50] Software patches are often released to fix identified vulnerabilities, but those that remain unknown (zero days) as well as those that have not been patched are still liable for exploitation.[51] Vulnerabilities vary in their ability to be exploited by malicious actors,[49] and the actual risk is dependent on the nature of the vulnerability as well as the value of the surrounding system.[52]  Although some vulnerabilities can only be used for denial of service attacks that compromise a system's availability, others allow the attacker to inject and run their own code (called malware), without the user being aware of it.[49] To thwart cyberattacks, all software in the system must be designed to withstand and recover from external attack.[48] Despite efforts to ensure security, a significant fraction of computers are infected with malware.[53]
Programming languages are the format in which software is written. Since the 1950s, thousands of different programming languages have been invented; some have been in use for decades, while others have fallen into disuse.[54] Some definitions classify machine code—the exact instructions directly implemented by the hardware—and assembly language—a more human-readable alternative to machine code whose statements can be translated one-to-one into machine code—as programming languages.[55] Programs written in the high-level programming languages used to create software share a few main characteristics: knowledge of machine code is not necessary to write them, they can be ported to other computer systems, and they are more concise and human-readable than machine code.[56] They must be both human-readable and capable of being translated into unambiguous instructions for computer hardware.[57]
The invention of high-level programming languages was simultaneous with the compilers needed to translate them automatically into machine code.[58] Most programs do not contain all the resources needed to run them and rely on external libraries. Part of the compiler's function is to link these files in such a way that the program can be executed by the hardware. Once compiled, the program can be saved as an object file and the loader (part of the operating system) can take this saved file and execute it as a process on the computer hardware.[59] Some programming languages use an interpreter instead of a compiler. An interpreter converts the program into machine code at run time, which makes them 10 to 100 times slower than compiled programming languages.[60][61]
Software is often released with the knowledge that it is incomplete or contains bugs. Purchasers knowingly buy it in this state, which has led to a legal regime where liability for software products is significantly curtailed compared to other products.[62]
Since the mid-1970s, software and its source code have been protected by copyright law that vests the owner with the exclusive right to copy the code. The underlying ideas or algorithms are not protected by copyright law, but are sometimes treated as a trade secret and concealed by such methods as non-disclosure agreements.[63] A software copyright is often owned by the person or company that financed or made the software (depending on their contracts with employees or contractors who helped to write it).[64] Some software is in the public domain and has no restrictions on who can use it, copy or share it, or modify it; a notable example is software written by the United States Government.  Free and open-source software also allow free use, sharing, and modification, perhaps with a few specified conditions.[64]  The use of some software is governed by an agreement (software license) written by the copyright holder and imposed on the user. Proprietary software is usually sold under a restrictive license that limits its use and sharing.[65]  Some free software licenses require that modified versions must be released under the same license, which prevents the software from being sold
or distributed under proprietary restrictions.[66]
Patents give an inventor an exclusive, time-limited license for a novel product or process.[67] Ideas about what software could accomplish are not protected by law and concrete implementations are instead covered by copyright law. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid.[68] Software patents have been historically controversial. Before the 1998 case State Street Bank & Trust Co. v. Signature Financial Group, Inc., software patents were generally not recognized in the United States. In that case, the Supreme Court decided that business processes could be patented.[69] Patent applications are complex and costly, and lawsuits involving patents can drive up the cost of products.[70] Unlike copyrights, patents generally only apply in the jurisdiction where they were issued.[71]
Engineer Capers Jones writes that "computers and software are making profound changes to every aspect of human life: education, work, warfare, entertainment, medicine, law, and everything else".[73] It has become ubiquitous in everyday life in developed countries.[74] In many cases, software augments the functionality of existing technologies such as household appliances and elevators.[75] Software also spawned entirely new technologies such as the Internet, video games, mobile phones, and GPS.[75][76] New methods of communication, including email, forums, blogs, microblogging, wikis, and social media, were enabled by the Internet.[77] Massive amounts of knowledge exceeding any paper-based library are now available with a quick web search.[76] Most creative professionals have switched to software-based tools such as computer-aided design, 3D modeling, digital image editing, and computer animation.[72] Almost every complex device is controlled by software.[76]
A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term "domain-specific language" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.
The line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as AWK and shell scripts, but was mostly used as a general-purpose programming language later on. By contrast, PostScript is a Turing-complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.
The design and use of appropriate DSLs is a key part of domain engineering, by using a language suitable to the domain at hand – this may consist of using an existing DSL or GPL, or developing a new DSL. Language-oriented programming considers the creation of special-purpose languages for expressing problems as standard part of the problem-solving process. Creating a domain-specific language (with software to support it), rather than reusing an existing language, can be worthwhile if the language allows a particular type of problem or solution to be expressed more clearly than an existing language would allow and the type of problem in question reappears sufficiently often. Pragmatically, a DSL may be specialized to a particular problem domain, a particular problem representation technique, a particular solution technique, or other aspects of a domain.
A domain-specific language is somewhere between a tiny programming language and a scripting language, and is often used in a way analogous to a programming library. The boundaries between these concepts are quite blurry, much like the boundary between scripting languages and general-purpose languages.
Domain-specific languages are languages (or often, declared syntaxes or grammars) with very specific goals in design and implementation. A domain-specific language can be one of a visual diagramming language, such as those created by the Generic Eclipse Modeling System, programmatic abstractions, such as the Eclipse Modeling Framework, or textual languages. For instance, the command line utility grep has a regular expression syntax which matches patterns in lines of text. The sed utility defines a syntax for matching and replacing regular expressions.  Often, these tiny languages can be used together inside a shell to perform more complex programming tasks.
The line between domain-specific languages and scripting languages is somewhat blurred, but domain-specific languages often lack low-level functions for filesystem access, interprocess control, and other functions that characterize full-featured programming languages, scripting or otherwise. Many domain-specific languages do not compile to byte-code or executable code, but to various kinds of media objects: GraphViz exports to PostScript, GIF, JPEG, etc., where Csound compiles to audio files, and a ray-tracing domain-specific language like POV compiles to graphics files.
A data definition language like SQL presents an interesting case: it can be deemed a domain-specific language because it is specific to a specific domain (in SQL's case, accessing and managing relational databases), and is often called from another application, but SQL has more keywords and functions than many scripting languages, and is often thought of as a language in its own right, perhaps because of the prevalence of database manipulation in programming and the amount of mastery required to be an expert in the language.
Further blurring this line, many domain-specific languages have exposed APIs, and can be accessed from other programming languages without breaking the flow of execution or calling a separate process, and can thus operate as programming libraries.
Some domain-specific languages expand over time to include full-featured programming tools, which further complicates the question of whether a language is domain-specific or not. A good example is the functional language XSLT, specifically designed for transforming one XML graph into another, which has been extended since its inception to allow (particularly in its 2.0 version) for various forms of filesystem interaction, string and date manipulation, and data typing.
Adopting a domain-specific language approach to software engineering involves both risks and opportunities. The well-designed domain-specific language manages to find the proper balance between these.
General purpose programming languages rarely support such idioms, but domain-specific languages can describe them, e.g.:
Examples of domain-specific programming languages include HTML, Logo for pencil-like drawing, Verilog and VHDL hardware description languages, MATLAB and GNU Octave for matrix programming, Mathematica, Maple and Maxima for symbolic mathematics, Specification and Description Language for reactive and distributed systems, spreadsheet formulas and macros, SQL for relational database queries, YACC grammars for creating parsers, regular expressions for specifying lexers, the Generic Eclipse Modeling System for creating diagramming languages, Csound for sound and music synthesis, and the input languages of GraphViz and GrGen, software packages used for graph layout and graph rewriting, Hashicorp Configuration Language used for Terraform and other Hashicorp tools, Puppet also has its own configuration language.
The GML scripting language used by GameMaker Studio is a domain-specific language targeted at novice programmers to easily be able to learn programming. While the language serves as a blend of multiple languages including Delphi, C++, and BASIC. Most of functions in that language after compiling in fact calls runtime functions written in language specific for targeted platform, so their final implementation is not visible to user. The language primarily serves to make it easy for anyone to pick up the language and develop a game, and thanks to GM runtime which handles main game loop and keeps implementation of called functions, few lines of code is required for simplest game, instead of thousands.
FilterMeister is a programming environment, with a programming language that is based on C, for the specific purpose of creating Photoshop-compatible image processing filter plug-ins; FilterMeister runs as a Photoshop plug-in itself and it can load and execute scripts or compile and export them as independent plug-ins.
Although the FilterMeister language reproduces a significant portion of the C language and function library, it contains only those features which can be used within the context of Photoshop plug-ins and adds a number of specific features only useful in this specific domain.
There has been much interest in domain-specific languages to improve the productivity and quality of software engineering.  Domain-specific language could possibly provide a robust set of tools for efficient software engineering. Such tools are beginning to make their way into the development of critical software systems.
The Software Cost Reduction Toolkit[6] is an example of this. The toolkit is a suite of utilities including a specification editor to create a requirements specification, a dependency graph browser to display variable dependencies, a consistency checker to catch missing cases in well-formed formulas in the specification, a model checker and a theorem prover to check program properties against the specification, and an invariant generator that automatically constructs invariants based on the requirements.
A newer development is language-oriented programming, an integrated software engineering methodology based mainly on creating, optimizing, and using domain-specific languages.
Complementing language-oriented programming, as well as all other forms of domain-specific languages, are the class of compiler writing tools called metacompilers. A metacompiler is not only useful for generating parsers and code generators for domain-specific languages, but a metacompiler itself compiles a domain-specific metalanguage specifically designed for the domain of metaprogramming.
Besides parsing domain-specific languages, metacompilers are useful for generating a wide range of software engineering and analysis tools.  The meta-compiler methodology is often found in program transformation systems.
Metacompilers that played a significant role in both computer science and the computer industry include Meta-II,[7] and its descendant TreeMeta.[8]
The purpose of business rules engines is to define a representation of business logic in as human-readable fashion as possible.  This allows both subject-matter experts and developers to work with and understand the same representation of the business logic.  Most rules engines provide both an approach to simplifying the control structures for business logic (for example, using declarative rules or decision tables) coupled with alternatives to programming syntax in favor of DSLs.
Gherkin is a language designed to define test cases to check the behavior of software, without specifying how that behavior is implemented. It is meant to be read and used by non-technical users using a natural language syntax and a line-oriented design. The tests defined with Gherkin must then be implemented in a general programming language. Then, the steps in a Gherkin program acts as a syntax for method invocation accessible to non-developers.
A query language, also known as data query language or database query language (DQL), is a computer language used to make queries in databases and information systems. In database systems, query languages rely on strict theory to retrieve information.[1] A well known example is the Structured Query Language (SQL).
Broadly, query languages can be classified according to whether they are database query languages or information retrieval query languages. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry. Other types of query languages include:
APT (Automatically Programmed Tool)[1] is a high-level computer programming language most commonly used to generate instructions for numerically controlled machine tools. Douglas T. Ross[2] is considered by many to be the father of APT: as head of the newly created Computer Applications Group of the Servomechanisms Laboratory at MIT in 1956, he led its technical effort. APT is a language and system that alleviates the tedious mathematics of writing toolpaths for numerically controlled equipment. This early language was used widely through the 1970s and is still a standard internationally.[3] Derivatives of APT were later developed.
APT is used to program numerically-controlled machine tools to create complex parts using a cutting tool moving in space. It is used to calculate a path that a tool must follow to generate a desired form. APT is a special-purpose language and the predecessor to modern computer aided manufacturing (CAM) systems. It was created and refined during the late 1950s and early 1960s to simplify the task of calculating geometry points that a tool must traverse in space to cut the complex parts required in the aerospace industry. It was a direct result of the new numerical control technology becoming available at that time and the daunting task that a machinist or engineer faced calculating the movements of the machine for the complex parts for which it was capable. Its development was centered at the same MIT labs that hosted the Numerical Control and the Milling Machine Projects. APT also was US Air Force sponsored and is notable for being the world's first major cooperative programming venture, combining government agencies, universities, and a 14-company team organized within the Aircraft Industries Association (now Aerospace Industries Association).[4] APT was created before graphical user interfaces were available, and so it relies on text to specify the geometry and toolpaths needed to machine a part. The original version was created before even FORTRAN was available and was the very first ANSI standard. Later versions were rewritten in FORTRAN.
APT shares many similarities with other computer programming languages such as FORTRAN. A general-purpose computer language takes source text and converts the statements to instructions that can be processed internally by a computer. APT converts source statements into programs for driving numerically-controlled machine tools. The output from an APT processor may be a cutter location (CL) file which is then run through a post-processor specific to the desired control - machine pair. The resulting file is then run by the control of the machine to generate tool motions and other machine actions. Most commonly, this file is in some form of RS-274 format instructions, commonly known as G-code.
Further derivatives of APT were developed, initially mainly to allow the programs to run on minicomputers instead of mainframes.  These included ADAPT from IBM; EXAPT from Germany; and UNIAPT, initially from United Computing, later used alongside the Unigraphics CADCAM systems.[5]
Machine tool manufacturer's systems utilize elements of APT to this day.[6] Standards Developers like STEP-NC took toolpath curves from APT and other sources.[7] APT formed the basis for two early programming languages in robotics: RAPT (Robot APT) was developed at the University of Edinburgh School of Informatics, and ROBEX was a derivative of EXAPT.[8]
In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term "database" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.
Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.
Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.
Formally, a "database" refers to a set of related data accessed through the use of a "database management system" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.
Because of the close relationship between them, the term "database" is often used casually to refer to both a database and the DBMS used to manipulate it.
Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.[1]
Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:
Both a database and its DBMS conform to the principles of a particular database model.[5] "Database system" refers collectively to the database model, database management system, and database.[6]
Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.[citation needed]
Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.
The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational,[8] SQL/relational, and post-relational.
The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018[update] they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS.[9] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.[citation needed]
Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch, which led to the coining of the term "post-relational" and also the development of hybrid object–relational databases.
The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases. A competing "next generation" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.
The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term "data-base" in a specific technical sense.[10]
As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market.
Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications.
IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014[update].[11]
Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that were primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a "search" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[12]
In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of "tables", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each "fact" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated.
The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit.
For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.
As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic.
Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a "language" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.
In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.
Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata).
IBM started working on a prototype system loosely based on Codd's concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large "chunk". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL[citation needed] – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2).
Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979.[18]
Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).
Another data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.[citation needed]
The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation."[19] dBASE was one of the top selling software titles in the 1980s and early 1990s.
The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields.[20] The term "object–relational impedance mismatch" described the inconvenience of translating between programmed objects and database tables. Object databases and object–relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object–relational mappings (ORMs) attempt to solve the same problem.
XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.
NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.
In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.
NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.
Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).
Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.
One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.
Connolly and Begg define database management system (DBMS) as a "software system that enables users to define, create, maintain and control access to the database."[24] Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access.
The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object–relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems.
It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities.[26] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.
Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.
Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.[28]
A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.
External interaction with the database will be via an application program that interfaces with the DBMS.[29] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information.
A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.
Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:
Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, "data about the data", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database.[33] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. "storage engine". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).
Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.
Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.
Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.
Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.
With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.[34]
Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).
Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.
This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called "subschemas". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.
Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause.[35]
Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).
The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.
A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs.
After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).
When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.
After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.
Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.
Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[36] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc.
Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as "DevOps for database".[37]
The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity–relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like "can a customer also be a supplier?", or "if a product is sold with two different forms of packaging, are those the same product or different products?", or "if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.
Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.
Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).
The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary "fact" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.
The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.
Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.
A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.
While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database.
The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model.[39] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.
The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.
Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more.
The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).
MAD (Michigan Algorithm Decoder) is a programming language and compiler for the IBM 704 and later the IBM 709, IBM 7090, IBM 7040, UNIVAC 1107, UNIVAC 1108, Philco 210-211, and eventually IBM System/370 mainframe computers. Developed in 1959 at the University of Michigan by Bernard Galler, Bruce Arden and Robert M. Graham, MAD is a variant of the ALGOL language. It was widely used to teach programming at colleges and universities during the 1960s and played a minor role in the development of Compatible Time-Sharing System (CTSS), Multics, and the Michigan Terminal System computer operating systems.[1] The original version of the chatbot ELIZA was written in MAD-SLIP.[2]
The following is an interesting quote from An Interview with Brian Kernighan[19] when he was asked "What hooked you on programming?":
GOM is essentially the 7090 MAD language modified and extended for the 360/370 architecture with some judicious tailoring to better fit current programming practices and problems.[12] The MTS Message System was written in GOM.
Logo is an educational programming language, designed in 1967 by Wally Feurzeig, Seymour Papert, and Cynthia Solomon.[1] Logo is not an acronym: the name was coined by Feurzeig while he was at Bolt, Beranek and Newman,[2] and derives from the Greek logos, meaning 'word' or 'thought'.
A general-purpose language, Logo is widely known for its use of turtle graphics, in which commands for movement and drawing produced line or vector graphics, either on screen or with a small robot termed a turtle. The language was conceived to teach concepts of programming related to Lisp and only later to enable what Papert called "body-syntonic reasoning", where students could understand, predict, and reason about the turtle's motion by imagining what they would do if they were the turtle. There are substantial differences among the many dialects of Logo, and the situation is confused by the regular appearance of turtle graphics programs that are named Logo.
Logo is a multi-paradigm adaptation and dialect of Lisp, a functional programming language.[3] There is no standard Logo, but UCBLogo has the best facilities[according to whom?] for handling lists, files, I/O, and recursion in scripts, and can be used to teach all computer science concepts, as UC Berkeley lecturer Brian Harvey did in his Computer Science Logo Style trilogy.[4]
Logo was created in 1967 at Bolt, Beranek and Newman (BBN), a Cambridge, Massachusetts, research firm, by Wally Feurzeig, Cynthia Solomon, and Seymour Papert.[5] Its intellectual roots are in artificial intelligence, mathematical logic and developmental psychology. For the first four years of Logo research, development and teaching work was done at BBN. The first implementation of Logo, called Ghost, was written in LISP on a PDP-1. The goal was to create a mathematical land where children could play with words and sentences.[6] Modeled on LISP, the design goals of Logo included accessible power[clarification needed]  and informative error messages. The use of virtual Turtles allowed for immediate visual feedback and debugging of graphic programming.
Logo was a primary influence on the Smalltalk programming language. It is also the main influence on the Etoys educational programming environment and language, which is essentially a Logo variant written in Squeak (itself a variant of Smalltalk). Logo influenced the procedure/method model in AgentSheets and AgentCubes to program agents similar to the notion of a turtle in Logo. Logo provided the underlying language for Boxer. Boxer was developed at University of California, Berkeley and MIT and is based on a literacy model, making it easier to use for nontechnical people.[28]
B was derived from BCPL, and its name may possibly be a contraction of BCPL.  Thompson's coworker Dennis Ritchie speculated that the name might be based on Bon, an earlier, but unrelated, programming language that Thompson designed for use on Multics.[note 1]
B was designed for recursive, non-numeric, machine-independent applications, such as system and language software.[3] It was a typeless language, with the only data type being the underlying machine's natural memory word format, whatever that might be. Depending on the context, the word was treated either as an integer or a memory address.
As machines with ASCII processing became common, notably the DEC PDP-11 that arrived at Bell Labs, support for character data stuffed in memory words became important. The typeless nature of the language was seen as a disadvantage, which led Thompson and Ritchie to develop an expanded version of the language supporting new internal and user-defined types, which became the C programming language.
Early implementations were for the DEC PDP-7 and PDP-11 minicomputers using early Unix, and Honeywell GE 645[7] 36-bit mainframes running the operating system GCOS. The earliest PDP-7 implementations compiled to threaded code, and Ritchie wrote a compiler using TMG which produced machine code.[8][9][10] In 1970 a PDP-11 was acquired and threaded code was used for the port; an assembler, dc, and the B language itself were written in B to bootstrap the computer. An early version of yacc was produced with this PDP-11 configuration. Ritchie took over maintenance during this period.[2][10]
Pandas (styled as pandas) is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license.[2] The name is derived from the term "panel data", an econometrics term for data sets that include observations over multiple time periods for the same individuals,[3] as well as a play on the phrase "Python data analysis".[4]: 5  Wes McKinney started building what would become Pandas at AQR Capital while he was a researcher there from 2007 to 2010.[5]
The development of Pandas introduced into Python many comparable features of working with DataFrames that were established in the R programming language.[6] The library is built upon another library, NumPy.
Pandas is built around data structures called Series and DataFrames. Data for these collections can be imported from various file formats such as comma-separated values, JSON, Parquet, SQL database tables or queries, and Microsoft Excel.[8]
A Series is a 1-dimensional data structure built on top of NumPy's array.[9]: 97  Unlike in NumPy, each data point has an associated label. The collection of these labels is called an index.[4]: 112  Series can be used arithmetically, as in the statement series_3 = series_1 + series_2: this will align data points with corresponding index values in series_1 and series_2, then add them together to produce new values in series_3.[4]: 114  A DataFrame is a 2-dimensional data structure of rows and columns, similar to a spreadsheet, and analogous to a Python dictionary mapping column names (keys) to Series (values), with each Series sharing an index.[4]: 115  DataFrames can be concatenated together or "merged" on columns or indices in a manner similar to joins in SQL.[4]: 177–182  Pandas implements a subset of relational algebra, and supports one-to-one, many-to-one, and many-to-many joins.[9]: 147–148  Pandas also supports the less common Panel and Panel4D, which are 3-dimensional and 4-dimension data structures respectively.[9]: 141
Pandas supports hierarchical indices with multiple values per data point. An index with this structure, called a "MultiIndex", allows a single DataFrame to represent multiple dimensions, similar to a pivot table in Microsoft Excel.[4]: 147–148  Each level of a MultiIndex can be given a unique name.[9]: 133  In practice, data with more than 2 dimensions is often represented using DataFrames with hierarchical indices, instead of the higher-dimension Panel and Panel4D data structures[9]: 128
Crystal is a high-level general-purpose, object-oriented programming language, designed and developed by Ary Borenszweig, Juan Wajnerman, Brian Cardiff and more than 400 contributors.[5] With syntax inspired by the language Ruby,[6] it is a compiled language with static type-checking, but specifying the types of variables or method arguments is generally unneeded. Types are resolved by an advanced global type inference algorithm.[7][8] Crystal 
is currently in active development. It is released as free and open-source software under the Apache License version 2.0.
Although resembling the Ruby language in syntax, Crystal compiles to much more efficient native code using an LLVM backend, at the cost of precluding the dynamic aspects of Ruby. The advanced global type inference used by the Crystal compiler, combined with union types, gives it more the feel of a higher-level scripting language than many other comparable programming languages. It has automated garbage collection and offers a Boehm collector. Crystal possesses a macro system and supports generics as well as method and operator overloading. Its concurrency model is inspired by communicating sequential processes (CSP) and implements lightweight fibers and channels (for interfiber communication), inspired by Go.[4]
D, also known as dlang, is a multi-paradigm system programming language created by Walter Bright at Digital Mars and released in 2001. Andrei Alexandrescu joined the design and development effort in 2007. Though it originated as a re-engineering of C++, D is now a very different language. As it has developed, it has drawn inspiration from other high-level programming languages. Notably, it has been influenced by Java, Python, Ruby, C#, and Eiffel.
D is a general-purpose systems programming language with a C-like syntax that compiles to native code. It is statically typed and supports both automatic (garbage collected) and manual memory management. D programs are structured as modules that can be compiled separately and linked with external libraries to create native libraries or executables.[11]
D is a systems programming language. Like C++, and unlike application languages such as Java and C#, D supports low-level programming, including inline assembler. Inline assembler allows programmers to enter machine-specific assembly code within standard D code. System programmers use this method to access the low-level features of the processor that are needed to run programs that interface directly with the underlying hardware, such as operating systems and device drivers. Low-level programming is also used to write higher performance code than would be produced by a compiler.
Imperative programming in D is almost identical to that in C. Functions, data, statements, declarations and expressions work just as they do in C, and the C runtime library may be accessed directly. On the other hand, unlike C, D's foreach loop construct allows looping over a collection. D also allows nested functions, which are functions that are declared inside another function, and which may access the enclosing function's local variables.
Object-oriented programming in D is based on a single inheritance hierarchy, with all classes derived from class Object. D does not support multiple inheritance; instead, it uses Java-style interfaces, which are comparable to C++'s pure abstract classes, and mixins, which separate common functionality from the inheritance hierarchy. D also allows the defining of static and final (non-virtual) methods in interfaces.
Many aspects of classes (and structs) can be introspected automatically at compile time (a form of reflective programming (reflection) using type traits) and at run time (RTTI / TypeInfo), to facilitate generic code or automatic code generation (usually using compile-time techniques).
D supports functional programming features such as function literals, closures, recursively-immutable objects and the use of higher-order functions. There are two syntaxes for anonymous functions, including a multiple-statement form and a "shorthand" single-expression notation:[14]
Other functional features such as currying and common higher-order functions such as map, filter, and reduce are available through the standard library modules std.functional and std.algorithm.
Parallel programming concepts are implemented in the library, and do not require extra support from the compiler. However the D type system and compiler ensure that data sharing can be detected and managed transparently.
The same module also supports taskPool which can be used for dynamic creation of parallel tasks, as well as map-filter-reduce and fold style operations on ranges (and arrays), which is useful when combined with functional operations. std.algorithm.map returns a lazily evaluated range rather than an array. This way, the elements are computed by each worker task in parallel automatically.
Metaprogramming is supported through templates, compile-time function execution, tuples, and string mixins. The following examples demonstrate some of D's compile-time features.
The current mechanisms in place primarily deal with function parameters and stack memory however it is a stated ambition of the leadership of the programming language to provide a more thorough treatment of lifetimes within the D programming language[25] (influenced by ideas from Rust programming language).
Because many other programming languages often provide the C API for writing extensions or running the interpreter of the languages, D can interface directly with these languages as well, using standard C bindings (with a thin D interface file). For example, there are bi-directional bindings for languages like Python,[27] Lua[28][29] and other languages, often using compile-time code generation and compile-time type reflection methods.
The D programming language has an official subset known as "Better C".[30] This subset forbids access to D features requiring use of runtime libraries other than that of C.
Walter Bright started working on a new language in 1999. D was first released in December 2001[1] and reached version 1.0 in January 2007.[31] The first version of the language (D1) concentrated on the imperative, object oriented and metaprogramming paradigms,[32] similar to C++.
Some members of the D community dissatisfied with Phobos, D's official runtime and standard library, created an alternative runtime and standard library named Tango. The first public Tango announcement came within days of D 1.0's release.[33] Tango adopted a different programming style, embracing OOP and high modularity. Being a community-led project, Tango was more open to contributions, which allowed it to progress faster than the official standard library. At that time, Tango and Phobos were incompatible due to different runtime support APIs (the garbage collector, threading support, etc.). This made it impossible to use both libraries in the same project. The existence of two libraries, both widely in use, has led to significant dispute due to some packages using Phobos and others using Tango.[34]
In June 2007, the first version of D2 was released.[35] The beginning of D2's development signaled D1's stabilization. The first version of the language has been placed in maintenance, only receiving corrections and implementation bugfixes. D2 introduced breaking changes to the language, beginning with its first experimental const system. D2 later added numerous other language features, such as closures, purity, and support for the functional and concurrent programming paradigms. D2 also solved standard library problems by separating the runtime from the standard library. The completion of a D2 Tango port was announced in February 2012.[36]
The release of Andrei Alexandrescu's book The D Programming Language on 12 June 2010, marked the stabilization of D2, which today is commonly referred to as just "D".
Code for the official D compiler, the Digital Mars D compiler by Walter Bright, was originally released under a custom license, qualifying as source available but not conforming to the Open Source Definition.[40] In 2014, the compiler front-end was re-licensed as open source under the Boost Software License.[3] This re-licensed code excluded the back-end, which had been partially developed at Symantec. On 7 April 2017, the whole compiler was made available under the Boost license after Symantec gave permission to re-license the back-end, too.[4][41][42][43] On 21 June 2017, the D Language was accepted for inclusion in GCC.[44]
Using above compilers and toolchains, it is possible to compile D programs to target many different architectures, including IA-32, amd64, AArch64, PowerPC, MIPS64, DEC Alpha, Motorola m68k, SPARC, s390, WebAssembly. The primary supported operating systems are Windows and Linux, but various compilers also support Mac OS X, FreeBSD, NetBSD, AIX, Solaris/OpenSolaris and Android, either as a host or target, or both. WebAssembly target (supported via LDC and LLVM) can operate in any WebAssembly environment, like modern web browser (Google Chrome, Mozilla Firefox, Microsoft Edge, Apple Safari), or dedicated Wasm virtual machines.
Notable organisations that use the D programming language for projects include Facebook,[75] eBay,[76] and Netflix.[77]
D has been successfully used for AAA games,[78] language interpreters, virtual machines,[79][80] an operating system kernel,[81] GPU programming,[82] web development,[83][84] numerical analysis,[85] GUI applications,[86][87] a passenger information system,[88] machine learning,[89] text processing, web and application servers and research.
Dart is a programming language designed by Lars Bak and Kasper Lund and developed by Google.[8] It can be used to develop web and mobile apps as well as server and desktop applications.
The Dart software development kit (SDK) ships with a standalone Dart runtime. This allows Dart code to run in a command-line interface environment. The SDK includes tools to compile and package Dart apps.[30] Dart ships with a complete standard library allowing users to write fully working system apps like custom web servers.[31]
On November 18, 2011, Google released Dart Editor, an open-source program based on Eclipse components, for macOS, Windows, and Linux-based operating systems.[50] The editor supports syntax highlighting, code completion, JavaScript compiling, running web and server Dart applications, and debugging.
In June 2015, Google transferred the CDE project to GitHub as a free software project and ceased active investment in CDE.[63] The Chrome Dev Editor project was archived on April 24, 2021.[64]
To provide an easier way to start using Dart, the Dart team created DartPad at the start of 2015. This online editor allows developers to experiment with Dart application programming interfaces (APIs) and run Dart code. It provides syntax highlighting, code analysis, code completion, documentation, and HTML and CSS editing.[65]
Delphi is a general-purpose programming language and a software product that uses the Delphi dialect of the Object Pascal programming language and provides an integrated development environment (IDE) for rapid application development of desktop, mobile, web, and console software,[3] currently developed and maintained by Embarcadero Technologies.
Delphi includes a code editor, a visual designer, an integrated debugger, a source code control component, and support for third-party plugins. The code editor features Code Insight (code completion), Error Insight (real-time error-checking), and refactoring. The visual forms designer has the option of using either the Visual Component Library (VCL) for pure Windows development or the FireMonkey (FMX) framework for cross-platform development. Database support is a key feature and is provided by FireDAC (Database Access Components). Delphi is known for its fast compilation speed, native code, and developer productivity.[citation needed]
Delphi was originally developed by Borland as a rapid application development tool for Windows as the successor of Turbo Pascal. Delphi added full object-oriented programming to the existing language, and the language has grown to support generics, anonymous methods, closures, and native Component Object Model (COM) support.
Delphi uses the Pascal-based programming language Object Pascal created by Anders Hejlsberg for Borland (now IDERA) as the successor to Turbo Pascal. It supports native cross-compilation to many platforms including Windows, Linux, iOS, and Android.
To better support development for Microsoft Windows and interoperate with code developed with other software development tools, Delphi supports independent interfaces of Component Object Model (COM) with reference counting class implementations, and support for many third-party components. Interface implementations can be delegated to fields or properties of classes. Message handlers are implemented by tagging a method of a class with the integer constant of the message to handle.[citation needed]
Database connectivity is extensively supported through VCL database-aware and database access components.
Delphi uses a strongly typed high-level programming language, intended to be easy to use and originally based on the earlier Object Pascal language. Pascal was originally developed as a general-purpose language "suitable for expressing the fundamental constructs known at the time in a concise and logical way", and "its implementation was to be efficient and competitive with existing FORTRAN compilers"[8] but without low-level programming facilities or access to hardware. Turbo Pascal and its descendants, including Delphi, support access to hardware and low-level programming, with the facility to incorporate code written in assembly language and other languages. Delphi's object-orientation features only class- and interface-based polymorphism.[9] Metaclasses are first class objects. Objects are references to the objects (as in Java), which Delphi implicitly de-references, so there is usually no need to manually allocate memory for pointers to objects or use similar techniques that some other languages need. There are dedicated reference-counted string types, and also null-terminated strings.
Delphi is the medium used to teach programming in South African schools as a subject of information technology (IT).[11]
Elixir is a functional, concurrent, high-level general-purpose programming language that runs on the BEAM virtual machine, which is also used to implement the Erlang programming language.[3] Elixir builds on top of Erlang and shares the same abstractions for building distributed, fault-tolerant applications. Elixir also provides tooling and an extensible design. The latter is supported by compile-time metaprogramming with macros and polymorphism via protocols.[4]
José Valim created the Elixir programming language as a research and development project at Plataformatec. His goals were to enable higher extensibility and productivity in the Erlang VM while maintaining compatibility with Erlang's ecosystem.[10][11]
In 2021, the Numerical Elixir effort was announced with the goal of bringing machine learning, neural networks, GPU compilation, data processing, and computational notebooks to the Elixir ecosystem.[13]
Haskell (/ˈhæskəl/[25]) is a general-purpose, statically typed, purely functional programming language with type inference and lazy evaluation.[26][27] Designed for teaching, research, and industrial applications, Haskell pioneered several programming language features such as type classes, which enable type-safe operator overloading, and monadic input/output (IO). It is named after logician Haskell Curry.[1] Haskell's main implementation is the Glasgow Haskell Compiler (GHC).
Haskell's semantics are historically based on those of the Miranda programming language, which served to focus the efforts of the initial Haskell working group.[28] The last formal specification of the language was made in July 2010, while the development of GHC continues to expand Haskell via language extensions.
Haskell is used in academia and industry.[29][30][31] As of May 2021[update], Haskell was the 28th most popular programming language by Google searches for tutorials,[32]  and made up less than 1% of active users on the GitHub source code repository.[33]
After the release of Miranda by Research Software Ltd. in 1985, interest in lazy functional languages grew. By 1987, more than a dozen non-strict, purely functional programming languages existed. Miranda was the most widely used, but it was proprietary software. At the conference on Functional Programming Languages and Computer Architecture (FPCA '87) in Portland, Oregon, there was a strong consensus that a committee be formed to define an open standard for such languages. The committee's purpose was to consolidate existing functional languages into a common one to serve as a basis for future research in functional-language design.[34]
Haskell features lazy evaluation, lambda expressions, pattern matching, list comprehension, type classes and type polymorphism. It is a purely functional programming language, which means that functions generally have no side effects. A distinct construct exists to represent side effects, orthogonal to the type of functions. A pure function can return a side effect that is subsequently executed, modeling the impure functions of other languages.
The construct that represents side effects is an example of a monad: a general framework which can model various computations such as error handling, nondeterminism, parsing and software transactional memory. They are defined as ordinary datatypes, but Haskell provides some syntactic sugar for their use.
An implementation of an algorithm similar to quick sort over lists, where the first element is taken as the pivot:
Ben Lippmeier designed Disciple[73] as a strict-by-default (lazy by explicit annotation) dialect of Haskell with a type-and-effect system, to address Haskell's difficulties in reasoning about lazy evaluation and in using traditional data structures such as mutable arrays.[74] He argues (p. 20) that "destructive update furnishes the programmer with two important and powerful tools ... a set of efficient array-like data structures for managing collections of objects, and ... the ability to broadcast a new value to all parts of a program with minimal burden on the programmer."
Robert Harper, one of the authors of Standard ML, has given his reasons for not using Haskell to teach introductory programming. Among these are the difficulty of reasoning about resource use with non-strict evaluation, that lazy evaluation complicates the definition of datatypes and inductive reasoning,[75] and the "inferiority" of Haskell's (old) class system compared to ML's module system.[76]
Starting in 2006, a series of organized hackathons has occurred, the Hac series, aimed at improving the programming language tools and libraries.[80]
JavaScript (/ˈdʒɑːvəskrɪpt/ ⓘ), often abbreviated as JS, is a programming language and core technology of the World Wide Web, alongside HTML and CSS. Ninety-nine percent of websites use JavaScript on the client side for webpage behavior.[10]
JavaScript is a high-level, often just-in-time compiled language that conforms to the ECMAScript standard.[11] It has dynamic typing, prototype-based object-orientation, and first-class functions. It is multi-paradigm, supporting event-driven, functional, and imperative programming styles. It has application programming interfaces (APIs) for working with text, dates, regular expressions, standard data structures, and the Document Object Model (DOM).
During these formative years of the Web, web pages could only be static, lacking the capability for dynamic behavior after the page was loaded in the browser. There was a desire in the flourishing web development scene to remove this limitation, so in 1995, Netscape decided to add a programming language to Navigator. They pursued two routes to achieve this: collaborating with Sun Microsystems to embed the Java language, while also hiring Brendan Eich to embed the Scheme language.[6]
The current JavaScript ecosystem has many libraries and frameworks, established programming practices, and substantial usage of JavaScript outside of web browsers.[17] Plus, with the rise of single-page applications and other JavaScript-heavy websites, several transpilers have been created to aid the development process.[36]
The use of JavaScript has expanded beyond its web browser roots. JavaScript engines are now embedded in a variety of other software systems, both for server-side website deployments and non-browser applications.
A JavaScript engine is a software component that executes JavaScript code. The first JavaScript engines were mere interpreters, but all relevant modern engines use just-in-time compilation for improved performance.[57]
JavaScript is a single-threaded language. The runtime processes messages from a queue one at a time, and it calls a function associated with each new message, creating a call stack frame with the function's arguments and local variables. The call stack shrinks and grows based on the function's needs. When the call stack is empty upon function completion, JavaScript proceeds to the next message in the queue. This is called the event loop, described as "run to completion" because each message is fully processed before the next message is considered. However, the language's concurrency model describes the event loop as non-blocking: program I/O is performed using events and callback functions. This means, for example, that JavaScript can process a mouse click while waiting for a database query to return information.[61]
JavaScript supports much of the structured programming syntax from C (e.g., if statements, while loops, switch statements, do while loops, etc.). One partial exception is scoping: originally JavaScript only had function scoping with var; block scoping was added in ECMAScript 2015 with the keywords let and const. Like C, JavaScript makes a distinction between expressions and statements. One syntactic difference from C is automatic semicolon insertion, which allow semicolons (which terminate statements) to be omitted.[62]
JavaScript and the DOM provide the potential for malicious authors to deliver scripts to run on a client computer via the Web. Browser authors minimize this risk using two restrictions. First, scripts run in a sandbox in which they can only perform Web-related actions, not general-purpose programming tasks like creating files. Second, scripts are constrained by the same-origin policy: scripts from one website do not have access to information such as usernames, passwords, or cookies sent to another site. Most JavaScript-related security bugs are breaches of either the same origin policy or the sandbox.
Some browsers include partial protection against reflected XSS attacks, in which the attacker provides a URL including malicious script. However, even users of those browsers are vulnerable to other XSS attacks, such as those where the malicious code is stored in a database. Only correct design of Web applications on the server-side can fully prevent XSS.
JavaScript provides an interface to a wide range of browser capabilities, some of which may have flaws such as buffer overflows. These flaws can allow attackers to write scripts that would run any code they wish on the user's system. This code is not by any means limited to another JavaScript application. For example, a buffer overrun exploit can allow an attacker to gain access to the operating system's API with superuser privileges.
The differences between the two languages are more prominent than their similarities. Java has static typing, while JavaScript's typing is dynamic. Java is loaded from compiled bytecode, while JavaScript is loaded as human-readable source code. Java's objects are class-based, while JavaScript's are prototype-based. Finally, Java did not support functional programming until Java 8, while JavaScript has done so from the beginning, being influenced by Scheme.
JSON is a data format derived from JavaScript; hence the name JavaScript Object Notation. It is a widely used format supported by many other programming languages.
Kotlin (/ˈkɒtlɪn/)[2] is a cross-platform, statically typed, general-purpose high-level programming language with type inference. Kotlin is designed to interoperate fully with Java, and the JVM version of Kotlin's standard library depends on the Java Class Library,
but type inference allows its syntax to be more concise. Kotlin mainly targets the JVM, but also compiles to JavaScript (e.g., for frontend web applications using React)[3] or native code via LLVM (e.g., for native iOS apps sharing business logic with Android apps).[4] Language development costs are borne by JetBrains, while the Kotlin Foundation protects the Kotlin trademark.[5]
On 7 May 2019, Google announced that the Kotlin programming language had become its preferred language for Android app developers.[6] Since the release of Android Studio 3.0 in October 2017, Kotlin has been included as an alternative to the standard Java compiler. The Android Kotlin compiler emits Java 8 bytecode by default (which runs in any later JVM), but allows targeting Java 9 up to 20, for optimizing,[7] or allows for more features; has bidirectional record class interoperability support for JVM, introduced in Java 16, considered stable as of Kotlin 1.5.
The name is derived from Kotlin Island, a Russian island in the Gulf of Finland, near Saint Petersburg. Andrey Breslav, Kotlin's former lead designer, mentioned that the team decided to name it after an island, in imitation of the Java programming language which shares a name with the  Indonesian island of Java.[10]
Kotlin 1.2 was released on November 28, 2017.[17] Sharing code between JVM and JavaScript platforms feature was newly added to this release (multiplatform programming is by now a beta feature[18] upgraded from "experimental"). A full-stack demo has been made with the new Kotlin/JS Gradle Plugin.[19][20]
Kotlin 1.3 was released on 29 October 2018, adding support for coroutines for use with asynchronous programming.[21]
On 7 May 2019, Google announced that the Kotlin programming language is now its preferred language for Android app developers.[6]
The influence of Scala in Kotlin can be seen in the extensive support for both object-oriented and functional programming[31] and in a number of specific features:
Kotlin 1.3 added support for contracts,[32] which are stable for the standard library declarations, but still experimental for user-defined declarations. Contracts are inspired by Eiffel's design by contract[33] programming paradigm.
In 2018, Kotlin was the fastest growing language on GitHub, with 2.6 times more developers compared to 2017.[53] It is the fourth most loved programming language according to the 2020 Stack Overflow Developer Survey.[54]
Lua (/ˈluːə/ LOO-ə; from Portuguese: lua [ˈlu(w)ɐ] meaning moon) is a lightweight, high-level, multi-paradigm programming language designed mainly for embedded use in applications.[3] Lua is cross-platform software, since the interpreter of compiled bytecode is written in ANSI C,[4] and Lua has a relatively simple C application programming interface (API) to embed it into applications.[5]
Lua originated in 1993 as a language for extending software applications to meet the increasing demand for customization at the time. It provided the basic facilities of most procedural programming languages, but more complicated or domain-specific features were not included; rather, it included mechanisms for extending the language, allowing programmers to implement such features. As Lua was intended to be a general embeddable extension language, the designers of Lua focused on improving its speed, portability, extensibility and ease-of-use in development.
From 1977 until 1992, Brazil had a policy of strong trade barriers (called a market reserve) for computer hardware and software, believing that Brazil could and should produce its own hardware and software. In that climate, Tecgraf's clients could not afford, either politically or financially, to buy customized software from abroad; under the market reserve, clients would have to go through a complex bureaucratic process to prove their needs couldn't be met by Brazilian companies. Those reasons led Tecgraf to implement the basic tools it needed from scratch.[6]
Lua's predecessors were the data-description/configuration languages Simple Object Language (SOL) and data-entry language (DEL).[7] They had been independently developed at Tecgraf in 1992–1993 to add some flexibility into two different projects (both were interactive graphical programs for engineering applications at Petrobras company). There was a lack of any flow-control structures in SOL and DEL, and Petrobras felt a growing need to add full programming power to them.
Lua 1.0 was designed in such a way that its object constructors, being then slightly different from the current light and flexible style, incorporated the data-description syntax of  SOL  (hence the name Lua: Sol meaning "Sun" in Portuguese, and Lua meaning "Moon"). Lua syntax for control structures was mostly borrowed from Modula (if, while, repeat/until), but also had taken influence from CLU (multiple assignments and multiple returns from function calls, as a simpler alternative to reference parameters or explicit pointers), C++ ("neat idea of allowing a local variable to be declared only where we need it"[6]), SNOBOL and AWK (associative arrays). In an article published in Dr. Dobb's Journal, Lua's creators also state that LISP and Scheme with their single, ubiquitous data-structure mechanism (the list) were a major influence on their decision to develop the table as the primary data structure of Lua.[8]
Versions of Lua prior to version 5.0 were released under a license similar to the BSD license. From version 5.0 onwards, Lua has been licensed under the MIT License. Both are permissive free software licences and are almost identical.
Lua is commonly described as a "multi-paradigm" language, providing a small set of general features that can be extended to fit different problem types. Lua does not contain explicit support for inheritance, but allows it to be implemented with metatables. Similarly, Lua allows programmers to implement namespaces, classes and other related features using its single table implementation; first-class functions allow the employment of many techniques from functional programming and full lexical scoping allows fine-grained information hiding to enforce the principle of least privilege.
In general, Lua strives to provide simple, flexible meta-features that can be extended as needed, rather than supply a feature-set specific to one programming paradigm. As a result, the base language is light; the full reference interpreter is only about 247 kB compiled[4] and easily adaptable to a broad range of applications.
As a dynamically typed language intended for use as an extension language or scripting language, Lua is compact enough to fit on a variety of host platforms. It supports only a small number of atomic data structures such as Boolean values, numbers (double-precision floating point and 64-bit integers by default) and strings. Typical data structures such as arrays, sets, lists and records can be represented using Lua's single native data structure, the table, which is essentially a heterogeneous associative array.
Tables are the most important data structures (and, by design, the only built-in composite data type) in Lua and are the foundation of all user-created types. They are associative arrays with addition of automatic numeric key and special syntax.
Tables are automatically assigned a numerical key, enabling them to be used as an array data type. The first automatic index is 1 rather than 0 as it is for many other programming languages (though an explicit index of 0 is allowed).
Extensible semantics is a key feature of Lua, and the metatable concept allows powerful customization of tables. The following example demonstrates an "infinite" table. For any n, fibs[n] will give the n-th Fibonacci number using dynamic programming and memoization.
Although Lua does not have a built-in concept of classes, object-oriented programming can be emulated using functions and tables. An object is formed by putting methods and fields in a table. Inheritance (both single and multiple) can be implemented with metatables, delegating nonexistent methods and fields to a parent object.
Besides standard library (core) modules it is possible to write extensions using the Lua API. Extension modules are shared objects which can be used to extend the functions of the interpreter by providing native facilities to Lua scripts. Lua scripts may load extension modules using require,[20] just like modules written in Lua itself, or with package.loadlib.[22] When a C library is loaded via require('foo') Lua will look for the function luaopen_foo and call it, which acts as any C function callable from Lua and generally returns a table filled with methods. A growing set of modules termed rocks are available through a package management system named LuaRocks,[23] in the spirit of CPAN, RubyGems and Python eggs. Prewritten Lua bindings exist for most popular programming languages, including other scripting languages.[24] For C++, there are a number of template-based approaches and some automatic binding generators.
In video game development, Lua is widely used as a scripting language, mainly due to its perceived easiness to embed, fast execution, and short learning curve.[25] Notable games which use Lua include Roblox,[26] Garry's Mod,  World of Warcraft, Payday 2,  Phantasy Star Online 2, Dota 2, Crysis,[27] and many others. Some games that do not natively support Lua programming or scripting, have this function added by mods, as ComputerCraft does for Minecraft. Also, Lua is used in non-video game software, such as Adobe Lightroom, Moho, iClone, Aerospike, and some system software in FreeBSD and NetBSD, and used as a template scripting language on MediaWiki using the Scribunto extension.[28]
In 2003, a poll conducted by GameDev.net showed Lua was the most popular scripting language for game programming.[29] On 12 January 2012, Lua was announced as a winner of the Front Line Award 2011 from the magazine Game Developer in the category Programming Tools.[30]
Many non-game applications also use Lua for extensibility, such as LuaTeX, an implementation of the TeX type-setting language, Redis, a key-value database, ScyllaDB, a wide-column store, Neovim, a text editor, Nginx, a web server, Wireshark, a network packet analyzer and Pure Data, a visual audio programming language (through the pdlua extension).
Through the Scribunto extension, Lua is available as a server-side scripting language in the MediaWiki software that runs Wikipedia and other wikis.[31] Among its uses are allowing the integration of data from Wikidata into articles,[32] and powering the automated taxobox system.
Oberon is a general-purpose programming language first published in 1987 by Niklaus Wirth and the latest member of the Wirthian family of ALGOL-like languages (Euler, ALGOL W, Pascal, Modula, and Modula-2).[1][2][3][4] Oberon was the result of a concentrated effort to increase the power of Modula-2, the direct successor of Pascal, and simultaneously to reduce its complexity. Its principal new feature is the concept of data type extension of record types.[5] It permits constructing new data types on the basis of existing ones and to relate them, deviating from the dogma of strict static typing of data. Type extension is Wirth's way of inheritance reflecting the viewpoint of the parent site. Oberon was developed as part of the implementation of an operating system, also named Oberon at ETH Zurich in Switzerland. The name was inspired both by the Voyager space probe's pictures of the moon of the planet Uranus, named Oberon, and because Oberon is famous as the king of the elves.[6]
Oberon is designed with a motto attributed to Albert Einstein in mind: "Make things as simple as possible, but not simpler." The principal guideline was to concentrate on features that are basic and essential and to omit ephemeral issues. Another factor was recognition of the growth of complexity in languages such as C++ and Ada. In contrast to these, Oberon emphasizes the use of the library concept to extend the language. Enumeration and subrange types, which were present in Modula-2, were omitted, and set types are limited to sets of integers. All imported items must be qualified by the name of the module where they are declared. Low-level facilities are highlighted by only allowing them to be used in a module which includes the identifier SYSTEM in its import list. Strict type checking, even across modules, and index checking at runtime, null pointer checking, and the safe type extension concept largely allow programming to rely on the language rules alone.
Oberon supports extension of record types for the construction of abstractions and heterogeneous structures. In contrast to the later dialects, Oberon-2 and Active Oberon, the original Oberon lacks a dispatch mechanism as a language feature but has it as a programming technique or design pattern. This gives great flexibility in OOP. In the Oberon operating system, two programming techniques are used together for the dispatch call: Method suite and Message handler.
In the Oberon operating system both of these techniques are used for dynamic dispatch. The first one is used for a known set of methods; the second is used for any new methods declared in the extension module. For example, if the extension module Rectangles were to implement a new Rotate() procedure, within the Figures module it could only be called via a message handler.
No-cost implementations of Oberon (the language) and Oberon (the operating system) can be found on the Internet (several are from ETHZ itself).
A few changes were made to the first released specification. For example, object-oriented programming (OOP) features were added, the FOR loop was reinstated. The result was Oberon-2. One release, named Native Oberon which includes an operating system, and can directly boot on IBM PC compatible class hardware. A .NET implementation of Oberon with some added minor .NET-related extensions was also developed at ETHZ. In 1993, an ETHZ university spin-off company brought a dialect of Oberon-2 to the market named Oberon-L. In 1997, it was renamed Component Pascal.
Oberon-2 compilers developed by ETH include versions for Microsoft Windows, Linux, Solaris, and classic Mac OS. Implementations from other sources exist for some other operating systems, including Atari TOS and AmigaOS.
Oberon-07 compilers have been developed for use with many different computer systems. Wirth's compiler targets a reduced instruction set computer (RISC) processor of his own design that was used to implement the 2013 version of the Project Oberon operating system on a Xilinx field-programmable gate array (FPGA) Spartan-3 board. Ports of the RISC processor to FPGA Spartan-6, Spartan-7, Artix-7 and a RISC emulator for Windows (compilable on Linux and macOS, and binaries available for Windows) also exist. OBNC compiles via C and can be used on any Portable Operating System Interface (POSIX) compatible operating system. The commercial Astrobe implementation targets STM ARM Cortex-M0, M3, M4, M7 and Raspberry Pi RP2040 and RP2350 microcontrollers. The Patchouli compiler produces 64-bit Windows binaries. Oberon-07M produces 32-bit Windows binaries and implements revision 2008 of the language. Akron's produces binaries for both Windows and Linux. OberonJS translates Oberon to JavaScript. There is online IDE for Oberon. oberonc is an implementation for the Java virtual machine.
Active Oberon is yet another variant of Oberon, which adds objects (with object-centered access protection and local activity control), system-guarded assertions, preemptive priority scheduling and a changed syntax for methods (named type-bound procedures in Oberon vocabulary). Objects may be active, which means that they may be threads or processes. Further, Active Oberon has a way to implement operators (including overloading), an advanced syntax for using arrays (see OberonX language extensions and Proceedings[14] of the 7th Joint Modular Languages Conference 2006 Oxford, UK), and knows about namespaces.[15] The operating system A2 (formerly Active Object System (AOS),[16] then Bluebottle), especially the kernel, synchronizes and coordinates different active objects.
ETHZ has released Active Oberon which supports active objects, and the operating systems based thereon (Active Object System (AOS), Bluebottle, A2), and environment (JDK, HTTP, FTP, etc.) for the language. As with many prior designs from ETHZ, versions of both are available for download on the Internet. As of 2003, supported central processing units (CPUs) include single and dual core x86, and StrongARM.
Pike is an interpreted, general-purpose, high-level, cross-platform, dynamic programming language, with a syntax similar to that of C. Unlike many other dynamic languages, Pike is both statically and dynamically typed, and requires explicit type definitions. It features a flexible type system that allows the rapid development and flexible code of dynamically typed languages, while still providing some of the benefits of a statically-typed language.
Pike features garbage collection, advanced data types, and first-class anonymous functions, with support for many programming paradigms, including object-oriented, functional and imperative programming. Pike is free software, released under the GPL, LGPL and MPL licenses.
In 1996, μLPC was renamed to Pike in order to provide a more commercially viable name. Although the name of the company has changed over the years, the company now known as Roxen Internet Software employed many Pike developers, and provided resources for Pike's development. Roxen is also the name of a web server developed by the company in Pike. In 2002, the programming environment laboratory at Linköping University took over maintenance of Pike from Roxen. Several Pike programmers have found their way to the Linköping office of Opera Software, where the language plays a central role in the server/gateway parts of the Opera Mini application.[2][3]
V, also known as vlang, is a statically typed, compiled programming language created by Alexander Medvednikov in early 2019.[4] It was inspired by the language Go, and other influences including Oberon, Swift, and Rust.[5][6][7] It is free and open-source software released under the MIT License, and currently in beta.[8]
According to various sources, the new language was created as a result of frustration with existing languages being used for personal projects.[11][12][13] The language was originally intended for personal use, but after it was mentioned publicly and gained interest, it was decided to make it public. V was initially created in order to develop a desktop messaging client known as Volt.[6] Upon public release, the compiler was written in V, and could compile itself.[4][12] Key design goals behind the creation of V were being easy to learn and use, higher readability, fast compilation, increased safety, efficient development, cross-platform usability, improved C interoperability, better error handling, modern features, and more maintainable software.[14][15][10][16]
Proprietary software is software that grants its creator, publisher, or other rightsholder or rightsholder partner a legal monopoly by modern copyright and intellectual property law to exclude the recipient from freely sharing the software or modifying it, and—in some cases, as is the case with some patent-encumbered and EULA-bound software—from making use of the software on their own, thereby restricting their freedoms.[1]
Proprietary software is a subset of non-free software, a term defined in contrast to free and open-source software; non-commercial licenses such as CC BY-NC are not deemed proprietary, but are non-free. Proprietary software may either be closed-source software or source-available software.[1][2]
Until the late 1960s, computers—especially large and expensive mainframe computers, machines in specially air-conditioned computer rooms—were usually leased to customers rather than sold.[3][4] Service and all software available were usually supplied by manufacturers without separate charge until 1969. Computer vendors usually provided the source code for installed software to customers.[citation needed] Customers who developed software often made it available to the public without charge.[5] Closed source means computer programs whose source code is not published except to licensees. It is available to be modified only by the organization that developed it and those licensed to use the software.
In 1969, IBM, which had antitrust lawsuits pending against it, led an industry change by starting to charge separately for mainframe software[6][7] and services, by unbundling hardware and software.[8]
Bill Gates' "Open Letter to Hobbyists" in 1976 decried computer hobbyists' rampant copyright infringement of software, particularly Microsoft's Altair BASIC interpreter, and asserted that their unauthorized use hindered his ability to produce quality software. But the legal status of software copyright, especially for object code, was not clear until the 1983 appeals court ruling in Apple Computer, Inc. v. Franklin Computer Corp.[9][10][11]
According to Brewster Kahle the legal characteristic of software changed also due to the U.S. Copyright Act of 1976.[12]
Starting in February 1983 IBM adopted an "object-code-only" model for a growing list of their software and stopped shipping much of the source code,[13][14] even to licensees.
In 1983, binary software became copyrightable in the United States as well by the Apple vs. Franklin law decision,[15] before which only source code was copyrightable.[16] Additionally, the growing availability of millions of computers based on the same microprocessor architecture created for the first time an unfragmented and big enough market for binary distributed software.[16]
The tendency to license proprietary software, rather than sell it, dates from the time period before the existence, then the scope of software copyright protection was clear. These licenses have continued in use after software copyright was recognized in the courts, and are considered to grant the company extra protection compared to copyright law.[17] According to United States federal law, a company can restrict the parties to which it sells but it cannot prevent a buyer from reselling the product. Software licensing agreements usually prohibit resale, enabling the company to maximize revenue.[18]
Software distributions considered as proprietary may in fact incorporate a "mixed source" model including both free and non-free software in the same distribution.[26] Most if not all so-called proprietary UNIX distributions are mixed source software, bundling open-source components like BIND, Sendmail, X Window System, DHCP, and others along with a purely proprietary kernel and system utilities.[27][28]
Some free software packages are also simultaneously available under proprietary terms. Examples include MySQL, Sendmail and ssh. The original copyright holders for a work of free software, even copyleft free software, can use dual-licensing to allow themselves or others to redistribute proprietary versions. Non-copyleft free software (i.e. software distributed under a permissive free software license or released to the public domain) allows anyone to make proprietary redistributions.[29][30] Free software that depends on proprietary software is considered "trapped" by the Free Software Foundation. This includes software written only for Microsoft Windows,[31] or software that could only run on Java, before it became free software.[32]
Most of the software is covered by copyright which, along with contract law, patents, and trade secrets, provides legal basis for its owner to establish exclusive rights.[33]
A software vendor delineates the specific terms of use in an end-user license agreement (EULA). The user may agree to this contract in writing, interactively on screen (clickwrap), or by opening the box containing the software (shrink wrap licensing). License agreements are usually not negotiable.[34] Software patents grant exclusive rights to algorithms, software features, or other patentable subject matter, with coverage varying by jurisdiction. Vendors sometimes grant patent rights to the user in the license agreement.[35] The source code for a piece of proprietary software is routinely handled as a trade secret.[36] Software can be made available with fewer restrictions on licensing or source-code access; software that satisfies certain conditions of freedom and openness is known as "free" or "open-source."[37]
Since license agreements do not override applicable copyright law or contract law, provisions in conflict with applicable law are not enforceable.[38] Some software is specifically licensed and not sold, in order to avoid limitations of copyright such as the first-sale doctrine.[39]
The owner of proprietary software exercises certain exclusive rights over the software. The owner can restrict the use, inspection of source code, modification of source code, and redistribution.
Vendors typically limit the number of computers on which software can be used, and prohibit the user from installing the software on extra computers.[citation needed] Restricted use is sometimes enforced through a technical measure, such as product activation, a product key or serial number, a hardware key, or copy protection.
Vendors typically distribute proprietary software in compiled form, usually the machine language understood by the computer's central processing unit. They typically retain the source code, or human-readable version of the software, often written in a higher level programming language.[43] This scheme is often referred to as closed source.[44]
While most proprietary software is distributed without the source code, some vendors distribute the source code or otherwise make it available to customers. For example, users who have purchased a license for the Internet forum software vBulletin can modify the source for their own site but cannot redistribute it. This is true for many web applications, which must be in source code form when being run by a web server. The source code is covered by a non-disclosure agreement or a license that allows, for example, study and modification, but not redistribution.[45] The text-based email client Pine and certain implementations of Secure Shell are distributed with proprietary licenses that make the source code available.[citation needed]Some licenses for proprietary software allow distributing changes to the source code, but only to others licensed for the product, and some[46] of those modifications are eventually picked up by the vendor.
Some governments fear that proprietary software may include defects or malicious features which would compromise sensitive information. In 2003 Microsoft established a Government Security Program (GSP) to allow governments to view source code and Microsoft security documentation, of which the Chinese government was an early participant.[47][48] The program is part of Microsoft's broader Shared Source Initiative which provides source code access for some products. The Reference Source License (Ms-RSL) and Limited Public License (Ms-LPL) are proprietary software licenses where the source code is made available.
Governments have also been accused of adding such malware to software themselves. According to documents released by Edward Snowden, the NSA has used covert partnerships with software companies to make commercial encryption software exploitable to eavesdropping, or to insert backdoors.[49][50]
Software vendors sometimes use obfuscated code to impede users who would reverse engineer the software.[51] This is particularly common with certain programming languages.[citation needed] For example, the bytecode for programs written in Java can be easily decompiled to somewhat usable code,[citation needed] and the source code for programs written in scripting languages such as PHP or JavaScript is available at run time.[52]
Proprietary software vendors can prohibit the users from sharing the software with others. Another unique license is required for another party to use the software.
In the case of proprietary software with source code available, the vendor may also prohibit customers from distributing their modifications to the source code.
Shareware is closed-source software whose owner encourages redistribution at no cost, but which the user sometimes must pay to use after a trial period. The fee usually allows use by a single user or computer. In some cases, software features are restricted during or after the trial period, a practice sometimes called crippleware.
Proprietary software often[citation needed] stores some of its data in file formats that are incompatible with other software, and may also communicate using protocols which are incompatible. Such formats and protocols may be restricted as trade secrets or subject to patents.[citation needed]
A proprietary application programming interface (API) is a software library interface "specific to one device or, more likely to a number of devices within a particular manufacturer's product range."[53] The motivation for using a proprietary API can be vendor lock-in or because standard APIs do not support the device's functionality.[53]
Any dependency on the future versions and upgrades for a proprietary software package can create vendor lock-in, entrenching a monopoly position.[56]
Proprietary software may also have licensing terms that limit the usage of that software to a specific set of hardware. Apple has such a licensing model for macOS, an operating system which is limited to Apple hardware, both by licensing and various design decisions. This licensing model has been affirmed by the United States Court of Appeals for the Ninth Circuit.[57]
Proprietary software which is no longer marketed, supported or sold by its owner is called abandonware, the digital form of orphaned works. If the proprietor of a software package should cease to exist, or decide to cease or limit production or support for a proprietary software package, recipients and users of the package may have no recourse if problems are found with the software. Proprietors can fail to improve and support software because of business problems.[58] Support for older or existing versions of a software package may be ended to force users to upgrade and pay for newer versions[59](planned obsolescence). Sometimes another vendor or a software's community themselves can provide support for the software, or the users can migrate to either competing systems with longer support life cycles or to FOSS-based systems.[60]
Some proprietary software is released by their owner at end-of-life as open-source or source available software, often to prevent the software from becoming unsupported and unavailable abandonware.[61][62][63] 3D Realms and id Software are famous for the practice of releasing closed source software into the open source.[further explanation needed] Some of those kinds are free-of-charge downloads (freeware), some are still commercially sold (e.g. Arx Fatalis).[further explanation needed] More examples of formerly closed-source software in the List of commercial software with available source code and List of commercial video games with available source code.
Proprietary software is not synonymous with commercial software,[64][65] although the two terms are sometimes used synonymously in articles about free software.[66][67] Proprietary software can be distributed at no cost or for a fee, and free software can be distributed at no cost or for a fee.[68] The difference is that whether proprietary software can be distributed, and what the fee would be, is at the proprietor's discretion. With free software, anyone who has a copy can decide whether, and how much, to charge for a copy or related services.[69]
Proponents of commercial proprietary software argue that requiring users to pay for software as a product increases funding or time available for the research and development of software. For example, Microsoft says that per-copy fees maximize the profitability of software development.[70]
Proprietary software generally creates greater commercial activity over free software, especially in regard to market revenues.[71] Proprietary software is often sold with a license that gives the end user right to use the software.
Free and open-source software (FOSS) is software available under a license that grants users the right to use, modify, and distribute the software – modified or not – to everyone free of charge. FOSS is an inclusive umbrella term encompassing free software and open-source software.[a][1] The rights guaranteed by FOSS originate from the "Four Essential Freedoms" of The Free Software Definition and the criteria of The Open Source Definition.[4][6] All FOSS must have publicly available source code, but not all source-available software is FOSS. FOSS is the opposite of proprietary software, which is licensed restrictively or has undisclosed source code.[4]
The historical precursor to FOSS was the hobbyist and academic public domain software ecosystem of the 1960s to 1980s. Free and open-source operating systems such as Linux distributions and descendants of BSD are widely used, powering millions of servers, desktops, smartphones, and other devices.[9][10] Free-software licenses and open-source licenses have been adopted by many software packages. Reasons for using FOSS include decreased software costs, increased security against malware, stability, privacy, opportunities for educational usage, and giving users more control over their own hardware.
The free software movement and the open-source software movement are online social movements behind widespread production, adoption and promotion of FOSS, with the former preferring to use the equivalent term free/libre and open-source software (FLOSS). FOSS is supported by a loosely associated movement of multiple organizations, foundations, communities and individuals who share basic philosophical perspectives and collaborate practically, but may diverge in detail questions.
"Free and open-source software" (FOSS) is an umbrella term for software that is considered free software and/or open-source software.[1] The precise definition of the terms "free software" and "open-source software" applies them to any software distributed under terms that allow users to use, modify, and redistribute said software in any manner they see fit, without requiring that they pay the author(s) of the software a royalty or fee for engaging in the listed activities.[11]
Although there is an almost complete overlap between free-software licenses and open-source-software licenses, there is a strong philosophical disagreement between the advocates of these two positions. The terminology of FOSS was created to be a neutral on these philosophical disagreements between the Free Software Foundation (FSF) and Open Source Initiative (OSI) and have a single unified term that could refer to both concepts, although Richard Stallman argues that it fails to be neutral unlike the similar term; "Free/Libre and Open Source Software" (FLOSS).[12]
Richard Stallman's The Free Software Definition, adopted by the FSF, defines free software as a matter of liberty, not price,[13][14] and that which upholds the Four Essential Freedoms. The earliest known publication of this definition of his free software definition was in the February 1986 edition[15] of the FSF's now-discontinued GNU's Bulletin publication. The canonical source for the document is in the philosophy section of the GNU Project website. As of August 2017[update], it is published in 40 languages.[16]
To meet the definition of "free software", the FSF requires the software's licensing respect the civil liberties / human rights of what the FSF calls the software user's "Four Essential Freedoms".[17]
The Open Source Definition is used by the Open Source Initiative (OSI) to determine whether a software license qualifies for the organization's insignia for open-source software. The definition was based on the Debian Free Software Guidelines, written and adapted primarily by Bruce Perens.[18][19] Perens did not base his writing on the Four Essential Freedoms of free software from the Free Software Foundation, which were only later available on the web.[20] Perens subsequently stated that he felt Eric Raymond's promotion of open-source unfairly overshadowed the Free Software Foundation's efforts and reaffirmed his support for free software.[21] In the following 2000s, he spoke about open source again.[22][23]
From the 1950s and on through the 1980s, it was common for computer users to have the source code for all programs they used, and the permission and ability to modify it for their own use. Software, including source code, was commonly shared by individuals who used computers, often as public-domain software[24] (FOSS is not the same as public domain software, as public domain software is not limited by copyrights[25]). Most companies had a business model based on hardware sales, and provided or bundled software with hardware, free of charge.[26]
By the late 1960s, the prevailing business model around software was changing. A growing and evolving software industry was competing with the hardware manufacturer's bundled software products; rather than funding software development from hardware revenue, these new companies were selling software directly. Leased machines required software support while providing no revenue for software, and some customers who were able to better meet their own needs did not want the costs of software bundled with hardware product costs. In United States vs. IBM, filed January 17, 1969, the government charged that bundled software was anticompetitive.[27] While some software was still being provided without monetary cost and license restriction, there was a growing amount of software that was only at a monetary cost with restricted licensing. In the 1970s and early 1980s, some parts of the software industry began using technical measures (such as distributing only binary copies of computer programs) to prevent computer users from being able to use reverse engineering techniques to study and customize software they had paid for. In 1980, the copyright law was extended to computer programs in the United States[28]—previously, computer programs could be considered ideas, procedures, methods, systems, and processes, which are not copyrightable.[29][30]
Early on, closed-source software was uncommon until the mid-1970s to the 1980s, when IBM implemented in 1983 an "object code only" policy, no longer distributing source code.[31][32][33]
In 1983, Richard Stallman, longtime member of the hacker community at the MIT Artificial Intelligence Laboratory, announced the GNU project, saying that he had become frustrated with the effects of the change in culture of the computer industry and its users.[34] Software development for the GNU operating system began in January 1984, and the Free Software Foundation (FSF) was founded in October 1985. An article outlining the project and its goals was published in March 1985 titled the GNU Manifesto. The manifesto included significant explanation of the GNU philosophy, The Free Software Definition and "copyleft" ideas. The FSF takes the position that the fundamental issue Free software addresses is an ethical one—to ensure software users can exercise what it calls "The Four Essential Freedoms".[17]
The Linux kernel, created by Linus Torvalds, was released as freely modifiable source code in 1991. Initially, Linux was not released under either a Free software or an Open-source software license. However, with version 0.12 in February 1992, he relicensed the project under the GNU General Public License.[35]
FreeBSD and NetBSD (both derived from 386BSD) were released as Free software when the USL v. BSDi lawsuit was settled out of court in 1993. OpenBSD forked from NetBSD in 1995. Also in 1995, The Apache HTTP Server, commonly referred to as Apache, was released under the Apache License 1.0.
In 1997, Eric Raymond published The Cathedral and the Bazaar, a reflective analysis of the hacker community and Free software principles. The paper received significant attention in early 1998, and was one factor in motivating Netscape Communications Corporation to release their popular Netscape Communicator Internet suite as Free software. This code is today better known as Mozilla Firefox and Thunderbird.
Netscape's act prompted Raymond and others to look into how to bring the FSF's Free software ideas and perceived benefits to the commercial software industry. They concluded that FSF's social activism was not appealing to companies like Netscape, and looked for a way to rebrand the Free software movement to emphasize the business potential of sharing and collaborating on software source code. The new name they chose was "Open-source", and quickly Bruce Perens, publisher Tim O'Reilly, Linus Torvalds, and others signed on to the rebranding. The Open Source Initiative was founded in February 1998 to encourage the use of the new term and evangelize open-source principles.[36]
While the Open Source Initiative sought to encourage the use of the new term and evangelize the principles it adhered to, commercial software vendors found themselves increasingly threatened by the concept of freely distributed software and universal access to an application's source code. A Microsoft executive publicly stated in 2001 that "Open-source is an intellectual property destroyer. I can't imagine something that could be worse than this for the software business and the intellectual-property business."[37] Companies have indeed faced copyright infringement issues when embracing FOSS.[38] For many years FOSS played a niche role outside of the mainstream of private software development. However the success of FOSS Operating Systems such as Linux, BSD and the companies based on FOSS such as Red Hat, has changed the software industry's attitude and there has been a dramatic shift in the corporate philosophy concerning its development.[39]
Users of FOSS benefit from the Four Essential Freedoms to make unrestricted use of, and to study, copy, modify, and redistribute such software with or without modification. If they would like to change the functionality of software they can bring about changes to the code and, if they wish, distribute such modified versions of the software or often − depending on the software's decision making model and its other users − even push or request such changes to be made via updates to the original software.[40][41][42][43][44]
Manufacturers of proprietary, closed-source software are sometimes pressured to building in backdoors or other covert, undesired features into their software.[45][46][47][48]  Instead of having to trust software vendors, users of FOSS can inspect and verify the source code themselves and can put trust on a community of volunteers and users.[44] As proprietary code is typically hidden from public view, only the vendors themselves and hackers may be aware of any vulnerabilities in them[44] while FOSS involves as many people as possible for exposing bugs quickly.[49][50]
FOSS is often free of charge although donations are often encouraged. This also allows users to better test and compare software.[44]
FOSS allows for better collaboration among various parties and individuals with the goal of developing the most efficient software for its users or use-cases while proprietary software is typically meant to generate profits. Furthermore, in many cases more organizations and individuals contribute to such projects than to proprietary software.[44] It has been shown that technical superiority is typically the primary reason why companies choose open source software.[44]
Sometimes, FOSS is not compatible with proprietary hardware or specific software. This is often due to manufacturers obstructing FOSS such as by not disclosing the interfaces or other specifications needed for members of the FOSS movement to write drivers for their hardware – for instance as they wish customers to run only their own proprietary software or as they might benefit from partnerships.[52][53][54][55][56][57][58]
While FOSS can be superior to proprietary equivalents in terms of software features and stability, in many cases it has more unfixed bugs and missing features when compared to similar commercial software.[59][additional citation(s) needed] This varies per case, and usually depends on the level of interest in a particular project. However, unlike close-sourced software, improvements can be made by anyone who has the motivation, time and skill to do so.[51][additional citation(s) needed]
There is often less certainty of FOSS projects gaining the required resources and participation for continued development than commercial software backed by companies.[61][additional citation(s) needed] However, companies also often abolish projects for being unprofitable, yet large companies may rely on, and hence co-develop, open source software.[50] On the other hand, if the vendor of proprietary software ceases development, there are no alternatives; whereas with FOSS, any user who needs it still has the right, and the source-code, to continue to develop it themself, or pay a 3rd party to do so.
As the FOSS operating system distributions of Linux has a lower market share of end users there are also fewer applications available.[62][63]
"We migrated key functions from Windows to Linux because we needed an operating system that was stable and reliable -- one that would give us in-house control. So if we needed to patch, adjust, or adapt, we could."
In 2017, the European Commission stated that "EU institutions should become open source software users themselves, even more than they already are" and listed open source software as one of the nine key drivers of innovation, together with big data, mobility, cloud computing and the internet of things.[95]
In 2020, the European Commission adopted its Open Source Strategy 2020-2023,[96] including encouraging sharing and reuse of software and publishing Commission's source code as key objectives. Among concrete actions there is also to set up an Open Source Programme Office in 2020[97] and in 2022 it launched its own FOSS repository https://code.europa.eu/.[98]
In 2021, the Commission Decision on the open source licensing and reuse of Commission software (2021/C 495 I/01)[99] was adopted, under which, as a general principle, the European Commission may release software under EUPL or another FOSS license, if more appropriate. There are exceptions though.
While copyright is the primary legal mechanism that FOSS authors use to ensure license compliance for their software, other mechanisms such as legislation, patents, and trademarks have implications as well. In response to legal issues with patents and the Digital Millennium Copyright Act (DMCA), the Free Software Foundation released version 3 of its GNU General Public License (GNU GPLv3) in 2007 that explicitly addressed the DMCA and patent rights.
Apple, a user of GCC and a heavy user of both DRM and patents, switched the compiler in its Xcode IDE from GCC to Clang, which is another FOSS compiler[109] but is under a permissive license.[110] LWN speculated that Apple was motivated partly by a desire to avoid GPLv3.[109] The Samba project also switched to GPLv3, so Apple replaced Samba in their software suite by a closed-source, proprietary software alternative.[111]
Leemhuis criticizes the prioritization of skilled developers who − instead of fixing issues in already popular open-source applications and desktop environments − create new, mostly redundant software to gain fame and fortune.[112]
Mergers have affected major open-source software. Sun Microsystems (Sun) acquired MySQL AB, owner of the popular open-source MySQL database, in 2008.[113]
Oracle in turn purchased Sun in January 2010, acquiring their copyrights, patents, and trademarks. Thus, Oracle became the owner of both the most popular proprietary database and the most popular open-source database. Oracle's attempts to commercialize the open-source MySQL database have raised concerns in the FOSS community.[114] Partly in response to uncertainty about the future of MySQL, the FOSS community forked the project into new database systems outside of Oracle's control. These include MariaDB, Percona, and Drizzle.[115] All of these have distinct names; they are distinct projects and cannot use the trademarked name MySQL.[116]
By defying ownership regulations in the construction and use of information—a key area of contemporary growth—the Free/Open Source Software (FOSS) movement counters neoliberalism and privatization in general.[119][120]
According to Yochai Benkler, Jack N. and Lillian R. Berkman Professor for Entrepreneurial Legal Studies at Harvard Law School, free software is the most visible part of a new economy of commons-based peer production of information, knowledge, and culture. As examples, he cites a variety of FOSS projects, including both free software and open-source.[121]
PLEX (Programming Language for EXchanges) is a special-purpose, concurrent, real-time programming language.  The proprietary PLEX language is closely tied to the architecture of Ericsson's AXE telephone exchanges which it was designed to control. PLEX was developed by Göran Hemdahl at Ericsson in the 1970s,[1]  and it has been continuously evolving since then.[2] PLEX was described in 2008 as "a cross between Fortran and a macro assembler."[3]
In programming languages, a closure, also lexical closure or function closure, is a technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function[a] together with an environment.[1] The environment is a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created.[b] Unlike a plain function, a closure allows the function to access those captured variables through the closure's copies of their values or references, even when the function is invoked outside their scope.
The concept of closures was developed in the 1960s for the mechanical evaluation of expressions in the λ-calculus and was first fully implemented in 1970 as a language feature in the PAL programming language to support lexically scoped first-class functions.[2]
Sussman and Abelson also use the term closure in the 1980s with a second, unrelated meaning: the property of an operator that adds data to a data structure to also be able to add nested data structures. This use of the term comes from mathematics use, rather than the prior use in computer science. The authors consider this overlap in terminology to be "unfortunate."[7]
The use of closures is associated with languages where functions are first-class objects, in which functions can be returned as results from higher-order functions, or passed as arguments to other function calls; if functions with free variables are first-class, then returning one creates a closure. This includes functional programming languages such as Lisp and ML, and many modern, multi-paradigm languages, such as Julia, Python, and Rust. Closures are also often used with callbacks, particularly for event handlers, such as in JavaScript, where they are used for interactions with a dynamic web page.
A closure can be used to associate a function with a set of "private" variables, which persist over several invocations of the function. The scope of the variable encompasses only the closed-over function, so it cannot be accessed from other program code. These are analogous to private variables in object-oriented programming, and in fact closures are similar to stateful function objects (or functors) with a single call-operator method.
Note: Some speakers call any data structure that binds a lexical environment a closure, but the term usually refers specifically to functions.
Closures are typically implemented with a special data structure that contains a pointer to the function code, plus a representation of the function's lexical environment (i.e., the set of available variables) at the time when the closure was created. The referencing environment binds the non-local names to the corresponding variables in the lexical environment at the time the closure is created, additionally extending their lifetime to at least as long as the lifetime of the closure. When the closure is entered at a later time, possibly with a different lexical environment, the function is executed with its non-local variables referring to the ones captured by the closure, not the current environment.
This explains why, typically, languages that natively support closures also use garbage collection. The alternatives are manual memory management of non-local variables (explicitly allocating on the heap and freeing when done), or, if using stack allocation, for the language to accept that certain use cases will lead to undefined behaviour, due to dangling pointers to freed automatic variables, as in lambda expressions in C++11[10] or nested functions in GNU C.[11] The funarg problem (or "functional argument" problem) describes the difficulty of implementing functions as first class objects in a stack-based programming language such as C or C++. Similarly in D version 1, it is assumed that the programmer knows what to do with delegates and automatic local variables, as their references will be invalid after return from its definition scope (automatic local variables are on the stack) – this still permits many useful functional patterns, but for complex cases needs explicit heap allocation for variables. D version 2 solved this by detecting which variables must be stored on the heap, and performs automatic allocation. Because D uses garbage collection, in both versions, there is no need to track usage of variables as they are passed.
In ML, local variables are lexically scoped, and hence define a stack-like model, but since they are bound to values and not to objects, an implementation is free to copy these values into the closure's data structure in a way that is invisible to the programmer.
Scheme, which has an ALGOL-like lexical scope system with dynamic variables and garbage collection, lacks a stack programming model and does not suffer from the limitations of stack-based languages. Closures are expressed naturally in Scheme. The lambda form encloses the code, and the free variables of its environment persist within the program as long as they can possibly be accessed, and so they can be used as freely as any other Scheme expression.[citation needed]
Closures are closely related to Actors in the Actor model of concurrent computation where the values in the function's lexical environment are called acquaintances. An important issue for closures in concurrent programming languages is whether the variables in a closure can be updated and, if so, how these updates can be synchronized. Actors provide one solution.[12]
Callbacks are used extensively in graphical user interface (GUI) widget toolkits to implement event-driven programming by associating general functions of graphical widgets (menus, buttons, check boxes, sliders, spinners, etc.) with application-specific functions implementing the specific desired behavior for the application.
C++ enables defining function objects by overloading operator(). These objects behave somewhat like functions in a functional programming language. They may be created at runtime and may contain state, but they do not implicitly capture local variables as closures do. As of the 2011 revision, the C++ language also supports closures, which are a type of function object constructed automatically from a special language construct called lambda-expression. A C++ closure may capture its context either by storing copies of the accessed variables as members of the closure object or by reference. In the latter case, if the closure object escapes the scope of a referenced object, invoking its operator() causes undefined behavior since C++ closures do not extend the lifetime of their context.
Defensive programming is a form of defensive design intended to develop programs that are capable of detecting potential security abnormalities and make predetermined responses.[1] It ensures the continuing function of a piece of software under unforeseen circumstances. Defensive programming practices are often used where high availability, safety, or security is needed.
Overly defensive programming, however, may safeguard against errors that will never be encountered, thus incurring run-time and maintenance costs.
Secure programming is the subset of defensive programming concerned with computer security. Security is the concern, not necessarily safety or availability (the software may be allowed to fail in certain ways). As with all kinds of defensive programming, avoiding bugs is a primary objective; however, the motivation is not as much to reduce the likelihood of failure in normal operation (as if safety were the concern), but to reduce the attack surface – the programmer must assume that the software might be misused actively to reveal bugs, and that bugs could be exploited maliciously.
Offensive programming is a category of defensive programming, with the added emphasis that certain errors should not be handled defensively. In this practice, only errors from outside the program's control are to be handled (such as user input); the software itself, as well as data from within the program's line of defense, are to be trusted in this methodology.
occam is a programming language which is concurrent and builds on the communicating sequential processes (CSP) process algebra,[1] and shares many of its features. It is named after philosopher William of Ockham after whom Occam's razor is named.
occam is an imperative procedural language (such as Pascal). It was developed by David May and others at Inmos (trademark INMOS), advised by Tony Hoare, as the native programming language for their transputer microprocessors, but implementations for other platforms are available. The most widely known version is occam 2; its programming manual was written by Steven Ericsson-Zenith and others at Inmos.
SEQ introduces a list of expressions that are evaluated sequentially. This is not implicit as it is in most other programming languages. Example:
With this revision, occam became a language able to express useful programs, whereas occam 1 was more suited to examining algorithms and exploring the new language (however, the occam 1 compiler was written in occam 1,[4] so there is an existence proof that reasonably sized, useful programs could be written in occam 1, despite its limits).
In computer science, dynamic software updating (DSU) is a field of research pertaining to upgrading programs while they are running. DSU is not currently widely used in industry. However, researchers have developed a wide variety of systems and techniques for implementing DSU. These systems are commonly tested on real-world programs.
Current operating systems and programming languages are typically not designed with DSU in mind. As such, DSU implementations commonly either utilize existing tools, or implement specialty compilers. These compilers preserve the semantics of the original program, but instrument either the source code or object code to produce a dynamically updateable program. Researchers compare DSU-capable variants of programs to the original program to assess safety and performance overhead.
Any running program can be thought of a tuple 



(
δ
,
P
)


{\displaystyle (\delta ,P)}

, where 



δ


{\displaystyle \delta }

 is the current program state and 



P


{\displaystyle P}

 is the current program code. Dynamic software updating systems transform a running program 



(
δ
,
P
)


{\displaystyle (\delta ,P)}

 to a new version 



(

δ
′

,

P
′

)


{\displaystyle (\delta ',P')}

. In order to do this, the state must be transformed into the representation 




P
′



{\displaystyle P'}

 expects. This requires a state transformer function. Thus, DSU transforms a program 



(
δ
,
P
)


{\displaystyle (\delta ,P)}

 to 



(
S
(
δ
)
,

P
′

)


{\displaystyle (S(\delta ),P')}

. An update is considered valid if and only if the running program 



(
S
(
δ
)
,

P
′

)


{\displaystyle (S(\delta ),P')}

 can be reduced to a point tuple 



(
δ
,

P
′

)


{\displaystyle (\delta ,P')}

 that is reachable from the starting point of the new version of the program, 



(

δ

i
n
i
t


,

P
′

)


{\displaystyle (\delta _{init},P')}

.[1]
Updating systems differ in the types of program changes that they support. For example, Ksplice only supports code changes in functions, and does not support changes to state representation. This is because Ksplice primarily targets security changes, rather than general updates. In contrast, Ekiden can update a program to any other program capable of being executed, even one written in a different programming language. Systems designers can extract valuable performance or safety assurances by limiting the scope of updates. For example, any update safety check limits the scope of updates to updates which pass that safety check. The mechanism used to transform code and state influences what kinds of updates a system will support.
The problem space addressed by dynamic updating can be thought of as an intersection of several others. Examples include checkpointing, dynamic linking, and persistence. As an example, a database that must be backward-compatible with previous versions of its on-disk file format, must accomplish the same type of state transformation expected of a dynamic updating system. Likewise, a program that has a plugin architecture, must be able to load and execute new code at runtime.
The earliest precursor to dynamic software updating is redundant systems. In a redundant environment, spare systems exist ready to take control of active computations in the event of a failure of the main system. These systems contain a main machine and a hot spare. The hot spare would be periodically seeded with a checkpoint of the primary system. In the event of a failure, the hot spare would take over, and the main machine would become the new hot spare. This pattern can be generalized to updating. In the event of an update, the hot spare would activate, the main system would update, and then the updated system would resume control.
The earliest true Dynamic Software Updating system is DYMOS (Dynamic Modification System).[4] Presented in 1983 in the PhD dissertation of Insup Lee, DYMOS was a fully integrated system that had access to an interactive user interface, a compiler and runtime for a Modula variant, and source code. This enabled DYMOS to type-check updates against the existing program.
If the target program is implemented in a virtual machine language, the VM can use existing infrastructure to load new code, since modern virtual machines support runtime loading for other use cases besides DSU (mainly debugging). The HotSpot JVM supports runtime code loading, and DSU systems targeting Java (programming language) can utilize this feature.
Ksplice is a DSU system that targets only the Linux kernel, making itself one of the specialized DSU systems that support an operating system kernel as the target program.  Ksplice uses source-level diffs to determine changes between current and updated versions of the Linux kernel, and then uses binary rewriting to insert the changes into the running kernel.[10] Ksplice was maintained by a commercial venture founded by its original authors, Ksplice Inc., which was acquired by Oracle Corporation in July 2011.[11] Ksplice is used on a commercial basis and exclusively in the Oracle Linux distribution.[12]
Erlang supports Dynamic Software Updating, though this is commonly referred to as "hot code loading". Erlang requires no safety guarantees on updates, but Erlang culture suggests that developers write in a defensive style that will gracefully handle type errors generated by updating.[citation needed]
Microsoft is utilizing internal patching technology for Microsoft Visual C++ that supports patching individual C++ functions while maintaining functional correctness of patches. Currently known applications is SQL Server in Azure SQL Database.[23]
Lisp Flavored Erlang (LFE) is a functional, concurrent, garbage collected, general-purpose programming language and Lisp dialect built on Core Erlang and the Erlang virtual machine (BEAM). LFE builds on Erlang to provide a Lisp syntax for writing distributed, fault-tolerant, soft real-time, non-stop applications. LFE also extends Erlang to support metaprogramming with Lisp macros and an improved developer experience with a feature-rich read–eval–print loop (REPL).[1] LFE is actively supported on all recent releases of Erlang; the oldest version of Erlang supported is R14.
Robert Virding has stated that there were several reasons why he started the LFE programming language:[2]
Like Lisp, LFE is an expression-oriented language. Unlike non-homoiconic programming languages, Lisps make no or little syntactic distinction between expressions and statements: all code and data are written as expressions. LFE brought homoiconicity to the Erlang VM.
Gleam is a general-purpose, concurrent, functional high-level programming language that compiles to Erlang or JavaScript source code.[2][7][8]
Gleam's toolchain is implemented in the Rust programming language.[17] The toolchain is a single native binary executable which contains the compiler, build tool, package manager, source code formatter, and language server. A WebAssembly binary containing the Gleam compiler is also available, enabling Gleam code to be compiled within a web browser.
In computer science, purely functional programming usually designates a programming paradigm—a style of building the structure and elements of computer programs—that treats all computation as the evaluation of mathematical functions.
Purely functional programming consists of ensuring that functions, inside the functional paradigm, will only depend on their arguments, regardless of any global or local state. A pure functional subroutine only has visibility of changes of state represented by state variables included in its scope.
The exact difference between pure and impure functional programming is a matter of controversy. Sabry's proposed definition of purity is that all common evaluation strategies (call-by-name, call-by-value, and call-by-need) produce the same result, ignoring strategies that error or diverge.[1]
A program is usually said to be functional when it uses some concepts of functional programming, such as first-class functions and higher-order functions.[2] However, a first-class function need not be purely functional, as it may use techniques from the imperative paradigm, such as arrays or input/output methods that use mutable cells, which update their state as side effects. In fact, the earliest programming languages cited as being functional, IPL and Lisp,[3][4] are both "impure" functional languages by Sabry's definition.
In a purely functional language, the only dependencies between computations are data dependencies, and computations are deterministic. Therefore, to program in parallel, the programmer need only specify the pieces that should be computed in parallel, and the runtime can handle all other details such as distributing tasks to processors, managing synchronization and communication, and collecting garbage in parallel. This style of programming avoids common issues such as race conditions and deadlocks, but has less control than an imperative language.[5]
Purely functional data structures are persistent. Persistency is required for functional programming; without it, the same computation could return different results. Functional programming  may use persistent non-purely functional data structures, while those data structures may not be used in purely functional programs.
Purely functional data structures are often represented in a different way than their imperative counterparts.[6] For example,  array  with constant-time access and update is a basic component of most imperative languages and many imperative data-structures, such as hash table and binary heap,  are based on arrays. Arrays can be replaced by map or random access list, which admits purely functional implementation, but the  access and update time is logarithmic. Therefore, purely functional data structures can be used in languages which are non-functional, but they may not be the most efficient tool available, especially if persistency is not required.
A purely functional language is a language which only admits purely functional programming. Purely functional programs can however be written in languages which are not purely functional.
The J programming language, developed in the early 1990s by Kenneth E. Iverson and Roger Hui,[5][6] is an array programming language based primarily on APL (also by Iverson).
J is a very terse array programming language, and is most suited to mathematical and statistical programming, especially when performing operations on matrices. It has also been used in extreme programming[8] and network performance analysis.[9]
Like John Backus's languages FP and FL, J supports function-level programming via its tacit programming features.
Unlike most languages that support object-oriented programming, J's flexible hierarchical namespace scheme (where every name exists in a specific locale) can be effectively used as a framework for both class-based and prototype-based object-oriented programming.
Since March 2011, J is free and open-source software under the GNU General Public License version 3 (GPLv3).[10][11][12] One may also purchase source under a negotiated license.[13]
The following is an implementation of quicksort demonstrating tacit programming. The latter involves composing functions together and not referring explicitly to any variables.  J's support for forks and hooks dictates rules on how arguments applied to this function will be applied to its component functions.
some of the modifiers control the order in which components are executed, allowing modifiers to be combined in any order to produce the unlimited variety of operations needed for practical programming.
The only collection type offered by J is the arbitrarily dimensioned array.  Most algorithms can be expressed very concisely using operations on these arrays.
K is a proprietary array processing programming language developed by Arthur Whitney and commercialized by Kx Systems. The language serves as the foundation for kdb+, an in-memory, column-based database, and other related financial products.[1] The language, originally developed in 1993, is a variant of APL and contains elements of Scheme. Advocates of the language emphasize its speed, facility in handling arrays, and expressive syntax.[2]
The contract ended in 1997 when UBS merged with Swiss Bank. In 1998, Kx Systems released kdb+, a database built on K. kdb was an in-memory, column-oriented database and included ksql, a query language with an SQL-like syntax. Since then, several financial products have been developed with K and kdb+. kdb+/tick and kdb+/taq were developed in 2001. kdb+, a 64-bit version of kdb+ was released in 2003 and kdb+/tick and kdb+/taq were released in 2004. kdb+ included Q, a language that merged the functions of the underlying K language and ksql.[4]
K is the foundation for a family of financial products. Kdb+ is an in-memory, column-based database with much of the same functions of a relational database management system. The database supports SQL, SQL-92 and ksql, a query language with a syntax similar to SQL and designed for column based queries and array analysis.
Kdb+ is available for several operating systems, including Solaris, Linux, macOS, and Windows (32-bit or 64-bit).
Q is a programming language for array processing, developed by Arthur Whitney. It is proprietary software, commercialized by Kx Systems. Q serves as the query language for kdb+, a disk based and in-memory, column-based database. Kdb+ is based on the language k, a terse variant of the language APL. Q is a thin wrapper around k, providing a more readable, English-like interface. One of the use cases is financial time series analysis, as one could do inexact time matches. An example is to match the a bid and the ask before that. Both timestamps slightly differ and are matched anyway.[3]
The fundamental building blocks of q are atoms, lists, and functions.  Atoms are scalars and include the data types numeric, character, date, and time.  Lists are ordered collections of atoms (or other lists) upon which the higher level data structures dictionaries and tables are internally constructed.  A dictionary is a map of a list of keys to a list of values.  A table is a transposed dictionary of symbol keys and equal length lists (columns) as values.  A keyed table, analogous to a table with a primary key placed on it, is a dictionary where the keys and values are arranged as two tables.
The following code demonstrates the relationships of the data structures. Expressions to evaluate appear prefixed with the q) prompt, with the output of the evaluation shown beneath:
The q programming language contains its own table query syntax called qSQL, which resembles traditional SQL but has important differences, mainly due to the fact that the underlying tables are oriented by column, rather than by row.
Lex reads an input stream specifying the lexical analyzer and writes source code which implements the lexical analyzer in the C programming language.
Lex was originally written by Mike Lesk and Eric Schmidt[5] and described in 1975.[6][7]
In the following years, Lex became standard lexical analyzer generator on many Unix and Unix-like systems. In 1983, Lex was one of several UNIX tools available for Charles River Data Systems' UNOS operating system under Bell Laboratories license.[8]
Although originally distributed as proprietary software, some versions of Lex are now open-source. Open-source versions of Lex, based on the original proprietary code, are now distributed with open-source operating systems such as OpenSolaris and Plan 9 from Bell Labs. One popular open-source version of Lex, called flex, or the "fast lexical analyzer", is not derived from proprietary coding.
A programming paradigm is a relatively high-level way to conceptualize and structure the implementation of a computer program. A programming language can be classified as supporting one or more paradigms.[1]
Paradigms are separated along and described by different dimensions of programming. Some paradigms are about implications of the execution model, such as allowing side effects, or whether the sequence of operations is defined by the execution model. Other paradigms are about the way code is organized, such as grouping into units that include both state and behavior. Yet others are about syntax and grammar.
Programming paradigms come from computer science research into existing practices of software development. The findings allow for describing and comparing programming practices and the languages used to code programs. For perspective, other fields of research study 
software engineering processes and describe various methodologies to describe and compare them.
A programming language can be described in terms of paradigms. Some languages support only one paradigm. For example, Smalltalk supports object-oriented and Haskell supports functional. Most languages support multiple paradigms. For example, a program written in C++, Object Pascal, or PHP can be purely procedural, purely object-oriented, or can contain aspects of both paradigms, or others.
In contrast, languages in the declarative paradigm do not state the order in which to execute operations. Instead, they supply a number of available operations in the system, along with the conditions under which each is allowed to execute.[7]  The implementation of the language's execution model tracks which operations are free to execute and chooses the order independently. More at  Comparison of multi-paradigm programming languages.
In object-oriented programming, code is organized into objects that contain state that is owned by and (usually) controlled by the code of the object.  Most object-oriented languages are also imperative languages.
In object-oriented programming, programs are treated as a set of interacting objects. In functional programming, programs are treated as a sequence of stateless function evaluations. When programming computers or systems with many processors, in process-oriented programming, programs are treated as sets of concurrent processes that act on a logical shared data structures.
Many programming paradigms are as well known for the techniques they forbid as for those they support. For instance, pure functional programming disallows side-effects, while structured programming disallows the goto construct. Partly for this reason, new paradigms are often regarded as doctrinaire or overly rigid by those accustomed to older ones.[8] Yet, avoiding certain techniques can make it easier to understand program behavior, and to prove theorems about program correctness.
Programming paradigms can also be compared with programming models, which allows invoking an execution model by using only an API. Programming models can also be classified into paradigms based on features of the execution model.
For parallel computing, using a programming model instead of a language is common.  The reason is that details of the parallel hardware leak into the abstractions used to program the hardware.  This causes the programmer to have to map patterns in the algorithm onto patterns in the execution model (which have been inserted due to leakage of hardware into the abstraction).  As a consequence, no one parallel programming language maps well to all computation problems.  Thus, it is more convenient to use a base sequential language and insert API calls to parallel execution models via a programming model.  Such parallel programming models can be classified according to abstractions that reflect the hardware, such as shared memory, distributed memory with message passing, notions of place visible in the code, and so forth.  These can be considered flavors of programming paradigm that apply to only parallel languages and programming models.
Some programming language researchers criticise the notion of paradigms as a classification of programming languages, e.g. Harper,[9] and Krishnamurthi.[10]  They argue that many programming languages cannot be strictly classified into one paradigm, but rather include features from several paradigms. See Comparison of multi-paradigm programming languages.
Different approaches to programming have developed over time. Classification of each approach was either described at the time the approach was first developed, but often not until some time later, retrospectively. An early approach consciously identified as such is structured programming, advocated since the mid 1960s. The concept of a programming paradigm as such dates at least to 1978, in the Turing Award lecture of Robert W. Floyd, entitled The Paradigms of Programming, which cites the notion of paradigm as used by Thomas Kuhn in his The Structure of Scientific Revolutions (1962).[11] Early programming languages did not have clearly defined programming paradigms and sometimes programs made extensive use of goto statements. Liberal use of which lead to spaghetti code which is difficult to understand and maintain. This led to the development of structured programming paradigms that disallowed the use of goto statements; only allowing the use of more structured programming constructs.[12]
Machine code is the lowest-level of computer programming as it is machine instructions that define behavior at the lowest level of abstract possible for a computer. As it is the most prescriptive way to code it is classified as imperative.
Assembly language introduced mnemonics for machine instructions and memory addresses. Assembly is classified as imperative and is sometimes called the second-generation programming language.
Procedural languages, also called the third-generation programming languages are the first described as high-level languages. They support vocabulary related to the problem being solved. For example,
In attempt to improve on procedural languages, object-oriented programming (OOP) languages were created, such as Simula, Smalltalk, C++, Eiffel, Python, PHP, Java, and C#. In these languages, data and methods to manipulate the data are in the same code unit called an object. This encapsulation ensures that the only way that an object can access data is via methods of the object that contains the data. Thus, an object's inner workings may be changed without affecting code that uses the object.
There is controversy raised by Alexander Stepanov, Richard Stallman[13] and other programmers, concerning the efficacy of the OOP paradigm versus the procedural paradigm. The need for every object to have associative methods leads some skeptics to associate OOP with software bloat; an attempt to resolve this dilemma came through polymorphism.
Although most OOP languages are third-generation, it is possible to create an object-oriented assembler language. High Level Assembly (HLA) is an example of this that fully supports advanced data types and object-oriented assembly language programming –  despite its early origins. Thus, differing programming paradigms can be seen rather like motivational memes of their advocates, rather than necessarily representing progress from one level to the next.[citation needed] Precise comparisons of competing paradigms' efficacy are frequently made more difficult because of new and differing terminology applied to similar entities and processes together with numerous implementation distinctions across languages.
A declarative programming program describes what the problem is, not how to solve it. The program is structured as a set of properties to find in the expected result, not as a procedure to follow. Given a database or a set of rules, the computer tries to find a solution matching all the desired properties. An archetype of a declarative language is the fourth generation language SQL, and the family of functional languages and logic programming.
Functional programming is a subset of declarative programming. Programs written using this paradigm use functions, blocks of code intended to behave like mathematical functions. Functional languages discourage changes in the value of variables through assignment, making a great deal of use of recursion instead.
The logic programming paradigm views computation as automated reasoning over a body of knowledge. Facts about the problem domain are expressed as logic formulas, and programs are executed by applying inference rules over them until an answer to the problem is found, or the set of formulas is proved inconsistent.
Symbolic programming is a paradigm that describes programs able to manipulate formulas and program components as data.[4] Programs can thus effectively modify themselves, and appear to "learn", making them suited for applications such as artificial intelligence, expert systems, natural-language processing and computer games. Languages that support this paradigm include Lisp and Prolog.[14]
Differentiable programming structures programs so that they can be differentiated throughout, usually via automatic differentiation.[15][16]
Literate programming, as a form of imperative programming, structures programs as a human-centered web, as in a hypertext essay: documentation is integral to the program, and the program is structured following the logic of prose exposition, rather than compiler convenience.
Symbolic programming techniques such as reflective programming (reflection), which allow a program to refer to itself, might also be considered as a programming paradigm. However, this is compatible with the major paradigms and thus is not a real paradigm in its own right.
Dylan is a multi-paradigm programming language that includes support for functional and object-oriented programming (OOP), and is dynamic and reflective while providing a programming model designed to support generating efficient machine code, including fine-grained control over dynamic and static behaviors. It was created in the early 1990s by a group led by Apple Computer.
Dylan derives from Scheme and Common Lisp and adds an integrated object system derived from the Common Lisp Object System (CLOS). In Dylan, all values (including numbers, characters, functions, and classes) are first-class objects. Dylan supports multiple inheritance, polymorphism, multiple dispatch, keyword arguments, object introspection, pattern-based syntax extension macros, and many other advanced features. Programs can express fine-grained control over dynamism, admitting programs that occupy a continuum between dynamic and static programming and supporting evolutionary development (allowing for rapid prototyping followed by incremental refinement and optimization).
Dylan's main design goal is to be a dynamic language well-suited for developing commercial software. Dylan attempts to address potential performance issues by introducing "natural" limits to the full flexibility of Lisp systems, allowing the compiler to clearly understand compilable units, such as libraries.
APL (named after the book A Programming Language)[3] is a programming language developed in the 1960s by Kenneth E. Iverson. Its central datatype is the multidimensional array. It uses a large range of special graphic symbols[4] to represent most functions and operators, leading to very concise code. It has been an important influence on the development of concept modeling, spreadsheets, functional programming,[5] and computer math packages.[6] It has also inspired several other programming languages.[7][8]
A mathematical notation for manipulating arrays was developed by Kenneth E. Iverson, starting in 1957 at Harvard University. In 1960, he began work for IBM where he developed this notation with Adin Falkoff and published it in his book A Programming Language in 1962.[3] The preface states its premise:
Applied mathematics is largely concerned with the design and analysis of explicit procedures for calculating the exact or approximate values of various functions. Such explicit procedures are called algorithms or programs. Because an effective notation for the description of programs exhibits considerable syntactic structure, it is called a programming language.
Iverson also used his notation in a draft of the chapter A Programming Language, written for a book he was writing with Fred Brooks, Automatic Data Processing, which would be published in 1963.[9][10]
After Lawrence M. Breed and Philip S. Abrams of Stanford University joined the team at IBM Research, they continued their prior work on an implementation programmed in FORTRAN IV for a part of the notation which had been done for the IBM 7090 computer running on the IBSYS operating system. This work was finished in late 1965 and later named IVSYS (for Iverson system). The basis of this implementation was described in detail by Abrams in a Stanford University Technical Report, "An Interpreter for Iverson Notation" in 1966. The academic aspect of this was formally supervised by Niklaus Wirth.[14] Like Hellerman's PAT system earlier, this implementation omitted the APL character set, but used special English reserved words for functions and operators. The system was later adapted for a time-sharing system and, by November 1966, it had been reprogrammed for the IBM System/360 Model 50 computer running in a time-sharing mode and was used internally at IBM.[15]
APL gained its foothold on mainframe timesharing systems from the late 1960s through the early 1980s, in part because it would support multiple users on lower-specification systems that had no dynamic address translation hardware.[20] Additional improvements in performance for selected IBM System/370 mainframe systems included the APL Assist Microcode in which some support for APL execution was included in the processor's firmware, as distinct from being implemented entirely by higher-level software. Somewhat later, as suitably performing hardware was finally growing available in the mid- to late-1980s, many users migrated their applications to the personal computer environment.
Early IBM APL interpreters for IBM 360 and IBM 370 hardware implemented their own multi-user management instead of relying on the host services, thus they were their own timesharing systems. First introduced for use at IBM in 1966, the APL\360[21][22][23] system was a multi-user interpreter. The ability to programmatically communicate with the operating system for information and setting interpreter system variables was done through special privileged "I-beam" functions, using both monadic and dyadic operations.[24]
In 1973, IBM released APL.SV, which was a continuation of the same product, but which offered shared variables as a means to access facilities outside of the APL system, such as operating system files. In the mid-1970s, the IBM mainframe interpreter was even adapted for use on the IBM 5100 desktop computer, which had a small CRT and an APL keyboard, when most other small computers of the time only offered BASIC. In the 1980s, the VSAPL program product enjoyed wide use with Conversational Monitor System (CMS), Time Sharing Option (TSO), VSPC, MUSIC/SP, and CICS users.
On microcomputers, which became available from the mid-1970s onwards, BASIC became the dominant programming language.[29] Nevertheless, some microcomputers provided APL instead – the first being the Intel 8008-based MCM/70 which was released in 1974[30][31] and which was primarily used in education.[32] Another machine of this time was the VideoBrain Family Computer, released in 1977, which was supplied with its dialect of APL called APL/S.[33]
In 1976, Bill Gates claimed in his Open Letter to Hobbyists that Microsoft Corporation was implementing APL for the Intel 8080 and Motorola 6800 but had "very little incentive to make [it] available to hobbyists" because of software piracy.[35] It was never released.
As other vendors were busy developing APL interpreters for new hardware, notably Unix-based microcomputers, APL2 was almost always the standard chosen for new APL interpreter developments. Even today, most APL vendors or their users cite APL2 compatibility as a selling point for those products.[42][43] IBM cites its use for problem solving, system design, prototyping, engineering and scientific computations, expert systems,[44] for teaching mathematics and other subjects, visualization and database access.[45]
Various implementations of APL by APLX, Dyalog, et al., include extensions for object-oriented programming, support for .NET, XML-array conversion primitives, graphing, operating system interfaces, and lambda calculus expressions. Freeware versions include GNU APL for Linux and NARS2000 for Windows (which also runs on Linux under Wine). Both of these are fairly complete versions of APL2 with various language extensions.
Over time, with the universal use of high-quality graphic displays, printing devices and Unicode support, the APL character font problem has largely been eliminated. However, entering APL characters requires the use of input method editors, keyboard mappings, virtual/on-screen APL symbol sets,[49][50] or easy-reference printed keyboard cards which can frustrate beginners accustomed to other programming languages.[51][52][53] With beginners who have no prior experience with other programming languages, a study involving high school students found that typing and using APL characters did not hinder the students in any measurable way.[54]
In defense of APL, it requires fewer characters to type, and keyboard mappings become memorized over time. Special APL keyboards are also made and in use today, as are freely downloadable fonts for operating systems such as Microsoft Windows.[49] The reported productivity gains assume that one spends enough time working in the language to make it worthwhile to memorize the symbols, their semantics, keyboard mappings, and many idioms for common tasks.[citation needed]
Unlike traditionally structured programming languages, APL code is typically structured as chains of monadic or dyadic functions, and operators[55] acting on arrays.[56] APL has many nonstandard primitives (functions and operators) that are indicated by a single symbol or a combination of a few symbols. All primitives are defined to have the same precedence, and always associate to the right. Thus, APL is read or best understood from right-to-left.
Early APL implementations (c. 1970 or so) had no programming loop control flow structures, such as do or while loops, and if-then-else constructs. Instead, they used array operations, and use of structured programming constructs was often unneeded, since an operation could be performed on a full array in one statement. For example, the iota function (ι) can replace for-loop iteration: ιN when applied to a scalar positive integer yields a one-dimensional array (vector), 1 2 3 ... N. Later APL implementations generally include comprehensive control structures, so that data structure and program control flow can be clearly and cleanly separated.
APL uses a set of non-ASCII symbols, which are an extension of traditional arithmetic and algebraic notation. Having single character names for single instruction, multiple data (SIMD) vector functions is one way that APL enables compact formulation of algorithms for data transformation such as computing Conway's Game of Life in one line of code.[58] In nearly all versions of APL, it is theoretically possible to express any computable function in one expression, that is, in one line of code.[citation needed]
Advocates of APL[who?] claim that the examples of so-called write-only code (badly written and almost incomprehensible code) are almost invariably examples of poor programming practice or novice mistakes, which can occur in any language. Advocates also claim that they are far more productive with APL than with more conventional computer languages, and that working software can be implemented in far less time and with far fewer programmers than using other technology.[citation needed]
They also may claim that because it is compact and terse, APL lends itself well to larger-scale software development and complexity, because the number of lines of code can be reduced greatly. Many APL advocates and practitioners also view standard programming languages such as COBOL and Java as being comparatively tedious. APL is often found where time-to-market is important, such as with trading systems.[62][63][64][65]
Some words used in APL literature have meanings that differ from those in both mathematics and the generality of computer science.
A design theme in APL is to define default actions in some cases that would produce syntax errors in most other programming languages.
Another example of this theme is that exponentiation in APL is written as 2*3, which indicates raising 2 to the power 3 (this would be written as 2^3 or 2**3 in some languages, or relegated to a function call such as pow(2, 3); in others). Many languages use * to signify multiplication, as in 2*3, but APL chooses to use 2×3. However, if no base is specified (as with the statement *3 in APL, or ^3 in other languages), most programming languages one would see this as a syntax error. APL, however, assumes the missing base to be the natural logarithm constant e, and interprets *3 as 2.71828*3.
The following function "life", written in Dyalog APL,[74][75] takes a Boolean matrix and calculates the new generation according to Conway's Game of Life. It demonstrates the power of APL to implement a complex algorithm in very little code, but understanding it requires some advanced knowledge of APL (as the same program would in many languages).
APL derives its name from the initials of Iverson's book A Programming Language,[3] even though the book describes Iverson's mathematical notation, rather than the implemented programming language described in this article. The name is used only for actual implementations, starting with APL\360.
Then one day Adin Falkoff walked into Ken's office and wrote "A Programming Language" on the board, and underneath it the acronym "APL". Thus it was born. It was just a week or so after this that Eric Iverson asked me his question, at a time when the name hadn't yet found its way the thirteen miles up the Taconic Parkway from IBM Research to IBM Mohansic.
APL is occasionally re-interpreted as Array Programming Language or Array Processing Language,[77] thereby making APL into a backronym.
Despite all these community efforts, no universal vendor-agnostic logo for the programming language emerged. As popular programming languages increasingly have established recognisable logos, Fortran getting one in 2020,[79] British APL Association launched a campaign in the second half of 2021, to establish such a logo for APL, and after a community election and multiple rounds of feedback, a logo was chosen in May 2022.[80]
APL is used for many purposes including financial and insurance applications,[81] artificial intelligence,[82][83]
neural networks[84]
and robotics.[85] It has been argued that APL is a calculation tool and not a programming language;[86] its symbolic nature and array capabilities have made it popular with domain experts and data scientists[87] who do not have or require the skills of a computer programmer.[citation needed]
In 1968, APL\1130 became the first publicly available APL system, created by IBM for the IBM 1130.[99] It became the most popular IBM Type-III Library software that IBM released.[100]
In 1972, APLGOL was released as an experimental version of APL that added structured programming language constructs to the language framework. New statements were added for interstatement control, conditional statement execution, and statement structuring, as well as statements to clarify the intent of the algorithm.[105] It was implemented for Hewlett-Packard in 1977.[106]
Dyalog APL was first released by British company Dyalog Ltd.[107] in 1983[108] and, as of 2018[update], is available for AIX, Linux (including on the Raspberry Pi), macOS and Microsoft Windows platforms. It is based on APL2, with extensions to support object-oriented programming,[109] functional programming,[110] and tacit programming.[111] Licences are free for personal/non-commercial use.[112]
APL is traditionally an interpreted language, having language characteristics such as weak variable typing not well suited to compilation.[119] However, with arrays as its core data structure[120] it provides opportunities for performance gains through parallelism,[121] parallel computing,[122][123] massively parallel applications,[124][125] and very-large-scale integration (VLSI),[126][127] and from the outset APL has been regarded as a high-performance language[128] – for example, it was noted for the speed with which it could perform complicated matrix operations "because it operates on arrays and performs operations like matrix inversion internally".[129]
FP (short for functional programming)[2] is a programming language created by John Backus to support the function-level programming[2] paradigm. It allows building programs from a set of generally useful primitives and avoiding named variables (a style also called tacit programming or "point free"). It was heavily influenced by APL developed by Kenneth E. Iverson in the early 1960s.[3]
The FP language was introduced in Backus's 1977 Turing Award paper, "Can Programming Be Liberated from the von Neumann Style?", subtitled "a functional style and its algebra of programs."  The paper sparked interest in functional programming research,[4] eventually leading to modern functional languages, which are largely founded on the lambda calculus paradigm, and not the function-level paradigm Backus had hoped. In his Turing award paper, Backus described how the FP style is different:
In computer science, function-level programming refers to one of the two contrasting programming paradigms identified by John Backus in his work on programs as mathematical objects, the other being value-level programming.
In his 1977 Turing Award lecture, Backus set forth what he considered to be the need to switch to a different philosophy in programming language design:[1]
Programming languages appear to be in trouble. Each successive language incorporates, with a little cleaning up, all the features of its predecessors plus a few more. [...] Each new language claims new and fashionable features... but the plain fact is that few languages make programming sufficiently cheaper or more reliable to justify the cost of producing and learning to use them.
He designed FP to be the first programming language to specifically support the function-level programming style.
A function-level program is variable-free (cf. point-free programming), since program variables, which are essential in value-level definitions, are not needed in function-level programs.
In the function-level style of programming, a program is built directly from programs that are given at the outset, by combining them with program-forming operations or functionals. Thus, in contrast with the value-level approach that applies the given programs to values to form a succession of values culminating in the desired result value, the function-level approach applies program-forming operations to the given programs to form a succession of programs culminating in the desired result program.
As a result, the function-level approach to programming invites study of the space of programs under  program-forming operations, looking to derive useful algebraic properties of these program-forming operations. The function-level approach offers the possibility of making the set of programs a mathematical space by emphasizing the algebraic properties of the program-forming operations over the space of programs.
Another potential advantage of the function-level view is the ability to use only strict functions and thereby have bottom-up semantics, which are the simplest kind of all. Yet another is the existence of function-level definitions that are not the lifted (that is, lifted from a lower value-level to a higher function-level) image of any existing value-level one: these (often terse) function-level definitions represent a more powerful style of programming not available at the value-level.
When Backus studied and publicized his function-level style of programming, his message was mostly misunderstood[2] as supporting the traditional functional programming style languages instead of his own FP and its successor FL.
Backus calls functional programming applicative programming;[clarification needed] his function-level programming is a particular, constrained type.
Even today, many users of lambda style languages often misinterpret Backus' function-level approach as a restrictive variant of the lambda style, which is a de facto value-level style. In fact, Backus would not have disagreed with the 'restrictive' accusation: he argued that it was precisely due to such restrictions that a well-formed mathematical space could arise, in a manner analogous to the way structured programming limits programming to a restricted version of all the control-flow possibilities available in plain, unrestricted unstructured programs.
SASL (St Andrews Static Language, alternatively  St Andrews Standard Language) is a purely functional programming language developed by David Turner at the University of St Andrews in 1972, based on the applicative subset of ISWIM.[1]  In 1976 Turner redesigned and reimplemented it as a non-strict (lazy) language.[2]  In this form it was the foundation of Turner's later languages Kent Recursive Calculator (KRC) and Miranda, but SASL appears to be untyped whereas Miranda has polymorphic types.
NPL is a functional programming language with pattern matching designed by Rod Burstall and John Darlington in 1977. The language allows certain sets and logic constructs to appear on the right hand side of definitions, e.g.
Hope is a programming language based on functional programming developed in the 1970s at the University of Edinburgh.[1][2]
It predates Miranda and Haskell and is contemporaneous with ML, also developed at the University. Hope was derived from NPL,[3] a simple functional language developed by Rod Burstall and John Darlington in their work on program transformation.[4] NPL and Hope are notable for being the first languages with call-by-pattern evaluation and algebraic data types.[5]
Hope was named for Sir Thomas Hope (c. 1681–1771), a Scottish agriculture reformer, after whom Hope Park Square in Edinburgh, the location of the artificial intelligence department at the time of the development of Hope, was also named.
Changing the order of clauses does not change the meaning of the program, because Hope's pattern matching always favors more specific patterns over less specific ones. Explicit declarations of data types in Hope are required; there is no type inference algorithm.
Miranda is a lazy, purely functional programming language designed by David Turner as a successor to his earlier programming languages SASL and KRC, using some concepts from ML and Hope. It was produced by Research Software Ltd. of England (which holds a trademark on the name Miranda)[3] and was the first purely functional language to be commercially supported.[citation needed]
Miranda was first released in 1985 as a fast interpreter in C for Unix-flavour operating systems, with subsequent releases in 1987 and 1989. It had a strong influence on the later Haskell language.[4] Turner stated that the benefits of Miranda over Haskell are:  "Smaller language, simpler type system, simpler arithmetic".[5]
In 2020 a version of Miranda was released as open source under a BSD licence. The code has been updated to conform to modern C standards (C11/C18) and to generate 64-bit binaries. This has been tested on operating systems including Debian, Ubuntu, WSL/Ubuntu, and macOS (Catalina).[5][6]
Miranda is a lazy, purely functional programming language. That is, it lacks side effects and imperative programming features. A Miranda program (called a script) is a set of equations that define various mathematical functions and algebraic data types. The word set is important here: the order of the equations is, in general, irrelevant, and there is no need to define an entity prior to its use.
Since the parsing algorithm makes intelligent use of layout (indentation, via off-side rule), bracketing statements are rarely needed and statement terminators are unneeded. This feature, inspired by ISWIM, is also used in occam and Haskell and was later popularized by Python.
The list instead is the most commonly used data structure in Miranda. It is written delimited by square brackets and with comma-separated elements, all of which must be of the same type:
In Miranda, as in most other purely functional languages, functions are first-class citizens, which is to say that they can be passed as arguments to other functions, returned as results, or included as elements of data structures. What is more, a function with two or more parameters may be "partially parameterised", or curried, by supplying fewer arguments than the full number of parameters. This gives another function which, given the remaining parameters, will return a result. For example:
Although Miranda is a strongly typed programming language, it does not insist on explicit type declarations. If a function's type is not explicitly declared, the interpreter infers it from the type of its parameters and how they are used within the function. In addition to the basic types (char, num, bool), it includes an "anything" type where the type of a parameter does not matter, as in the list-reversing function:
In computer science, recursion is a method of solving a computational problem where the solution depends on solutions to smaller instances of the same problem.[1][2] Recursion solves such recursive problems by using functions that call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.[3]
Most computer programming languages support recursion by allowing a function to call itself from within its own code. Some functional programming languages (for instance, Clojure)[5] do not define any looping constructs but rely solely on recursion to repeatedly call code. It is proved in computability theory that these recursive-only languages are Turing complete; this means that they are as powerful (they can be used to solve the same problems) as imperative languages based on control structures such as while and for.
Repeatedly calling a function from within itself may cause the call stack to have a size equal to the sum of the input sizes of all involved calls. It follows that, for problems that can be solved easily by iteration, recursion is generally less efficient, and, for certain problems, algorithmic or compiler-optimization techniques such as tail call optimization may improve computational performance over a naive recursive implementation.
A common algorithm design tactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the divide-and-conquer method; when combined with a lookup table that stores the results of previously solved sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as dynamic programming or memoization.
Similarly recursive definitions are often used to model the structure of expressions and statements in programming languages.  Language designers often express grammars in a syntax such as Backus–Naur form; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition:
A coinductive data definition is one that specifies the operations that may be performed on a piece of data; typically, self-referential coinductive definitions are used for data structures of infinite size.
This is very similar to an inductive definition of lists of strings; the difference is that this definition specifies how to access the contents of the data structure—namely, via the accessor functions head and tail—and what those contents may be, whereas the inductive definition specifies how to create the structure and what it may be created from.
Corecursion is related to coinduction, and can be used to compute particular instances of (possibly) infinite objects.  As a programming technique, it is used most often in the context of lazy programming languages, and can be preferable to recursion when the desired size or precision of a program's output is unknown.  In such cases the program requires both a definition for an infinitely large (or infinitely precise) result, and a mechanism for taking a finite portion of that result.  The problem of computing the first n prime numbers is one that can be solved with a corecursive program (e.g. here).
Many well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it.  HtDP (How to Design Programs) refers to this kind as generative recursion.  Examples of generative recursion include: gcd, quicksort, binary search, mergesort, Newton's method, fractals, and adaptive integration.
On the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this.
Short-circuiting is primarily a concern when many base cases are encountered, such as Null pointers in a tree, which can be linear in the number of function calls, hence significant savings for O(n) algorithms; this is illustrated below for a depth-first search. Short-circuiting on a tree corresponds to considering a leaf (non-empty node with no children) as the base case, rather than considering an empty node as the base case. If there is only a single base case, such as in computing the factorial, short-circuiting provides only O(1) savings.
Recursive algorithms are often inefficient for small data, due to the overhead of repeated function calls and returns. For this reason efficient implementations of recursive algorithms often start with the recursive algorithm, but then switch to a different algorithm when the input becomes small. An important example is merge sort, which is often implemented by switching to the non-recursive insertion sort when the data is sufficiently small, as in the tiled merge sort. Hybrid recursive algorithms can often be further refined, as in Timsort, derived from a hybrid merge sort/insertion sort.
Recursion and iteration are equally expressive: recursion can be replaced by iteration with an explicit call stack, while iteration can be replaced with tail recursion. Which approach is preferable depends on the problem under consideration and the language used. In imperative programming, iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion. By contrast, in functional languages recursion is preferred, with tail recursion optimization leading to little overhead. Implementing an algorithm using iteration may not be easily achievable.
Most programming languages in use today allow the direct specification of recursive functions and procedures. When such a function is called, the program's runtime environment keeps track of the various instances of the function (often using a call stack, although other methods may be used). Every recursive function can be transformed into an iterative function by replacing recursive calls with iterative control constructs and simulating the call stack with a stack explicitly managed by the program.[9][10]
In some programming languages, the maximum size of the call stack is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms. Consequently, these languages sometimes place a limit on the depth of recursion to avoid stack overflows; Python is one such language.[13] Note the caveat below regarding the special case of tail recursion.
Because recursive algorithms can be subject to stack overflows, they may be vulnerable to pathological or malicious input.[14] Some malware specifically targets a program's call stack and takes advantage of the stack's inherently recursive nature.[15] Even in the absence of malware, a stack overflow caused by unbounded recursion can be fatal to the program, and exception handling logic may not prevent the corresponding process from being terminated.[16]
Multiply recursive problems are inherently recursive, because of prior state they need to track. One example is tree traversal as in depth-first search; though both recursive and iterative methods are used,[17] they contrast with list traversal and linear search in a list, which is a singly recursive and thus naturally iterative method. Other examples include divide-and-conquer algorithms such as Quicksort, and functions such as the Ackermann function. All of these algorithms can be implemented iteratively with the help of an explicit stack, but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution.
Recursive algorithms can be replaced with non-recursive counterparts.[18] One method for replacing recursive algorithms is to simulate them using heap memory in place of stack memory.[19] An alternative is to develop a replacement algorithm entirely based on non-recursive methods, which can be challenging.[20] For example, recursive algorithms for matching wildcards, such as Rich Salz' wildmat algorithm,[21] were once typical. Non-recursive algorithms for the same purpose, such as the Krauss matching wildcards algorithm, have been developed to avoid the drawbacks of recursion[22] and have improved only gradually based on techniques such as collecting tests and profiling performance.[23]
This factorial function can also be described without using recursion by making use of the typical looping constructs found in imperative programming languages:
The definition above translates straightforwardly to functional programming languages such as Scheme; this is an example of iteration implemented recursively.
The Euclidean algorithm, which computes the greatest common divisor of two integers, can be written recursively.
The recursive program above is tail-recursive; it is equivalent to an iterative algorithm, and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls.  Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls.  By maintaining its state entirely in the variables x and y and using a looping construct, the program avoids making recursive calls and growing the call stack.
The iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps.
The binary search algorithm is a method of searching a sorted array for a single element by cutting the array in half with each recursive pass.  The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for.
Recursion is used in this algorithm because with each pass a new array is created by cutting the old one in half.  The binary search procedure is then called recursively, this time on the new (and smaller) array.  Typically the array's size is adjusted by manipulating a beginning and ending index.  The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass.
An important application of recursion in computer science is in defining dynamic data structures such as lists and trees.  Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time.
"Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms."[27]
Because the struct node data structure is defined recursively, procedures that operate on it can be implemented naturally as recursive procedures.  The list_print procedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL).  For each node it prints the data element (an integer).  In the C implementation, the list remains unchanged by the list_print procedure.
The time efficiency of recursive algorithms can be expressed in a recurrence relation of Big O notation.  They can (usually) then be simplified into a single Big-O term.
The logical reading frees the reader from needing to know how the clause is used to solve problems. The clause can be used top-down, as in Prolog, to reduce problems to subproblems. Or it can be used bottom-up (or forwards), as in Datalog, to derive conclusions from conditions. This separation of concerns is a form of abstraction, which separates declarative knowledge from problem solving methods (see Algorithm#Algorithm = Logic + Control).[28]
A common mistake among programmers is not providing a way to exit a recursive function, often by omitting or incorrectly checking the base case, letting it run (at least theoretically) infinitely by endlessly calling itself recursively. This is called infinite recursion, and the program will never terminate. In practice, this typically exhausts the available stack space. In most programming environments, a program with infinite recursion will not really run forever. Eventually, something will break and the program will report an error.[29]
The Association for Computing Machinery awarded Thierry Coquand, Gérard Huet, Christine Paulin-Mohring, Bruno Barras, Jean-Christophe Filliâtre, Hugo Herbelin, Chetan Murthy, Yves Bertot, and Pierre Castéran with the 2013 ACM Software System Award for Rocq.
The old name Coq is a wordplay on the name of Thierry Coquand, calculus of constructions or CoC and follows the French computer science tradition of naming software after animals (coq in French meaning rooster).[2] On October 11, 2023, the development team announced that Coq will be renamed The Rocq Prover in coming months, and began updating the code base, website, and associated tools.[3]
When viewed as a programming language, Rocq implements a dependently typed functional programming model;[4] when viewed as a logical system, it implements a higher-order type theory. The development of Rocq has been supported since 1984 by French Institute for Research in Computer Science and Automation (INRIA), now in collaboration with École Polytechnique, University of Paris-Sud, Paris Diderot University, and French National Centre for Scientific Research (CNRS). In the 1990s, École normale supérieure de Lyon (ENS Lyon) was also part of the project. The development of Rocq was initiated by Gérard Huet and Thierry Coquand, and more than 40 people, mainly researchers, have contributed features to the core system since its inception. The implementation team has successively been coordinated by Gérard Huet, Christine Paulin-Mohring, Hugo Herbelin, and Matthieu Sozeau. Rocq is mainly implemented in OCaml with a bit of C. The core system can be extended by way of a plug-in mechanism.[5]
Rocq provides a specification language called Gallina[8] ("hen" in Latin, Spanish, Italian and Catalan).
Programs written in Gallina have the weak normalization property, implying that they always terminate.
This is a distinctive property of the language, since infinite loops (non-terminating programs) are common in other programming languages,[9]
and is one way to avoid the halting problem.
Georges Gonthier of Microsoft Research in Cambridge, England and Benjamin Werner of INRIA used Rocq to create a surveyable proof of the four color theorem, which was completed in 2002.[11]  Their work led to the development of the SSReflect ("Small Scale Reflection") package, which was a significant extension to Rocq.[12] Despite its name, most of the features added to Rocq by SSReflect are general-purpose features and are not limited to the computational reflective programming style of proof. These features include:
Total functional programming (also known as strong functional programming,[1] to be contrasted with ordinary, or weak functional programming) is a programming paradigm that restricts the range of programs to those that are provably terminating.[2]
These restrictions mean that total functional programming is not Turing-complete. However, the set of algorithms that can be used is still huge. For example, any algorithm for which an asymptotic upper bound can be calculated (by a program that itself only uses Walther recursion) can be trivially transformed into a provably-terminating function by using the upper bound as an extra argument decremented on each iteration or recursion.
Some classes of algorithms have no theoretical upper bound but do have a practical upper bound (for example, some heuristic-based algorithms can be programmed to "give up" after so many recursions, also ensuring termination).
Another outcome of total functional programming is that both strict evaluation and lazy evaluation result in the same behaviour, in principle; however, one or the other may still be preferable (or even required) for performance reasons.[4]
In total functional programming, a distinction is made between data and codata—the former is finitary, while the latter is potentially infinite. Such potentially infinite data structures are used for applications such as I/O. Using codata entails the usage of such operations as corecursion. However, it is possible to do I/O in a total functional programming language (with dependent types) also without codata.[5]
Both Epigram and Charity could be considered total functional programming languages, even though they do not work in the way Turner specifies in his paper. So could programming directly in plain System F, in Martin-Löf type theory or the Calculus of Constructions.
Clean is a general-purpose purely functional programming language. Originally called the Concurrent Clean System[3] or the Clean System,[4][5] it has been developed by a group of researchers from the Radboud University in Nijmegen since 1987.[6][7] Although development of the language has slowed, some researchers are still working in the language.[8] In 2018, a spin-off company was founded that uses Clean.[9]
Agda is a dependently typed functional programming language originally developed by Ulf Norell at Chalmers University of Technology with implementation described in his PhD thesis.[2] The original Agda system was developed at Chalmers by Catarina Coquand in 1999.[3] The current version, originally named Agda 2, is a full rewrite, which should be considered a new language that shares a name and tradition.
Agda is also a proof assistant based on the propositions-as-types paradigm (Curry–Howard correspondence), but unlike Coq, has no separate tactics language, and proofs are written in a functional programming style. The language has ordinary programming constructs such as data types, pattern matching, records, let expressions and modules, and a Haskell-like syntax. The system has Emacs, Atom, and VS Code interfaces[4][5][6] but can also be run in batch processing mode from a command-line interface.
The main way of defining data types in Agda is via inductive data types which are similar to algebraic data types in non-dependently typed programming languages.
Programming in pure type theory involves a lot of tedious and repetitive proofs. Although Agda has no separate tactics language, it is possible to program useful tactics within Agda. Typically, this works by writing an Agda function that optionally returns a proof of some property of interest. A tactic is then constructed by running this function at type-checking time, for example using the following auxiliary definitions:
Further, to write more complex tactics, Agda supports automation via reflective programming (reflection). The reflection mechanism allows quoting program fragments into, or unquoting them from, the abstract syntax tree. The way reflection is used is similar to the way Template Haskell works.[10]
Agda is a total functional programming language, i.e., each program in it must terminate and all possible patterns must be matched. Without this feature, the logic behind the language becomes inconsistent, and it becomes possible to prove arbitrary statements. For termination checking, Agda uses the approach of the Foetus termination checker.[12]
Agda has an extensive de facto standard library, which includes many useful definitions and theorems about basic data structures, such as natural numbers, lists, and vectors. The library is in beta, and is under active development.
Epigram is a functional programming language with dependent types, and the integrated development environment (IDE) usually packaged with the language. Epigram's type system is strong enough to express program specifications. The goal is to support a smooth transition from ordinary programming to integrated programs and proofs whose correctness can be checked and certified by the compiler. Epigram exploits the Curry–Howard correspondence, also termed the propositions as types principle, and is based on intuitionistic type theory.
In computer science, a purely functional data structure is a data structure that can be directly implemented in a purely functional language. The main difference between an arbitrary data structure and a purely functional one is that the latter is (strongly) immutable. This restriction ensures the data structure possesses the advantages of immutable objects: (full) persistency, quick copy of objects, and thread safety. Efficient purely functional data structures may require the use of lazy evaluation and memoization.
Persistent data structures have the property of keeping previous versions of themselves unmodified. On the other hand, non-persistent structures such as arrays admit a destructive update,[1] that is, an update which cannot be reversed. Once a program writes a value in some index of the array, its previous value can not be retrieved anymore.[citation needed]
Formally, a purely functional data structure is a data structure which can be implemented in a purely functional language, such as Haskell. In practice, it means that the data structures must be built using only persistent data structures such as tuples, sum types, product types, and basic types such as integers, characters, strings. Such a data structure is necessarily persistent. However, not all persistent data structures are purely functional.[1]: 16  For example, a persistent array is a data-structure which is persistent and which is implemented using an array, thus is not purely functional.[citation needed]
In the book Purely functional data structures, Okasaki compares destructive updates to master chef's knives.[1]: 2  Destructive updates cannot be undone, and thus they should not be used unless it is certain that the previous value is not required anymore. However, destructive updates can also allow efficiency that can not be obtained using other techniques. For example, a data structure using an array and destructive updates may be replaced by a similar data structure where the array is replaced by a map, a random access list, or a balanced tree, which admits a purely functional implementation. But the access cost may increase from constant time to logarithmic time.[citation needed]
A data structure is never inherently functional. For example, a stack can be implemented as a singly-linked list. This implementation is purely functional as long as the only operations on the stack return a new stack without altering the old stack. However, if the language is not purely functional, the run-time system may be unable to guarantee immutability. This is illustrated by Okasaki,[1]: 9–11  where he shows the concatenation of two singly-linked lists can still be done using an imperative setting.[citation needed]
In order to ensure that a data structure is used in a purely functional way in an impure functional language, modules or classes can be used to ensure manipulation via authorized functions only.[citation needed]
One of the central challenges in adapting existing code to use purely functional data structures lies in the fact that mutable data
structures provide "hidden outputs" for functions that use them. Rewriting these functions to use purely functional data structures
requires adding these data structures as explicit outputs.
In his book Purely Functional Data Structures, computer scientist Chris Okasaki describes techniques used to design and implement purely functional data structures, a small subset of which are summarized below.
Lazy evaluation is particularly interesting in a purely functional language[1]: 31  because the order of the evaluation never changes the result of a function. Therefore, lazy evaluation naturally becomes an important part of the construction of purely functional data structures. It allows a computation to be done only when its result is actually required. Therefore, the code of a purely functional data structure can, without loss of efficiency, consider similarly data that will effectively be used and data that will be ignored. The only computation required is for the first kind of data; that is what will actually be performed.[citation needed]
One of the key tools in building efficient, purely functional data structures is memoization.[1]: 31  When a computation is done, it is saved and does not have to be performed a second time. This is particularly important in lazy implementations; additional evaluations may require the same result, but it is impossible to know which evaluation will require it first.[citation needed]
Some data structures, even those that are not purely functional such as dynamic arrays, admit operations that are efficient most of the time (e.g., constant time for dynamic arrays), and rarely inefficient (e.g., linear time for dynamic arrays). Amortization can then be used to prove that the average running time of the operations is efficient.[1]: 39  That is to say, the few inefficient operations are rare enough, and do not change the asymptotical evolution of time complexity when a sequence of operations is considered.[citation needed]
In general, having inefficient operations is not acceptable for persistent data structures, because this very operation can be called many times. It is not acceptable either for real-time or for imperative systems, where the user may require the time taken by the operation to be predictable. Furthermore, this unpredictability complicates the use of parallelism.[1]: 83 [citation needed]
In order to avoid those problems, some data structures allow for the inefficient operation to be postponed—this is called scheduling.[1]: 84  The only requirement is that the computation of the inefficient operation should end before its result is actually needed. A constant part of the inefficient operation is performed simultaneously with the following call to an efficient operation, so that the inefficient operation is already totally done when it is needed, and each individual operation remains efficient.[clarification needed]
In functional programming, monads are a way to structure computations as a sequence of steps, where each step not only produces a value but also some extra information about the computation, such as a potential failure, non-determinism, or side effect. More formally, a monad is a type constructor M equipped with two operations, return : <A>(a : A) -> M(A) which lifts a value into the monadic context, and bind : <A,B>(m_a : M(A), f : A -> M(B)) -> M(B) which chains monadic computations. In simpler terms, monads can be thought of as interfaces implemented on type constructors, that allow for functions to abstract over various type constructor variants that implement monad (e.g. Option, List, etc.).[1][2]
One example of a monad is the Maybe type. Undefined null results are one particular pain point that many procedural languages don't provide specific tools for dealing with, requiring use of the null object pattern or checks to test for invalid values at each operation to handle undefined values. This causes bugs and makes it harder to build robust software that gracefully handles errors. The Maybe type forces the programmer to deal with these potentially undefined results by explicitly defining the two states of a result: Just ⌑result⌑, or Nothing. For example the programmer might be constructing a parser, which is to return an intermediate result, or else signal a condition which the parser has detected, and which the programmer must also handle. With just a little extra functional spice on top, this Maybe type transforms into a fully-featured monad.[c]: 12.3 pages 148–151
The more common definition for a monad in functional programming, used in the above example, is actually based on a Kleisli triple ⟨T, η, μ⟩ rather than category theory's standard definition. The two constructs turn out to be mathematically equivalent, however, so either definition will yield a valid monad. Given any well-defined basic types T and U, a monad consists of three parts:
The value of the monad pattern goes beyond merely condensing code and providing a link to mathematical reasoning.
Whatever language or default programming paradigm a developer uses, following the monad pattern brings many of the benefits of purely functional programming.
By reifying a specific kind of computation, a monad not only encapsulates the tedious details of that computational pattern, but it does so in a declarative way, improving the code's clarity.
As monadic values explicitly represent not only computed values, but computed effects, a monadic expression can be replaced with its value in referentially transparent positions, much like pure expressions can be, allowing for many techniques and optimizations based on rewriting.[4]
The monad structure can also be seen as a uniquely mathematical and compile time variation on the decorator pattern.
Some monads can pass along extra data that is inaccessible to functions, and some even exert finer control over execution, for example only calling a function under certain conditions.
Because they let application programmers implement domain logic while offloading boilerplate code onto pre-developed modules, monads can even be considered a tool for aspect-oriented programming.[12]
If a language does not support monads by default, it is still possible to implement the pattern, often without much difficulty.
When translated from category-theory to programming terms, the monad structure is a generic concept and can be defined directly in any language that supports an equivalent feature for bounded polymorphism.
A concept's ability to remain agnostic about operational details while working on underlying types is powerful, but the unique features and stringent behavior of monads set them apart from other concepts.[13]
The term "monad" in programming dates to the APL and J programming languages, which do tend toward being purely functional. However, in those languages, "monad" is only shorthand for a function taking one parameter (a function with two parameters being a "dyad", and so on).[19]
Starting in the 1980s, a vague notion of the monad pattern began to surface in the computer science community.
According to programming language researcher Philip Wadler, computer scientist John C. Reynolds anticipated several facets of it in the 1970s and early 1980s, when he discussed the value of continuation-passing style, of category theory as a rich source for formal semantics, and of the type distinction between values and computations.[4]
The research language Opal, which was actively designed up until 1990, also effectively based I/O on a monadic type, but the connection was not realized at the time.[21]
The computer scientist Eugenio Moggi was the first to explicitly link the monad of category theory to functional programming, in a conference paper in 1989,[22] followed by a more refined journal submission in 1991. In earlier work, several computer scientists had advanced using category theory to provide semantics for the lambda calculus. Moggi's key insight was that a real-world program is not just a function from values to other values, but rather a transformation that forms computations on those values. When formalized in category-theoretic terms, this leads to the conclusion that monads are the structure to represent these computations.[3]
Several others popularized and built on this idea, including Philip Wadler and Simon Peyton Jones, both of whom were involved in the specification of Haskell. In particular, Haskell used a problematic "lazy stream" model up through v1.2 to reconcile I/O with lazy evaluation, until switching over to a more flexible monadic interface.[23] The Haskell community would go on to apply monads to many problems in functional programming, and in the 2010s, researchers working with Haskell eventually recognized that monads are applicative functors;[24][j] and that both monads and arrows are monoids.[26]
At first, programming with monads was largely confined to Haskell and its derivatives, but as functional programming has influenced other paradigms, many languages have incorporated a monad pattern (in spirit if not in name). Formulations now exist in Scheme, Perl, Python, Racket, Clojure, Scala, F#, and have also been considered for a new ML standard.[citation needed]
Though rarer in computer science, one can use category theory directly, which defines a monad as a functor with two additional natural transformations.[k]
So to begin, a structure requires a higher-order function (or "functional") named map to qualify as a functor:
One application for this monadic list is representing nondeterministic computation.
List can hold results for all execution paths in an algorithm, then condense itself at each step to "forget" which paths led to which results (a sometimes important distinction from deterministic, exhaustive algorithms).[citation needed]
Another benefit is that checks can be embedded in the monad; specific paths can be pruned transparently at their first point of failure, with no need to rewrite functions in the pipeline.[28]
Besides generating monads with extra properties, for any given monad, one can also define a comonad.
Conceptually, if monads represent computations built up from underlying values, then comonads can be seen as reductions back down to values.
Monadic code, in a sense, cannot be fully "unpacked"; once a value is wrapped within a monad, it remains quarantined there along with any side-effects (a good thing in purely functional programming).
Sometimes though, a problem is more about consuming contextual data, which comonads can model explicitly.
A less trivial example is the Stream comonad, which can be used to represent data streams and attach filters to the incoming signals with extend.
In fact, while not as popular as monads, researchers have found comonads particularly useful for stream processing and modeling dataflow programming.[35][36]
In computer science, array programming refers to solutions that allow the application of operations to an entire set of values at once. Such solutions are commonly used in scientific and engineering settings.
Modern programming languages that support array programming (also known as vector or multidimensional languages) have been engineered specifically to generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays. These include APL, J, Fortran, MATLAB, Analytica, Octave, R, Cilk Plus, Julia, Perl Data Language (PDL), Raku (programming language). In these languages, an operation that operates on entire arrays can be called a vectorized operation,[1] regardless of whether it is executed on a vector processor, which implements vector instructions. Array programming primitives concisely express broad ideas about data manipulation. The level of concision can be dramatic in certain cases: it is not uncommon[example needed] to find array programming language one-liners that require several pages of object-oriented code.
The fundamental idea behind array programming is that operations apply at once to an entire set of values. This makes it a high-level programming model as it allows the programmer to think and operate on whole aggregates of data, without having to resort to explicit loops of individual scalar operations.
Kenneth E. Iverson described the rationale behind array programming (actually referring to APL) as follows:[2]
most programming languages are decidedly inferior to mathematical notation and are little used as tools of thought in ways that would be considered significant by, say, an applied mathematician.
The thesis is that the advantages of executability and universality found in programming languages can be effectively combined, in a single coherent language, with the advantages offered by mathematical notation. it is important to distinguish the difficulty of describing and of learning a piece of notation from the difficulty of mastering its implications. For example, learning the rules for computing a matrix product is easy, but a mastery of its implications (such as its associativity, its distributivity over addition, and its ability to represent linear functions and geometric operations) is a different and much more difficult matter.
Users of computers and programming languages are often concerned primarily with the efficiency of execution of algorithms, and might, therefore, summarily dismiss many of the algorithms presented here. Such dismissal would be short-sighted since a clear statement of an algorithm can usually be used as a basis from which one may easily derive a more efficient algorithm.
The basis behind array programming and thinking is to find and exploit the properties of data where individual elements are similar or adjacent. Unlike object orientation which implicitly breaks down data to its constituent parts (or scalar quantities), array orientation looks to group data and apply a uniform handling.
Function rank is an important concept to array programming languages in general, by analogy to tensor rank in mathematics: functions that operate on data may be classified by the number of dimensions they act on. Ordinary multiplication, for example, is a scalar ranked function because it operates on zero-dimensional data (individual numbers). The cross product operation is an example of a vector rank function because it operates on vectors, not scalars. Matrix multiplication is an example of a 2-rank function, because it operates on 2-dimensional objects (matrices). Collapse operators reduce the dimensionality of an input data array by one or more dimensions. For example, summing over elements collapses the input array by 1 dimension.
Array programming is very well suited to implicit parallelization; a topic of much research nowadays. Further, Intel and compatible CPUs developed and produced after 1997 contained various instruction set extensions, starting from MMX and continuing through SSSE3 and 3DNow!, which include rudimentary SIMD array capabilities. This has continued into the 2020s with instruction sets such as AVX-512, making modern CPUs sophisticated vector processors. Array processing is distinct from parallel processing in that one physical processor performs operations on a group of items simultaneously while parallel processing aims to split a larger problem into smaller ones (MIMD) to be solved piecemeal by numerous processors. Processors with multiple cores and GPUs with thousands of general computing cores are common as of 2023.
The canonical examples of array programming languages are Fortran, APL, and J. Others include: A+, Analytica, Chapel, IDL, Julia, K, Klong, Q, MATLAB, GNU Octave, Scilab, FreeMat, Perl Data Language (PDL), R, Raku, S-Lang, SAC, Nial, ZPL, Futhark, and TI-BASIC.
While scalar languages like C do not have native array programming elements as part of the language proper, this does not mean programs written in these languages never take advantage of the underlying techniques of vectorization (i.e., utilizing a CPU's vector-based instructions if it has them or by using multiple CPU cores). Some C compilers like GCC at some optimization levels detect and vectorize sections of code that its heuristics determine would benefit from it. Another approach is given by the OpenMP API, which allows one to parallelize applicable sections of code by taking advantage of multiple CPU cores.
An array language simplifies programming but possibly at a cost known as the abstraction penalty.[3][4][5] Because the additions are performed in isolation from the rest of the coding, they may not produce the optimally most efficient code. (For example, additions of other elements of the same array may be subsequently encountered during the same execution, causing unnecessary repeated lookups.) Even the most sophisticated optimizing compiler would have an extremely hard time amalgamating two or more apparently disparate functions which might appear in different program sections or sub-routines, even though a programmer could do this easily, aggregating sums on the same pass over the array to minimize overhead).
The previous C code would become the following in the Ada language,[6] which supports array-programming syntax.
Stata's matrix programming language Mata supports array programming. Below, we illustrate addition, multiplication, addition of a matrix and a scalar, element by element multiplication, subscripting, and one of Mata's many inverse matrix functions.
The rasdaman query language is a database-oriented array-programming language. For example, two arrays could be added with the following query:
This is not only an example of terse array programming from the coding point of view but also from the computational efficiency perspective, which in several array programming languages benefits from quite efficient linear algebra libraries such as ATLAS or LAPACK.[10]
The use of specialized and efficient libraries to provide more terse abstractions is also common in other programming languages. In C++ several linear algebra libraries exploit the language's ability to overload operators. In some cases a very terse abstraction in those languages is explicitly influenced by the array programming paradigm, as the NumPy extension library to Python, Armadillo and Blitz++ libraries do.[11][12]
Leiningen is a build automation and dependency management tool for the simple configuration of software projects written in the Clojure programming language.
Leiningen is the most widely-contributed-to open-source Clojure project. It is featured in chapter 8 of the book Clojure Programming.[2]
Ciao is a general-purpose programming language which supports logic, constraint, functional, higher-order, and object-oriented programming styles. Its main design objectives are high expressive power, extensibility, safety, reliability, and efficient execution.[1]
Ciao provides a full Prolog system (supporting ISO-Prolog),
declarative subsets and extensions of Prolog, functional programming (including lazy evaluation), higher-order (with predicate abstractions), constraint programming, and objects, as well as feature terms
(records), persistence, several control rules (breadth-first search, iterative deepening, ...), concurrency (threads/engines), distributed execution (agents), and parallel execution. Libraries also support WWW programming, sockets, external interfaces (C, Java, TclTk, relational databases, etc.), etc.
Ciao has also support for programming in the small: the compiler is
capable of producing small executables (including only those builtins
used by the program) and the interpreter supports scripting.
The International Conference on Functional Programming (ICFP) is an annual academic conference in the field of computer science sponsored by the ACM SIGPLAN, in association with IFIP Working Group 2.8 (Functional Programming). The conference focuses on functional programming and related areas of programming languages, logic, compilers and software development.[1]
The ICFP was first held in 1996, replacing two biennial conferences: the Functional Programming and Computer Architecture (FPCA) and LISP and Functional Programming (LFP). The conference location alternates between Europe and North America, with occasional appearances in other continents.[2] The conference usually lasts 3 days, surrounded by co-located workshops devoted to particular functional languages or application areas.
The ICFP has also held an open annual programming contest since 1998, called the ICFP Programming Contest.
The Journal of Functional Programming[2] is a peer-reviewed scientific journal covering the design, implementation, and application of functional programming languages, spanning the range from mathematical theory to industrial practice. Topics covered include functional languages and extensions, implementation techniques, reasoning and proof, program transformation and synthesis, type systems, type theory, language-based security, memory management, parallelism and applications. The journal is of interest to computer scientists, software engineers, programming language researchers, and mathematicians interested in the logical foundations of programming. Philip Wadler was editor-in-chief from 1990 to 2004. The journal is indexed in Zentralblatt MATH.
The Symposium on Trends in Functional Programming (TFP) is focused on research in the field of functional programming and investigating relationships with other branches of computer science.
A model consists of the equations used to capture the behavior of a system. By contrast, computer simulation is the actual running of the program that perform algorithms which solve those equations, often in an approximate manner. Simulation, therefore, is the process of running a model. Thus one would not "build a simulation"; instead, one would "build a model (or a simulator)", and then either "run the model" or equivalently "run a simulation".
Computer simulation developed hand-in-hand with the rapid growth of the computer, following its first large-scale deployment during the Manhattan Project in World War II to model the process of nuclear detonation. It was a simulation of 12 hard spheres using a Monte Carlo algorithm. Computer simulation is often used as an adjunct to, or substitute for, modeling systems for which simple closed form analytic solutions are not possible. There are many types of computer simulations; their common feature is the attempt to generate a sample of representative scenarios for a model in which a complete enumeration of all possible states of the model would be prohibitive or impossible.[7]
Another way of categorizing models is to look at the underlying data structures. For time-stepped simulations, there are two main classes:
In the context of an operating system, a device driver is a computer program that operates or controls a particular type of device that is attached to a computer or automaton.[1] A driver provides a software interface to hardware devices, enabling operating systems and other computer programs to access hardware functions without needing to know precise details about the hardware being used.
The main purpose of device drivers is to provide abstraction by acting as a translator between a hardware device and the applications or operating systems that use it.[1] Programmers can write higher-level application code independently of whatever specific hardware the end-user is using.
For example, a high-level application for interacting with a serial port may simply have two functions for "send data" and "receive data". At a lower level, a device driver implementing these functions would communicate to the particular serial port controller installed on a user's computer. The commands needed to control a 16550 UART are much different from the commands needed to control an FTDI serial port converter, but each hardware-specific device driver abstracts these details into the same (or similar) software interface.
Writing a device driver requires an in-depth understanding of how the hardware and the software works for a given platform function. Because drivers require low-level access to hardware functions in order to operate, drivers typically operate in a highly privileged environment and can cause system operational issues if something goes wrong. In contrast, most user-level software on modern operating systems can be stopped without greatly affecting the rest of the system. Even drivers executing in user mode can crash a system if the device is erroneously programmed. These factors make it more difficult and dangerous to diagnose problems.[3]
The task of writing drivers thus usually falls to software engineers or computer engineers who work for hardware-development companies. This is because they have better information than most outsiders about the design of their hardware. Moreover, it was traditionally considered in the hardware manufacturer's interest to guarantee that their clients can use their hardware in an optimal way. Typically, the Logical Device Driver (LDD) is written by the operating system vendor, while the Physical Device Driver (PDD) is implemented by the device vendor. However, in recent years, non-vendors have written numerous device drivers for proprietary devices, mainly for use with free and open source operating systems. In such cases, it is important that the hardware manufacturer provide information on how the device communicates. Although this information can instead be learned by reverse engineering, this is much more difficult with hardware than it is with software.
Windows uses a combination of driver and minidriver, where the full class/port driver is provided with the operating system, and miniclass/miniport drivers are developed by vendors and implement hardware- or function-specific subset of the full driver stack.[4] Miniport model is used by NDIS,  WDM, WDDM, WaveRT, StorPort, WIA, and HID drivers; each of them uses device-specific APIs and still requires the developer to handle tedious device management tasks.
Depending on the operating system, device drivers may be permitted to run at various different privilege levels. The choice of which level of privilege the drivers are in is largely decided by the type of kernel an operating system uses. An operating system that uses a monolithic kernel, such as the Linux kernel, will typically run device drivers with the same privilege as all other kernel objects. By contrast, a system designed around microkernel, such as Minix, will place drivers as processes independent from the kernel but that use it for essential input-output functionalities and to pass messages between user programs and each other.[6]
On Windows NT, a system with a hybrid kernel, it is common for device drivers to run in either kernel-mode or user-mode.[7]
The most common mechanism for segregating memory into various privilege levels is via protection rings. On many systems, such as those with x86 and ARM processors, switching between rings imposes a performance penalty, a factor that operating system developers and embedded software engineers consider when creating drivers for devices which are preferred to be run with low latency, such as network interface cards. The primary benefit of running a driver in user mode is improved stability since a poorly written user-mode device driver cannot crash the system by overwriting kernel memory.[8]
Because of the diversity of modern[update] hardware and operating systems, drivers operate in many different environments.[9] Drivers may interface with:
Virtual device drivers represent a particular variant of device drivers. They are used to emulate a hardware device, particularly in virtualization environments, for example when a DOS program is run on a Microsoft Windows computer or when a guest operating system is run on, for example, a Xen host. Instead of enabling the guest operating system to dialog with hardware, virtual device drivers take the opposite role and emulates a piece of hardware, so that the guest operating system and its drivers running inside a virtual machine can have the illusion of accessing real hardware. Attempts by the guest operating system to access the hardware are routed to the virtual device driver in the host operating system as e.g., function calls. The virtual device driver can also send simulated processor-level events like interrupts into the virtual machine.
Computers often have many diverse and customized device drivers running in their operating system (OS) kernel which often contain various bugs and vulnerabilities, making them a target for exploits.[18]  A Bring Your Own Vulnerable Driver (BYOVD) attacker installs any signed, old third-party driver with known vulnerabilities that allow malicious code to be inserted into the kernel.[19]
Drivers that may be vulnerable include those for WiFi and Bluetooth,[20][21] gaming/graphics drivers,[22] and drivers for printers.[23]
A group of security researchers considers the lack of isolation as one of the main factors undermining kernel security,[30] and published an isolation framework to protect operating system kernels, primarily the monolithic Linux kernel whose drivers they say get ~80,000 commits per year.[31][32]
Elm is a domain-specific programming language for declaratively creating web browser-based graphical user interfaces. Elm is purely functional, and is developed with emphasis on usability, performance, and robustness. It advertises "no runtime exceptions in practice",[10] made possible by the Elm compiler's static type checking.
Elm was initially designed by Evan Czaplicki as his thesis in 2012.[11] The first release of Elm came with many examples and an online editor that made it easy to try out in a web browser.[12] Mr. Czaplicki joined Prezi in 2013 to work on Elm,[13] and in 2016 moved to NoRedInk as an Open Source Engineer, also starting the Elm Software Foundation.[14]
All values in Elm are immutable, meaning that a value cannot be modified after it is created. Elm uses persistent data structures to implement its arrays, sets, and dictionaries in the standard library.[23]
Types include primitives like integers and strings, and basic data structures such as lists, tuples, and records. Functions have types written with arrows, for example round : Float -> Int. Custom types allow the programmer to create custom types to represent data in a way that matches the problem domain.[24]
The Elm Architecture is a software design pattern and as a TLA called TEA pattern for building interactive web applications. Elm applications are naturally constructed in that way, but other projects may find the concept useful.
This means that, for example, Elm does not have a generic map function which works across multiple data structures such as List and Set. In Elm, such functions are typically invoked qualified by their module name, for example calling List.map and Set.map. In Haskell or PureScript, there would be only one function map. This is a known feature request that is on Mr. Czaplicki's rough roadmap since at least 2015.[33] On the other hand, implementations of TEA pattern in advanced languages like Scala does not suffer from such limitations and can benefit from Scala's type classes, type-level and kind-level programming constructs.[34]
Functional reactive programming (FRP) is a programming paradigm for reactive programming (asynchronous dataflow programming) using the building blocks of functional programming (e.g., map, reduce, filter). FRP has been used for programming graphical user interfaces (GUIs), robotics, games, and music, aiming to simplify these problems by explicitly modeling time.[citation needed]
The original formulation of functional reactive programming can be found in the ICFP 97 paper Functional Reactive Animation by Conal Elliott and Paul Hudak.[1]
This semantic model of FRP in side-effect free languages is typically in terms of continuous functions, and typically over time.[4] This formulation is also referred to as denotative continuous time programming (DCTP).[5]
ReactiveX, popularized by its JavaScript implementation rxjs, is functional and reactive but differs from functional reactive programming.[19]
Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.
Depending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations  such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.
Inductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases.
Output of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language.
In many applications the output program must be correct with respect to the examples and partial specification,  and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis,[1][2] usually opposed to 'deductive' program synthesis,[3][4][5] where the specification is usually complete.
In other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples.
The diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages.
The advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro[9] eventually spawning the new field of inductive logic programming (ILP).[10] The early works of Plotkin,[11][12] and his "relative least general generalization (rlgg)", had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM.[13] But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs[14][15][16] with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery.[17]
In parallel to work in ILP, Koza[18] proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE[19] and the systematic-search-based system MagicHaskeller.[20] Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.
The early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem.[21] The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold.[22] More recently, the language learning problem was addressed by the inductive programming community.[23][24]
In the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below).
One powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming).[30][31][29][32]
The first workshop on Approaches and Applications of Inductive Programming (AAIP)  held in conjunction with ICML 2005 identified all applications where "learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations".
Since then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming,[33] the related areas of programming by example[34] and programming by demonstration,[35] and intelligent tutoring systems.
Agent-oriented programming allows the developer to build, extend and use software agents, which are abstractions of objects that can message other agents.
Array programming (also termed vector or multidimensional) languages generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays.
Aspect-oriented programming enables developers to add new functionality to code, known as "advice", without modifying that code itself; rather, it uses a pointcut to implement the advice into code blocks.
An authoring language is a programming language designed for use by a non-computer expert to easily create tutorials, websites, and other interactive computer programs.
A concatenative programming language is a point-free computer programming language in which all expressions denote functions, and the juxtaposition of expressions denotes function composition.[4] Concatenative programming replaces function application, which is common in other programming styles, with function composition as the default way to build subroutines.
A constraint programming language is a declarative programming language where relationships between variables are expressed as constraints. Execution proceeds by attempting to find values for the variables which satisfy all declared constraints.
Dataflow programming languages rely on a (usually visual) representation of the flow of data to specify the program.  Frequently used for reacting to discrete events or for processing streams of data.  Examples of dataflow languages include:
Declarative languages express the logic of a computation without describing its control flow in detail. Declarative programming stands in contrast to imperative programming via imperative programming languages, where control flow is specified by serial orders (imperatives). (Pure) functional and logic-based programming languages are also declarative, and constitute the major subcategories of the declarative category. This section lists additional examples not in those subcategories.
An esoteric programming language is a programming language designed as a test of the boundaries of computer programming language design, as a proof of concept, or as a joke.
Extension programming languages are languages embedded into another program and used to harness its features in extension scripts.
Fourth-generation programming languages are high-level programming languages built around database systems. They are generally used in commercial environments.
Functional programming languages define programs and subroutines as mathematical functions and treat them as first-class. Many so-called functional languages are "impure", containing imperative features. Many functional languages are tied to mathematical calculation tools. Functional languages include:
Imperative programming languages may be multi-paradigm and appear in other classifications. Here is a list of programming languages that follow the imperative paradigm:
Interpreted languages are programming languages in which programs may be executed from source code form, by an interpreter. Theoretically, any language can be compiled or interpreted, so the term interpreted language generally refers to languages that are usually interpreted rather than compiled.
Some programming languages without the inherent ability to manually manage memory, like Cython,[27] Swift,[c] and Scala[28] (Scala Native only), are able to import or call functions like malloc and free from C through a foreign function interface.
List-based languages are a type of data-structured language that are based on the list data structure.
Metaprogramming is the writing of programs that write or manipulate other programs, including themselves, as their data or that do part of the work that is otherwise done at run time during compile time. In many cases, this allows programmers to get more done in the same amount of time as they would take to write all the code manually.
Multiparadigm languages support more than one programming paradigm. They allow a program to use more than one programming style. The goal is to allow programmers to use the best tool for a job, admitting that no one paradigm solves all problems in the easiest or most efficient way.
Several general-purpose programming languages, such as C and Python, are also used for technical computing, this list focuses on languages almost exclusively used for technical computing.
Class-based object-oriented programming languages support objects defined by their class. Class definitions include member data. Message passing is a key concept, if not the main concept, in object-oriented languages.
Procedural programming languages are based on the concept of the unit and scope (the data viewing range) of an executable code statement. A procedural program is composed of one or more units or modules, either user coded or provided in a code library; each module is composed of one or more procedures, also called a function, routine, subroutine, or method, depending on the language. Examples of procedural languages include:
Reflective programming languages let programs examine and possibly modify their high-level structure at runtime or compile-time. This is most common in high-level virtual machine programming languages like Smalltalk, and less common in lower-level programming languages like C. Languages and platforms supporting reflection:
Stack-based languages are a type of data-structured language that are based on the stack data structure.
Synchronous programming languages are optimized for programming reactive systems, systems that are often interrupted and must respond quickly. Many such systems are also called realtime systems, and are used often in embedded systems.
A shading language is a graphics programming language adapted to programming shader effects. Such language forms usually consist of special data types, like "color" and "normal". Due to the variety of target markets for 3D computer graphics.
They provide both higher hardware abstraction and a more flexible programming model than previous paradigms which hardcoded transformation and shading equations. This gives the programmer greater control over the rendering process and delivers richer content at lower overhead.
The system programming languages are for low-level tasks like memory management or task management. A system programming language usually refers to a programming language used for system programming; such languages are designed for writing system software, which usually requires different development approaches when compared with application software.
System software is computer software designed to operate and control the computer hardware, and to provide a platform for running application software. System software includes software categories such as operating systems, utility software, device drivers, compilers, and linkers. Examples of system languages include:
Visual programming languages let users specify programs in a two-(or more)-dimensional way, instead of as one-dimensional text strings, via graphic layouts of various types. Some dataflow programming languages are also visual languages.
In computer science, dynamic software updating (DSU) is a field of research pertaining to upgrading programs while they are running. DSU is not currently widely used in industry. However, researchers have developed a wide variety of systems and techniques for implementing DSU. These systems are commonly tested on real-world programs.
Current operating systems and programming languages are typically not designed with DSU in mind. As such, DSU implementations commonly either utilize existing tools, or implement specialty compilers. These compilers preserve the semantics of the original program, but instrument either the source code or object code to produce a dynamically updateable program. Researchers compare DSU-capable variants of programs to the original program to assess safety and performance overhead.
Any running program can be thought of a tuple 



(
δ
,
P
)


{\displaystyle (\delta ,P)}

, where 



δ


{\displaystyle \delta }

 is the current program state and 



P


{\displaystyle P}

 is the current program code. Dynamic software updating systems transform a running program 



(
δ
,
P
)


{\displaystyle (\delta ,P)}

 to a new version 



(

δ
′

,

P
′

)


{\displaystyle (\delta ',P')}

. In order to do this, the state must be transformed into the representation 




P
′



{\displaystyle P'}

 expects. This requires a state transformer function. Thus, DSU transforms a program 



(
δ
,
P
)


{\displaystyle (\delta ,P)}

 to 



(
S
(
δ
)
,

P
′

)


{\displaystyle (S(\delta ),P')}

. An update is considered valid if and only if the running program 



(
S
(
δ
)
,

P
′

)


{\displaystyle (S(\delta ),P')}

 can be reduced to a point tuple 



(
δ
,

P
′

)


{\displaystyle (\delta ,P')}

 that is reachable from the starting point of the new version of the program, 



(

δ

i
n
i
t


,

P
′

)


{\displaystyle (\delta _{init},P')}

.[1]
Updating systems differ in the types of program changes that they support. For example, Ksplice only supports code changes in functions, and does not support changes to state representation. This is because Ksplice primarily targets security changes, rather than general updates. In contrast, Ekiden can update a program to any other program capable of being executed, even one written in a different programming language. Systems designers can extract valuable performance or safety assurances by limiting the scope of updates. For example, any update safety check limits the scope of updates to updates which pass that safety check. The mechanism used to transform code and state influences what kinds of updates a system will support.
The problem space addressed by dynamic updating can be thought of as an intersection of several others. Examples include checkpointing, dynamic linking, and persistence. As an example, a database that must be backward-compatible with previous versions of its on-disk file format, must accomplish the same type of state transformation expected of a dynamic updating system. Likewise, a program that has a plugin architecture, must be able to load and execute new code at runtime.
The earliest precursor to dynamic software updating is redundant systems. In a redundant environment, spare systems exist ready to take control of active computations in the event of a failure of the main system. These systems contain a main machine and a hot spare. The hot spare would be periodically seeded with a checkpoint of the primary system. In the event of a failure, the hot spare would take over, and the main machine would become the new hot spare. This pattern can be generalized to updating. In the event of an update, the hot spare would activate, the main system would update, and then the updated system would resume control.
The earliest true Dynamic Software Updating system is DYMOS (Dynamic Modification System).[4] Presented in 1983 in the PhD dissertation of Insup Lee, DYMOS was a fully integrated system that had access to an interactive user interface, a compiler and runtime for a Modula variant, and source code. This enabled DYMOS to type-check updates against the existing program.
If the target program is implemented in a virtual machine language, the VM can use existing infrastructure to load new code, since modern virtual machines support runtime loading for other use cases besides DSU (mainly debugging). The HotSpot JVM supports runtime code loading, and DSU systems targeting Java (programming language) can utilize this feature.
Ksplice is a DSU system that targets only the Linux kernel, making itself one of the specialized DSU systems that support an operating system kernel as the target program.  Ksplice uses source-level diffs to determine changes between current and updated versions of the Linux kernel, and then uses binary rewriting to insert the changes into the running kernel.[10] Ksplice was maintained by a commercial venture founded by its original authors, Ksplice Inc., which was acquired by Oracle Corporation in July 2011.[11] Ksplice is used on a commercial basis and exclusively in the Oracle Linux distribution.[12]
Erlang supports Dynamic Software Updating, though this is commonly referred to as "hot code loading". Erlang requires no safety guarantees on updates, but Erlang culture suggests that developers write in a defensive style that will gracefully handle type errors generated by updating.[citation needed]
Microsoft is utilizing internal patching technology for Microsoft Visual C++ that supports patching individual C++ functions while maintaining functional correctness of patches. Currently known applications is SQL Server in Azure SQL Database.[23]
Interactive programming is the procedure of writing parts of a program while it is already active. This focuses on the program text as the main interface for a running process, rather than an interactive application, where the program is designed in development cycles and used thereafter (usually by a so-called "user", in distinction to the "developer"). Consequently, here, the activity of writing a program becomes part of the program itself.
It thus forms a specific instance of interactive computation as an extreme opposite to batch processing, where neither writing the program nor its use happens in an interactive way. The principle of rapid feedback in extreme programming is radicalized and becomes more explicit.
Interactive programming techniques are especially useful in cases where no clear specification of the problem that is to be solved can be given in advance. In such situations (which are not unusual in research), the formal language provides the necessary environment for the development of an appropriate question or problem formulation.
Interactive programming has also been used in applications that need to be rewritten without stopping them, a feature which the computer language Smalltalk is famous for. Generally, dynamic programming languages provide the environment for such an interaction, so that typically prototyping and iterative and incremental development is done while other parts of the program are running.
As this feature is an apparent need in sound design and algorithmic composition, it has evolved significantly there. More recently, researchers have been using this method to develop sonification algorithms.
Using dynamic programming languages for sound and graphics, interactive programming is also used as an improvisational performance style live coding, mainly in algorithmic music and video.
Extempore is a live coding environment focused on real-time audiovisual software development. It is designed to accommodate the demands of cyber-physical computing.[2] Extempore consists of two integrated languages, Scheme (with extensions) and Extempore Language. It uses the LLVM cross-language compiler to achieve performant digital signal processing and related low-level features, on-the-fly.[citation needed]
In computer programming, a statement is a syntactic unit of an imperative programming language that expresses some action to be carried out.[1] A program written in such a language is formed by a sequence of one or more statements. A statement may have internal components (e.g. expressions).
Many programming languages (e.g. Ada, Algol 60, C, Java, Pascal) make a distinction between statements and definitions/declarations. A definition or declaration specifies the data on which a program is to operate, while a statement specifies the actions to be taken with that data.
Many compound statements are loop commands or choice commands. In theory only one of each of these types of commands is required. In practice there are various special cases which occur quite often; these may make a program easier to understand, may make programming easier, and can often be implemented much more efficiently.  There are many subtleties not mentioned here; see the linked articles for details.
Some programming language grammars reserve keywords or mark them specially, and do not allow them to be used as identifiers. This often leads to grammars which are easier to parse, requiring less lookahead.
In Algol 60 and Algol 68, special tokens were distinguished explicitly: for publication, in boldface e.g. begin; for programming, with some special marking, e.g., a flag ('begin), quotation marks ('begin'), or underlined (begin on the Elliott 503). This is called "stropping".
Certain names are reserved as part of the programming language and can not be used as programmer-defined names.
The majority of the most popular programming languages use reserved keywords. Early examples include FLOW-MATIC (1953) and COBOL (1959). Since 1970 other examples include Ada, C, C++, Java, and Pascal.  The number of reserved words depends on the language: C has about 30 while COBOL has about 400.
Semantics is concerned with the meaning of a program. The standards documents for many programming languages use BNF or some equivalent to express the syntax/grammar in a fairly formal and precise way, but the semantics/meaning of the program is generally described using examples and English prose.  This can result in ambiguity.[8]  In some language descriptions the meaning of compound statements is defined by the use of 'simpler' constructions, e.g. a while loop can be defined by a combination of tests, jumps, and labels, using if and goto.
In most programming languages, a statement can consist of little more than an expression, usually by following the expression with a statement terminator (semicolon). In such a case, while the expression evaluates to a value, the complete statement does not (the expression's value is discarded). For instance, in C, C++, C#, and many similar languages, x = y + 1 is an expression that will set x to the value of y plus one, and the whole expression itself will evaluate to the same value that x is set to. However, x = y + 1; (note the semicolon at the end) is a statement that will still set x to the value of y plus one because the expression within the statement is still evaluated, but the result of the expression is discarded, and the statement itself does not evaluate to any value.[9]
In computer programming, a sigil (/ˈsɪdʒəl/) is a symbol affixed to a variable name, showing the variable's datatype or scope, usually a prefix, as in $foo, where $ is the sigil.
The use of sigils was popularized by the BASIC programming language. The best known example of a sigil in BASIC is the dollar sign ("$") appended to the names of all strings. Many BASIC dialects use other sigils (like "%") to denote integers and floating-point numbers and their precision, and sometimes other types as well.
Larry Wall adopted shell scripting's use of sigils for his Perl programming language.[citation needed] In Perl, the sigils do not specify fine-grained data types like strings and integers, but the more general categories of scalars (using a prefixed "$"), arrays (using "@"), hashes (using "%"), and subroutines (using "&"). Raku also uses secondary sigils, or twigils,[2] to indicate the scope of variables. Prominent examples of twigils in Raku include "^" (caret), used with self-declared formal parameters ("placeholder variables"), and ".", used with object attribute accessors (i.e., instance variables).
In the MUMPS programming language, "$" precedes intrinsic function names and "special variable names" (built-in variables for accessing the execution state). "$Z" precedes non-standard intrinsic function names. "$$" precedes extrinsic function names. Routines (used for procedures, subroutines, functions) and global variables (database storage) are prefixed by a caret (^). The last global variable subtree may be referenced indirectly by a caret and the last subscript; this is referred to as a "naked reference".  System-wide routines and global variables (stored in certain shared database(s)) are prefixed with ^%; these are referred to as "percent routines" and "percent globals".
Various programming languages including Prolog, Haskell, Ruby and Go treat identifiers beginning with a capital letter differently from identifiers beginning with a small letter, a practice related to the use of sigils.
In the C programming language, struct is the keyword used to define a composite, a.k.a. record, data type – a named set of values that occupy a block of memory. It allows for the different values to be accessed via a single identifier, often a pointer. A struct can contain other data types so is used for mixed-data-type records. For example a bank customer struct might contains fields: name, address, telephone, balance.
Via the keyword typedef, a struct type can be referenced without using the struct keyword. However, some[who?] programming style guides advise against this, claiming that it can obfuscate the type.
The C-family programming languages share significant features of the C programming language. Many of these 70 languages were developmentally influenced by C due to its success and ubiquity. The family also includes predecessors that influenced C's design such as BCPL.
Notable programming sources use terms like C-style, C-like, a dialect of C, having C-like syntax. The term curly bracket programming language denotes a language that shares C's block syntax.[1][2]
C-family languages span multiple programming paradigms, conceptual models, and run-time environments.
Limbo is a programming language for writing distributed systems and is the language used to write applications for the Inferno operating system. It was designed at Bell Labs by Sean Dorward, Phil Winterbottom, and Rob Pike.[1]
The 3rd edition of the Inferno operating system and Limbo programming language are described in the textbook Inferno Programming with Limbo ISBN 0-470-84352-7 (Chichester: John Wiley & Sons, 2003), by Phillip Stanley-Marbell.  Another textbook The Inferno Programming Book: An Introduction to Programming for the Inferno Distributed System, by  Martin Atkins, Charles Forsyth, Rob Pike and Howard Trickey, was started, but never released.
LPMud, abbreviated LP, is a family of multi-user dungeon (MUD) server software. Its first instance, the original LPMud game driver, was developed in 1989 by Lars Pensjö (the LP in LPMud).[1][2][3] LPMud was innovative in its separation of the MUD infrastructure into a virtual machine (termed the driver) and a development framework written in the programming language LPC (termed the mudlib).[4]
Lars Pensjö had been an avid player of TinyMUD and AberMUD. He had wanted to create a world with the flexibility of TinyMUD and the style of AberMUD[5] but did not want to have sole responsibility for creating and maintaining the game world. He once said, "I didn't think I would be able to design a good adventure. By allowing wizards coding rights, I thought others could help me with this."[6]  The result was the creation of a new, C-based, object-oriented programming language, LPC, that made it simple for people with minimal programming skills to add elements like rooms, weapons, and monsters to a virtual world.[7]
To accomplish his goal, Lennart Augustsson convinced Pensjö to write what today would be called a virtual machine, the LPMud driver. The driver managed the interpretation of LPC code as well as providing basic operating system services to the LPC code. By virtue of this design, Pensjö made it more difficult for common programming errors like infinite loops and infinite recursion made by content builders to harm the overall stability of the server. His choice of an OO approach made it easy for new programmers to concentrate on the task of "building a room" rather than programming logic.[3]
In 1993, the TMI-2 mudlib was used to create PangaeaMud, an academic research project designed as an interactive geologic database tool.[21]
MudOS is a major family of LPMud server software, implementing its own variant of the LPC (programming language).[28][29]  It first came into being on February 18, 1992.[30]  It pioneered important technical innovations in MUDs, including the network socket support that made InterMUD communications possible[6][31] and LPC-to-C compilation.[32]
A system programming language is a programming language used for system programming; such languages are designed for writing system software, which usually requires different development approaches when compared with application software. Edsger Dijkstra referred to these languages as machine oriented high order languages, or mohol.[1]
General-purpose programming languages tend to focus on generic features to allow programs written in the language to use the same code on different platforms. Examples of such languages include ALGOL and Pascal. This generic quality typically comes at the cost of denying direct access to the machine's internal workings, and this often has negative effects on performance.
System languages, in contrast, are designed not for compatibility, but for performance and ease of access to the underlying hardware while still providing high-level programming concepts like structured programming. Examples include ESPOL and SPL, both of which are similar to ALGOL in syntax but tuned to their respective platforms. Others are cross-platform but designed to work close to the hardware, like BLISS, JOVIAL and BCPL.
Some languages straddle the system and application domains, bridging the gap between these uses. The canonical example is C, which is used widely for both system and application programming. Some modern languages also do this such as Rust and Swift.
In contrast with application languages, system programming languages typically offer more-direct access to the physical hardware of the machine: an archetypical system programming language in this sense was BCPL. System programming languages often lack built-in input/output (I/O) facilities because a system-software project usually develops its own I/O mechanisms or builds on basic monitor I/O or screen management facilities. The distinction between languages used for system programming and application programming became blurred over time with the widespread popularity of PL/I, C and Pascal.
The earliest system software was written in assembly language primarily because there was no alternative, but also for reasons including efficiency of object code, compilation time, and ease of debugging. Application languages such as FORTRAN were used for system programming, although they usually still required some routines to be written in assembly language.[2]
While PL360 is at the semantic level of assembly language, another kind of system programming language operates at a higher semantic level, but has specific extensions designed to make the language suitable for system programming. An early example of this kind of language is LRLTRAN,[4] which extended Fortran with features for character and bit manipulation, pointers, and directly addressed jump tables.
Subsequently, languages such as C were developed, where the combination of features was sufficient to write system software, and a compiler could be developed that generated efficient object programs on modest hardware. Such a language generally omits features that cannot be implemented efficiently, and adds a small number of machine-dependent features needed to access specific hardware abilities; inline assembly code, such as C's asm statement, is often used for this purpose. Although many such languages were developed,[1] C and C++ are the ones which survived.
Lint is the computer science term for a static code analysis tool used to flag programming errors, bugs, stylistic errors and suspicious constructs.[1] The term originates from a Unix utility that examined C language source code.[2]  A program which performs this function is also known as a "linter".
Stephen C. Johnson, a computer scientist at Bell Labs, came up with the term "lint" in 1978 while debugging the yacc grammar he was writing for C and dealing with portability issues stemming from porting Unix to a 32-bit machine.[5][2] The term was borrowed from the word lint, the tiny bits of fiber and fluff shed by clothing, as the command he wrote would act like a lint trap in a clothes dryer, capturing waste fibers while leaving whole fabrics intact. In 1979, lint programming was used outside of Bell Labs for the first time, in the seventh version (V7) of Unix.
In his original 1978 paper Johnson stated his reasoning in creating a separate program to detect errors, distinct from that which it analyzed: "...the general notion of having two programs is a good one" [because they concentrate on different things, thereby allowing the programmer to] "concentrate at one stage of the programming process solely on the algorithms, data structures, and correctness of the program, and then later retrofit, with the aid of lint, the desirable properties of universality and portability".[2]
Lint-like tools have also been developed for other aspects of software development, such as enforcing grammar and style guides for given language source code.[9] Some tools (such as ESLint) also allow rules to be auto-fixable: a rule definition can also come with the definition of a transform that resolves the warning. Rules about style are especially likely to come with an auto-fix. If the linter is run in "fix all" mode on a file that triggers only rules about formatting, the linter will act just like a formatter.
Windows uses specific language and territory strings.
The locale identifier (LCID) for unmanaged code on Microsoft Windows is a number such as 1033 for English (United States), or 2057 for English (United Kingdom), or 1041 for Japanese (Japan). These numbers consist of a language code (lower 10 bits) and a culture code (upper bits), and are therefore often written in hexadecimal notation, such as 0x0409, 0x0809 or 0x0411.
Microsoft is starting to introduce managed code application programming interfaces (APIs) for .NET that use this format. One of the first to be generally released is a function to mitigate issues with internationalized domain names,[4] but more are in Windows Vista Beta 1.
In computer programming, a function (also procedure, method, subroutine, routine, or subprogram) is a callable unit[1] of software logic that has a well-defined interface and behavior and can be invoked multiple times.
Callable units provide a powerful programming tool.[2] The primary purpose is to allow for the decomposition of a large and/or complicated problem into chunks that have relatively low cognitive load and to assign the chunks meaningful names (unless they are anonymous). Judicious application can reduce the cost of developing and maintaining software, while increasing its quality and reliability.[3]
Callable units are present at multiple levels of abstraction in the programming environment. For example, a programmer may write a function in source code that is compiled to machine code that implements similar semantics. There is a callable unit in the source code and an associated one in the machine code, but they are different kinds of callable units – with different implications and features.
Some programming languages, such as COBOL and BASIC, make a distinction between functions that return a value (typically called "functions") and those that do not (typically called "subprogram", "subroutine", or "procedure"). Other programming languages, such as C, C++, and Rust, only use the term "function" irrespective of whether they return a value or not. Some object-oriented languages, such as Java and C#, refer to functions inside classes as "methods".
Kay McNulty had worked closely with John Mauchly on the ENIAC team and developed an idea for subroutines for the ENIAC computer she was programming during World War II.[11] She and the other ENIAC programmers used the subroutines to help calculate missile trajectories.[11]
One of the first programming languages to support user-written subroutines and functions was FORTRAN II.  The IBM FORTRAN II compiler was released in 1958.  ALGOL 58 and other early programming languages also supported procedural programming.
Most modern implementations of a function call use a call stack, a special case of the stack data structure, to implement function calls and returns. Each procedure call creates a new entry, called a stack frame, at the top of the stack; when the procedure returns, its stack frame is deleted from the stack, and its space may be used for other procedure calls. Each stack frame contains the private data of the corresponding call, which typically includes the procedure's parameters and internal variables, and the return address.
In a multi-threaded environment, there is generally more than one stack.[17] An environment that fully supports coroutines or lazy evaluation may use data structures other than stacks to store their activation records.
Most modern programming languages provide features to define and call functions, including syntax for accessing such features, including:
Most implementations, especially in modern languages, support parameters which the callable declares as formal parameters. A caller passes actual parameters, a.k.a. arguments, to match. Different programming languages provide different conventions for passing arguments.
In strictly functional programming languages such as Haskell, a function can have no side effects, which means it cannot change the state of the program. Functions always return the same result for the same input. Such languages typically only support functions that return a value, since there is no value in a function that has neither return value nor side effect.
If supported by the language, a callable may call itself, causing its execution to suspend while another nested execution of the same callable executes. Recursion is a useful means to simplify some complex algorithms and break down complex problems. Recursive languages provide a new copy of local variables on each call. If the programmer desires the recursive callable to use the same variables instead of using locals, they typically declare them in a shared context such static or global.
Recursion allows direct implementation of functionality defined by mathematical induction and recursive divide and conquer algorithms. Here is an example of a recursive function in C/C++ to find Fibonacci numbers:
Some languages support overloading – allow multiple callables with the same name in the same scope, but operating on different types of input. Consider the square root function applied to real number, complex number and matrix input. The algorithm for each type of input is different, and the return value may have a different type. By writing three separate callables with the same name. i.e. sqrt, the resulting code may be easier to write and to maintain since each one has a name that is relatively easy to understand and to remember instead of giving longer and more complicated names like sqrt_real, sqrt_complex, qrt_matrix.
A closure is a callable plus values of some of its variables captured from the environment in which it was created. Closures were a notable feature of the Lisp programming language, introduced by John McCarthy. Depending on the implementation, closures can serve as a mechanism for side-effects.
Some optimizations for minimizing call overhead may seem straight forward, but cannot be used if the callable has side effects. For example, in the expression (f(x)-1)/(f(x)+1), the function f cannot be called only once with its value used two times since the two calls may return different results. Moreover, in the few languages which define the order of evaluation of the division operator's operands, the value of x must be fetched again before the second call, since the first call may have changed it. Determining whether a callable has a side effect is difficult – indeed, undecidable by virtue of Rice's theorem. So, while this optimization is safe in a purely functional programming language, a compiler for an language not limited to functional typically assumes the worst case, that every callable may have side effects.
A built-in function, or builtin function, or intrinsic function, is a function for which the compiler generates code at compile time or provides in a way other than for other functions.[23] A built-in function does not need to be defined like other functions since it is built in to the programming language.[24]
Some programmers suggest that a callable should perform exactly one task, and if it performs more than one task, it should be split up into multiple callables. They argue that callables are key components in software maintenance, and their roles in the program must remain distinct.
Proponents of modular programming advocate that each callable should have minimal dependency on the rest of the codebase. For example, the use of global variables is generally deemed unwise, because it adds coupling between all callables that use the global variables. If such coupling is not necessary, they advise to refactor callables to accept passed parameters instead.
Notice that the motherhood function, X = mother(Y) is represented by a relation, as in a relational database. However, relations in Prolog function as callable units.
In computer programming, an operator is a programming language construct that provides functionality that may not be possible to define as a user-defined function (i.e. sizeof in C) or has syntax different than a function (i.e. infix addition as in a+b). Like other programming language concepts, operator has a generally accepted, although debatable meaning among practitioners while at the same time each language gives it specific meaning in that context, and therefore the meaning varies by language.
In general, an operator may be prefix, infix, postfix, matchfix, circumfix or bifix,[1][2][3][4][5] and the syntax of an expression involving an operator depends on its arity (number of operands), precedence, and (if applicable), associativity. Most programming languages support binary operators and a few unary operators, with a few supporting more operands, such as the ?: operator in C, which is ternary. There are prefix unary operators, such as unary minus -x, and postfix unary operators, such as post-increment x++; and binary operations are infix, such as x + y or x = y. Infix operations of higher arity require additional symbols, such as the ternary operator ?: in C, written as a ? b : c – indeed, since this is the only common example, it is often referred to as the ternary operator. Prefix and postfix operations can support any desired arity, however, such as 1 2 3 4 +.
Some languages (e.g. C, C++ and PHP) define a fixed set of operators, while others (e.g. Prolog,[6] Seed7,[7] F#, OCaml, Haskell) allow for user-defined operators. Some programming languages restrict operator symbols to special characters like + or := while others allow names like div (e.g. Pascal).
If a language does allow for defining new operators, the mechanics of doing so may involve meta-programming – specifying the operator in a separate language.
In mathematics and computer programming, the order of operations  is a collection of rules that reflect conventions about which operations to perform first in order to evaluate a given mathematical expression.
These rules are formalized with a ranking of the operations. The rank of an operation is called its precedence, and an operation with a higher precedence is performed before operations with lower precedence. Calculators generally perform operations with the same precedence from left to right,[1] but some programming languages and calculators adopt different conventions.
The order of operations, that is, the order in which the operations in an expression are usually performed, results from a convention adopted  throughout mathematics, science, technology and many computer programming languages. It is summarized as:[2][5]
The radical symbol ⁠








t







{\displaystyle {\sqrt {\vphantom {t}}}}

⁠ is traditionally extended by a bar (called vinculum) over the radicand (this avoids the need for parentheses around the radicand). Other functions use parentheses around the input to avoid ambiguity.[6][7][a] The parentheses can be omitted if the input is a single numerical variable or constant,[2] as in the case of sin x = sin(x) and sin π = sin(π).[a]  Traditionally this convention extends to monomials; thus, sin 3x = sin(3x) and even sin ⁠1/2⁠xy = sin(xy/2), but sin x + y = sin(x) + y, because x + y is not a monomial. However, this convention is not universally understood, and some authors prefer explicit parentheses.[b] Some calculators and programming languages require parentheses around function inputs, some do not.
In some applications and programming languages, notably Microsoft Excel, PlanMaker (and other spreadsheet applications) and the programming language bc, unary operations have a higher priority than binary operations, that is, the unary minus has higher precedence than exponentiation, so in those languages −32 will be interpreted as (−3)2 = 9.[9] This does not apply to the binary minus operation '−'; for example in Microsoft Excel while the formulas =-2^2, =(-2)^2 and =0+-2^2 return 4, the formulas =0-2^2 and =-(2^2) return −4.
However, when exponentiation is represented by an explicit symbol such as a caret (^) or arrow (↑), there is no common standard. For example, Microsoft Excel and computation programming language MATLAB evaluate a^b^c as (ab)c, but Google Search and Wolfram Alpha as a(bc). Thus 4^3^2 is evaluated to 4,096 in the first case and to 262,144 in the second case.
Most programming languages use precedence levels that conform to the order commonly used in mathematics,[41] though others, such as APL, Smalltalk, Occam and Mary, have no operator precedence rules (in APL, evaluation is strictly right to left; in Smalltalk, it is strictly left to right).
Dennis Ritchie, creator of the C language, said of the precedence in C (shared by programming languages that borrow those rules from C, for example, C++, Perl and PHP) that it would have been preferable to move the bitwise operators above the comparison operators.[42] Many programmers have become accustomed to this order, but more recent popular languages like Python[43] and Ruby[44] do have this order reversed. The relative precedence levels of operators found in many C-style languages are as follows:
The accuracy of software developer knowledge about binary operator precedence has been found to closely follow their frequency of occurrence in source code.[46]
In computer programming, a variable is an abstract storage location paired with an associated symbolic name, which contains some known or unknown quantity of data or object referred to as a value; or in simpler terms, a variable is a named container for a particular set of bits or type of data (like integer, float, string, etc...).[1] A variable can eventually be associated with or identified by a memory address. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may thus change during the course of program execution.[2][3][4][5]
Variables in programming may not directly correspond to the concept of variables in mathematics. The latter is abstract, having no reference to a physical object such as storage location. The value of a computing variable is not necessarily part of an equation or formula as in mathematics. Variables in computer programming are frequently given long names to make them relatively descriptive of their use, whereas variables in mathematics often have terse, one- or two-character names for brevity in transcription and manipulation.
In imperative programming languages, values can generally be accessed or changed at any time. In pure functional and logic languages, variables are bound to expressions and keep a single value during their entire lifetime due to the requirements of referential transparency. In imperative languages, the same behavior is exhibited by (named) constants (symbolic constants), which are typically contrasted with (normal) variables.
Depending on the type system of a programming language, variables may only be able to store a specified data type (e.g. integer or string). Alternatively, a datatype may be associated only with the current value, allowing a single variable to store anything supported by the programming language. Variables are the containers for storing the values.
Unless the programming language features garbage collection, a variable whose extent permanently outlasts its scope can result in a memory leak, whereby the memory allocated for the variable can never be freed since the variable which would be used to reference it for deallocation purposes is no longer accessible. However, it can be permissible for a variable binding to extend beyond its scope, as occurs in Lisp closures and C static local variables; when execution passes back into the variable's scope, the variable may once again be used. A variable whose scope begins before its extent does is said to be uninitialized and often has an undefined, arbitrary value if accessed (see wild pointer), since it has yet to be explicitly given a particular value. A variable whose extent ends before its scope may become a dangling pointer and deemed uninitialized once more since its value has been destroyed. Variables described by the previous two cases may be said to be out of extent or unbound. In many languages, it is an error to try to use the value of a variable when it is out of extent. In other languages, doing so may yield unpredictable results. Such a variable may, however, be assigned a new value, which gives it a new extent.
It is considered good programming practice to make the scope of variables as narrow as feasible so that different parts of a program do not accidentally interact with each other by modifying each other's variables. Doing so also prevents action at a distance. Common techniques for doing so are to have different sections of a program use different name spaces, or to make individual variables "private" through either dynamic variable scoping or lexical variable scoping.
Many programming languages employ a reserved value (often named null or nil) to indicate an invalid or uninitialized variable.
Variables often store simple data, like integers and literal strings, but some programming languages allow a variable to store values of other datatypes as well. Such languages may also enable functions to be parametric polymorphic. These functions operate like variables to represent data of multiple types. For example, a function named length may determine the length of a list. Such a length function may be parametric polymorphic by including a type variable in its type signature, since the number of elements in the list is independent of the elements' types.
The specifics of variable allocation and the representation of their values vary widely, both among programming languages and among implementations of a given language. Many language implementations allocate space for local variables, whose extent lasts for a single function call on the call stack, and whose memory is automatically reclaimed when the function returns. More generally, in name binding, the name of a variable is bound to the address of some particular block (contiguous sequence) of bytes in memory, and operations on the variable manipulate that block. Referencing is more common for variables whose values have large or unknown sizes when the code is compiled. Such variables reference the location of the value instead of storing the value itself, which is allocated from a pool of memory called the heap.
When a variable refers to a data structure created dynamically, some of its components may be only indirectly accessed through the variable. In such circumstances, garbage collectors (or analogous program features in languages that lack garbage collectors) must deal with a case where only a portion of the memory reachable from the variable needs to be reclaimed.
Unlike their mathematical counterparts, programming variables and constants commonly take multiple-character names, e.g. COST or total. Single-character names are most commonly used only for auxiliary variables; for instance, i, j, k for array index variables.
Some naming conventions are enforced at the language level as part of the language syntax which involves the format of valid identifiers. In almost all languages, variable names cannot start with a digit (0–9) and cannot contain whitespace characters. Whether or not punctuation marks are permitted in variable names varies from language to language; many languages only permit the underscore ("_") in variable names and forbid all other punctuation. In some programming languages, sigils (symbols or punctuation) are affixed to variable identifiers to indicate the variable's datatype or scope.
Embedded software is computer software, written to control machines or devices that are not typically thought of as computers, commonly known as embedded systems. It is typically specialized for the particular hardware that it runs on and has time and memory constraints.[1] This term is sometimes used interchangeably with firmware.[2]
A precise and stable characteristic feature is that no or not all functions of embedded software are initiated/controlled via a human interface, but through machine-interfaces instead.[3]
Manufacturers build embedded software into the electronics of cars, telephones, modems, robots, appliances, toys, security systems, pacemakers, televisions and set-top boxes, and digital watches, for example.[4] This software can be very simple, such as lighting controls running on an 8-bit microcontroller with a few kilobytes of memory with the suitable level of processing complexity determined with a Probably Approximately Correct Computation framework[5] (a methodology based on randomized algorithms). However, embedded software can become very sophisticated in applications such as routers, optical network elements, airplanes, missiles, and process control systems.[6]
Unlike standard computers that generally use an operating systems such as macOS, Windows or Linux, embedded software may use no operating system. When they do use one, a wide variety of operating systems can be chosen from, typically a real-time operating system. Code for embedded software is typically written in C or C++, but various high-level programming languages, such as Java, Python and JavaScript, are now also in common use to target microcontrollers and embedded systems.[7] Assembly languages are often used too, especially in booting and interrupt handling. Ada is used in some military and aviation projects.
Most consumers are familiar with application software that provide functionality on a computer. However embedded software is often less visible, but no less complicated. Unlike application software, embedded software has fixed hardware requirements and capabilities, and addition of third-party hardware or software is strictly controlled.
Embedded software needs to include all needed device drivers at manufacturing time, and the device drivers are written for the various hardware devices. These device drivers, called BSP (Board support package), form the layer of software containing hardware-specific drivers and other routines that allow a particular operating system (traditionally a real-time operating system, or RTOS) to function in a particular hardware environment (a computer or CPU card), integrated with the RTOS itself. The software is highly dependent on the CPU and specific chips chosen. Most embedded software engineers have at least a passing knowledge of reading schematics, and reading data sheets for components to determine usage of registers and communication system. Conversion between decimal, hexadecimal and binary is useful as well as using bit manipulation.[8]
Web applications are often used for managing hardware, although XML files and other output may be passed to a computer for display. File systems with folders are typically used, however SQL databases are often absent.
Software development requires use of a cross compiler, which runs on a computer but produces executable code for the target device. Debugging requires use of an in-circuit emulator, and debugging hardware such as JTAG or SWD debuggers. Software developers often have access to the complete kernel (OS) source code.
Lint is the computer science term for a static code analysis tool used to flag programming errors, bugs, stylistic errors and suspicious constructs.[1] The term originates from a Unix utility that examined C language source code.[2]  A program which performs this function is also known as a "linter".
Stephen C. Johnson, a computer scientist at Bell Labs, came up with the term "lint" in 1978 while debugging the yacc grammar he was writing for C and dealing with portability issues stemming from porting Unix to a 32-bit machine.[5][2] The term was borrowed from the word lint, the tiny bits of fiber and fluff shed by clothing, as the command he wrote would act like a lint trap in a clothes dryer, capturing waste fibers while leaving whole fabrics intact. In 1979, lint programming was used outside of Bell Labs for the first time, in the seventh version (V7) of Unix.
In his original 1978 paper Johnson stated his reasoning in creating a separate program to detect errors, distinct from that which it analyzed: "...the general notion of having two programs is a good one" [because they concentrate on different things, thereby allowing the programmer to] "concentrate at one stage of the programming process solely on the algorithms, data structures, and correctness of the program, and then later retrofit, with the aid of lint, the desirable properties of universality and portability".[2]
Lint-like tools have also been developed for other aspects of software development, such as enforcing grammar and style guides for given language source code.[9] Some tools (such as ESLint) also allow rules to be auto-fixable: a rule definition can also come with the definition of a transform that resolves the warning. Rules about style are especially likely to come with an auto-fix. If the linter is run in "fix all" mode on a file that triggers only rules about formatting, the linter will act just like a formatter.
A race condition or race hazard is the condition of an electronics, software, or other system where the system's substantive behavior is dependent on the sequence or timing of other uncontrollable events, leading to unexpected or inconsistent results. It becomes a bug when one or more of the possible behaviors is undesirable.
Race conditions can occur especially in logic circuits or multithreaded or distributed software programs. Using mutual exclusion can prevent race conditions in distributed software systems.
A race condition can arise in software when a computer program has multiple code paths that are executing at the same time. If the multiple code paths take a different amount of time than expected, they can finish in a different order than expected, which can cause software bugs due to unanticipated behavior. A race can also occur between two programs, resulting in security issues.
Critical race conditions cause invalid execution and software bugs. Critical race conditions often happen when the processes or threads depend on some shared state. Operations upon shared states are done in critical sections that must be mutually exclusive. Failure to obey this rule can corrupt the shared state.
A race condition can be difficult to reproduce and debug because the end result is nondeterministic and depends on the relative timing between interfering threads. Problems of this nature can therefore disappear when running in debug mode, adding extra logging, or attaching a debugger. A bug that disappears like this during debugging attempts is often referred to as a "Heisenbug". It is therefore better to avoid race conditions by careful software design.
Many software race conditions have associated computer security implications. A race condition allows an attacker with access to a shared resource to cause other actors that utilize that resource to malfunction, resulting in effects including denial of service[13] and privilege escalation.[14][15]
A different form of race condition exists in file systems where unrelated programs may affect each other by suddenly using up available resources such as disk space, memory space, or processor cycles. Software not carefully designed to anticipate and handle this race situation may then become unpredictable. Such a risk may be overlooked for a long time in a system that seems very reliable. But eventually enough data may accumulate or enough other software may be added to critically destabilize many parts of a system. An example of this occurred with the near loss of the Mars Rover "Spirit" not long after landing, which occurred due to deleted file entries causing the file system library to consume all available memory space.[17] A solution is for software to request and reserve all the resources it will need before beginning a task; if this request fails then the task is postponed, avoiding the many points where failure could have occurred. Alternatively, each of those points can be equipped with error handling, or the success of the entire task can be verified afterwards, before continuing. A more common approach is to simply verify that enough system resources are available before starting a task; however, this may not be adequate because in complex systems the actions of other running programs can be unpredictable.
Software flaws in life-critical systems can be disastrous. Race conditions were among the flaws in the Therac-25 radiation therapy machine, which led to the death of at least three patients and injuries to several more.[18]
Another example is the energy management system provided by GE Energy and used by Ohio-based FirstEnergy Corp (among other power facilities). A race condition existed in the alarm subsystem; when three sagging power lines were tripped simultaneously, the condition prevented alerts from being raised to the monitoring technicians, delaying their awareness of the problem. This software flaw eventually led to the North American Blackout of 2003.[19] GE Energy later developed a software patch to correct the previously undiscovered error.
Many software tools exist to help detect race conditions in software. They can be largely categorized into two groups: static analysis tools and dynamic analysis tools.
CH /ˌsiːˈeɪtʃ/ is a proprietary cross-platform C and C++ interpreter and scripting language environment. It was designed by Harry Cheng as a scripting language for beginners to learn mathematics, computing, numerical analysis (numeric methods), and programming in C/C++. Ch is now developed and marketed by SoftIntegration, Inc.. Free versions include the student edition, and the non-commercial Professional Edition for Raspberry Pi.
CH supports LEGO Mindstorms NXT and EV3, Arduino, Linkbot, Finch Robot, RoboTalk and Raspberry Pi, Pi Zero, and ARM for robot programming and learning.[12][13][14] It can also be embedded into the LabVIEW system design platform and development environment.[15]
A method in object-oriented programming (OOP) is a procedure associated with an object, and generally also a message. An object consists of state data and behavior; these compose an interface, which specifies how the object may be used. A method is a behavior of an object parametrized by a user.
In class-based programming, methods are defined within a class, and objects are instances of a given class. One of the most important capabilities that a method provides is method overriding - the same name (e.g., area) can be used for multiple different kinds of classes. This allows the sending objects to invoke behaviors and to delegate the implementation of those behaviors to the receiving object. A method in Java programming sets the behavior of a class object. For example, an object can send an area message to another object and the appropriate formula is invoked whether the receiving object is a rectangle, circle, triangle, etc.
These methods provide an abstraction layer that facilitates encapsulation and modularity. For example, if a bank-account class provides a getBalance() accessor method to retrieve the current balance (rather than directly accessing the balance data fields), then later revisions of the same code can implement a more complex mechanism for balance retrieval (e.g., a database fetch), without the dependent code needing to be changed. The concepts of encapsulation and modularity are not unique to object-oriented programming. Indeed, in many ways the object-oriented approach is simply the logical extension of previous paradigms such as abstract data types and structured programming.[3]
An abstract method is one with only a signature and no implementation body. It is often used to specify that a subclass must provide an implementation of the method, as in an abstract class. Abstract methods are used to specify interfaces in some programming languages.[6]
Some procedural languages were extended with object-oriented capabilities to leverage the large skill sets and legacy code for those languages but still provide the benefits of object-oriented development. Perhaps the most well-known example is C++, an object-oriented extension of the C programming language. Due to the design requirements to add the object-oriented paradigm on to an existing procedural language, message passing in C++ has some unique capabilities and terminologies. For example, in C++ a method is known as a member function. C++ also has the concept of virtual functions which are member functions that can be overridden in derived classes and allow for dynamic dispatch.
A property, in some object-oriented programming languages, is a special sort of class member, intermediate in functionality between a field (or data member) and a method. The syntax for reading and writing of properties is like for fields, but property reads and writes are (usually) translated to 'getter' and 'setter' method calls. The field-like syntax is easier to read and write than many method calls,[citation needed] yet the interposition of method calls "under the hood" allows for data validation, active updating (e.g., of GUI elements), or implementation of what may be called "read-only fields".
Programming languages that support properties include ActionScript 3, C#, D, Delphi/Free Pascal, eC, F#, Kotlin, JavaScript, Objective-C 2.0, Python, Scala, Swift, Lua, and Visual Basic.
Vala is an object-oriented programming language with a self-hosting compiler that generates C code and uses the GObject system.
Vala is a programming language that combines the high-level build-time performance of scripting languages with the run-time performance of low-level programming languages. It aims to bring modern programming language features to GNOME developers without imposing any additional runtime requirements and without using a different ABI, compared to applications and libraries written in C. The syntax of Vala is similar to C#, modified to better fit the GObject type system.[3]
Vala uses GLib and its submodules (GObject, GModule, GThread, GIO) as the core library, which is available for most operating systems and offers things like platform independent threading, input/output, file management, network sockets, plugins, regular expressions, etc. The syntax of Vala currently supports modern language features as follows:
Vala is intended to provide runtime access to existing C libraries, especially GObject-based libraries, without the need for runtime bindings. To use a library with Vala, all that needed is an API file (.vapi) containing the class and method declarations in Vala syntax. However, C++ libraries are not supported. At present, vapi files for a large part of the GNU project and GNOME platform are included with each release of Vala, including GTK. There is also a library called Gee, written in Vala, that provides GObject-based interfaces and classes for commonly used data structures.[8]
Tooling for Vala development has seen significant improvement over the recent years. The following is a list of some popular IDEs and text editors with plug-ins that add support for programming in Vala:
CLU is a  programming language created at the Massachusetts Institute of Technology (MIT) by Barbara Liskov and her students starting in 1973.[8] While it did not find extensive use, it introduced many features that are used widely now, and is seen as a step in the development of object-oriented programming (OOP).
Another key feature of the CLU type system are iterators, which return objects from a collection serially, one after another.[10] Iterators offer an identical application programming interface (API) no matter what data they are being used with. Thus the iterator for a collection of complex_numbers can be used interchangeably with that for an array of integers. A distinctive feature of CLU iterators is that they are implemented as coroutines, with each value being provided to the caller via a yield statement. Iterators like those in CLU are now a common feature of many modern languages, such as C#, Ruby, and Python, though recently they are often referred to as generators.
Alphard is a Pascal-like programming language for data abstraction and verification, proposed and designed by William A. Wulf, Ralph L. London, and Mary Shaw.[1] The language was the subject of several research publications in the late 1970s, but was never implemented. Its main innovative feature was the introduction of the 'form' datatype, which combines a specification and a procedural (executable) implementation. It also took the generator from IPL-V,[2] as well as the mapping functions from Lisp[3] and made it general case.[4]
Flavors,[1] an early object-oriented extension to Lisp developed by Howard Cannon at the MIT Artificial Intelligence Laboratory for the Lisp machine and its programming language Lisp Machine Lisp, was the first programming language to include mixins.[2] Symbolics used it for its Lisp machines, and eventually developed it into New Flavors; both the original and new Flavors were message passing OO models. It was hugely influential in the development of the Common Lisp Object System (CLOS).[3]
Cocoa is Apple's native object-oriented application programming interface (API) for its desktop operating system macOS.
Cocoa applications are typically developed using the development tools provided by Apple, specifically Xcode (formerly Project Builder) and Interface Builder (now part of Xcode), using the programming languages Objective-C or Swift. However, the Cocoa programming environment can be accessed using other tools. It is also possible to write Objective-C Cocoa programs in a simple text editor and build it manually with GNU Compiler Collection (GCC) or Clang from the command line or from a makefile.
For end users, Cocoa applications are those written using the Cocoa programming environment. Such applications usually have a familiar look and feel, since the Cocoa programming environment provides a lot of common UI elements (such as buttons, scroll bars, etc.), and automates many aspects of an application to comply with Apple's human interface guidelines.
For iOS, iPadOS, tvOS, and watchOS, APIs similar to Application Kit, named UIKit and WatchKit, are available; they include gesture recognition, animation, and a different set of graphical control elements that are designed to accommodate the specific platforms they target. Foundation Kit and Core Data are also available in those operating systems. It is used in applications for Apple devices such as the iPhone, the iPod Touch, the iPad, the Apple TV, and the Apple Watch.
Cocoa continues the lineage of several software frameworks (mainly the App Kit and Foundation Kit)  from the NeXTSTEP and OpenStep programming environments developed by NeXT in the 1980s and 1990s. Apple acquired NeXT in December 1996, and subsequently went to work on the Rhapsody operating system that was to be the direct successor of OPENSTEP. It was to have had an emulation base for classic Mac OS applications, named Blue Box. The OpenStep base of libraries and binary support was termed Yellow Box. Rhapsody evolved into Mac OS X, and the Yellow Box became Cocoa. Thus, Cocoa classes begin with the letters NS, such as NSString or NSArray.  These stand for the original proprietary term for the OpenStep framework, NeXTSTEP.[2]
The Cocoa frameworks are written in Objective-C. Java bindings for the Cocoa frameworks (termed the Java bridge) were also made available with the aim of replacing Objective-C with a more popular language[8] but these bindings were unpopular among Cocoa developers and Cocoa's message passing semantics did not translate well to a statically-typed language such as Java.[9] Cocoa's need for runtime binding means many of Cocoa's key features are not available with Java. In 2005, Apple announced that the Java bridge was to be deprecated, meaning that features added to Cocoa in macOS versions later than 10.4 would not be added to the Cocoa-Java programming interface.
At Apple Worldwide Developers Conference (WWDC) 2014, Apple introduced a new programming language named Swift, which is intended to replace Objective-C.[10]
Originally, AppleScript Studio could be used to develop simpler Cocoa applications.[11]  However, as of Snow Leopard, it has been deprecated. It was replaced with AppleScriptObjC, which allows programming in AppleScript, while using Cocoa frameworks.[12]
The Cocoa programming environment can be accessed using other tools with the aid of bridge mechanisms such as PasCocoa, PyObjC, CamelBones, RubyCocoa, and a D/Objective-C Bridge.
There are also open source implementations of major parts of the Cocoa framework, such as GNUstep and Cocotron,[16] which allow cross-platform Cocoa application development to target other operating systems, such as Microsoft Windows and Linux.
In computer programming, event-driven programming is a programming paradigm in which the flow of the program is determined by external events. UI events from mice, keyboards, touchpads and touchscreens, and external sensor inputs are common cases. Events may also be programmatically generated, such as from messages from other programs, notifications from other threads, or other network events.
Event-driven programming is the dominant paradigm used in graphical user interfaces applications and network servers.
Event-driven programs can be written in any programming language, although the task is easier in languages that provide high-level abstractions.
Because the event loop of retrieving/dispatching of events are common amongst applications, many programming frameworks take care of their implementation and expect the user to provide only the code for the event handlers.
RPG, an early programming language from IBM, whose 1960s design concept was similar to event-driven programming discussed above, provided a built-in main I/O loop (known as the "program cycle") where the calculations responded in accordance to 'indicators' (flags) that were set earlier in the cycle.
The actual logic is contained in event-handler routines. These routines handle the events to which the main program will respond. For example, a single left-button mouse-click on a command button in a GUI program may trigger a routine that will open another window, save data to a database or exit the application. Many IDEs provide the programmer with GUI event templates, allowing the programmer to focus on writing the event code.
Most existing GUI architectures use event-driven programming.[2] Windows has an event loop. The Java AWT framework processes all UI changes on a single thread, called the Event dispatching thread. Similarly, all UI updates in the Java framework JavaFX occur on the JavaFX Application Thread.[3]
Ruby is an interpreted, high-level, general-purpose programming language. It was designed with an emphasis on programming productivity and simplicity. In Ruby, everything is an object, including primitive data types. It was developed in the mid-1990s by Yukihiro "Matz" Matsumoto in Japan.
Ruby is dynamically typed and uses garbage collection and just-in-time compilation. It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. According to the creator, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, BASIC, and Lisp.[10][3]
By 2000, Ruby was more popular than Python in Japan.[20] In September 2000, the first English language book Programming Ruby was printed, which was later freely released to the public, further widening the adoption of Ruby amongst English speakers. In early 2002, the English-language ruby-talk mailing list was receiving more messages than the Japanese-language ruby-list, demonstrating Ruby's increasing popularity in the non-Japanese speaking world.
Ruby 3.2 was released on December 25, 2022.[44] It brings support for being run inside of a WebAssembly environment via a WASI interface. Regular expressions also receives some improvements, including a faster, memoized matching algorithm to protect against certain ReDoS attacks, and configurable timeouts for regular expression matching. Additional debugging and syntax features are also included in this release, which include syntax suggestion, as well as error highlighting. The MJIT compiler has been re-implemented as a standard library module, while the YJIT, a Rust-based JIT compiler now supports more architectures on Linux.
Matsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design.[45] At a Google Tech Talk in 2008 he said, "I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language."[46] He stresses that systems design needs to emphasize human, rather than computer, needs:[47]
Often people, especially computer engineers, focus on the machines. They think, "By doing this, the machine will run fast. By doing this, the machine will run more effectively. By doing this, the machine will something something something." They are focusing on machines. But in fact we need to focus on humans, on how humans care about doing programming or operating the application of the machines. We are the masters. They are the slaves.
Everyone has an individual background. Someone may come from Python, someone else may come from Perl, and they may be surprised by different aspects of the language. Then they come up to me and say, 'I was surprised by this feature of the language, so Ruby violates the principle of least surprise.' Wait. Wait. The principle of least surprise is not for you only. The principle of least surprise means principle of least my surprise. And it means the principle of least surprise after you learn Ruby very well. For example, I was a C++ programmer before I started designing Ruby. I programmed in C++ exclusively for two or three years. And after two years of C++ programming, it still surprises me.
Ruby has been described as a multi-paradigm programming language: it allows procedural programming (defining functions/variables outside classes makes them part of the root, 'self' Object), with object orientation (everything is an object) or functional programming (it has anonymous functions, closures, and continuations; statements all have values, and functions return the last evaluation). It has support for introspection, reflective programming, metaprogramming, and interpreter-based threads. Ruby features dynamic typing, and supports parametric polymorphism.
One of the differences from Python and Perl is that Ruby keeps all of its instance variables completely private to the class and only exposes them through accessor methods (attr_writer, attr_reader, etc.). Unlike the "getter" and "setter" methods of other languages like C++ or Java, accessor methods in Ruby can be created with a single line of code via metaprogramming; however, accessor methods can also be created in the traditional fashion of C++ and Java. As invocation of these methods does not require the use of parentheses, it is trivial to change an instance variable into a full function, without modifying a single line of calling code or having to do any refactoring achieving similar functionality to C# and VB.NET property members.
Matsumoto originally developed Ruby on the 4.3BSD-based Sony NEWS-OS 3.x, but later migrated his work to SunOS 4.x, and finally to Linux.[61][62] By 1999, Ruby was known to work across many different operating systems. Modern Ruby versions and implementations are available on all major desktop, mobile and server-based operating systems. Ruby is also supported across a number of cloud hosting platforms like Jelastic, Heroku, Google Cloud Platform and others.
The Ruby Application Archive, which hosted applications, documentation, and libraries for Ruby programming, was maintained until 2013, when its function was transferred to RubyGems.[65]
This comparison of programming languages compares how object-oriented programming languages such as C++, Java, Smalltalk, Object Pascal, Perl, Python, and others manipulate data structures.
Logic programming is a programming, database and knowledge representation paradigm based on formal logic. A logic program is a set of sentences in logical form, representing knowledge about some problem domain. Computation is performed by applying logical reasoning to that knowledge, to solve problems in the domain.  Major logic programming language families include Prolog, Answer Set Programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:
Logic programming languages that include negative conditions have the knowledge representation capabilities of a non-monotonic logic.
Much of the research in the field of logic programming has been concerned with trying to develop a logical semantics for negation as failure and with developing other semantics and other implementations for negation. These developments have been important, in turn, for supporting the development of formal methods for logic-based program verification and program transformation.
The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green.[3] This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language that places no constraints on the order in which operations are performed.[4]
Logic programming, with its current syntax of facts and rules, can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in artificial intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.[5]
Although it was based on the proof methods of logic, Planner, developed by Carl Hewitt at MIT, was the first language to emerge within this proceduralist paradigm.[6] Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. Winograd used Micro-Planner to implement the landmark, natural-language understanding program SHRDLU.[7] For the sake of efficiency, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA4,[8] Popler,[9] Conniver,[10] QLISP,[11] and the concurrent language Ether.[12]
Colmerauer, with Philippe Roussel, used the procedural interpretation as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David H. D. Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp.[17] Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.
Logic programming gained international attention during the 1980s, when it was chosen by the Japanese  Ministry of International Trade and Industry to develop the software for the Fifth Generation Computer Systems (FGCS) project. The FGCS project aimed to use logic programming to develop advanced Artificial Intelligence applications on massively parallel computers. Although the project initially explored the use of Prolog, it later adopted the use of concurrent logic programming, because it was closer to the FGCS computer architecture.
However, the committed choice feature of concurrent logic programming interfered with the language's logical semantics[18] and with its suitability for knowledge representation and problem solving applications. Moreover, the parallel computer systems developed in the project failed to compete with advances taking place in the development of more conventional, general-purpose computers. Together these two issues resulted in the FGCS project failing to meet its objectives. Interest in both logic programming and AI fell into world-wide decline.[19]
In the meanwhile, more declarative logic programming approaches, including those based on the use of Prolog, continued to make progress independently of the FGCS project. In particular, although Prolog was developed to combine declarative and procedural representations of knowledge, the purely declarative interpretation of logic programs became the focus for applications in the field of deductive databases. Work in this field became prominent around 1977, when Hervé Gallaire and Jack Minker organized a workshop on logic and databases in Toulouse.[20] The field was eventually renamed as Datalog.
This focus on the logical, declarative reading of logic programs was given further impetus by the development of constraint logic programming in the 1980s and Answer Set Programming in the 1990s. It is also receiving renewed emphasis in recent applications of Prolog[21]
The Association for Logic Programming (ALP) was founded in 1986 to promote Logic Programming. Its official journal until 2000, was The Journal of Logic Programming. Its  founding editor-in-chief was J. Alan Robinson.[22]  In 2001, the journal was renamed The Journal of Logic and Algebraic Programming, and the official journal of ALP became Theory and Practice of Logic Programming, published by Cambridge University Press.
Logic programs enjoy a rich variety of semantics and problem solving methods, as well as a wide range of applications in programming, databases, knowledge representation and problem solving.
The procedural interpretation of logic programs, which uses backward reasoning to reduce goals to subgoals, is a special case of the use of a problem-solving strategy to control the use of a declarative, logical representation of knowledge to obtain the behaviour of an algorithm. More generally, different problem-solving strategies can be applied to the same logical representation to obtain different algorithms. Alternatively, different algorithms can be obtained with a given problem-solving strategy by using different logical representations.[23]
In the more general, non-propositional case, where sub-goals can share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies.[26] Such strategies are used, for example, in concurrent logic programming.
In most cases, backward reasoning from a query or goal is more efficient than forward reasoning. But sometimes with Datalog and Answer Set Programming, there may be no query that is separate from the set of clauses as a whole, and then generating all the facts that can be derived from the clauses is a sensible problem-solving strategy. Here is another example, where forward reasoning beats backward reasoning in a more conventional computation task, where the goal ?- fibonacci(n, Result) is to find the nth fibonacci number:
Logic programming can be viewed as a generalisation of functional programming, in which functions are a special case of relations.[28]
For example, the function, mother(X) = Y, (every X has only one mother Y) can be represented by the relation mother(X, Y). In this respect, logic programs are similar to relational databases, which also represent functions as relations.
The term relational programming has been used to cover a variety of programming languages that treat functions as a special case of relations. Some of these languages, such as miniKanren[28]
and relational linear programming[30]
are logic programming languages in the sense of this article.
However, the relational language RML is an imperative programming language
[31] whose core construct is a
relational expression, which is similar to an expression in first-order predicate logic.
Other relational programming languages are based on the relational calculus[32] or relational algebra.[33]
This property of withdrawing a conclusion when new information is added, is called non-monotonicity, and it makes logic programming a non-monotonic logic.
In the alternative stable model semantics, there may be no intended models or several intended models, all of which are minimal and two-valued. The stable model semantics underpins answer set programming (ASP).
Attempts to understand negation in logic programming have also contributed to the development of abstract argumentation frameworks.[41] In an argumentation interpretation of negation, the initial argument that tom should be punished because he is a thief, is attacked by the argument that he should be rehabilitated because he is a minor. But the fact that tom is violent undermines the argument that tom should be rehabilitated and reinstates the argument that tom should be punished.
Metaprogramming, in which programs are treated as data, was already a feature of early Prolog implementations.[42][43] For example, the Edinburgh DEC10 implementation of Prolog included "an interpreter and a compiler, both written in Prolog itself".[43] The simplest metaprogram is the so-called "vanilla" meta-interpreter:
Metaprogramming is an application of the more general use of a metalogic or metalanguage  to describe and reason about another language, called the object language.
Metalogic programming allows object-level and metalevel representations to be combined, as in natural language. For example, in the following program, the atomic formula attends(Person, Meeting) occurs both as an object-level formula, and as an argument of the metapredicates prohibited and approved.
He states that "unlike logic, rule-based systems can also easily represent strategic information
about what to do" (page 45). For example, "IF you want to go home for the weekend, and you have bus fare, THEN
you can catch a bus". He does not observe that the same strategy of reducing a goal to subgoals can be interpreted, in the manner of logic programming, as applying backward reasoning to a logical conditional:
All of these characteristics of rule-based systems - search, forward and backward reasoning, default reasoning, and goal-reduction - are also defining characteristics of logic programming. This suggests that Thagard's conclusion (page 56) that:
Other arguments showing how logic programming can be used to model aspects of human thinking are presented by Keith Stenning and Michiel van Lambalgen in their book,
Human Reasoning and Cognitive Science.[45] They show how the non-monotonic character of logic programs can be used to explain human performance on a variety of psychological tasks. They also show (page 237) that  "closed–world reasoning in its guise as logic programming has an appealing neural implementation, unlike classical logic."
In The Proper Treatment of Events,[46]
Michiel van Lambalgen and Fritz Hamm investigate the use of constraint logic programming to code "temporal notions in natural language by looking at the way human beings construct time".
The use of logic to represent procedural knowledge and strategic information was one of the main goals contributing to the early development of logic programming. Moreover, it continues to be an important feature of the Prolog family of logic programming languages today. However, many applications of logic programming, including Prolog applications, increasingly focus on the use of logic to represent purely declarative knowledge. These applications include both the representation of general commonsense knowledge and the representation of domain specific expertise.
Logic programming has also proved to be useful for representing domain-specific expertise in expert systems.[48] But human expertise, like general-purpose commonsense, is mostly implicit and tacit, and it is often difficult to represent such implicit knowledge in explicit rules. This difficulty does not arise, however, when logic programs are used to represent the existing, explicit rules of a business organisation or legal authority.
Historically, the representation of a large portion of the British Nationality Act as a logic program in the 1980s[49] was "hugely influential for the development of computational representations of legislation, showing how logic programming enables intuitively appealing representations that can be directly deployed to generate automatic inferences".[50]
Various extensions of logic programming have been developed to provide a logical framework for such destructive change of state.[53][54][55]
Prolog has also contributed to the development of other programming languages, including ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog.
Constraint logic programming (CLP) combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of a clause. Constraint predicates are not defined by the facts and rules in the program, but are predefined by some domain-specific model-theoretic structure or theory.
Procedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are simplified and checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.
The following constraint logic program represents a toy temporal database of john's history as a teacher:
Here ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:
Constraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.
Datalog is a database definition language, which combines a relational view of data, as in relational databases, with a logical view, as in logic programming.
Relational databases use a relational calculus or relational algebra, with relational operations, such as union, intersection, set difference and cartesian product to specify queries, which access  a database. Datalog uses logical connectives, such as or, and and not in the bodies of rules to define relations as part of the database itself.
It was recognized early in the development of relational databases that recursive queries cannot be expressed in either relational algebra or relational calculus, and that this defficiency can be remedied by introducing a least-fixed-point operator.[56][57] In contrast, recursive relations can be defined naturally by rules in logic programs, without the need for any new logical connectives or operators.
Datalog differs from more general logic programming by having only constants and variables as terms. Moreover, all facts are variable-free, and rules are restricted, so that if they are executed bottom-up, then the derived facts are also variable-free.
Like Datalog, Answer Set programming (ASP) is not Turing-complete. Moreover, instead of separating goals (or queries) from the program to be used in solving the goals, ASP  treats the whole program as a goal, and solves the goal by generating a stable model that makes the goal true. For this purpose, it uses the stable model semantics, according to which a logic program can have zero, one or more intended models. For example, the following program represents a degenerate variant of the map colouring problem of colouring two countries red or green:
Confusingly, constraints in ASP are different from constraints in CLP. Constraints in CLP are predicates that qualify answers to queries (and solutions of goals). Constraints in ASP are clauses that eliminate models that would otherwise satisfy goals. Constraints in ASP are like integrity constraints in databases.
This combination of ordinary logic programming clauses and constraint clauses illustrates the generate-and-test methodology of problem solving in ASP: The ordinary clauses define a search space of possible solutions, and the constraints filter out unwanted solutions.[58]
Most implementations of ASP proceed in two steps: First they instantiate the program in all possible ways, reducing it to a propositional logic program (known as grounding). Then they apply a propositional logic problem solver, such as the DPLL algorithm or a Boolean SAT solver. However, some implementations, such as s(CASP)[59] use a goal-directed, top-down, SLD resolution-like procedure without
grounding.
Abductive logic programming[60] (ALP), like CLP, extends normal logic programming by allowing the bodies of clauses to contain literals whose predicates are not defined by clauses. In ALP, these predicates are declared as abducible (or assumable), and are used as in abductive reasoning to explain observations, or more generally to add new facts to the program (as assumptions) to solve goals.
Abductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret negation as failure as a form of abductive reasoning.[62]
Inductive logic programming (ILP) is an approach to machine learning that induces logic programs as hypothetical generalisations of positive and negative examples. Given a logic program representing background knowledge and positive examples together with constraints representing negative examples, an ILP system induces a logic program that generalises the positive examples while excluding the negative examples.
Recent work in ILP, combining logic programming, learning and probability, has given rise to the fields of statistical relational learning and probabilistic inductive logic programming.
Concurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).[67]
However, procedurally, when there are several clauses whose heads H match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, ... , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceeds with the subgoals B1, ..., Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of "don't care nondeterminism", rather than "don't know nondeterminism".
Carl Hewitt has argued[68] that, because of the indeterminacy of concurrent computation, concurrent logic programming cannot implement general concurrency. However, according to the logical semantics, any result of a computation of a concurrent logic program is a logical consequence of the program, even though not all logical consequences can be derived.
Concurrent constraint logic programming[69] combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to use only one.
Several researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog[70] and λProlog.[71]
Basing logic programming within linear logic has resulted in the design of logic programming languages that are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO,[72] Lolli,[73] ACL,[74] and Forum.[75] Forum provides a goal-directed interpretation of all linear logic.
Logtalk[77] extends the Prolog programming language with support for objects, protocols, and other OOP concepts. It supports most standard-compliant Prolog systems as backend compilers.
Transaction logic[53] is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2[78] system. Other prototypes are also available.
In software development, an object is an entity that has state, behavior, and identity.[1]: 78  An object can model some part of reality or can be an invention of the design process whose collaborations with other such objects serve as the mechanisms that provide some higher-level behavior. Put another way, an object represents an individual, identifiable item, unit, or entity, either real or abstract, with a well-defined role in the problem domain.[1]: 76
A programming language can be classified based on its support for objects. A language that provides an encapsulation construct for state, behavior, and identity is classified as object-based. If the language also provides polymorphism and inheritance it is classified as object-oriented. A language that supports creating an object from a class is classified as class-based. A language that supports object creation via a template object is classified as prototype-based.
Object-Oriented Software Construction, also called OOSC, is a book by Bertrand Meyer, widely considered a foundational text of object-oriented programming.[citation needed] The first edition was published in 1988; the second edition, extensively revised and expanded (more than 1300 pages), in 1997. Many translations are available including Dutch (first edition only), French (1+2), German (1), Italian (1), Japanese (1+2), Persian (1), Polish (2), Romanian (1), Russian (2), Serbian (2), and Spanish (2).[1] The book has been cited thousands of times. As of 15 December 2011[update], The Association for Computing Machinery's (ACM) Guide to Computing Literature counts 2,233 citations,[2] for the second edition alone in computer science journals and technical books; Google Scholar lists 7,305 citations. As of September 2006[update], the book is number 35 in the list of all-time most cited works (books, articles, etc.) in computer science literature, with 1,260 citations.[3] 
The book won a Jolt award in 1994.[4] The second edition is available online free.[5]
The book presents object technology as an answer to major issues of software engineering, with a special emphasis on addressing the software quality factors of correctness, robustness, extendibility and reusability. It starts with an examination of the issues of software quality, then introduces   abstract data types as the theoretical basis for object technology and proceeds with the main object-oriented techniques: classes, objects, genericity, inheritance, Design by Contract, concurrency, and persistence. It includes extensive discussions of methodological issues.
The first edition of the book used the programming language Eiffel for the examples and served as a justification of the language design choices for Eiffel. The second edition also uses Eiffel as its notation, but in an effort to separate the notation from the concepts it does not name the language until the Epilogue, on page 1162, where Eiffel appears as the last word. A few months after publication of the second edition, a reader posted on Usenet[citation needed] his discovery that the book's 36 chapters alternatively start with the letters E, I, F, F, E, L, a pattern being repeated 6 times. Also, in the Appendix, titled "Epilogue, In Full Frankness Exposing the Language" (in first initials), the first letters of each paragraph spell the same pattern.
Prototype-based programming is a style of object-oriented programming in which behavior reuse (known as inheritance) is performed via a process of reusing existing objects that serve as prototypes. This model can also be known as prototypal, prototype-oriented, classless, or instance-based programming.
Prototype-based programming uses the process generalized objects, which can then be cloned and extended. Using fruit as an example, a "fruit" object would represent the properties and functionality of fruit in general. A "banana" object would be cloned from the "fruit" object and general properties specific to bananas would be appended. Each individual "banana" object would be cloned from the generic "banana" object. Compare to the class-based paradigm, where a "fruit" class would be extended by a "banana" class.
The first prototype-based programming languages were Director a.k.a. Ani (on top of MacLisp) (1976-1979), and contemporaneously and not independently, ThingLab (on top of Smalltalk) (1977-1981), respective PhD projects by Kenneth Michael Kahn at MIT and Alan Hamilton Borning at Stanford (but working with Alan Kay at Xerox PARC). Borning introduced the word "prototype" in his TOPLAS 1981 paper. The first prototype-based programming language with more than one implementer or user was probably Yale T Scheme (1981-1984), though like Director and ThingLab initially, it just speaks of objects without classes. The language that made the name and notion of prototypes popular was Self (1985-1995), developed by David Ungar and Randall Smith to research topics in object-oriented language design.
Since the 2010s, a new generation of languages with pure functional prototypes has appeared, that reduce OOP to its very core: Jsonnet is a dynamic lazy pure functional language with a builtin prototype object system using mixin inheritance; Nix is a dynamic lazy pure functional language that builds an equivalent object system (Nix "extensions") in just two short function definitions (plus many other convenience functions). Both languages are used to define large distributed software configurations (Jsonnet being directly inspired by GCL, the Google Configuration Language, with which Google defines all its deployments, and has similar semantics though with dynamic binding of variables). Since then, other languages like Gerbil Scheme have implemented pure functional lazy prototype systems based on similar principles.
Advocates of prototype-based programming argue that it encourages the programmer to focus on the behavior of some set of examples and only later worry about classifying these objects into archetypal objects that are later used in a fashion similar to classes.[2] Many prototype-based systems encourage the alteration of prototypes during run-time, whereas only very few class-based object-oriented systems (such as the dynamic object-oriented system, Common Lisp, Dylan, Objective-C, Perl, Python, Ruby, or Smalltalk) allow classes to be altered during the execution of a program.
Almost all prototype-based systems are based on interpreted and dynamically typed languages. Systems based on statically typed languages are technically feasible, however. The Omega language discussed in Prototype-Based Programming[3] is an example of such a system, though according to Omega's website even Omega is not exclusively static, but rather its "compiler may choose to use static binding where this is possible and may improve the efficiency of a program."
Cloning refers to a process whereby a new object is constructed by copying the behavior of an existing object (its prototype). The new object then carries all the qualities of the original. From this point on, the new object can be modified. In some systems the resulting child object maintains an explicit link (via delegation or resemblance) to its prototype, and changes in the prototype cause corresponding changes to be apparent in its clone. Other systems, such as the Forth-like programming language Kevo, do not propagate change from the prototype in this fashion and instead follow a more concatenative model where changes in cloned objects do not automatically propagate across descendants.[2]
In concatenative prototyping - the approach implemented by the Kevo programming language - there are no visible pointers or links to the original prototype from which an object is cloned. The prototype (parent) object is copied rather than linked to and there is no delegation. As a result, changes to the prototype will not be reflected in cloned objects.[5] Incidentally, the Cosmos programming language achieves the same through the use of persistent data structures.[6]
Advocates of class-based object models who criticize prototype-based systems often have concerns similar to the concerns that proponents of static type systems for programming languages have of dynamic type systems (see datatype). Usually, such concerns involve correctness, safety, predictability, efficiency and programmer unfamiliarity.
A common criticism made against prototype-based languages is that the community of software developers is unfamiliar with them, despite the popularity and market permeation of JavaScript. However, knowledge about prototype-based systems is increasing with the proliferation of JavaScript frameworks and the complex use of JavaScript as the World Wide Web (Web) matures.[8][citation needed] ECMAScript 6 introduced classes as syntactic sugar over JavaScript's existing prototype-based inheritance, providing an alternative way to create objects and manage inheritance.[9]
Class-based programming, or more commonly class-orientation, is a style of object-oriented programming (OOP) in which inheritance occurs via defining classes of objects, instead of inheritance occurring via the objects alone (compare prototype-based programming).
In class-based programming, inheritance is done by defining new classes as extensions of existing classes: the existing class is the parent class and the new class is the child class. If a child class has only one parent class, this is known as single inheritance, while if a child class can have more than one parent class, this is known as multiple inheritance. This organizes classes into a hierarchy, either a tree (if single inheritance) or lattice (if multiple inheritance).
The defining feature of inheritance is that both interface and implementation are inherited; if only interface is inherited, this is known as interface inheritance or subtyping. Inheritance can also be done without classes, as in prototype-based programming.
Class-based languages, or, to be more precise, typed languages, where subclassing is the only way of subtyping, have been criticized for mixing up implementations and interfaces—the essential principle in object-oriented programming. The critics say one might create a bag class that stores a collection of objects, then extend it to make a new class called a set class where the duplication of objects is eliminated.[1][2]  Now, a function that takes an object of the bag class may expect that adding two objects increases the size of a bag by two, yet if one passes an object of a set class, then adding two objects may or may not increase the size of a bag by two. The problem arises precisely because subclassing implies subtyping even in the instances where the principle of subtyping, known as the Liskov substitution principle, does not hold. Barbara Liskov and Jeannette Wing formulated the principle succinctly in a 1994 paper as follows:
In class-based, object-oriented programming, a constructor (abbreviation: ctor) is a special type of function called to create an object. It prepares the new object for use, often accepting arguments that the constructor uses to set required member variables.
Static constructors allow complex static variable initialization.[7]
Static constructors are called implicitly when the class is first accessed. Any call to a class (static or constructor call), triggers the static constructor execution.
Static constructors are thread safe and implement a singleton pattern. When used in a generic programming class, static constructors are called at every new generic instantiation one per type.[8]: 38 [4]: 111  Static variables are instantiated as well.
As of ES6, JavaScript has direct constructors like many other programming languages. They are written as such
In computer programming, a trait is a language concept that represents a set of methods that can be used to extend the functionality of a class.[1][2]
In object-oriented programming, behavior is sometimes shared between classes which are not related to each other. For example, many unrelated classes may have methods to serialize objects to JSON. Historically, there have been several approaches to solve this without duplicating the code in every class needing the behavior. Other approaches include multiple inheritance and mixins, but these have drawbacks: the behavior of the code may unexpectedly change if the order in which the mixins are applied is altered, or if new methods are added to the parent classes or mixins.
In case of a naming collision between methods provided by different traits, the programmer must explicitly disambiguate which one of those methods will be used in the class; thus manually solving the diamond problem of multiple inheritance. This is different from other composition methods in object-oriented programming, where conflicting names are automatically resolved by scoping rules.
Traits come originally from the programming language Self[5] and are supported by the following programming languages:
In object-oriented programming, delegation refers to evaluating a member (property or method) of one object (the receiver) in the context of another original object (the sender). Delegation can be done explicitly, by passing the responsibilities of the sending object to the receiving object, which can be done in any object-oriented language; or implicitly, by the member lookup rules of the language, which requires language support for the feature. Implicit delegation is the fundamental method for behavior reuse in prototype-based programming, corresponding to inheritance in class-based programming. The best-known languages that support delegation at the language level are Self, which incorporates the notion of delegation through its notion of mutable parent slots that are used upon method lookup on self calls, and JavaScript; see JavaScript delegation.
The term delegation is also used loosely for various other relationships between objects; see delegation (programming) for more. Frequently confused concepts are simply using another object, more precisely referred to as consultation or aggregation; and evaluating a member on one object by evaluating the corresponding member on another object, notably in the context of the receiving object, which is more precisely referred to as forwarding (when a wrapper object doesn't pass itself to the wrapped object).[1][2][a] The delegation pattern is a software design pattern for implementing delegation, though this term is also used loosely for consultation or forwarding.
This sense of delegation as programming language feature making use of the method lookup rules for dispatching so-called self-calls was defined by Lieberman in his 1986 paper "Using Prototypical Objects to Implement Shared Behavior in Object-Oriented Systems".
It has been argued that delegation may in some cases be preferred to inheritance to make program code more readable and understandable.[5] Despite explicit delegation being fairly widespread, relatively few major programming languages implement delegation as an alternative model to inheritance. The precise relationship between delegation and inheritance is complicated; some authors consider them equivalent, or one a special case of the other.[6]
Programming languages in general do not support this unusual form of delegation as a language concept, but there are a few exceptions[citation needed].
Delegation can be described as a low level mechanism for sharing code and data between entities. Thus it builds the foundation for other language constructs. Notably role-oriented programming languages have been utilizing delegation, but especially the older ones factually used aggregation while claiming to use delegation. This should not be considered cheating, merely the plural definitions of what delegation means (as described above).
Delegation has also been suggested for advice resolution in aspect-oriented programming by Ernst and Lorenz in 2003.
In software systems, encapsulation refers to the bundling of data with the mechanisms or methods that operate on the data.  It may also refer to the limiting of direct access to some of that data, such as an object's components.[1] Essentially, encapsulation prevents external code from being concerned with the internal workings of an object.
All object-oriented programming (OOP) systems support encapsulation,[2][3] but encapsulation is not unique to OOP. Implementations of abstract data types, modules, and libraries also offer encapsulation. The similarity has been explained by programming language theorists in terms of existential types.[4]
In object-oriented programming languages, and other related fields,   encapsulation refers to one of two related but distinct notions, and sometimes to the combination thereof:[5][6]
Some programming language researchers and academics use the first meaning alone or in combination with the second as a distinguishing feature of object-oriented programming, while some programming languages that provide lexical closures view encapsulation as a feature of the language orthogonal to object orientation.
Encapsulation may also refer to containing a repetitive or complex process in a single unit to be invoked. Object-oriented programming facilitate this at both the method and class levels. This definition is also applicable to procedural programming.[10]
Under the definition that encapsulation "can be used to hide data members and member functions", the internal representation of an object is generally hidden outside of the object's definition. Typically, only the object's own methods can directly inspect or manipulate its fields. Hiding the internals of the object protects its integrity by preventing users from setting the internal data of the component into an invalid or inconsistent state. A supposed benefit of encapsulation is that it can reduce system complexity, and thus increase robustness, by allowing the developer to limit the interdependencies between software components.[citation needed]
In software engineering, coupling is the degree of interdependence between software modules, a measure of how closely connected two routines or modules are,[1] and the strength of the relationships between modules.[2] Coupling is not binary but multi-dimensional. [3]
The software quality metrics of coupling and cohesion were invented by Larry Constantine in the late 1960s as part of a structured design, based on characteristics of “good” programming practices that reduced maintenance and modification costs. Structured design, including cohesion and coupling, were published in the article Stevens, Myers & Constantine (1974)[4] and the book Yourdon & Constantine (1979),[5] and the latter subsequently became standard terms.
The goal of defining and measuring this type of coupling is to provide a run-time evaluation of a software system. It has been argued that static coupling metrics lose precision when dealing with an intensive use of dynamic binding or inheritance.[8] In the attempt to solve this issue, dynamic coupling measures have been taken into account.
This kind of a coupling metric considers the conceptual similarities between software entities using, for example, comments and identifiers and relying on techniques such as latent semantic indexing (LSI).
Logical coupling (or evolutionary coupling or change coupling) analysis exploits the release history of a software system to find change patterns among modules or classes: e.g., entities that are likely to be changed together or sequences of changes (a change in a class A is always followed by a change in a class B).
Whether loosely or tightly coupled, a system's performance is often reduced by message and parameter creation, transmission, translation (e.g. marshaling) and message interpretation (which might be a reference to a string, array or data structure), which require less overhead than creating a complicated message such as a SOAP message. Longer messages require more CPU and memory to produce. To optimize runtime performance, message length must be minimized and message meaning must be maximized.
Coupling describes the degree and nature of dependency between software components, focusing on what they share (e.g., data, control flow, technology) and how tightly they are bound. It evaluates two key dimensions: strength, which measures how difficult it is to change the dependency, and scope (or visibility), which indicates how widely the dependency is exposed across modules or boundaries. Traditional coupling types typically include content coupling, common coupling, control coupling, stamp coupling, external coupling, and data coupling. [9][10][11]
Connascence, introduced by Meilir Page-Jones, provides a systematic framework for analyzing and measuring coupling dependencies. It evaluates dependencies based on three dimensions: strength, which measures the effort required to refactor or modify the dependency; locality, which considers how physically or logically close dependent components are in the codebase; and degree, which measures how many components are affected by the dependency. Connascence can be categorized into static (detectable at compile-time) and dynamic (detectable at runtime) forms. Static connascence refers to compile-time dependencies, such as method signatures, while dynamic connascence refers to runtime dependencies, which can manifest in forms like connascence of timing, values, or algorithm. [9][10][11]
Dependencies spanning module boundaries or distributed systems typically have higher coordination costs, increasing the difficulty of refactoring and propagating changes across distant boundaries. Modern practices, such as dependency injection and interface-based programming, are often employed to reduce coupling strength and improve the maintainability of dependencies. [9][10][11]
C (pronounced /ˈsiː/ – like the letter c)[6] is a general-purpose programming language. It was created in the 1970s by Dennis Ritchie and remains very widely used and influential. By design, C's features cleanly reflect the capabilities of the targeted CPUs. It has found lasting use in operating systems code (especially in kernels[7]), device drivers, and protocol stacks, but its use in application software has been decreasing.[8] C is commonly used on computer architectures that range from the largest supercomputers to the smallest microcontrollers and embedded systems.
A successor to the programming language B, C was originally developed at Bell Labs by Ritchie between 1972 and 1973 to construct utilities running on Unix. It was applied to re-implementing the kernel of the Unix operating system.[9] During the 1980s, C gradually gained popularity. It has become one of the most widely used programming languages,[10][11] with C compilers available for practically all modern computer architectures and operating systems. The book The C Programming Language, co-authored by the original language designer, served for many years as the de facto standard for the language.[12][1] C has been standardized since 1989 by the American National Standards Institute (ANSI) and, subsequently, jointly by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC).
C is an imperative procedural language, supporting structured programming, lexical variable scope, and recursion, with a static type system. It was designed to be compiled to provide low-level access to memory and language constructs that map efficiently to machine instructions, all with minimal runtime support. Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant C program written with portability in mind can be compiled for a wide variety of computer platforms and operating systems with few changes to its source code.
Since 2000, C has consistently ranked among the top four languages in the TIOBE index, a measure of the popularity of programming languages.[13]
C is an imperative, procedural language in the ALGOL tradition.  It has a static type system. In C, all executable code is contained within subroutines (also called "functions", though not in the sense of functional programming). Function parameters are passed by value, although arrays are passed as pointers, i.e. the address of the first item in the array. Pass-by-reference is simulated in C by explicitly passing pointers to the thing being referenced.
The origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Dennis Ritchie and Ken Thompson, incorporating several ideas from colleagues.  Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was also developed in assembly language.[9]
Thompson wanted a programming language for developing utilities for the new platform. He first tried writing a Fortran compiler, but he soon gave up the idea and instead created a cut-down version of the recently developed systems programming language called BCPL. The official description of BCPL was not available at the time,[14] and Thompson modified the syntax to be less 'wordy' and similar to a simplified ALGOL known as SMALGOL.[15] He called the result B,[9] describing it as "BCPL semantics with a lot of SMALGOL syntax".[15] Like BCPL, B had a bootstrapping compiler to facilitate porting to new machines.[15] Ultimately, few utilities were written in B because it was too slow and could not take advantage of PDP-11 features such as byte addressability.
Unix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system (which was written in PL/I) and Master Control Program (MCP) for the Burroughs B5000 (which was written in ALGOL) in 1961. In around  1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system.  Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.[16]
In 1978 Brian Kernighan and Dennis Ritchie published the first edition of The C Programming Language.[18] Known as K&R from the initials of its authors, the book served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as "K&R C". As this was released in 1978, it is now also referred to as C78.[19] The second edition of the book[20] covers the later ANSI C standard, described below.
In 1983 the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 "Programming Language C".  This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.
In 1990 the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms "C89" and "C90" refer to the same programming language.
C17 is an informal name for ISO/IEC 9899:2018, a standard for the C programming language published in June 2018. It introduces no new language features, only technical corrections, and clarifications to defects in C11. The standard macro __STDC_VERSION__ is defined as 201710L to indicate that C17 support is available.
Historically, embedded C programming requires non-standard extensions to the C language to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.
As an imperative language, C uses statements to specify actions. The most common statement is an expression statement, consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by if ... [else] conditional execution and by do ... while, while, and for iterative execution (looping). The for statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. break and continue can be used within the loop. Break is used to leave the innermost enclosing loop statement and continue is used to skip to its reinitialisation. There is also a non-structured goto statement which branches directly to the designated label within the function. switch selects a case to be executed based on the value of an integer expression. Different from many other languages, control-flow will fall through to the next case unless terminated by a break.
Expressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next "sequence point"; sequence points include the end of each expression statement, and the entry to and return from each function call.  Sequence points also occur during evaluation of expressions containing certain operators (&&, ||, ?: and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.
Kernighan and Ritchie say in the Introduction of The C Programming Language: "C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better."[31] The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.
Most of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations.  Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. Some of those words were added as keywords with their conventional spelling in C23 and the corresponding macros were removed.
Prior to C89, entry was reserved as a keyword. In the second edition of their book The C Programming Language, which describes what became known as C89, Kernighan and Ritchie wrote, "The ... [keyword] entry, formerly reserved but never used, is no longer reserved." and "The stillborn entry keyword is withdrawn."[33]
The "hello, world" example that appeared in the first edition of K&R has become the model for an introductory program in most programming textbooks. The program prints "hello, world" to the standard output, which is usually a terminal or screen display.
C is often used in low-level systems programming where escapes from the type system may be necessary.  The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a type cast to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.
Careless use of pointers is potentially dangerous.  Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects.  Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer.  In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.
C does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing.  The index values of the resulting "multi-dimensional array" can be thought of as increasing in row-major order. Multi-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, in early versions of C the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this was to allocate the array with an additional "row vector" of pointers to the columns.) C99 introduced "variable-length arrays" which address this issue.
One of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three principal ways to allocate memory for objects:[37]
Where possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary.[37]  Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on C dynamic memory allocation for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated.  (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)
The C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single "archive" file.  Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. For a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., -lm, shorthand for "link the math library").[37]
File input and output (I/O) is not part of the C language itself but instead is handled by libraries (such as the C standard library) and their associated header files (e.g. stdio.h). File handling is generally implemented through high-level I/O which works through streams. A stream is from this perspective a data flow that is independent of devices, while a file is a concrete device. The high-level I/O is done through the association of a stream to a file. In the C standard library, a buffer (a memory area or queue) is temporarily used to store data before it is sent to the final destination. This reduces the time spent waiting for slower devices, for example a hard drive or solid-state drive. Low-level I/O functions are not part of the standard C library[clarification needed] but are generally part of "bare metal" programming (programming that is independent of any operating system such as most embedded programming). With few exceptions, implementations include low-level I/O.
There are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.
C is widely used for systems programming in implementing operating systems and embedded system applications.[46] This is for several reasons:
C enables programmers to create efficient implementations of algorithms and data structures, because the layer of abstraction from hardware is thin, and its overhead is low, an important criterion for computationally intensive programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C.  Many languages support calling library functions in C, for example, the Python-based framework NumPy uses C for the high-performance and hardware-interacting aspects.
A consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C.[48] For example, the reference implementations of Python,[49] Perl,[50] Ruby,[51] and PHP[52] are written in C.
The two most popular web servers, Apache HTTP Server and Nginx, are both written in C.  These web servers interact with the operating system, listen on TCP ports for HTTP requests, and then serve up static web content, or cause the execution of other languages handling to 'render' content such as PHP, which is itself primarily written in C.  C's close-to-the-metal approach allows for the construction of these high-performance software systems.
For some purposes, restricted styles of C have been adopted, e.g. MISRA C or CERT C, in an attempt to reduce the opportunity for bugs.  Databases such as CWE attempt to count the ways C etc. has vulnerabilities, along with recommendations for mitigation.
When object-oriented programming languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.[66]
The C++ programming language (originally named "C with Classes") was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax.[67] C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now[when?] supports most of C, with a few exceptions.
Objective-C was originally a very "thin" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.
Joseph Leslie Armstrong (27 December 1950 – 20 April 2019) was a computer scientist working in the area of fault-tolerant distributed systems.  He is best known as one of the co-designers of the Erlang programming language.
He received a Ph.D. in Computer Science from the Royal Institute of Technology (KTH) in Stockholm, Sweden in 2003.[2][3] His dissertation was titled Making reliable distributed systems in the presence of software errors.[4] He was a professor at KTH from 2014 until his death.[2]
After briefly working for Donald Michie at the University of Edinburgh, Armstrong moved to Sweden in 1974 and joined the Ericsson Computer Science Laboratory at Kista in 1984.[2]
Originally a physicist, he switched to computer science when he ran out of money in the middle of his physics PhD and landed a job as a researcher working for Donald Michie — one of the founders of the field of artificial intelligence in Britain. At Michie's lab, Armstrong was exposed to the full range of AI goodies, becoming a founding member of the British Robotics Association and writing papers about robotic vision.
When funding for AI dried up as a result of the famous Lighthill report, it was back to physics-related programming for more than half a decade, first at the EISCAT scientific association and later the Swedish Space Corporation, before finally joining the Ericsson Computer Science Lab where he invented Erlang.[5]
It was at Ericsson in 1986, that he worked with Robert Virding and Mike Williams, to invent the Erlang programming language,[2] which was released as open source in 1998.[6]
In object-oriented programming, an interface or protocol type[a] is a data type that acts as an abstraction of a class. It describes a set of method signatures, the implementations of which may be provided by multiple classes that are otherwise not necessarily related to each other.[1] A class which provides the methods listed in a protocol is said to adopt the protocol,[2] or to implement the interface.[1]
Some programming languages provide explicit language support for protocols (Ada, C#, D, Dart, Delphi, Go, Java, Logtalk, Object Pascal, Objective-C, OCaml, PHP, Racket, Seed7, Swift, Python 3.8). In languages supporting multiple inheritance, such as C++, interfaces are implemented as abstract classes.
this, self, and Me are keywords used in some computer programming languages to refer to the object, class, or other entity which the currently running code is a part of.  The entity referred to thus depends on the execution context (such as which object has its method called).  Different programming languages use these keywords in slightly different ways.  In languages where a keyword like "this" is mandatory, the keyword is the only way to access data and methods stored in the current object.  Where optional, these keywords can disambiguate variables and functions with the same name.
In many object-oriented programming languages, this (also called self or Me) is a variable that is used in instance methods to refer to the object on which they are working. The first OO language, SIMULA 67, used this to explicitly reference the local object.[1]: 4.3.2.3  C++ and languages which derive in style from it (such as Java, C#, D, and PHP) also generally use this. Smalltalk and others, such as Object Pascal, Perl, Python, Ruby, Rust, Objective-C, DataFlex and Swift, use self. Microsoft's Visual Basic uses Me.
In the programming language Dylan, which is an object-oriented language that supports multimethods and doesn't have a concept of this, sending a message to an object is still kept in the syntax. The two forms below work in the same way; the differences are just syntactic sugar.
In JavaScript, which is a programming or scripting language used extensively in web browsers, this is an important keyword, although what it evaluates to depends on where it is used.
Lua itself is not object-oriented, but when combined with another feature called metatables, the use of self lets programmers define functions in a manner resembling object-oriented programming.
This is a list of notable programming languages with features designed for object-oriented programming (OOP).
Emerald is a distributed, object-oriented programming language developed in the 1980s by Andrew P. Black, Norman C. Hutchinson, Eric B. Jul, and Henry M. Levy, in the Department of Computer Science at the University of Washington.[1]
Emerald was designed to support high performance distribution, location, and high performance of objects, to simplify distributed programming, to exploit information hiding, and to be a small language.
JADE is a proprietary object-oriented software development and deployment platform product from the New Zealand-based Jade Software Corporation, first released in 1996.[1][2] It consists of the JADE programming language, Integrated development environment and debugger, integrated application server and object database management system.
Designed as an end-to-end development environment to allow systems to be coded in one language from the database server down to the clients, it also provides APIs for other languages, including .NET Framework,[3] Java, C/C++ and Web services.
Like all of the other popular programming languages used to create database-driven software, JADE is fully object-oriented. JADE was designed to have all the most important features of object-oriented programming, but does not support the overloading of methods or operators, and until Jade 2018 lacked parameterised constructors.[4]
Classes in JADE are kept together in schemas. Schemas serve the same purpose as Java packages or namespaces in .NET, but have a hierarchy, and inherit classes from superschemas. This becomes useful especially when programming using the model–view–controller methodology, as model classes can be put in one schema, then the controller and view classes can be built on top of the model classes in a subschema.
All the code for a JADE application is stored in its object-oriented database. This allows for multi-user development as the database maintains concurrency control, and with each piece of the code being a separate object in the database, it is often possible to recode a system while it is live and online as long as the parts of the system being changed are not in use.
The main goal of JADE was to create a seamlessly integrated programming language that would allow developers to create one application that would go from end-to-end instead of having to write three separate applications for the database server, application server and presentation client and then write the code for them to communicate with each other.
The most striking difference between JADE and other object-oriented programming languages is that its object database is a native part of its language. For example, when creating an object in JADE, it can be created as transient or persistent. Creating an object as transient is similar to creating objects in other object-oriented programming languages – the object is simply created in memory, and then lost when the program ends. On the other hand, when an object is created as persistent, when the program ends, the object will still exist and be there the next time the program starts up. So, when an object is persistent JADE automatically works in the background to store and retrieve the object in the database when necessary. Persistent objects can be distributed across multiple co-operating servers, with JADE automatically handling object caching and cache coherency.
There are very few differences between manipulating transient and persistent objects so JADE makes it appear to the programmer as if all the objects in the entire database were in local memory. Most of the time, JADE's object-oriented database is used in a multi-user system, and so JADE makes it appear to the programmer as if all the objects in the database were stored in shared memory that all users connected to the system can access, even from different computers.
With all of the program code centralised on the database server as well the data, all client nodes can be programmed as if they were running on the database server.
JADE's database is inherently object-oriented, and ACID-compliant, and has all of the standard features such as atomic transactions, locking, rollback, crash recovery and the ability to keep one or more secondary database servers synchronised with the main database for backup, disaster recovery and performance reasons.
To interoperate with relational databases for reporting, business intelligence and data warehouse purposes JADE since 2010 has provided a "Relational Population Service" that enables automatically replicating objects from its native object-orientated database to one or more relational database. This feature supports Microsoft SQL Server versions 2000, 2005 and 2008.
Database-driven software, often uses a three-tier methodology with applications being split into three tiers – data storage, processing and presentation. In the data storage and processing tiers, JADE systems are a collection of co-operating servers, called nodes, and multiple nodes may be involved in each tier. Each type of node has a different name and all are capable of manipulating objects and executing business logic. A collection of nodes can be deployed across one or several machines. Database servers handle data storage and can also execute business logic, while application servers handle processing. In a three-tier model, presentation clients provide the user interface. JADE also provides a two-tier client that combines the abilities of an application server and presentation client. Traditionally, these three tiers would be created by combining three programs and having them communicate to form one system. Having the different programs separate like this has many advantages, the main one is that the system becomes scalable, that is, raising the power of the system involves simply adding more nodes.
Designing a system like this gives the programmer a decision to consider every time they want to code in a particular function. They need to decide whether the function would run best on the database server, application server or presentation client before they begin coding as it will be difficult to change that decision once the functionality is coded into one of the tiers.
This is different for JADE applications, as they are coded as one application from end-to-end. When creating a JADE application, the programmer can think as if they were creating a program that will be running on the database server – as if all the data storage, processing and presentation were happening on one computer. When the program runs on three tiers, JADE automatically knows to run all the code by default on the application server, and to send database requests up to the database server and user interface information down to the presentation client. However, it is very easy for the programmer to switch the location at which a particular method is run and move it to a different tier by changing the method signature. Because of this, the decision on where a particular piece of code should run can be made late in the development cycle, and refactoring code to run on different parts of the system ends up being a lot easier because of the way JADE allows end-to-end development of software.
In the same schema, a JADE developer can create many completely separate applications which may provide different interfaces to access the same database.
The Smart Thin Client works by connecting to an Application Server which generally does all the processing on behalf of the Smart Thin Client, meaning the thin client only needs to be responsible for displaying forms and taking input. This means the computer running the thin client does not have to be a very powerful computer, and it does not require a fast network connection as it is not loading data from the database – JADE thin clients are often run over a dial-up connection. This is the reason they are called thin clients, as there is not a heavy requirement on computational power to run these clients.
The Standard Client is just the Smart Thin Client combined with the Application Server on one machine. In this case, the node running the client does all of the processing as well as the presentation. Standard clients have greater demands on computational power than thin clients, as they must load data from the database as well as do their own processing.
Web services are used to allow different programs to communicate with each other from remote locations in an object-oriented form. Web services cannot be accessed directly by human users. One of the uses of Web services with JADE is to allow other technologies such as .NET or Java to use JADE as the backend object-oriented database. Web services also allow JADE systems to interoperate with services provided by other non-JADE systems.
JADE was originally conceived by Sir Gilbert Simpson and is currently developed by the Jade Software Corporation.[6]
Self is a general-purpose, high-level, object-oriented programming language based on the concept of prototypes. Self began as a dialect of Smalltalk, being dynamically typed and using just-in-time compilation (JIT) with the prototype-based approach to objects: it was first used as an experimental test system for language design in the 1980s and 1990s. In 2006, Self was still being developed as part of the Klein project, which was a Self virtual machine written fully in Self. The latest version, 2024.1 was released in August 2024.[2]
Self was designed mostly by David Ungar and Randall Smith in 1986 while working at Xerox PARC. Their objective was to advance the state of the art in object-oriented programming language research, once Smalltalk-80 was released by the labs and began to be taken seriously by the industry. They moved to Stanford University and continued work on the language, building the first working Self compiler in 1987. Then, focus changed to working to build a full system for Self, in contrast to only the language.
The Morphic user interface construction environment was originally developed by Randy Smith and John Maloney for the Self programming language.[6] Morphic has been ported to other notable programming languages including Squeak, JavaScript, Python, and Objective-C.
Objects in Self can be modified to include additional slots. This can be done using the graphical programming environment, or with the primitive '_AddSlots:'. A primitive has the same syntax as a normal keyword message, but its name starts with the underscore character. The _AddSlots primitive should be avoided because it is a left over from early implementations. However, we will show it in the example below because it makes the code shorter.
One feature of Self is that it is based on the same sort of virtual machine system that earlier Smalltalk systems used. That is, programs are not stand-alone entities as they are in languages such as C, but need their entire memory environment in order to run. This requires that applications be shipped in chunks of saved memory known as snapshots or images. One disadvantage of this approach is that images are sometimes large and unwieldy; however, debugging an image is often simpler than debugging traditional programs because the runtime state is easier to inspect and modify. (The difference between source-based and image-based development is analogous to the difference between class-based and prototypical object-oriented programming.)
From a technical perspective, BETA provides several unique features. Classes and Procedures are unified to one concept, a Pattern. Also, classes are defined as properties/attributes of objects.  This means that a class cannot be instantiated without an explicit object context. A consequence of this is that BETA supports nested classes. Classes can be virtually defined, much like virtual methods can be in most object-oriented programming languages. Virtual entities (such as methods and classes) are never overwritten; instead they are redefined or specialized.
BETA supports the object-oriented perspective on programming and has comprehensive facilities for procedural and functional programming. It has powerful abstraction mechanisms to support identification of objects, classification and composition. BETA is a statically typed language like Simula, Eiffel and C++, with most type checking done at compile-time.[1] BETA aims to achieve an optimal balance between compile-time type checking and run-time type checking.
A major and peculiar feature of the language is the concept of patterns. In another programming language, such as C++, one would have several classes and procedures. BETA expresses both of these concepts using patterns.
A dynamic programming language is a type of programming language that allows various operations to be determined and executed at runtime. This is different from the compilation phase. Key decisions about variables, method calls, or data types are made when the program is running, unlike in static languages, where the structure and types are fixed during compilation. Dynamic languages provide flexibility. This allows developers to write more adaptable and concise code.
Static programming languages (possibly indirectly) require developers to define the size of utilized memory before compilation (unless working around with pointer logic). Consistent with object runtime alteration, dynamic languages implicitly need to (re-)allocate memory based on program individual operations.
A limited number of dynamic programming languages provide features which combine code introspection (the ability to examine classes, functions, and keywords to know what they are, what they do and what they know) and eval in a feature called macros. Most programmers today who are aware of the term macro have encountered them in C or C++, where they are a static feature which is built in a small subset of the language, and are capable only of string substitutions on the text of the program. In dynamic languages, however, they provide access to the inner workings of the compiler, and full access to the interpreter, virtual machine, or runtime, allowing the definition of language-like constructs which can optimize code or modify the syntax or grammar of the language.
Popular dynamic programming languages include JavaScript, Python, Ruby, PHP, Lua and Perl. The following are generally considered dynamic languages:
Apache Groovy is a Java-syntax-compatible object-oriented programming language for the Java platform. It is both a static and dynamic language with features similar to those of Python, Ruby, and Smalltalk. It can be used as both a programming language and a scripting language for the Java Platform, is compiled to Java virtual machine (JVM) bytecode, and interoperates seamlessly with other Java code and libraries. Groovy uses a curly-bracket syntax similar to Java's. Groovy supports closures, multiline strings, and expressions embedded in strings. Much of Groovy's power lies in its AST transformations, triggered through annotations.
Groovy 1.0 was released on January 2, 2007, and Groovy 2.0 in July, 2012. Since version 2, Groovy can be compiled statically, offering type inference and performance near that of Java.[4][5] Groovy 2.4 was the last major release under Pivotal Software's sponsorship which ended in March 2015.[6] Groovy has since changed its governance structure to a Project Management Committee in the Apache Software Foundation.[7]
When the Pivotal Software joint venture was spun-off by EMC Corporation (EMC) and VMware in April 2013, Groovy and Grails formed part of its product portfolio. Pivotal ceased sponsoring Groovy and Grails from April 2015.[6]
That same month, Groovy changed its governance structure from a Codehaus repository to a Project Management Committee (PMC) in the Apache Software Foundation via its incubator.[7]
Groovy graduated from Apache's incubator and became a top-level project in November 2015.[15]
Most valid Java files are also valid Groovy files. Although the two languages are similar, Groovy code can be more compact, because it does not need all the elements that Java needs.[18] This makes it possible for Java programmers to learn Groovy gradually by starting with familiar Java syntax before acquiring more Groovy programming idioms.[19]
Groovy natively supports markup languages such as XML and HTML by using an inline Document Object Model (DOM) syntax. This feature enables the definition and manipulation of many types of heterogeneous data assets with a uniform and concise syntax and programming methodology.[citation needed]
The Grails framework uses metaprogramming extensively to enable GORM dynamic finders, like User.findByName('Josh') and others.[26]
Although Groovy is mostly an object-oriented language, it also offers functional programming features.
On JavaScript Object Notation (JSON) and XML processing, Groovy employs the Builder pattern, making the production of the data structure less verbose. For example, the following XML:
According to Groovy's own documentation, "When the Groovy compiler compiles Groovy scripts and classes, at some point in the process, the source code will end up being represented in memory in the form of a Concrete Syntax Tree, then transformed into an Abstract Syntax Tree. The purpose of AST Transformations is to let developers hook into the compilation process to be able to modify the AST before it is turned into bytecode that will be run by the JVM. AST Transformations provides Groovy with improved compile-time metaprogramming capabilities allowing powerful flexibility at the language level, without a runtime performance penalty."[36]
Distributed Relational Database Architecture (DRDA) is a database interoperability standard from The Open Group.
DRDA describes the architecture for distributed relational databases. It defines the rules for accessing the distributed data, but it does not provide the actual application programming interfaces (APIs) to perform the access. It was first used in DB2 2.3.
In software engineering, a software design pattern or design pattern is a general, reusable solution to a commonly occurring problem in many contexts in software design.[1] A design pattern is not a rigid structure to be transplanted directly into source code. Rather, it is a description or a template for solving a particular type of problem that can be deployed in many different situations.[2] Design patterns can be viewed as formalized best practices that the programmer may use to solve common problems when designing a software application or system.
Object-oriented design patterns typically show relationships and interactions between classes or objects, without specifying the final application classes or objects that are involved.[citation needed] Patterns that imply mutable state may be unsuited for functional programming languages. Some patterns can be rendered unnecessary in languages that have built-in support for solving the problem they are trying to solve, and object-oriented patterns are not necessarily suitable for non-object-oriented languages.[citation needed]
Design patterns may be viewed as a structured approach to computer programming intermediate between the levels of a programming paradigm and a concrete algorithm.[citation needed]
Patterns originated as an architectural concept by Christopher Alexander as early as 1977 in A Pattern Language (c.f. his article, "The Pattern of Streets," JOURNAL OF THE AIP, September, 1966, Vol. 32, No. 5, pp. 273–278). In 1987, Kent Beck and Ward Cunningham began experimenting with the idea of applying patterns to programming – specifically pattern languages – and presented their results at the OOPSLA conference that year.[3][4] In the following years, Beck, Cunningham and others followed up on this work.
Design patterns gained popularity in computer science after the book Design Patterns: Elements of Reusable Object-Oriented Software was published in 1994  by the so-called "Gang of Four" (Erich Gamma, Richard Helm, Ralph Johnson and John Vlissides), which is frequently abbreviated as "GoF". That same year, the first Pattern Languages of Programming Conference was held, and the following year the Portland Pattern Repository was set up for documentation of design patterns. The scope of the term remains a matter of dispute. Notable books in the design pattern genre include:
Design patterns can speed up the development process by providing proven development paradigms.[6] Effective software design requires considering issues that may not become apparent until later in the implementation. Freshly written code can often have hidden, subtle issues that take time to be detected; issues that sometimes can cause major problems down the road. Reusing design patterns can help to prevent such issues,[7] and enhance code readability for those familiar with the patterns.
Software design techniques are difficult to apply to a broader range of problems.[citation needed] Design patterns provide general solutions, documented in a format that does not require specifics tied to a particular problem.
In 1996, Christopher Alexander was invited to give a Keynote Speech to the 1996 OOPSLA Convention.  Here he reflected on how his work on Patterns in Architecture had developed and his hopes for how the Software Design community could help Architecture extend Patterns to create living structures that use generative schemes that are more like computer code.
The annual Pattern Languages of Programming Conference proceedings [14] include many examples of domain-specific patterns.
Object-oriented design patterns typically show relationships and interactions between classes or objects, without specifying the final application classes or objects that are involved. Patterns that imply mutable state may be unsuited for functional programming languages. Some patterns can be rendered unnecessary in languages that have built-in support for solving the problem they are trying to solve, and object-oriented patterns are not necessarily suitable for non-object-oriented languages.
Some suggest that design patterns may be a sign that features are missing in a given programming language (Java or C++ for instance). Peter Norvig demonstrates that 16 out of the 23 patterns in the Design Patterns book (which is primarily focused on C++) are simplified or eliminated (via direct language support) in Lisp or Dylan.[29] Related observations were made by Hannemann and Kiczales who implemented several of the 23 design patterns using an aspect-oriented programming language (AspectJ) and showed that code-level dependencies were removed from the implementations of 17 of the 23 design patterns and that aspect-oriented programming could simplify the implementations of design patterns.[30]
See also Paul Graham's essay "Revenge of the Nerds".[31]
By definition, a pattern must be programmed anew into each application that uses it. Since some authors see this as a step backward from software reuse as provided by components, researchers have worked to turn patterns into components. Meyer and Arnout were able to provide full or partial componentization of two-thirds of the patterns they attempted.[34]
Software design patterns offer finer granularity compared to software architecture patterns and software architecture styles, as design patterns focus on solving detailed, low-level design problems within individual components or subsystems. Examples include Singleton, Factory Method, and Observer. [35][36][37]
Software Architecture Pattern refers to a reusable, proven solution to a recurring problem at the system level, addressing concerns related to the overall structure, component interactions, and quality attributes of the system.[citation needed] Software architecture patterns operate at a higher level of abstraction than design patterns, solving broader system-level challenges. While these patterns typically affect system-level concerns, the distinction between architectural patterns and architectural styles can sometimes be blurry. Examples include Circuit Breaker. [35][36][37]
Software Architecture Style refers to a high-level structural organization that defines the overall system organization, specifying how components are organized, how they interact, and the constraints on those interactions.[citation needed] Architecture styles typically include a vocabulary of component and connector types, as well as semantic models for interpreting the system's properties. These styles represent the most coarse-grained level of system organization. Examples include Layered Architecture, Microservices, and Event-Driven Architecture. [35][36][37]
In software engineering, a software design pattern or design pattern is a general, reusable solution to a commonly occurring problem in many contexts in software design.[1] A design pattern is not a rigid structure to be transplanted directly into source code. Rather, it is a description or a template for solving a particular type of problem that can be deployed in many different situations.[2] Design patterns can be viewed as formalized best practices that the programmer may use to solve common problems when designing a software application or system.
Object-oriented design patterns typically show relationships and interactions between classes or objects, without specifying the final application classes or objects that are involved.[citation needed] Patterns that imply mutable state may be unsuited for functional programming languages. Some patterns can be rendered unnecessary in languages that have built-in support for solving the problem they are trying to solve, and object-oriented patterns are not necessarily suitable for non-object-oriented languages.[citation needed]
Design patterns may be viewed as a structured approach to computer programming intermediate between the levels of a programming paradigm and a concrete algorithm.[citation needed]
Patterns originated as an architectural concept by Christopher Alexander as early as 1977 in A Pattern Language (c.f. his article, "The Pattern of Streets," JOURNAL OF THE AIP, September, 1966, Vol. 32, No. 5, pp. 273–278). In 1987, Kent Beck and Ward Cunningham began experimenting with the idea of applying patterns to programming – specifically pattern languages – and presented their results at the OOPSLA conference that year.[3][4] In the following years, Beck, Cunningham and others followed up on this work.
Design patterns gained popularity in computer science after the book Design Patterns: Elements of Reusable Object-Oriented Software was published in 1994  by the so-called "Gang of Four" (Erich Gamma, Richard Helm, Ralph Johnson and John Vlissides), which is frequently abbreviated as "GoF". That same year, the first Pattern Languages of Programming Conference was held, and the following year the Portland Pattern Repository was set up for documentation of design patterns. The scope of the term remains a matter of dispute. Notable books in the design pattern genre include:
Design patterns can speed up the development process by providing proven development paradigms.[6] Effective software design requires considering issues that may not become apparent until later in the implementation. Freshly written code can often have hidden, subtle issues that take time to be detected; issues that sometimes can cause major problems down the road. Reusing design patterns can help to prevent such issues,[7] and enhance code readability for those familiar with the patterns.
Software design techniques are difficult to apply to a broader range of problems.[citation needed] Design patterns provide general solutions, documented in a format that does not require specifics tied to a particular problem.
In 1996, Christopher Alexander was invited to give a Keynote Speech to the 1996 OOPSLA Convention.  Here he reflected on how his work on Patterns in Architecture had developed and his hopes for how the Software Design community could help Architecture extend Patterns to create living structures that use generative schemes that are more like computer code.
The annual Pattern Languages of Programming Conference proceedings [14] include many examples of domain-specific patterns.
Object-oriented design patterns typically show relationships and interactions between classes or objects, without specifying the final application classes or objects that are involved. Patterns that imply mutable state may be unsuited for functional programming languages. Some patterns can be rendered unnecessary in languages that have built-in support for solving the problem they are trying to solve, and object-oriented patterns are not necessarily suitable for non-object-oriented languages.
Some suggest that design patterns may be a sign that features are missing in a given programming language (Java or C++ for instance). Peter Norvig demonstrates that 16 out of the 23 patterns in the Design Patterns book (which is primarily focused on C++) are simplified or eliminated (via direct language support) in Lisp or Dylan.[29] Related observations were made by Hannemann and Kiczales who implemented several of the 23 design patterns using an aspect-oriented programming language (AspectJ) and showed that code-level dependencies were removed from the implementations of 17 of the 23 design patterns and that aspect-oriented programming could simplify the implementations of design patterns.[30]
See also Paul Graham's essay "Revenge of the Nerds".[31]
By definition, a pattern must be programmed anew into each application that uses it. Since some authors see this as a step backward from software reuse as provided by components, researchers have worked to turn patterns into components. Meyer and Arnout were able to provide full or partial componentization of two-thirds of the patterns they attempted.[34]
Software design patterns offer finer granularity compared to software architecture patterns and software architecture styles, as design patterns focus on solving detailed, low-level design problems within individual components or subsystems. Examples include Singleton, Factory Method, and Observer. [35][36][37]
Software Architecture Pattern refers to a reusable, proven solution to a recurring problem at the system level, addressing concerns related to the overall structure, component interactions, and quality attributes of the system.[citation needed] Software architecture patterns operate at a higher level of abstraction than design patterns, solving broader system-level challenges. While these patterns typically affect system-level concerns, the distinction between architectural patterns and architectural styles can sometimes be blurry. Examples include Circuit Breaker. [35][36][37]
Software Architecture Style refers to a high-level structural organization that defines the overall system organization, specifying how components are organized, how they interact, and the constraints on those interactions.[citation needed] Architecture styles typically include a vocabulary of component and connector types, as well as semantic models for interpreting the system's properties. These styles represent the most coarse-grained level of system organization. Examples include Layered Architecture, Microservices, and Event-Driven Architecture. [35][36][37]
An object database or object-oriented database  is a database management system in which information is represented in the form of objects as used in object-oriented programming. Object databases are different from relational databases which are table-oriented. A third type, object–relational databases, is a hybrid of both approaches.
Object databases have been considered since the early 1980s.[2]
Object-oriented database management systems (OODBMSs) also called ODBMS (Object Database Management System)  combine database capabilities with object-oriented programming language capabilities.
OODBMSs allow object-oriented programmers to develop the product, store them as objects, and replicate or modify existing objects to make new objects within the OODBMS. Because the database is integrated with the programming language, the programmer can maintain consistency within one environment, in that both the OODBMS and the programming language will use the same model of representation. Relational DBMS projects, by way of contrast, maintain a clearer division between the database model and the application.
Some object-oriented databases are designed to work well with object-oriented programming languages such as Delphi, Ruby, Python, JavaScript, Perl, Java, C#, Visual Basic .NET, C++, Objective-C and Smalltalk; others such as JADE have their own programming languages. OODBMSs use exactly the same model as object-oriented programming languages.
Object database management systems grew out of research during the early to mid-1970s into having intrinsic database management support for graph-structured objects. The term "object-oriented database system" first appeared around 1985.[4] Notable research projects included Encore-Ob/Server (Brown University), EXODUS (University of Wisconsin–Madison), IRIS (Hewlett-Packard), ODE (Bell Labs), ORION (Microelectronics and Computer Technology Corporation or MCC), Vodak (GMD-IPSI), and Zeitgeist (Texas Instruments). The ORION project had more published papers than any of the other efforts. Won Kim of MCC compiled the best of those papers in a book published by The MIT Press.[5]
Early commercial products included Gemstone (Servio Logic, name changed to GemStone Systems), Gbase (Graphael), and Vbase (Ontologic). Additional commercial products entered the market in the late 1980s through the mid 1990s. These included ITASCA (Itasca Systems), Jasmine (Fujitsu, marketed by Computer Associates), Matisse (Matisse Software), Objectivity/DB (Objectivity, Inc.), ObjectStore (Progress Software, acquired from eXcelon which was originally Object Design, Incorporated), ONTOS (Ontos, Inc., name changed from Ontologic), O2[6] (O2 Technology, merged with several companies, acquired by Informix, which was in turn acquired by IBM), POET (now FastObjects from Versant which acquired Poet Software), Versant Object Database (Versant Corporation), VOSS (Logic Arts) and JADE (Jade Software Corporation). Some of these products remain on the market and have been joined by new open source and commercial products such as InterSystems Caché.
Object database management systems added the concept of persistence to object programming languages. The early commercial products were integrated with various languages: GemStone (Smalltalk), Gbase (LISP), Vbase (COP) and VOSS (Virtual Object Storage System for Smalltalk). For much of the 1990s, C++ dominated the commercial object database management market. Vendors added Java in the late 1990s and more recently, C#.
Starting in 2004, object databases have seen a second growth period when open source object databases emerged that were widely affordable and easy to use, because they are entirely written in OOP languages like Smalltalk, Java, or C#, such as Versant's db4o (db4objects), DTS/S1 from Obsidian Dynamics and Perst (McObject), available under dual open source and commercial licensing.
Object databases based on persistent programming acquired a niche in application areas such as
engineering and spatial databases, telecommunications, and scientific areas such as high energy physics[13] and molecular biology.[14]
Another group of object databases focuses on embedded use in devices, packaged software, and real-time systems.
Most object databases also offer some kind of query language, allowing objects to be found using a declarative programming approach. It is in the area of object query languages, and the integration of the query and navigational interfaces, that the biggest differences between products are found. An attempt at standardization was made by the ODMG with the Object Query Language, OQL.
Another area of variation between products is in the way that the schema of a database is defined. A general characteristic, however, is that the programming language and the database schema use the same type definitions.
Many object databases, for example Gemstone or VOSS, offer support for versioning. An object can be viewed as the set of all its versions. Also, object versions can be treated as objects in their own right. Some object databases also provide systematic support for triggers and constraints which are the basis of active databases.
The efficiency of such a database is also greatly improved in areas which demand massive amounts of data about one item. For example, a banking institution could get the user's account information and provide them efficiently with extensive information such as transactions, account information entries etc.
The Object Data Management Group  was a consortium of object database and object–relational mapping vendors, members of the academic community, and interested parties. Its goal was to create a set of specifications that would allow for portable applications that store objects in database management systems. It published several versions of its specification. The last release was ODMG 3.0. By 2001, most of the major object database and object–relational mapping vendors claimed conformance to the ODMG Java Language Binding. Compliance to the other components of the specification was mixed. In 2001, the ODMG Java Language Binding was submitted to the Java Community Process as a basis for the Java Data Objects specification. The ODMG member companies then decided to concentrate their efforts on the Java Data Objects specification. As a result, the ODMG disbanded in 2001.
Many object database ideas were also absorbed into SQL:1999 and have been implemented in varying degrees in object–relational database products.
In 2005 Cook, Rai, and Rosenberger proposed to drop all standardization efforts to introduce additional object-oriented query APIs but rather use the OO programming language itself, i.e., Java and .NET, to express queries.  As a result, Native Queries emerged. Similarly, Microsoft announced Language Integrated Query (LINQ) and DLINQ, an implementation of LINQ, in September 2005, to provide close, language-integrated database query capabilities with its programming languages C# and VB.NET 9.
In February 2006, the Object Management Group (OMG) announced that they had been granted the right to develop new specifications based on the ODMG 3.0 specification and the formation of the Object Database Technology Working Group (ODBT WG). The ODBT WG planned to create a set of standards that would incorporate advances in object database technology (e.g., replication), data management (e.g., spatial indexing), and data formats (e.g., XML) and to include new features into these standards that support domains where object databases are being adopted (e.g., real-time systems).  The work of the ODBT WG was suspended in March 2009 when, subsequent to the economic turmoil in late 2008, the ODB vendors involved in this effort decided to focus their resources elsewhere.
In January 2007 the World Wide Web Consortium gave final recommendation status to the XQuery language.  XQuery uses XML as its data model. Some of the ideas developed originally for object databases found their way into XQuery, but XQuery is not intrinsically object-oriented. Because of the popularity of XML, XQuery engines compete with object databases as a vehicle for storage of data that is too complex or variable to hold conveniently in a relational database. XQuery also allows modules to be written to provide encapsulation features that have been provided by Object-Oriented systems.
XQuery v1 and XPath v2 and later are powerful and are available in both open source and libre (FOSS) software,[15][16][17] as well as in commercial systems. They are easy to learn and use, and very powerful and fast. They are not relational and XQuery is not based on SQL (although one of the people who designed XQuery also co-invented SQL). But they are also not object-oriented, in the programming sense: XQuery does not use encapsulation with hiding, implicit dispatch, and classes and methods. XQuery databases generally use XML and JSON as an interchange format, although other formats are used.
Since the early 2000s JSON has gained community adoption and popularity in applications where developers are in control of the data format. JSONiq, a query-analog of XQuery for JSON (sharing XQuery's core expressions and operations), demonstrated the functional equivalence of the JSON and XML formats for data-oriented information. In this context, the main strategy of OODBMS maintainers was to retrofit JSON to their databases (by using it as the internal data type).
An object database stores complex data and relationships between data directly, without mapping to relational rows and columns, and this makes them suitable for applications dealing with very complex data.[19] Objects have a many-to-many relationship and are accessed by the use of pointers. Pointers are linked to objects to establish relationships. Another benefit of an OODBMS is that it can be programmed with small procedural differences without affecting the entire system.[20]
A relational database (RDB[1]) is a database  based on the relational model of data, as proposed by E. F. Codd in 1970.[2]
A Relational Database Management System (RDBMS) is a type of database management system that stores data in a structured format using rows and columns.
Many relational database systems are equipped with the option of using SQL (Structured Query Language) for querying and updating the database.[3]
The concept of relational database was defined by E. F. Codd at IBM in 1970. Codd introduced the term relational in his research paper "A Relational Model of Data for Large Shared Data Banks".[2] In this paper and later papers, he defined what he meant by relation. One well-known definition of what constitutes a relational database system is composed of Codd's 12 rules.
However, no commercial implementations of the relational model conform to all of Codd's rules,[4] so the term has gradually come to describe a broader class of database systems, which at a minimum:
In 1974, IBM began developing System R, a research project to develop a prototype RDBMS.[5][6]
The first system sold as an RDBMS was Multics Relational Data Store (June 1976).[7][8][citation needed] Oracle was released in 1979 by Relational Software, now Oracle Corporation.[9] Ingres and IBM BS12 followed. Other examples of an RDBMS include IBM Db2, SAP Sybase ASE, and Informix. In 1984, the first RDBMS for Macintosh began being developed, code-named Silver Surfer, and was released in 1987 as 4th Dimension and known today as 4D.[10]
A second school of thought argues that if a database does not implement all of Codd's rules (or the current understanding on the relational model, as expressed by Christopher J. Date, Hugh Darwen and others), it is not relational. This view, shared by many theorists and other strict adherents to Codd's principles, would disqualify most DBMSs as not relational. For clarification, they often refer to some RDBMSs as truly-relational database management systems (TRDBMS), naming others pseudo-relational database management systems (PRDBMS).[citation needed]
The primary keys within a database are used to define the relationships among the tables. When a PK migrates to another table, it becomes a foreign key (FK) in the other table. When each cell can contain only one value and the PK migrates into a regular entity table, this design pattern can represent either a one-to-one or one-to-many relationship. Most relational database designs resolve many-to-many relationships by creating an additional table that contains the PKs from both of the other entity tables –  the relationship becomes an entity; the resolution table is then named appropriately and the two FKs are combined to form a PK. The migration of PKs to other tables is the second major reason why system-assigned integers are used normally as PKs; there is usually neither efficiency nor clarity in migrating a bunch of other types of columns.
In order for a database management system (DBMS) to operate efficiently and accurately, it must use ACID transactions.[18][19][20]
Part of the programming within a RDBMS is accomplished using stored procedures (SPs). Often procedures can be used to greatly reduce the amount of information transferred within and outside of a system. For increased security, the system design may grant access to only the stored procedures and not directly to the tables. Fundamental stored procedures contain the logic needed to insert new and update existing data. More complex procedures may be written to implement additional rules and logic related to processing or selecting the data.
The relational database was first defined in June 1970 by Edgar Codd, of IBM's San Jose Research Laboratory.[2] Codd's view of what qualifies as an RDBMS is summarized in Codd's 12 rules. A relational database has become the predominant type of database. Other models besides the relational model include the hierarchical database model and the network model.
The table below summarizes some of the most important relational database terms and the corresponding SQL term:
In a relational database, a relation is a set of tuples that have the same attributes. A tuple usually represents an object and information about that object. Objects are typically physical objects or concepts. A relation is usually described as a table, which is organized into rows and columns. All the data referenced by an attribute are in the same domain and conform to the same constraints.
Constraints are often used to make it possible to further restrict the domain of an attribute. For instance, a constraint can restrict a given integer attribute to values between 1 and 10. Constraints provide one method of implementing business rules in the database and support subsequent data use within the application layer. SQL implements constraint functionality in the form of check constraints.
Constraints restrict the data that can be stored in relations. These are usually defined using expressions that result in a Boolean value, indicating whether or not the data satisfies the constraint. Constraints can apply to single attributes, to a tuple (restricting combinations of attributes) or to an entire relation.
Since every attribute has an associated domain, there are constraints (domain constraints). The two principal rules for the relational model are known as entity integrity and referential integrity.
A stored procedure is executable code that is associated with, and generally stored in, the database. Stored procedures usually collect and customize common operations, like inserting a tuple into a relation, gathering statistical information about usage patterns, or encapsulating complex business logic and calculations. Frequently they are used as an application programming interface (API) for security or simplicity. Implementations of stored procedures on SQL RDBMS's often allow developers to take advantage of procedural extensions (often vendor-specific) to the standard declarative SQL syntax.
Stored procedures are not part of the relational database model, but all commercial implementations include them.
An index is one way of providing quicker access to data. Indices can be created on any combination of attributes on a relation. Queries that filter using those attributes can find matching tuples directly using the index (similar to Hash table lookup), without having to check each tuple in turn. This is analogous to using the index of a book to go directly to the page on which the information you are looking for is found, so that you do not have to read the entire book to find what you are looking for. Relational databases typically supply multiple indexing techniques, each of which is optimal for some combination of data distribution, relation size, and typical access pattern. Indices are usually implemented via B+ trees, R-trees, and bitmaps.
Indices are usually not considered part of the database, as they are considered an implementation detail, though indices are usually maintained by the same group that maintains the other parts of the database. The use of efficient indexes on both primary and foreign keys can dramatically improve query performance. This is because B-tree indexes result in query times proportional to log(n) where n is the number of rows in a table and hash indexes result in constant time queries (no size dependency as long as the relevant part of the index fits into memory).
Queries made against the relational database, and the derived relvars in the database are expressed in a relational calculus or a relational algebra. In his original relational algebra, Codd introduced eight relational operators in two groups of four operators each. The first four operators were based on the traditional mathematical set operations:
The remaining operators proposed by Codd involve special operations specific to relational databases:
Normalization was first proposed by Codd as an integral part of the relational model. It encompasses a set of procedures designed to eliminate non-simple domains (non-atomic values) and the redundancy (duplication) of data, which in turn prevents data manipulation anomalies and loss of data integrity. The most common forms of normalization applied to databases are called the normal forms.
Connolly and Begg define database management system (DBMS) as a "software system that enables users to define, create, maintain and control access to the database".[23] RDBMS is an extension of that initialism that is sometimes used when the underlying database is relational.
An alternative definition for a relational database management system is a database management system (DBMS) based on the relational model. Most databases in widespread use today are based on this model.[24]
RDBMSs have been a common option for the storage of information in databases used for financial records, manufacturing and logistical information, personnel data, and other applications since the 1980s. Relational databases have often replaced legacy hierarchical databases and network databases, because RDBMS were easier to implement and administer. Nonetheless, relational stored data received continued, unsuccessful challenges by object database management systems in the 1980s and 1990s, (which were introduced in an attempt to address the so-called object–relational impedance mismatch between relational databases and object-oriented application programs), as well as by XML database management systems in the 1990s.[25] However, due to the expanse of technologies, such as horizontal scaling of computer clusters, NoSQL databases have recently become popular as an alternative to RDBMS databases.[26]
Distributed Relational Database Architecture (DRDA) was designed by a workgroup within IBM in the period 1988 to 1994. DRDA enables network connected relational databases to cooperate to fulfill SQL requests.[27][28]
The messages, protocols, and structural components of DRDA are defined by the Distributed Data Management Architecture.
According to research company Gartner, in 2011, the five leading proprietary software relational database vendors by revenue were Oracle (48.8%), IBM (20.2%), Microsoft (17.0%), SAP including Sybase (4.6%), and Teradata (3.7%).[30]
A relational database (RDB[1]) is a database  based on the relational model of data, as proposed by E. F. Codd in 1970.[2]
A Relational Database Management System (RDBMS) is a type of database management system that stores data in a structured format using rows and columns.
Many relational database systems are equipped with the option of using SQL (Structured Query Language) for querying and updating the database.[3]
The concept of relational database was defined by E. F. Codd at IBM in 1970. Codd introduced the term relational in his research paper "A Relational Model of Data for Large Shared Data Banks".[2] In this paper and later papers, he defined what he meant by relation. One well-known definition of what constitutes a relational database system is composed of Codd's 12 rules.
However, no commercial implementations of the relational model conform to all of Codd's rules,[4] so the term has gradually come to describe a broader class of database systems, which at a minimum:
In 1974, IBM began developing System R, a research project to develop a prototype RDBMS.[5][6]
The first system sold as an RDBMS was Multics Relational Data Store (June 1976).[7][8][citation needed] Oracle was released in 1979 by Relational Software, now Oracle Corporation.[9] Ingres and IBM BS12 followed. Other examples of an RDBMS include IBM Db2, SAP Sybase ASE, and Informix. In 1984, the first RDBMS for Macintosh began being developed, code-named Silver Surfer, and was released in 1987 as 4th Dimension and known today as 4D.[10]
A second school of thought argues that if a database does not implement all of Codd's rules (or the current understanding on the relational model, as expressed by Christopher J. Date, Hugh Darwen and others), it is not relational. This view, shared by many theorists and other strict adherents to Codd's principles, would disqualify most DBMSs as not relational. For clarification, they often refer to some RDBMSs as truly-relational database management systems (TRDBMS), naming others pseudo-relational database management systems (PRDBMS).[citation needed]
The primary keys within a database are used to define the relationships among the tables. When a PK migrates to another table, it becomes a foreign key (FK) in the other table. When each cell can contain only one value and the PK migrates into a regular entity table, this design pattern can represent either a one-to-one or one-to-many relationship. Most relational database designs resolve many-to-many relationships by creating an additional table that contains the PKs from both of the other entity tables –  the relationship becomes an entity; the resolution table is then named appropriately and the two FKs are combined to form a PK. The migration of PKs to other tables is the second major reason why system-assigned integers are used normally as PKs; there is usually neither efficiency nor clarity in migrating a bunch of other types of columns.
In order for a database management system (DBMS) to operate efficiently and accurately, it must use ACID transactions.[18][19][20]
Part of the programming within a RDBMS is accomplished using stored procedures (SPs). Often procedures can be used to greatly reduce the amount of information transferred within and outside of a system. For increased security, the system design may grant access to only the stored procedures and not directly to the tables. Fundamental stored procedures contain the logic needed to insert new and update existing data. More complex procedures may be written to implement additional rules and logic related to processing or selecting the data.
The relational database was first defined in June 1970 by Edgar Codd, of IBM's San Jose Research Laboratory.[2] Codd's view of what qualifies as an RDBMS is summarized in Codd's 12 rules. A relational database has become the predominant type of database. Other models besides the relational model include the hierarchical database model and the network model.
The table below summarizes some of the most important relational database terms and the corresponding SQL term:
In a relational database, a relation is a set of tuples that have the same attributes. A tuple usually represents an object and information about that object. Objects are typically physical objects or concepts. A relation is usually described as a table, which is organized into rows and columns. All the data referenced by an attribute are in the same domain and conform to the same constraints.
Constraints are often used to make it possible to further restrict the domain of an attribute. For instance, a constraint can restrict a given integer attribute to values between 1 and 10. Constraints provide one method of implementing business rules in the database and support subsequent data use within the application layer. SQL implements constraint functionality in the form of check constraints.
Constraints restrict the data that can be stored in relations. These are usually defined using expressions that result in a Boolean value, indicating whether or not the data satisfies the constraint. Constraints can apply to single attributes, to a tuple (restricting combinations of attributes) or to an entire relation.
Since every attribute has an associated domain, there are constraints (domain constraints). The two principal rules for the relational model are known as entity integrity and referential integrity.
A stored procedure is executable code that is associated with, and generally stored in, the database. Stored procedures usually collect and customize common operations, like inserting a tuple into a relation, gathering statistical information about usage patterns, or encapsulating complex business logic and calculations. Frequently they are used as an application programming interface (API) for security or simplicity. Implementations of stored procedures on SQL RDBMS's often allow developers to take advantage of procedural extensions (often vendor-specific) to the standard declarative SQL syntax.
Stored procedures are not part of the relational database model, but all commercial implementations include them.
An index is one way of providing quicker access to data. Indices can be created on any combination of attributes on a relation. Queries that filter using those attributes can find matching tuples directly using the index (similar to Hash table lookup), without having to check each tuple in turn. This is analogous to using the index of a book to go directly to the page on which the information you are looking for is found, so that you do not have to read the entire book to find what you are looking for. Relational databases typically supply multiple indexing techniques, each of which is optimal for some combination of data distribution, relation size, and typical access pattern. Indices are usually implemented via B+ trees, R-trees, and bitmaps.
Indices are usually not considered part of the database, as they are considered an implementation detail, though indices are usually maintained by the same group that maintains the other parts of the database. The use of efficient indexes on both primary and foreign keys can dramatically improve query performance. This is because B-tree indexes result in query times proportional to log(n) where n is the number of rows in a table and hash indexes result in constant time queries (no size dependency as long as the relevant part of the index fits into memory).
Queries made against the relational database, and the derived relvars in the database are expressed in a relational calculus or a relational algebra. In his original relational algebra, Codd introduced eight relational operators in two groups of four operators each. The first four operators were based on the traditional mathematical set operations:
The remaining operators proposed by Codd involve special operations specific to relational databases:
Normalization was first proposed by Codd as an integral part of the relational model. It encompasses a set of procedures designed to eliminate non-simple domains (non-atomic values) and the redundancy (duplication) of data, which in turn prevents data manipulation anomalies and loss of data integrity. The most common forms of normalization applied to databases are called the normal forms.
Connolly and Begg define database management system (DBMS) as a "software system that enables users to define, create, maintain and control access to the database".[23] RDBMS is an extension of that initialism that is sometimes used when the underlying database is relational.
An alternative definition for a relational database management system is a database management system (DBMS) based on the relational model. Most databases in widespread use today are based on this model.[24]
RDBMSs have been a common option for the storage of information in databases used for financial records, manufacturing and logistical information, personnel data, and other applications since the 1980s. Relational databases have often replaced legacy hierarchical databases and network databases, because RDBMS were easier to implement and administer. Nonetheless, relational stored data received continued, unsuccessful challenges by object database management systems in the 1980s and 1990s, (which were introduced in an attempt to address the so-called object–relational impedance mismatch between relational databases and object-oriented application programs), as well as by XML database management systems in the 1990s.[25] However, due to the expanse of technologies, such as horizontal scaling of computer clusters, NoSQL databases have recently become popular as an alternative to RDBMS databases.[26]
Distributed Relational Database Architecture (DRDA) was designed by a workgroup within IBM in the period 1988 to 1994. DRDA enables network connected relational databases to cooperate to fulfill SQL requests.[27][28]
The messages, protocols, and structural components of DRDA are defined by the Distributed Data Management Architecture.
According to research company Gartner, in 2011, the five leading proprietary software relational database vendors by revenue were Oracle (48.8%), IBM (20.2%), Microsoft (17.0%), SAP including Sybase (4.6%), and Teradata (3.7%).[30]
In computer programming, data-driven programming is a programming paradigm in which the program statements describe the data to be matched and the processing required rather than defining a sequence of steps to be taken.[1] Standard examples of data-driven languages are the text-processing languages sed and AWK,[1] and the document transformation language XSLT, where the data is a sequence of lines in an input stream – these are thus also known as line-oriented languages – and pattern matching is primarily done via regular expressions or line numbers.
Data-driven programming is similar to event-driven programming, in that both are structured as pattern matching and resulting processing, and are usually implemented by a main loop, though they are typically applied to different domains. The condition/action model is also similar to aspect-oriented programming, where when a join point (condition) is reached, a pointcut (action) is executed. A similar paradigm is used in some tracing frameworks such as DTrace, where one lists probes (instrumentation points) and associated actions, which execute when the condition is satisfied.
Adapting abstract data type design methods to object-oriented programming results in a data-driven design.[2] This type of design is sometimes used in object-oriented programming to define classes during the conception of a piece of software.
Data-driven programming is typically applied to streams of structured data, for filtering, transforming, aggregating (such as computing statistics), or calling other programs. Typical streams include log files, delimiter-separated values, or email messages, notably for email filtering. For example, an AWK program may take as input a stream of log statements, and for example send all to the console, write ones starting with WARNING to a "WARNING" file, and send an email to a sysadmin in case any line starts with "ERROR". It could also record how many warnings are logged per day. Alternatively, one can process streams of delimiter-separated values, processing each line or aggregated lines, such as the sum or max. In email, a language like procmail can specify conditions to match on some emails, and what actions to take (deliver, bounce, discard, forward, etc.).
While data-driven design does prevent coupling of data and functionality, in some cases, data-driven programming has been argued to lead to bad object-oriented design, especially when dealing with more abstract data. This is because a purely data-driven object or entity is defined by the way it is represented.  Any attempt to change the structure of the object would immediately break the functions that rely on it.
As an example, one might represent driving directions as a series of intersections (two intersecting streets) where the driver must turn right or left.  If an intersection (in the United States) is represented in data by the zip code (5-digit number) and two street names (strings of text), bugs may appear when a city where streets intersect multiple times is encountered.  While this example may be oversimplified, restructuring of data is a fairly common problem in software engineering, either to eliminate bugs, increase efficiency, or support new features.
Component-based software engineering (CBSE), also called component-based development (CBD), is a style of software engineering that aims to construct a software system from components that are loosely-coupled and reusable. This emphasizes the separation of concerns among components.[1][2]
To find the right level of component granularity, software architects have to continuously iterate their component designs with developers. Architects need to take into account user requirements, responsibilities and architectural characteristics.[3]
In software and systems engineering, a use case is a potential scenario in which a system receives an external request (such as user input) and responds to it.
A use case is a list of actions or event steps typically defining the interactions between a role (known in the Unified Modeling Language (UML) as an actor) and a system to achieve a goal. The actor can be a human or another external system. In systems engineering, use cases are used at a higher level than within software engineering, often representing missions or stakeholder goals. The detailed requirements may then be captured in the Systems Modeling Language (SysML) or as contractual statements.
In 1992 he co-authored the book Object-Oriented Software Engineering - A Use Case Driven Approach,[4] which laid the foundation of the OOSE system engineering method and helped to popularize use cases for capturing functional requirements, especially in software development. In 1994 he published a book about use cases and object-oriented techniques applied to business models and business process reengineering.[5]
At the same time, Grady Booch and James Rumbaugh worked at unifying their object-oriented analysis and design methods, the Booch method and Object Modeling Technique (OMT) respectively.  In 1995 Ivar Jacobson joined them and together they created the Unified Modelling Language (UML), which includes use case modeling. UML was standardized by the Object Management Group (OMG) in 1997.[6] Jacobson, Booch and Rumbaugh also worked on a refinement of the Objectory software development process. The resulting Unified Process was published in 1999 and promoted a use case driven approach.[7]
According to the Software Engineering Body of Knowledge (SWEBOK),[16] use cases belong to the scenario-based requirement elicitation techniques, as well as the model-based analysis, techniques. But the use cases also support narrative-based requirement gathering, incremental requirement acquisition, system documentation, and acceptance testing.[1]
Cockburn's approach has influenced other authors; for example, Alexander and Beus-Dukic generalize Cockburn's "Fully dressed use case" template from software to systems of all kinds, with the following fields differing from Cockburn:[26]
A use case defines the interactions between external actors and the system under consideration to accomplish a goal. Actors must be able to make decisions, but need not be human: "An actor might be a person, a company or organization, a computer program, or a computer system—hardware, software, or both."[28] Actors are always stakeholders, but not all stakeholders are actors, since they may "never interact directly with the system, even though they have the right to care how the system behaves."[28] For example, "the owners of the system, the company's board of directors, and regulatory bodies such as the Internal Revenue Service and the Department of Insurance" could all be stakeholders but are unlikely to be actors.[28]
Since the inception of the agile movement, the user story technique from Extreme Programming has been so popular that many think it is the only and best solution for the agile requirements of all projects. Alistair Cockburn lists five reasons why he still writes use cases in agile development.[32]
Use cases constitute a powerful, user-centric tool for the software requirements specification process.[33] Use case modeling typically starts from identifying key stakeholder roles (actors) interacting with the system, and their goals or objectives the system must fulfill (an outside perspective). These user goals then become the ideal candidates for the names or titles of the use cases which represent the desired functional features or services provided by the system. This user-centered approach ensures that what has real business value and the user really want is developed, not those trivial functions speculated from a developer or system (inside) perspective.
Genetic programming (GP) is an evolutionary algorithm, an artificial intelligence technique mimicking natural evolution, which operates on a population of programs. It applies the genetic operators selection according to a predefined fitness measure, mutation and crossover.
It may and often does happen that a particular run of the algorithm results in premature convergence to some local maximum which is not a globally optimal or even good solution.  Multiple runs (dozens to hundreds) are usually necessary to produce a very good result.  It may also be necessary to have a large starting population size and variability of the individuals to avoid pathologies.
Although the idea of evolving programs, initially in the computer language Lisp, was current amongst John Holland's students,[3] it was not until they organised the first Genetic Algorithms (GA) conference in Pittsburgh that Nichael Cramer[4] published evolved programs in two specially designed languages, which included the first statement of modern "tree-based" Genetic Programming (that is, procedural languages organized in tree-based structures and operated on by suitably defined GA-operators). In 1988, John Koza (also a PhD student of John Holland) patented his invention of a GA for program evolution.[5] This was followed by publication in the International Joint Conference on Artificial Intelligence IJCAI-89.[6]
Koza followed this with 205 publications on “Genetic Programming” (GP), name coined by David Goldberg, also a PhD student of John Holland.[7] However, it is the series of 4 books by Koza, starting in 1992[8] with accompanying videos,[9] that really established GP. Subsequently, there was an enormous expansion of the number of publications with the Genetic Programming Bibliography, surpassing 10,000 entries.[10] In 2010, Koza[11] listed 77 results where Genetic Programming was human competitive.
In 1996, Koza started the annual Genetic Programming conference[12] which was followed in 1998 by the annual EuroGP conference,[13] and the first book[14] in a GP series edited by Koza. 1998 also saw the first GP textbook.[15] GP continued to flourish, leading to the first specialist GP journal[16] and three years later (2003) the annual Genetic Programming Theory and Practice (GPTP) workshop was established by Rick Riolo.[17][18] Genetic Programming papers continue to be published at a diversity of conferences and associated journals. Today there are nineteen GP books including several for students.[15]
Early work that set the stage for current genetic programming research topics and applications is diverse, and includes software synthesis and repair, predictive modeling, data mining,[30] financial modeling,[31] soft sensors,[32] design,[33] and image processing.[34] Applications in some areas, such as design, often make use of intermediate representations,[35] such as Fred Gruau's cellular encoding.[36] Industrial uptake has been significant in several areas including finance, the chemical industry, bioinformatics[37][38] and the steel industry.[39]
GP evolves computer programs, traditionally represented in memory as tree structures.[40] Trees can be easily evaluated in a recursive manner. Every internal node has an operator function and every terminal node has an operand, making mathematical expressions easy to evolve and evaluate. Thus traditionally GP favors the use of programming languages that naturally embody tree structures (for example, Lisp; other functional programming languages are also suitable).
Non-tree representations have been suggested and successfully implemented, such as linear genetic programming which perhaps suits the more traditional imperative languages.[41][42] The commercial GP software Discipulus uses automatic induction of binary machine code ("AIM")[43] to achieve better performance. μGP[44] uses directed multigraphs to generate programs that fully exploit the syntax of a given assembly language. Multi expression programming uses Three-address code for encoding solutions. Other program representations on which significant research and development have been conducted include programs for stack-based virtual machines,[45][46][47] and sequences of integers that are mapped to arbitrary programming languages via grammars.[48][49] Cartesian genetic programming is another form of GP, which uses a graph representation instead of the usual tree based representation to encode computer programs.
In Genetic Programming two fit individuals are chosen from the population to be parents for one or two children. In tree genetic programming, these parents are represented as inverted lisp like trees, with their root nodes at the top. In subtree crossover in each parent a subtree is randomly chosen. (Highlighted with yellow in the animation.) In the root donating parent (in the animation on the left) the chosen subtree is removed and replaced with a copy of the randomly chosen subtree from the other parent, to give a new child tree.
There are many types of mutation in genetic programming. They start from a fit syntactically correct parent and aim to randomly create a syntactically correct child. In the animation
a subtree is randomly chosen (highlighted by yellow). It is removed and replaced by a randomly generated subtree.
Similarly there are many types of linear genetic programming mutation, each of which tries to ensure the mutated child is still syntactically correct.
GP has been successfully used as an automatic programming tool, a machine learning tool and an automatic problem-solving engine.[18] GP is especially useful in the domains where the exact form of the solution is not known in advance or an approximate solution is acceptable (possibly because finding the exact solution is very difficult). Some of the applications of GP are curve fitting, data modeling, symbolic regression, feature selection, classification, etc. John R. Koza mentions 76 instances where Genetic Programming has been able to produce results that are competitive with human-produced results (called Human-competitive results).[54] Since 2004, the annual Genetic and Evolutionary Computation Conference (GECCO) holds Human Competitive Awards (called Humies) competition,[55] where cash awards are presented to human-competitive results produced by any form of genetic and evolutionary computation. GP has won many awards in this competition over the years.
Meta-genetic programming is the proposed meta-learning technique of evolving a genetic programming system using genetic programming itself. It suggests that chromosomes, crossover, and mutation were themselves evolved, therefore like their real life counterparts should be allowed to change on their own rather than being determined by a human programmer. Meta-GP was formally proposed by Jürgen Schmidhuber in 1987.[56] Doug Lenat's Eurisko is an earlier effort that may be the same technique. It is a recursive but terminating algorithm, allowing it to avoid infinite recursion. In the "autoconstructive evolution" approach to meta-genetic programming, the methods for the production and variation of offspring are encoded within the evolving programs themselves, and programs are executed to produce new programs to be added to the population.[46][57]
Critics of this idea often say this approach is overly broad in scope. However, it might be possible to constrain the fitness criterion onto a general class of results, and so obtain an evolved GP that would more efficiently produce results for sub-classes. This might take the form of a meta evolved GP for producing human walking algorithms which is then used to evolve human running, jumping, etc. The fitness criterion applied to the meta GP would simply be one of efficiency.
Nim is a general-purpose, multi-paradigm, statically typed, compiled high-level system programming language,[9] designed and developed by a team around Andreas Rumpf. Nim is designed to be "efficient, expressive, and elegant",[10] supporting metaprogramming, functional, message passing,[11] procedural, and object-oriented programming styles by providing several features such as compile time code generation, algebraic data types, a foreign function interface (FFI) with C, C++, Objective-C, and JavaScript, and supporting compiling to those same languages as intermediate representations.
Nim is statically typed.[12] It supports compile-time metaprogramming features such as syntactic macros and term rewriting macros.[13] Term rewriting macros enable library implementations of common data structures, such as bignums and matrices, to be implemented efficiently and with syntactic integration, as if they were built-in language facilities.[14] Iterators are supported and can be used as first class entities,[13] as can functions, allowing for the use of functional programming methods. Object-oriented programming is supported by inheritance and multiple dispatch. Functions can be generic and overloaded, and generics are further enhanced by Nim's support for type classes. Operator overloading is also supported.[13] Nim includes multiple tunable memory management strategies, including tracing garbage collection, reference counting, and fully manual systems, with the default being deterministic reference counting with optimizations via move semantics and cycle collection via trial deletion.[15]
Andreas Rumpf is the designer and original implementer of Nim. He received a diploma in computer science from the University of Kaiserslautern-Landau, Germany. His research interests include hard realtime systems, embedded systems, compiler construction and artificial intelligence.[20]
The Nim programming language is a concise, fast programming language that compiles to C, C++ and JavaScript. Nim's initial development was started in 2005 by Andreas Rumpf. It was originally named Nimrod when the project was made public in 2008.[21]: 4–11
The first version of the Nim compiler was written in Pascal using the Free Pascal compiler.[22] In 2008, a version of the compiler written in Nim was released.[23] The compiler is free and open-source software, and is being developed by a community of volunteers working with Andreas Rumpf.[24] The language was officially renamed from Nimrod to Nim with the release of version 0.10.2 in December 2014.[25] On September 23, 2019, version 1.0 of Nim was released, signifying the maturing of the language and its toolchain. On August 1, 2023, version 2.0 of Nim was released, signifying the completion, stabilization of, and switch to the ARC/ORC memory model.[26]
The syntax of Nim resembles that of Python.[27] Code blocks and nesting statements are identified through use of whitespace, according to the offside-rule. Many keywords are identical to their Python equivalents, which are mostly English keywords, whereas other programming languages usually use punctuation. With the goal of improving upon its influence languages, even though Nim supports indentation-based syntax like Python, it introduced additional flexibility. For example, a single statement may span multiple lines if a comma or binary operator is at the end of each line. Nim also supports user-defined operators.
Unlike Python, Nim implements (native) static typing. Nim's type system allows for easy type conversion, casting, and provides syntax for generic programming. Nim notably provides type classes which can stand in for multiple types, and provides several such type classes 'out of the box'. Type classes allow working with several types as if they were a single type. For example:
The Nim compiler emits fast, optimized C code by default. It defers compiling-to-object code to an external C compiler[32] to leverage existing compiler optimization and portability. Many C compilers are supported, including Clang, Microsoft Visual C++ (MSVC), MinGW, and GNU Compiler Collection (GCC). The Nim compiler can also emit C++, Objective-C, and JavaScript code to allow easy interfacing with application programming interfaces (APIs) written in those languages;[9] developers can simply write in Nim, then compile to any supported language. This also allows writing applications for iOS and Android. There is also an unofficial LLVM backend, allowing use of the Nim compiler in a stand-alone way.[18]
The Nim compiler is self-hosting, meaning it is written in the Nim language.[33] The compiler supports cross-compiling, so it is able to compile software for any of the supported operating systems, no matter the development machine. This is useful for compiling applications for embedded systems, and for uncommon and obscure computer architectures.[citation needed]
niminst is a tool to generate an installer for a Nim program.[44] 
It creates .msi installers for Windows via Inno Setup, and install and uninstall scripts for Linux, macOS, and Berkeley Software Distribution (BSD).
Pure libraries are modules written in Nim only. They include no wrappers to access libraries written in other programming languages.
Impure libraries are modules of Nim code which depend on external libraries that are written in other programming languages such as C.
A Nim program can use any library which can be used in a C, C++, or JavaScript program. Language bindings exist for many libraries, including GTK,[50][51] Qt QML,[52] wxWidgets,[53] SDL 2,[54][55] Raylib,[56] Godot,[57] UE5,[58] Cairo,[59] OpenGL,[60] Vulkan,[61] WinAPI,[62] zlib, libzip, OpenSSL and cURL.[63] Nim works with PostgreSQL, MySQL, and SQLite databases.
There are open source tools of various degree of support that can be used to interface Nim with Lua,[64] Julia,[65] Rust,[66]
C#,[67]
and Python[68] programming languages or transpile Nim to TypeScript.[69]
Functional programming is supported in Nim through first-class functions and code without side effects via the noSideEffect pragma or the func keyword.[70] Nim will perform side effect analysis and raise compiling errors for code that does not obey the contract of producing no side effects when compiled with the experimental feature strictFuncs, planned to become the default in later versions.[71]
Contrary to purely functional programming languages, Nim is a multi-paradigm programming language, so functional programming restrictions are opt-in on a function-by-function basis.
Nim's uniform function call syntax enables calling ordinary functions with syntax similar to method call invocations in other programming languages. This is functional for "getters": and Nim also provides syntax for the creation of such "setters" as well. Objects may be made public on a per-field basis, providing for encapsulation.
Nim supports both constrained and unconstrained generic programming.
Generics may be used in procedures, templates and macros. Unconstrained generic identifiers (T in this example) are defined after the routine's name in square brackets. Constrained generics can be placed on generic identifiers, or directly on parameters.
Nim's FFI is used to call functions written in the other programming languages that it can compile to. This means that libraries written in C, C++, Objective-C, and JavaScript can be used in the Nim source code. One should be aware that both JavaScript and C, C++, or Objective-C libraries cannot be combined in the same program, as they are not as compatible with JavaScript as they are with each other. Both C++ and Objective-C are based on and compatible with C, but JavaScript is incompatible, as a dynamic, client-side web-based language.[21]: 226
The first Nim conference, NimConf, took place on June 20, 2020. It was held digitally due to COVID-19, with an open call for contributor talks in the form of YouTube videos.[87] The conference began with language overviews by Nim developers Andreas Rumpf and Dominik Picheta. Presentation topics included talks about web frameworks, mobile development, Internet of things (IoT) devices, and game development, including a talk about writing Nim for Game Boy Advance.[88] NimConf 2020 is available as a YouTube playlist.[89] NimConf 2021 occurred the following year, was also held digitally, and included talks about game development, REPLs, real-time operating systems, Nim in the industry, object-relational mapping (ORM), fuzzing, language design, and graphics libraries.[90]
In addition to official conferences, Nim has been featured at various other conventions. A presentation on Nim was given at the O'Reilly Open Source Convention (OSCON) in 2015.[91][92][93] Four speakers represented Nim at FOSDEM 2020, including the creator of the language, Andreas Rumpf.[94] At FOSDEM 2022, Nim hosted their own developer room virtually due to the COVID-19 pandemic.[95] Talks were held on concurrency, embedded programming, programming for GPUs, entity-component systems, game development, rules engines, Python interop, and metaprogramming.[96]
In generic programming, a concept is a description of supported operations on a type, including syntax and semantics. In this way, concepts are related to abstract types but concepts do not require a subtype relationship.
The Nim programming language implements concepts as a series of arbitrary compile-time boolean predicates.[6]
Many algorithms rely on these properties to function properly.
For example the min function can be safely defined on totally ordered types:
Templates are a feature of the C++ programming language that allows functions and classes to operate with generic types. This allows a function or class declaration to reference via a generic variable another different class (built-in or newly declared data type) without creating full declaration for each of these different classes.
Although C++ templates, Java generics, and .NET generics are often considered similar, generics only mimic the basic behavior of C++ templates.[6] Some of the advanced template features utilized by libraries such as Boost and STLSoft, and implementations of the STL, for template metaprogramming (explicit or partial specialization, default template arguments, template non-type arguments, template template arguments, ...) are unavailable with generics.
Oxygene (formerly known as Chrome) is a programming language developed by RemObjects Software for Microsoft's Common Language Infrastructure, the Java Platform and Cocoa. Oxygene is based on Delphi's Object Pascal, but also has influences from  C#, Eiffel, Java, F# and other languages.
From 2008 to 2012, RemObjects Software licensed its compiler and IDE technology to Embarcadero to be used as the backend compiler in their Embarcadero Prism product.[2] Starting in the Fall of 2011, Oxygene became available in two separate editions, with the second edition adding support for the Java and Android runtimes.  Starting with the release of XE4, Embarcadero Prism is no longer part of the RAD Studio SKU. Numerous support and upgrade paths for Prism customers exist to migrate to Oxygene.[3] As of 2016, there is only one edition of Oxygene, which allows development on Windows or macOS, and which can create executables for Windows, Linux, WebAssembly .NET, iOS, Android, Java and macOS.
The Oxygene language has its origins in Object Pascal in general and Delphi in particular, but was designed to reflect the guidelines of .NET programming and to create fully CLR-compliant assemblies. Therefore, some minor language features known from Object Pascal – Delphi have been dropped or revised, while many new and more modern features, such as Generics or Sequences and Queries have been added to the language.
Oxygene is an Object-oriented programming language. Thus, it uses classes, which can hold data and execute code, to design programs.[clarification needed] Classes are "prototypes" for objects, like the idea of an apple is the prototype for the apple one can actually buy in a shop. It is known that an apple has a colour, and that it can be peeled: those are the data and executable "code" for the apple class.
Oxygene provides language-level support for some features of parallel programming. The goal is to use all cores or processors of a computer to improve performance. To reach this goal, tasks must be distributed among several threads. The .NET Framework's ThreadPool class offered a way to efficiently work with several threads. The Task Parallel Library (TPL) was introduced in .NET 4.0 to provide more features for parallel programming.
Template metaprogramming (TMP) is a metaprogramming technique in which templates are used by a compiler to generate temporary source code, which is merged by the compiler with the rest of the source code and then compiled. The output of these templates can include compile-time constants, data structures, and complete functions. The use of templates can be thought of as compile-time polymorphism. The technique is used by a number of languages, the best-known being C++, but also Curl, D, Nim, and XL.
The use of templates as a metaprogramming technique requires two distinct operations: a template must be defined, and a defined template must be instantiated. The generic form of the generated source code is described in the template definition, and when the template is instantiated, the generic form in the template is used to generate a specific set of source code.
Template metaprogramming is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram.[3]
Template metaprograms have no mutable variables— that is, no variable can change value once it has been initialized, therefore template metaprogramming can be seen as a form of functional programming. In fact many template implementations implement flow control only through recursion, as seen in the example below.
Though the syntax of template metaprogramming is usually very different from the programming language it is used with, it has practical uses. Some common reasons to use templates are to implement generic programming (avoiding sections of code which are similar except for some minor variations) or to perform automatic compile-time optimization such as doing something once at compile time rather than every time the program is run — for instance, by having the compiler unroll loops to eliminate jumps and loop count decrements whenever the program is executed.
What exactly "programming at compile-time" means can be illustrated with an example of a factorial function, which in non-template C++ can be written using recursion as follows:
The code above will execute at run time to determine the factorial value of the literals 0 and 4.
By using template metaprogramming and template specialization to provide the ending condition for the recursion, the factorials used in the program—ignoring any factorial not used—can be calculated at compile time by this code:
As another, more significant, example of compile-time loop unrolling, template metaprogramming can be used to create length-n vector classes (where n is known at compile time). The benefit over a more traditional length-n vector is that the loops can be unrolled, resulting in very optimized code. As an example, consider the addition operator. A length-n vector addition might be written as
Polymorphism is a common standard programming facility where derived objects can be used as instances of their base object but where the derived objects' methods will be invoked, as in this code
However, in many cases the polymorphic behaviour needed is invariant and can be determined at compile time. Then the Curiously Recurring Template Pattern (CRTP) can be used to achieve static polymorphism, which is an imitation of polymorphism in programming code but which is resolved at compile time and thus does away with run-time virtual-table lookups. For example:
Compile-time versus execution-time tradeoffs get visible if a great deal of template metaprogramming is used.
When an array is numerically indexed, its range is the upper and lower bound of the array. Depending on the environment, a warning, a fatal exception, or unpredictable behavior will occur if the program attempts to access an array element that is outside the range. In some programming languages, such as C, arrays have a fixed lower bound (zero) and will contain data at each position up to the upper bound (so an array with 5 elements will have a range of 0 to 4). In others, such as PHP, an array may have holes where no element is defined, and therefore an array with a range of 0 to 4 will have up to 5 elements (and a minimum of 2).
Another meaning of range in computer science is an alternative to iterator. When used in this sense, range is defined as "a pair of begin/end iterators packed together".[1] It is argued [1] that "Ranges are a superior abstraction" (compared to iterators) for several reasons, including better safety.
A programming language is a system of notation for writing computer programs.[1]
Programming languages are described in terms of their syntax (form) and semantics (meaning), usually defined by a formal language. Languages usually provide features such as a type system, variables, and mechanisms for error handling. An implementation of a programming language is required in order to execute programs, namely an interpreter or a compiler. An interpreter directly executes the source code, while a compiler produces an executable program.
Computer architecture has strongly influenced the design of programming languages, with the most common type (imperative languages—which implement operations in a specified order) developed to perform well on the popular von Neumann architecture. While early programming languages were closely tied to the hardware, over time they have developed more abstraction to hide implementation details for greater simplicity.
Thousands of programming languages—often classified as imperative, functional, logic, or object-oriented—have been developed for a wide variety of uses. Many aspects of programming language design involve tradeoffs—for example, exception handling simplifies error handling, but at a performance cost. Programming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.
Programming languages differ from natural languages in that natural languages are used for interaction between people, while programming languages are designed to allow humans to communicate instructions to machines.[citation needed]
The term computer language is sometimes used interchangeably with "programming language".[2] However, usage of these terms varies among authors.
In one usage, programming languages are described as a subset of computer languages.[3] Similarly, the term "computer language" may be used in contrast to the term "programming language" to describe languages used in computing but not considered programming languages[citation needed].  Most practical programming languages are Turing complete,[4] and as such are equivalent in what programs they can compute.
Another usage regards programming languages as theoretical constructs for programming abstract machines and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources.[5] John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.[6]
The first programmable computers were invented at the end of the 1940s, and with them, the first programming languages.[7] The earliest computers were programmed in first-generation programming languages (1GLs), machine language (simple instructions that could be directly executed by the processor). This code was very difficult to debug and was not portable between different computer systems.[8] In order to improve the ease of programming, assembly languages (or second-generation programming languages—2GLs) were invented, diverging from the machine language to make programs easier to understand for humans, although they did not increase portability.[9]
Initially, hardware resources were scarce and expensive, while human resources were cheaper. Therefore, cumbersome languages that were time-consuming to use, but were closer to the hardware for higher efficiency were favored.[10] The introduction of high-level programming languages (third-generation programming languages—3GLs)—revolutionized programming. These languages abstracted away the details of the hardware, instead being designed to express algorithms that could be understood more easily by humans. For example, arithmetic expressions could now be written in symbolic notation and later translated into machine code that the hardware could execute.[9] In 1957, Fortran (FORmula TRANslation) was invented. Often considered the first compiled high-level programming language,[9][11] Fortran has remained in use into the twenty-first century.[12]
Around 1960, the first mainframes—general purpose computers—were developed, although they could only be operated by professionals and the cost was extreme. The data and instructions were input by punch cards, meaning that no input could be added while the program was running. The languages developed at this time therefore are designed for minimal interaction.[14] After the invention of the microprocessor, computers in the 1970s became dramatically cheaper.[15] New computers also allowed more user interaction, which was supported by newer programming languages.[16]
Lisp, implemented in 1958, was the first functional programming language.[17] Unlike Fortran, it supported recursion and conditional expressions,[18] and it also introduced dynamic memory management on a heap and automatic garbage collection.[19] For the next decades, Lisp dominated artificial intelligence applications.[20] In 1978, another functional language, ML, introduced inferred types and polymorphic parameters.[16][21]
After ALGOL (ALGOrithmic Language) was released in 1958 and 1960,[22] it became the standard in computing literature for describing algorithms. Although its commercial success was limited, most popular imperative languages—including C, Pascal, Ada, C++, Java, and C#—are directly or indirectly descended from ALGOL 60.[23][12] Among its innovations adopted by later programming languages included greater portability and the first use of context-free, BNF grammar.[24] Simula, the first language to support object-oriented programming (including subtypes, dynamic dispatch, and inheritance), also descends from ALGOL and achieved commercial success.[25] C, another ALGOL descendant, has sustained popularity into the twenty-first century. C allows access to lower-level machine operations more than other contemporary languages. Its power and efficiency, generated in part with flexible pointer operations, comes at the cost of making it more difficult to write correct code.[16]
Prolog, designed in 1972, was the first logic programming language, communicating with a computer using formal logic notation.[26][27] With logic programming, the programmer specifies a desired result and allows the interpreter to decide how to achieve it.[28][27]
During the 1980s, the invention of the personal computer transformed the roles for which programming languages were used.[29] New languages introduced in the 1980s included C++, a superset of C that can compile C programs but also supports classes and inheritance.[30] Ada and other new languages introduced support for concurrency.[31] The Japanese government invested heavily into the so-called fifth-generation languages that added support for concurrency to logic programming constructs, but these languages were outperformed by other concurrency-supporting languages.[32][33]
Due to the rapid growth of the Internet and the World Wide Web in the 1990s, new programming languages were introduced to support Web pages and networking.[34] Java, based on C++ and designed for increased portability across systems and security, enjoyed large-scale success because these features are essential for many Internet applications.[35][36] Another development was that of dynamically typed scripting languages—Python, JavaScript, PHP, and Ruby—designed to quickly produce small programs that coordinate existing applications. Due to their integration with HTML, they have also been used for building web pages hosted on servers.[37][38]
During the 2000s, there was a slowdown in the development of new programming languages that achieved widespread popularity.[39] One innovation was service-oriented programming, designed to exploit distributed systems whose components are connected by a network. Services are similar to objects in object-oriented programming, but run on a separate process.[40] C# and F# cross-pollinated ideas between imperative and functional programming.[41] After 2010, several new languages—Rust, Go, Swift, Zig and Carbon —competed for the performance-critical software for which C had historically been used.[42] Most of the new programming languages use static typing while a few numbers of new languages use dynamic typing like Ring and Julia.[43][44]
Some of the new programming languages are classified as visual programming languages like Scratch, LabVIEW and PWCT. Also, some of these languages mix between textual and visual programming usage like Ballerina.[45][46][47][48] Also, this trend lead to developing projects that help in developing new VPLs like Blockly by Google.[49] Many game engines like Unreal and Unity added support for visual scripting too.[50][51]
Every programming language includes fundamental elements for describing data and the operations or transformations applied to them, such as adding two numbers or selecting an item from a collection. These elements are governed by syntactic and semantic rules that define their structure and meaning, respectively.
A programming language's surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, some programming languages are graphical, using visual relationships between symbols to specify a program.
The programming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:
The grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars.[52] Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution.[53] In contrast to Lisp's macro system and Perl's BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.[54]
Static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms.[1][failed verification] For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct.[55] Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Programming languages such as Java and C# have definite assignment analysis, a form of data flow analysis, as part of their respective static semantics.[56]
Once data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research goes into formal semantics of programming languages, which allows execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.[56]
A data type is a set of allowable values and operations that can be performed on these values.[57] Each programming language's type system defines which data types exist, the type of an expression, and how type equivalence and type compatibility function in the language.[58]
Early programming languages often supported only built-in, numeric types such as the integer (signed and unsigned) and floating point (to support operations on real numbers that are not integers). Most programming languages support multiple sizes of floats (often called float and double) and integers depending on the size and precision required by the programmer. Storing an integer in a type that is too small to represent it leads to integer overflow. The most common way of representing negative numbers with signed types is twos complement, although ones complement is also used.[62] Other common types include Boolean—which is either true or false—and character—traditionally one byte, sufficient to represent all ASCII characters.[63]
Arrays are a data type whose elements, in many languages, must consist of a single type of fixed length. Other languages define arrays as references to data stored elsewhere and support elements of varying types.[64] Depending on the programming language, sequences of multiple characters, called strings, may be supported as arrays of characters or their own primitive type.[65] Strings may be of fixed or variable length, which enables greater flexibility at the cost of increased storage space and more complexity.[66] Other data types that may be supported include lists,[67] associative (unordered) arrays accessed via keys,[68] records in which data is mapped to names in an ordered structure,[69] and tuples—similar to records but without names for data fields.[70] Pointers store memory addresses, typically referencing locations on the heap where other data is stored.[71]
The simplest user-defined type is an ordinal type, often called an enumeration, whose values can be mapped onto the set of positive integers.[72] Since the mid-1980s, most programming languages also support abstract data types, in which the representation of the data and operations are hidden from the user, who can only access an interface.[73] The benefits of data abstraction can include increased reliability, reduced complexity, less potential for name collision, and allowing the underlying data structure to be changed without the client needing to alter its code.[74]
In static typing, all expressions have their types determined before a program executes, typically at compile-time.[59] Most widely used, statically typed programming languages require the types of variables to be specified explicitly. In some languages, types are implicit; one form of this is when the compiler can infer types based on context. The downside of implicit typing is the potential for errors to go undetected.[75] Complete type inference has traditionally been associated with functional languages such as Haskell and ML.[76]
With dynamic typing, the type is not attached to the variable but only the value encoded in it. A single variable can be reused for a value of a different type. Although this provides more flexibility to the programmer, it is at the cost of lower reliability and less ability for the programming language to check for errors.[77] Some languages allow variables of a union type to which any type of value can be assigned, in an exception to their usual static typing rules.[78]
In computing, multiple instructions can be executed simultaneously. Many programming languages support instruction-level and subprogram-level concurrency.[79] By the twenty-first century, additional processing power on computers was increasingly coming from the use of additional processors, which requires programmers to design software that makes use of multiple processors simultaneously to achieve improved performance.[80] Interpreted languages such as Python and Ruby do not support the concurrent use of multiple processors.[81] Other programming languages do support managing data shared between different threads by controlling the order of execution of key instructions via the use of semaphores, controlling access to shared data via monitor, or enabling message passing between threads.[82]
Many programming languages include exception handlers, a section of code triggered by runtime errors that can deal with them in two main ways:[83]
Some programming languages support dedicating a block of code to run regardless of whether an exception occurs before the code is reached; this is called finalization.[84]
One of the most important influences on programming language design has been computer architecture. Imperative languages, the most commonly used type, were designed to perform well on von Neumann architecture, the most common computer architecture.[89] In von Neumann architecture, the memory stores both data and instructions, while the CPU that performs instructions on data is separate, and data must be piped back and forth to the CPU. The central elements in these languages are variables, assignment, and iteration, which is more efficient than recursion on these machines.[90]
Many programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse.[citation needed] The birth of programming languages in the 1950s was stimulated by the desire to make a universal programming language suitable for all machines and uses, avoiding the need to write code for different computers.[91] By the early 1960s, the idea of a universal language was rejected due to the differing requirements of the variety of purposes for which code was written.[92]
Desirable qualities of programming languages include readability, writability, and reliability.[93] These features can reduce the cost of training programmers in a language, the amount of time needed to write and maintain programs in the language, the cost of compiling the code, and increase runtime performance.[94]
Programming language design often involves tradeoffs.[104] For example, features to improve reliability typically come at the cost of performance.[105] Increased expressivity due to a large number of operators makes writing code easier but comes at the cost of readability.[105]
Natural-language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs.[106] Alan Perlis was similarly dismissive of the idea.[107]
The specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.
An implementation of a programming language is the conversion of a program into machine code that can be executed by the hardware. The machine code then can be executed with the help of the operating system.[111] The most common form of interpretation in production code is by a compiler, which translates the source code via an intermediate-level language into machine code, known as an executable. Once the program is compiled, it will run more quickly than with other implementation methods.[112] Some compilers are able to provide further optimization to reduce memory or computation usage when the executable runs, but increasing compilation time.[113]
Another implementation method is to run the program with an interpreter, which translates each line of software into machine code just before it executes. Although it can make debugging easier, the downside of interpretation is that it runs 10 to 100 times slower than a compiled executable.[114] Hybrid interpretation methods provide some of the benefits of compilation and some of the benefits of interpretation via partial compilation. One form this takes is just-in-time compilation, in which the software is compiled ahead of time into an intermediate language, and then into machine code immediately before execution.[115]
Although most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain-specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.[citation needed]
Some programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language,[116] and Microsoft's C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.[117]
Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language. Some languages may make the transition from closed to open; for example, Erlang was originally Ericsson's internal programming language.[118]
Open source programming languages are particularly helpful for open science applications, enhancing the capacity for replication and code sharing.[119]
Thousands of different programming languages have been created, mainly in the computing field.[120]
Individual software projects commonly use five programming languages or more.[121]
Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers "do exactly what they are told to do", and cannot "understand" what code the programmer intended to write. The combination of the language definition, a program, and the program's inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.
A programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives).[122] Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.
Determining which is the most widely used programming language is difficult since the definition of usage varies by context. One language may occupy the greater number of programmer hours, a different one has more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes;[124][125] Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time, and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.
Combining and averaging information from various internet sites, stackify.com reported the ten most popular programming languages (in descending order by overall popularity): Java, C, C++, Python, C#, JavaScript, VB .NET, R, PHP, and MATLAB.[129]
As of June 2024, the top five programming languages as measured by TIOBE index are Python, C++, C, Java and C#. TIOBE provide a list of top 100 programming languages according to popularity and update this list every month.[130]
A dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate, or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC language has many dialects.
Programming languages are often placed into four main categories: imperative, functional, logic, and object oriented.[131]
Although markup languages are not programming languages, some have extensions that support limited programming. Additionally, there are special-purpose languages that are not easily compared to other programming languages.[135]
The free software movement is a social movement with the goal of obtaining and guaranteeing certain freedoms for software users, namely the freedoms to run, study, modify, and share copies of software.[1][2] Software which meets these requirements, The Four Essential Freedoms of Free Software, is termed free software.
Although drawing on traditions and philosophies among members of the 1970s hacker culture and academia, Richard Stallman formally founded the movement[3] in 1983 by launching the GNU Project.[4] Stallman later established the Free Software Foundation in 1985 to support the movement.
The philosophy of the Free Software Movement is based on promoting collaboration between programmers and computer users. This process necessitates the rejection of
proprietary software and the promotion of free software.[5] Stallman notes that this action would not hinder the progression of technology, as he states, "Wasteful duplication of system programming effort will be avoided. This effort can go instead into advancing the state of the art."[6]
Members of the Free Software Movement believe that all software users should have the freedoms listed in The Free Software Definition. Members hold the belief that it is immoral to prohibit or prevent people from exercising these freedoms, and that they are required in creating a community where software users can help each other and have control over their technology.[7] Regarding proprietary software, some believe that it is not strictly immoral, citing increased profitability in the business models available for proprietary software, along with technical features and convenience.[8]
The Free Software Foundation espouses the principle that all software needs free documentation, as programmers should have the ability to update manuals to reflect modifications made to the software.[9] Within the movement, the FLOSS Manuals foundation specializes in providing such documentation.
The core work of the free software movement is focused on software development. The free software movement also rejects proprietary software, refusing to install software that does not give them the freedoms of free software. According to Stallman, "The only thing in the software field that is worse than an unauthorised copy of a proprietary program, is an authorised copy of the proprietary program because this does the same harm to its whole community of users, and in addition, usually the developer, the perpetrator of this evil, profits from it."[10]
Some supporters of the free software movement take up public speaking, or host a stall at software-related conferences to raise awareness of software freedom. This is seen as important since people who receive free software, but who are not aware that it is free software, will later accept a non-free replacement or will add software that is not free software.[11]
A lot of lobbying work has been done against software patents and expansions of copyright law. Other lobbying focuses directly on the use of free software by government agencies and government-funded projects.
In June 1997, the Society for Study, Application, and Development of Free Software was established under the China Software Industry Association in Beijing. Through this organization, the website freesoft.cei.gov.cn was developed, though the website is currently inaccessible on IP addresses located in the United States. The use of open-source software Linux in China has moved beyond government and educational institutions and has extended to other organizations such as financial institutions, telecommunications, and public security. Several Chinese researchers and scholars have claimed that the existence of FOSS in China has been important in challenging the presence of Microsoft, which Guangnan Ni, a member of the Chinese Academy of Engineering stated, "The monopoly of (Microsoft Windows) is even more powerful in China than other places in the world".[12] Yi Zhou, a professor of mathematics at Fudan University, has also alleged that, "Government procurement of FLOSS for a number of years in China has compelled Microsoft to cut its prices of Office software substantially" [12]
Government of India had issued Policy on Adoption of Open Source Software for Government of India in 2015 to drive uptake within the government. With the vision to transform India as a Software Product Nation, National Policy on Software Products-2019 was approved by the Government.[13]
Free and Open Source Software (Foss) is crucial for countries such as Pakistan which is set up by Union of Information Technology.  For the case of Pakistan, Pakistan Software Export Board (PSEB) aids in the creation and advocate of FOSS usage in various government departments in addition to curbing illegality of copying that is software piracy. Promotion of adoption of FOSS is essential however it comes with problems of proprietary anti competition software practices including indulging in bribing and corruption by government departments. Pakistan works on the introduction  of usage of open type  basis of source Solutions in the curricula  in schools and colleges. This is because of FOSS uniqueness in terms of political, democratic and social varieties of aspect regarding  information communication and technology.[14]
In the United States, there have been efforts to pass legislation at the state level encouraging the use of free software by state government agencies.[15]
On January 11, 2022, two bills were shown on the New Hampshire legislating floor. The first bill called "HB 1273" was introduced by Democratic New Hampshire representative Eric Gallager, the bill prioritized "replacing proprietary software used by state agencies with free software." Gallager stated that to an extent, the proposed legislation will help distinguish "free software" and "open-source software", this will also put these two into state regulation. The second bill called "HB 1581" was proposed by Grafton Republican representative Lex Berezhny. The bill would've restored a requisite forcing "state agencies to use proprietary software" and as Lex put it, "when it is the most effective solution." He also said that requisite was happening between 2012 and 2018. According to the Concord Monitor, the state of New Hampshire had an already  "thriving open source software community" with a view of "live free or die" but they had difficulty getting that notion with the state.[16]
Congressmen Edgar David Villanueva and Jacques Rodrich Ackerman have been instrumental in introducing free software in Peru, with bill 1609 on "Free Software in Public Administration".[17] The incident invited the attention of Microsoft, Peru, whose general manager wrote a letter to Villanueva. His response received worldwide attention and is seen as a classic piece of argumentation favouring use of free software in governments.[18]
Uruguay has a sanctioned law requiring that the state give priority to free software. It also requires that information be exchanged in open formats.[19]
The Government of Venezuela implemented a free software law in January 2006. Decree No. 3,390 mandated all government agencies to migrate to free software over a two-year period.[20]
Publiccode.eu is a campaign launched demanding a legislation requiring that publicly financed software developed for the public sector be made publicly available under a Free and Open Source Software licence. If it is public money, it should be public code as well.[21]
The French Gendarmerie and the French National Assembly utilize the open source operating system Linux.[22]
Free Software events happening all around the world connects people to increase visibility for Free software projects and foster collaborations.
The free software movement has been extensively analyzed using economic methodologies, including perspectives from heterodox economics. Of particular interest to economists[who?] is the willingness of programmers in the free software movement to work, often producing higher-quality than proprietary programmers, without financial compensation[citation needed].
In his 1998 article "The High-Tech Gift Economy", Richard Barbrook suggested that the then-nascent free software movement represented a return to the gift economy building on hobbyism and the absence of economic scarcity on the Internet.[24]
Gabriella Coleman has emphasized the importance of accreditation, respect, and honour within the free software community as a form of compensation for contributions to projects, over and against financial motivations.[25]
The Swedish Marxian economist Johan Söderberg has argued that the free software movement represents a complete alternative to capitalism that may be expanded to create a post-work society. He argues that the combination of a manipulation of intellectual property law and private property to make goods available to the public and a thorough blend between labor and fun make the free software movement a communist economy.[26]
Since its inception, there is an ongoing contention between the many FLOSS organizations (FSF, OSI, Debian, Mozilla Foundation, Apache Foundation, etc.) within the free software movement, with the main conflicts centered around the organization's needs for compromise and pragmatism rather than adhering to founding values and philosophies.[27]
The Open Source Initiative (OSI) was founded in February 1998 by Eric Raymond and Bruce Perens to promote the term "open-source software" as an alternative term for free software. The OSI aimed to address the perceived shortcomings and ambiguity of the term "free software", as well as shifting the focus of free software from a social and ethical issue to instead emphasize open source as a superior model for software development.[28][29][30][31] The latter became the view of Eric Raymond and Linus Torvalds, while Bruce Perens argued that open source was meant to popularize free software under a new brand and called for a return to basic ethical principles.[32]
Some free software advocates use the terms "Free and Open-Source Software" (FOSS) or "Free/Libre and Open-Source Software" (FLOSS) as a form of inclusive compromise, which brings free and open-source software advocates together to work on projects cohesively. Some users believe this is an ideal solution in order to promote both the user's freedom with the software and the pragmatic efficiency of an open-source development model. This view is reinforced by fact that majority of OSI-approved licenses and self-avowed open-source programs are also compatible with the free software formalisms and vice versa.[33]
While free and open source software are often linked together, they offer two separate ideas and values. Richard Stallman has referred to open source as "a non-movement", as it "does not campaign for anything".[34]
"Open source" addresses software being open as a practical question  rather than an ethical dilemma – non-free software is not the best solution but nonetheless a solution. The free software movement views free software as a moral imperative: that proprietary software should be rejected, and that only free software should be developed and taught in order to make computing technology beneficial to the general public.[35]
Although the movements have differing values and goals, collaborations between the Free Software Movement and Open Source Initiative have taken place when it comes to practical projects.[36] By 2005, Richard Glass considered the differences to be a "serious fracture" but "vitally important to those on both sides of the fracture" and "of little importance to anyone else studying the movement from a software engineering perspective" since they have had "little effect on the field".[37]
Eric Raymond criticises the speed at which the free software movement is progressing, suggesting that temporary compromises should be made for long-term gains. Raymond argues that this could raise awareness of the software and thus increase the free software movement's influence on relevant standards and legislation.[38]
Stallman said that this is where people get the misconception of "free": there is no wrong in programmers' requesting payment for a proposed project, or charging for copies of free software.[40] Restricting and controlling the user's decisions on use is the actual violation of freedom. Stallman defends that in some cases, monetary incentive is not necessary for motivation since the pleasure in expressing creativity is a reward in itself.[6] Conversely, Stallman admits that it is not easy to raise money for free software projects.[41]
The free software movement champions copyleft licensing schema (often pejoratively called "viral licenses"). In its strongest form, copyleft mandates that any works derived from copyleft-licensed software must also carry a copyleft license, so the license spreads from work to work like a computer virus might spread from machine to machine. Stallman has previously stated his opposition to describing the GNU GPL as "viral". These licensing terms can only be enforced through asserting copyrights.[42]
Critics of copyleft licensing challenge the idea that restricting modifications is in line with the free software movement's emphasis on various "freedoms", especially when alternatives like MIT, BSD, and Apache licenses are more permissive.[43][44] Proponents enjoy the assurance that copylefted work cannot usually be incorporated into non-free software projects.[45] They emphasize that copyleft licenses may not attach for all uses and that in any case, developers can simply choose not to use copyleft-licensed software.[46][47]
The Free Software Directory (FSD) is a project of the Free Software Foundation (FSF).  It catalogs free software that runs under free operating systems—particularly GNU and Linux. The cataloged projects are often able to run in several other operating systems. The project was formerly co-run by UNESCO.
Unlike some other directories that focus on free software,[1] Free Software Directory staff verify the licenses of software listed in the directory.
FSD has been used as a source for assessing the share of free software, for example finding in September 2002 an amount of "1550 entries, of which 1363 (87.9%) used the GPL license, 103 (6.6%) used the LGPL license, 32 (2.0%) used a BSD or BSD-like license, 29 (1.9%) used the Artistic license, 5 (0.3%) used the MIT license".[2] By September 2009, the Directory listed 6,000 packages whose number grew up to 6,500 in October 2011, when the newly updated directory was launched. All listed packages are "free for any computer user to download, run and share. Each entry is individually checked and tested ... so users know that any program they come across in the directory will be truly free software ... with free documentation and without proprietary software requirements".[3]
Several scientific publications review or refer to the directory.[4][5][6] It has been remarked that the Directory "only includes software that runs on free operating systems. The FSF/UNESCO Free Software Directory is also a collaborative project, offering a web interface for users to enter and update entries".[7]
Among the critical issues of the previous version, it has been pointed out that while "available software is described using a variety of textual metadata, including the components upon which a particular piece of software depends", "unfortunately, those dependencies are only listed by name, and locating and retrieving them is left to the user".[8]
On the other hand, the accuracy of the directory review on licenses is acknowledged.[9]
The code review from the directory's editorial board is suitable for obtaining statistics on subsets of free software packages reliably clustered by license.[10][11]
In September 2011, the Free Software Directory was re-implemented as a wiki, using MediaWiki and the Semantic MediaWiki extension, to allow users to directly add to and modify its contents.[12]
Semantic MediaWiki provides the directory with semantic web technologies by adding "advanced search and presentation capabilities, structured to be useful for reading by both humans and data-mining programs".[13]
The new edition of the directory has been described as designed to ease and support with semantics the discovery and harvesting of information on free software programs. "An extensive and flexible category system, plus over 40,000 keywords and more than 40 different fields of information, enhance both simple and advanced searching".[13]
There are three major databases of open source software available on the Internet today: the GNU Free Software Directory (https://www.gnu.org/directory/), SourceForge (https://sourceforge.net/), and Freshmeat (http://freshmeat.net/).
The FSF prefers the GPL license, so the FSF directory statistic may be biased in the percentage of GPLed software it registers, but the directory still provides strong additional evidence that the GPL is a widely used license for FLOSS.
The Free Software Definition, written by Richard Stallman and published by the Free Software,free software as being software that ensures that the users have freedom in using, studying, sharing and modifying that software. The term "free" is used in the sense of "free speech," not of "free of charge. The earliest-known publication of the definition was in the February 1986 edition of the now-discontinued GNU's Bulletin publication by the FSF. The canonical source for the document is in the philosophy section of the website. , it is published in 39 languages. The FSF publishes
In 1996, when the gnu.org website was launched, "free software" was defined referring to "three levels of freedom" by adding an explicit mention of the freedom to study the software (which could be read in the two-point definition as being part of the freedom to change the program).[2][3]  Stallman later avoided the word "levels", saying that all of the freedoms are needed, so it is misleading to think in terms of levels[citation needed].
The modern definition defines free software by whether or not the recipient has the following four freedoms:[6]
Freedoms 1 and 3 require source code to be available because studying and modifying software without its source code is highly impractical.
In July 1997, Bruce Perens published the Debian Free Software Guidelines.[7] A definition based on the DFSG was also used by the Open Source Initiative (OSI) under the name "The Open Source Definition".
Despite the philosophical differences between the free software movement and the open-source-software movement, the official definitions of free software by the FSF and of open-source software by the OSI basically refer to the same software licences, with a few minor exceptions. While stressing these philosophical differences, the Free Software Foundation comments:
The term "open source" software is used by some people to mean more or less the same category as free software. It is not exactly the same class of software: they accept some licences that we consider too restrictive, and there are free software licences they have not accepted. However, the differences in extension of the category are small: nearly all free software is open source, and nearly all open source software is free.
The Free Software Foundation (FSF) grants two annual awards. Since 1998, FSF has granted the award for Advancement of Free Software and since 2005, also the Free Software Award for Projects of Social Benefit.
In 1999 the award for Advancement of Free Software was presented at the Jacob Javits Center European Meeting (FOSDEM). Since 2006, the awards have been presented at the FSF's annual members meeting in Cambridge, Massachusetts.
The Advancement of Free Software award is annually presented by the Free Software Foundation (FSF) to a person whom it deems to have made a great contribution to the progress and development of free software, through activities that accord with the spirit of free software.[1]
The Free Software Award for Projects of Social Benefit is an annual award granted by the Free Software Foundation (FSF). In announcing the award, the FSF explained that:
This award is presented to the project or team responsible for applying free software, or the ideas of the free software movement, in a project that intentionally and significantly benefits society in other aspects of life.[30]
This is the second annual award created by the FSF. The first was the Award for the Advancement of Free Software (AAFS).
The third annual award created by the FSF, the award is presented to an exceptional newcomer to the free software community.[36]
The Free Software Foundation (FSF) grants two annual awards. Since 1998, FSF has granted the award for Advancement of Free Software and since 2005, also the Free Software Award for Projects of Social Benefit.
In 1999 the award for Advancement of Free Software was presented at the Jacob Javits Center European Meeting (FOSDEM). Since 2006, the awards have been presented at the FSF's annual members meeting in Cambridge, Massachusetts.
The Advancement of Free Software award is annually presented by the Free Software Foundation (FSF) to a person whom it deems to have made a great contribution to the progress and development of free software, through activities that accord with the spirit of free software.[1]
The Free Software Award for Projects of Social Benefit is an annual award granted by the Free Software Foundation (FSF). In announcing the award, the FSF explained that:
This award is presented to the project or team responsible for applying free software, or the ideas of the free software movement, in a project that intentionally and significantly benefits society in other aspects of life.[30]
This is the second annual award created by the FSF. The first was the Award for the Advancement of Free Software (AAFS).
The third annual award created by the FSF, the award is presented to an exceptional newcomer to the free software community.[36]
The Free Software Foundation (FSF) grants two annual awards. Since 1998, FSF has granted the award for Advancement of Free Software and since 2005, also the Free Software Award for Projects of Social Benefit.
In 1999 the award for Advancement of Free Software was presented at the Jacob Javits Center European Meeting (FOSDEM). Since 2006, the awards have been presented at the FSF's annual members meeting in Cambridge, Massachusetts.
The Advancement of Free Software award is annually presented by the Free Software Foundation (FSF) to a person whom it deems to have made a great contribution to the progress and development of free software, through activities that accord with the spirit of free software.[1]
The Free Software Award for Projects of Social Benefit is an annual award granted by the Free Software Foundation (FSF). In announcing the award, the FSF explained that:
This award is presented to the project or team responsible for applying free software, or the ideas of the free software movement, in a project that intentionally and significantly benefits society in other aspects of life.[30]
This is the second annual award created by the FSF. The first was the Award for the Advancement of Free Software (AAFS).
The third annual award created by the FSF, the award is presented to an exceptional newcomer to the free software community.[36]
The Free Software Foundation (FSF) grants two annual awards. Since 1998, FSF has granted the award for Advancement of Free Software and since 2005, also the Free Software Award for Projects of Social Benefit.
In 1999 the award for Advancement of Free Software was presented at the Jacob Javits Center European Meeting (FOSDEM). Since 2006, the awards have been presented at the FSF's annual members meeting in Cambridge, Massachusetts.
The Advancement of Free Software award is annually presented by the Free Software Foundation (FSF) to a person whom it deems to have made a great contribution to the progress and development of free software, through activities that accord with the spirit of free software.[1]
The Free Software Award for Projects of Social Benefit is an annual award granted by the Free Software Foundation (FSF). In announcing the award, the FSF explained that:
This award is presented to the project or team responsible for applying free software, or the ideas of the free software movement, in a project that intentionally and significantly benefits society in other aspects of life.[30]
This is the second annual award created by the FSF. The first was the Award for the Advancement of Free Software (AAFS).
The third annual award created by the FSF, the award is presented to an exceptional newcomer to the free software community.[36]
The free software movement is a social movement with the goal of obtaining and guaranteeing certain freedoms for software users, namely the freedoms to run, study, modify, and share copies of software.[1][2] Software which meets these requirements, The Four Essential Freedoms of Free Software, is termed free software.
Although drawing on traditions and philosophies among members of the 1970s hacker culture and academia, Richard Stallman formally founded the movement[3] in 1983 by launching the GNU Project.[4] Stallman later established the Free Software Foundation in 1985 to support the movement.
The philosophy of the Free Software Movement is based on promoting collaboration between programmers and computer users. This process necessitates the rejection of
proprietary software and the promotion of free software.[5] Stallman notes that this action would not hinder the progression of technology, as he states, "Wasteful duplication of system programming effort will be avoided. This effort can go instead into advancing the state of the art."[6]
Members of the Free Software Movement believe that all software users should have the freedoms listed in The Free Software Definition. Members hold the belief that it is immoral to prohibit or prevent people from exercising these freedoms, and that they are required in creating a community where software users can help each other and have control over their technology.[7] Regarding proprietary software, some believe that it is not strictly immoral, citing increased profitability in the business models available for proprietary software, along with technical features and convenience.[8]
The Free Software Foundation espouses the principle that all software needs free documentation, as programmers should have the ability to update manuals to reflect modifications made to the software.[9] Within the movement, the FLOSS Manuals foundation specializes in providing such documentation.
The core work of the free software movement is focused on software development. The free software movement also rejects proprietary software, refusing to install software that does not give them the freedoms of free software. According to Stallman, "The only thing in the software field that is worse than an unauthorised copy of a proprietary program, is an authorised copy of the proprietary program because this does the same harm to its whole community of users, and in addition, usually the developer, the perpetrator of this evil, profits from it."[10]
Some supporters of the free software movement take up public speaking, or host a stall at software-related conferences to raise awareness of software freedom. This is seen as important since people who receive free software, but who are not aware that it is free software, will later accept a non-free replacement or will add software that is not free software.[11]
A lot of lobbying work has been done against software patents and expansions of copyright law. Other lobbying focuses directly on the use of free software by government agencies and government-funded projects.
In June 1997, the Society for Study, Application, and Development of Free Software was established under the China Software Industry Association in Beijing. Through this organization, the website freesoft.cei.gov.cn was developed, though the website is currently inaccessible on IP addresses located in the United States. The use of open-source software Linux in China has moved beyond government and educational institutions and has extended to other organizations such as financial institutions, telecommunications, and public security. Several Chinese researchers and scholars have claimed that the existence of FOSS in China has been important in challenging the presence of Microsoft, which Guangnan Ni, a member of the Chinese Academy of Engineering stated, "The monopoly of (Microsoft Windows) is even more powerful in China than other places in the world".[12] Yi Zhou, a professor of mathematics at Fudan University, has also alleged that, "Government procurement of FLOSS for a number of years in China has compelled Microsoft to cut its prices of Office software substantially" [12]
Government of India had issued Policy on Adoption of Open Source Software for Government of India in 2015 to drive uptake within the government. With the vision to transform India as a Software Product Nation, National Policy on Software Products-2019 was approved by the Government.[13]
Free and Open Source Software (Foss) is crucial for countries such as Pakistan which is set up by Union of Information Technology.  For the case of Pakistan, Pakistan Software Export Board (PSEB) aids in the creation and advocate of FOSS usage in various government departments in addition to curbing illegality of copying that is software piracy. Promotion of adoption of FOSS is essential however it comes with problems of proprietary anti competition software practices including indulging in bribing and corruption by government departments. Pakistan works on the introduction  of usage of open type  basis of source Solutions in the curricula  in schools and colleges. This is because of FOSS uniqueness in terms of political, democratic and social varieties of aspect regarding  information communication and technology.[14]
In the United States, there have been efforts to pass legislation at the state level encouraging the use of free software by state government agencies.[15]
On January 11, 2022, two bills were shown on the New Hampshire legislating floor. The first bill called "HB 1273" was introduced by Democratic New Hampshire representative Eric Gallager, the bill prioritized "replacing proprietary software used by state agencies with free software." Gallager stated that to an extent, the proposed legislation will help distinguish "free software" and "open-source software", this will also put these two into state regulation. The second bill called "HB 1581" was proposed by Grafton Republican representative Lex Berezhny. The bill would've restored a requisite forcing "state agencies to use proprietary software" and as Lex put it, "when it is the most effective solution." He also said that requisite was happening between 2012 and 2018. According to the Concord Monitor, the state of New Hampshire had an already  "thriving open source software community" with a view of "live free or die" but they had difficulty getting that notion with the state.[16]
Congressmen Edgar David Villanueva and Jacques Rodrich Ackerman have been instrumental in introducing free software in Peru, with bill 1609 on "Free Software in Public Administration".[17] The incident invited the attention of Microsoft, Peru, whose general manager wrote a letter to Villanueva. His response received worldwide attention and is seen as a classic piece of argumentation favouring use of free software in governments.[18]
Uruguay has a sanctioned law requiring that the state give priority to free software. It also requires that information be exchanged in open formats.[19]
The Government of Venezuela implemented a free software law in January 2006. Decree No. 3,390 mandated all government agencies to migrate to free software over a two-year period.[20]
Publiccode.eu is a campaign launched demanding a legislation requiring that publicly financed software developed for the public sector be made publicly available under a Free and Open Source Software licence. If it is public money, it should be public code as well.[21]
The French Gendarmerie and the French National Assembly utilize the open source operating system Linux.[22]
Free Software events happening all around the world connects people to increase visibility for Free software projects and foster collaborations.
The free software movement has been extensively analyzed using economic methodologies, including perspectives from heterodox economics. Of particular interest to economists[who?] is the willingness of programmers in the free software movement to work, often producing higher-quality than proprietary programmers, without financial compensation[citation needed].
In his 1998 article "The High-Tech Gift Economy", Richard Barbrook suggested that the then-nascent free software movement represented a return to the gift economy building on hobbyism and the absence of economic scarcity on the Internet.[24]
Gabriella Coleman has emphasized the importance of accreditation, respect, and honour within the free software community as a form of compensation for contributions to projects, over and against financial motivations.[25]
The Swedish Marxian economist Johan Söderberg has argued that the free software movement represents a complete alternative to capitalism that may be expanded to create a post-work society. He argues that the combination of a manipulation of intellectual property law and private property to make goods available to the public and a thorough blend between labor and fun make the free software movement a communist economy.[26]
Since its inception, there is an ongoing contention between the many FLOSS organizations (FSF, OSI, Debian, Mozilla Foundation, Apache Foundation, etc.) within the free software movement, with the main conflicts centered around the organization's needs for compromise and pragmatism rather than adhering to founding values and philosophies.[27]
The Open Source Initiative (OSI) was founded in February 1998 by Eric Raymond and Bruce Perens to promote the term "open-source software" as an alternative term for free software. The OSI aimed to address the perceived shortcomings and ambiguity of the term "free software", as well as shifting the focus of free software from a social and ethical issue to instead emphasize open source as a superior model for software development.[28][29][30][31] The latter became the view of Eric Raymond and Linus Torvalds, while Bruce Perens argued that open source was meant to popularize free software under a new brand and called for a return to basic ethical principles.[32]
Some free software advocates use the terms "Free and Open-Source Software" (FOSS) or "Free/Libre and Open-Source Software" (FLOSS) as a form of inclusive compromise, which brings free and open-source software advocates together to work on projects cohesively. Some users believe this is an ideal solution in order to promote both the user's freedom with the software and the pragmatic efficiency of an open-source development model. This view is reinforced by fact that majority of OSI-approved licenses and self-avowed open-source programs are also compatible with the free software formalisms and vice versa.[33]
While free and open source software are often linked together, they offer two separate ideas and values. Richard Stallman has referred to open source as "a non-movement", as it "does not campaign for anything".[34]
"Open source" addresses software being open as a practical question  rather than an ethical dilemma – non-free software is not the best solution but nonetheless a solution. The free software movement views free software as a moral imperative: that proprietary software should be rejected, and that only free software should be developed and taught in order to make computing technology beneficial to the general public.[35]
Although the movements have differing values and goals, collaborations between the Free Software Movement and Open Source Initiative have taken place when it comes to practical projects.[36] By 2005, Richard Glass considered the differences to be a "serious fracture" but "vitally important to those on both sides of the fracture" and "of little importance to anyone else studying the movement from a software engineering perspective" since they have had "little effect on the field".[37]
Eric Raymond criticises the speed at which the free software movement is progressing, suggesting that temporary compromises should be made for long-term gains. Raymond argues that this could raise awareness of the software and thus increase the free software movement's influence on relevant standards and legislation.[38]
Stallman said that this is where people get the misconception of "free": there is no wrong in programmers' requesting payment for a proposed project, or charging for copies of free software.[40] Restricting and controlling the user's decisions on use is the actual violation of freedom. Stallman defends that in some cases, monetary incentive is not necessary for motivation since the pleasure in expressing creativity is a reward in itself.[6] Conversely, Stallman admits that it is not easy to raise money for free software projects.[41]
The free software movement champions copyleft licensing schema (often pejoratively called "viral licenses"). In its strongest form, copyleft mandates that any works derived from copyleft-licensed software must also carry a copyleft license, so the license spreads from work to work like a computer virus might spread from machine to machine. Stallman has previously stated his opposition to describing the GNU GPL as "viral". These licensing terms can only be enforced through asserting copyrights.[42]
Critics of copyleft licensing challenge the idea that restricting modifications is in line with the free software movement's emphasis on various "freedoms", especially when alternatives like MIT, BSD, and Apache licenses are more permissive.[43][44] Proponents enjoy the assurance that copylefted work cannot usually be incorporated into non-free software projects.[45] They emphasize that copyleft licenses may not attach for all uses and that in any case, developers can simply choose not to use copyleft-licensed software.[46][47]
Proprietary software is software that grants its creator, publisher, or other rightsholder or rightsholder partner a legal monopoly by modern copyright and intellectual property law to exclude the recipient from freely sharing the software or modifying it, and—in some cases, as is the case with some patent-encumbered and EULA-bound software—from making use of the software on their own, thereby restricting their freedoms.[1]
Proprietary software is a subset of non-free software, a term defined in contrast to free and open-source software; non-commercial licenses such as CC BY-NC are not deemed proprietary, but are non-free. Proprietary software may either be closed-source software or source-available software.[1][2]
Until the late 1960s, computers—especially large and expensive mainframe computers, machines in specially air-conditioned computer rooms—were usually leased to customers rather than sold.[3][4] Service and all software available were usually supplied by manufacturers without separate charge until 1969. Computer vendors usually provided the source code for installed software to customers.[citation needed] Customers who developed software often made it available to the public without charge.[5] Closed source means computer programs whose source code is not published except to licensees. It is available to be modified only by the organization that developed it and those licensed to use the software.
In 1969, IBM, which had antitrust lawsuits pending against it, led an industry change by starting to charge separately for mainframe software[6][7] and services, by unbundling hardware and software.[8]
Bill Gates' "Open Letter to Hobbyists" in 1976 decried computer hobbyists' rampant copyright infringement of software, particularly Microsoft's Altair BASIC interpreter, and asserted that their unauthorized use hindered his ability to produce quality software. But the legal status of software copyright, especially for object code, was not clear until the 1983 appeals court ruling in Apple Computer, Inc. v. Franklin Computer Corp.[9][10][11]
According to Brewster Kahle the legal characteristic of software changed also due to the U.S. Copyright Act of 1976.[12]
Starting in February 1983 IBM adopted an "object-code-only" model for a growing list of their software and stopped shipping much of the source code,[13][14] even to licensees.
In 1983, binary software became copyrightable in the United States as well by the Apple vs. Franklin law decision,[15] before which only source code was copyrightable.[16] Additionally, the growing availability of millions of computers based on the same microprocessor architecture created for the first time an unfragmented and big enough market for binary distributed software.[16]
The tendency to license proprietary software, rather than sell it, dates from the time period before the existence, then the scope of software copyright protection was clear. These licenses have continued in use after software copyright was recognized in the courts, and are considered to grant the company extra protection compared to copyright law.[17] According to United States federal law, a company can restrict the parties to which it sells but it cannot prevent a buyer from reselling the product. Software licensing agreements usually prohibit resale, enabling the company to maximize revenue.[18]
Software distributions considered as proprietary may in fact incorporate a "mixed source" model including both free and non-free software in the same distribution.[26] Most if not all so-called proprietary UNIX distributions are mixed source software, bundling open-source components like BIND, Sendmail, X Window System, DHCP, and others along with a purely proprietary kernel and system utilities.[27][28]
Some free software packages are also simultaneously available under proprietary terms. Examples include MySQL, Sendmail and ssh. The original copyright holders for a work of free software, even copyleft free software, can use dual-licensing to allow themselves or others to redistribute proprietary versions. Non-copyleft free software (i.e. software distributed under a permissive free software license or released to the public domain) allows anyone to make proprietary redistributions.[29][30] Free software that depends on proprietary software is considered "trapped" by the Free Software Foundation. This includes software written only for Microsoft Windows,[31] or software that could only run on Java, before it became free software.[32]
Most of the software is covered by copyright which, along with contract law, patents, and trade secrets, provides legal basis for its owner to establish exclusive rights.[33]
A software vendor delineates the specific terms of use in an end-user license agreement (EULA). The user may agree to this contract in writing, interactively on screen (clickwrap), or by opening the box containing the software (shrink wrap licensing). License agreements are usually not negotiable.[34] Software patents grant exclusive rights to algorithms, software features, or other patentable subject matter, with coverage varying by jurisdiction. Vendors sometimes grant patent rights to the user in the license agreement.[35] The source code for a piece of proprietary software is routinely handled as a trade secret.[36] Software can be made available with fewer restrictions on licensing or source-code access; software that satisfies certain conditions of freedom and openness is known as "free" or "open-source."[37]
Since license agreements do not override applicable copyright law or contract law, provisions in conflict with applicable law are not enforceable.[38] Some software is specifically licensed and not sold, in order to avoid limitations of copyright such as the first-sale doctrine.[39]
The owner of proprietary software exercises certain exclusive rights over the software. The owner can restrict the use, inspection of source code, modification of source code, and redistribution.
Vendors typically limit the number of computers on which software can be used, and prohibit the user from installing the software on extra computers.[citation needed] Restricted use is sometimes enforced through a technical measure, such as product activation, a product key or serial number, a hardware key, or copy protection.
Vendors typically distribute proprietary software in compiled form, usually the machine language understood by the computer's central processing unit. They typically retain the source code, or human-readable version of the software, often written in a higher level programming language.[43] This scheme is often referred to as closed source.[44]
While most proprietary software is distributed without the source code, some vendors distribute the source code or otherwise make it available to customers. For example, users who have purchased a license for the Internet forum software vBulletin can modify the source for their own site but cannot redistribute it. This is true for many web applications, which must be in source code form when being run by a web server. The source code is covered by a non-disclosure agreement or a license that allows, for example, study and modification, but not redistribution.[45] The text-based email client Pine and certain implementations of Secure Shell are distributed with proprietary licenses that make the source code available.[citation needed]Some licenses for proprietary software allow distributing changes to the source code, but only to others licensed for the product, and some[46] of those modifications are eventually picked up by the vendor.
Some governments fear that proprietary software may include defects or malicious features which would compromise sensitive information. In 2003 Microsoft established a Government Security Program (GSP) to allow governments to view source code and Microsoft security documentation, of which the Chinese government was an early participant.[47][48] The program is part of Microsoft's broader Shared Source Initiative which provides source code access for some products. The Reference Source License (Ms-RSL) and Limited Public License (Ms-LPL) are proprietary software licenses where the source code is made available.
Governments have also been accused of adding such malware to software themselves. According to documents released by Edward Snowden, the NSA has used covert partnerships with software companies to make commercial encryption software exploitable to eavesdropping, or to insert backdoors.[49][50]
Software vendors sometimes use obfuscated code to impede users who would reverse engineer the software.[51] This is particularly common with certain programming languages.[citation needed] For example, the bytecode for programs written in Java can be easily decompiled to somewhat usable code,[citation needed] and the source code for programs written in scripting languages such as PHP or JavaScript is available at run time.[52]
Proprietary software vendors can prohibit the users from sharing the software with others. Another unique license is required for another party to use the software.
In the case of proprietary software with source code available, the vendor may also prohibit customers from distributing their modifications to the source code.
Shareware is closed-source software whose owner encourages redistribution at no cost, but which the user sometimes must pay to use after a trial period. The fee usually allows use by a single user or computer. In some cases, software features are restricted during or after the trial period, a practice sometimes called crippleware.
Proprietary software often[citation needed] stores some of its data in file formats that are incompatible with other software, and may also communicate using protocols which are incompatible. Such formats and protocols may be restricted as trade secrets or subject to patents.[citation needed]
A proprietary application programming interface (API) is a software library interface "specific to one device or, more likely to a number of devices within a particular manufacturer's product range."[53] The motivation for using a proprietary API can be vendor lock-in or because standard APIs do not support the device's functionality.[53]
Any dependency on the future versions and upgrades for a proprietary software package can create vendor lock-in, entrenching a monopoly position.[56]
Proprietary software may also have licensing terms that limit the usage of that software to a specific set of hardware. Apple has such a licensing model for macOS, an operating system which is limited to Apple hardware, both by licensing and various design decisions. This licensing model has been affirmed by the United States Court of Appeals for the Ninth Circuit.[57]
Proprietary software which is no longer marketed, supported or sold by its owner is called abandonware, the digital form of orphaned works. If the proprietor of a software package should cease to exist, or decide to cease or limit production or support for a proprietary software package, recipients and users of the package may have no recourse if problems are found with the software. Proprietors can fail to improve and support software because of business problems.[58] Support for older or existing versions of a software package may be ended to force users to upgrade and pay for newer versions[59](planned obsolescence). Sometimes another vendor or a software's community themselves can provide support for the software, or the users can migrate to either competing systems with longer support life cycles or to FOSS-based systems.[60]
Some proprietary software is released by their owner at end-of-life as open-source or source available software, often to prevent the software from becoming unsupported and unavailable abandonware.[61][62][63] 3D Realms and id Software are famous for the practice of releasing closed source software into the open source.[further explanation needed] Some of those kinds are free-of-charge downloads (freeware), some are still commercially sold (e.g. Arx Fatalis).[further explanation needed] More examples of formerly closed-source software in the List of commercial software with available source code and List of commercial video games with available source code.
Proprietary software is not synonymous with commercial software,[64][65] although the two terms are sometimes used synonymously in articles about free software.[66][67] Proprietary software can be distributed at no cost or for a fee, and free software can be distributed at no cost or for a fee.[68] The difference is that whether proprietary software can be distributed, and what the fee would be, is at the proprietor's discretion. With free software, anyone who has a copy can decide whether, and how much, to charge for a copy or related services.[69]
Proponents of commercial proprietary software argue that requiring users to pay for software as a product increases funding or time available for the research and development of software. For example, Microsoft says that per-copy fees maximize the profitability of software development.[70]
Proprietary software generally creates greater commercial activity over free software, especially in regard to market revenues.[71] Proprietary software is often sold with a license that gives the end user right to use the software.
Video editing software or a video editor is software used for performing the post-production video editing of digital video sequences on a non-linear editing system (NLE). It has replaced traditional flatbed celluloid film editing tools and analog video tape editing machines.
Video editing software serves a lot of purposes, such as filmmaking, audio commentary, and general editing of video content.
In NLE software, the user manipulates sections of video, images, and audio on a sequence. These clips can be trimmed, cut, and manipulated in many different ways. When editing is finished, the user exports the sequence as a video file.
NLE software is typically based on a timeline interface where sections moving image video recordings, known as clips, are laid out in sequence and played back. The NLE offers a range of tools for trimming, splicing, cutting, and arranging clips across the timeline.
As digital video editing advanced, visual effects became possible, and is part of the standard toolkit, usually found in prosumer and professional grade software.
A different kind of visual effects is motion capture. Software such as Blender can perform motion capture to make animated objects follow an actor's movements.
Video editing software can be divided into consumer grade, which focuses on ease-of-use, along with professional grade software, which focuses on feature availability, and advanced editing techniques. The typical use case for the former is to edit personal videos on the go, when more advanced editing is not required.
While most video editing software has been separate from the operating systems, some operating systems have had a video editor installed by default, such as Windows Movie Maker in Windows XP, or as a component of the default photo viewer, such as the Photos app on iOS.
Software Freedom Conservancy, Inc. (also known as "Conservancy") is an organization that provides a non-profit home, infrastructure support, and legal support for free and open source software projects. The organization was established in 2006, and as of June 2022, had over 40 member projects.[2]
In February 2015, the Outreachy program (formerly the Free and Open Source Software Program for Women) announced that it was moving from The GNOME Project to become part of Conservancy.[10]
In November 2017, Conservancy reported that the Software Freedom Law Center had demanded the invalidation of the SFC's trademark.[13][non-primary source needed]
The Software Freedom Law Center (SFLC) is an organization that provides pro bono legal representation and related services to not-for-profit developers of free software/open source software.  It was launched in February 2005 with Eben Moglen as chairman.  Initial funding of US$4 million was pledged by Open Source Development Labs.
SFLC represented and advised the Free Software Foundation, one of its principal clients, throughout the process of drafting and public discussion of version 3 of the GNU General Public License (GPLv3) during 2005–2007. Along with FSF president Richard M. Stallman, SFLC director Eben Moglen and then-SFLC counsel Richard Fontana were principal authors of GPLv3, LGPLv3, and the GNU Affero General Public License.[2][3][4]
On December 14, 2009, SFLC announced the filing of a lawsuit on behalf of its clients, Software Freedom Conservancy and Erik Andersen, against 14 companies, including Best Buy, Samsung, and Westinghouse alleging these companies had violated GPLv2 by distributing BusyBox in some of their products without releasing BusyBox source code.[15][16] 
For instance, Samsung released its LN52A650 TV firmware in 2010,[17][18][19] which was used later as the base for the community SamyGO project.
On December 11, 2008, SFLC announced the filing of a lawsuit on behalf of its client, the Free Software Foundation, against Cisco Systems, Inc.[21]  This lawsuit was the first suit ever initiated by the Free Software Foundation,.[22] On May 20, 2009, the parties announced a settlement.
Software relicensing is applied in open-source software development when software licenses of software modules are incompatible and are required to be compatible for a greater combined work. Licenses applied to software as copyrightable works, in source code as binary form,[1] can contain contradictory clauses. These requirements can make it impossible to combine source code or content of several software works to create a new combined one.[2][3]
Sometimes open-source software projects get stuck in a license incompatibility situation. Often the only feasible way to resolve this situation is re-licensing of all participating software parts. For successful relicensing the agreement of all involved copyright holders, typically the developers, to a changed license is required. While in the free and open-source domain achieving 100% coverage of all authors is often impossible due to the many contributors involved, often it is assumed that a great majority is sufficient. For instance, Mozilla assumed an author coverage of 95% to be sufficient.[4] Others in the free and open-source software (FOSS) domain, such as Eric S. Raymond, came to different conclusions regarding the requirements for relicensing of a whole code base.[5]
In August 2016 the MariaDB Corporation relicensed the database proxy server MaxScale from GPL to the non-FOSS but source-available and time-limited Business source license (BSL)[51] which defaults back after three years to GPL.[52][53] In 2017 followed version 1.1, revised with feedback also from Bruce Perens.[54][55]
For a long time D back-end source code was available but under a non-open source conform license,[56] because it was partially developed at Symantec and couldn't be relicensed as open source.[57] On April 9, 2017, also the back-end part could be relicensed to the open-source Boost Software License.[58][59][60]
League for Programming Freedom (LPF) was founded in 1989 by Richard Stallman to unite free software developers as well as developers of proprietary software to fight against software patents and the extension of the scope of copyright. Their logo is the Statue of Liberty holding a floppy disk and tape spool.
The League produced a newsletter, Programming Freedom,[1] in 11 issues from 1991 to 1995. These primary source materials chronicle the work of the organization.
The single event that had the most influence on the creation of the League was Apple's lawsuits against Microsoft about supposed copyrights violations of the look and feel of the Macintosh in the development of Windows.[2] After the lawsuit ended, the League went dormant, to be resurrected by those who were increasingly troubled by the enforcement of software patents.
The Free Software Foundation Europe e.V. (FSFE) is an organization that supports free software and all aspects of the free software movement in Europe, with registered chapters in several European countries.[2][3] It is a registered voluntary association (German: eingetragener Verein) incorporated under German law.
FSFE was founded in 2001. It is the European sister organization of the US‑based Free Software Foundation (FSF), although each foundation exists as a separate organization.[4] Following the return of Richard Stallman to the FSF in 2021, the FSFE declared themselves "unable to collaborate" with the FSF.[5][6]
FSFE believes that access to (and control of) software determines who may participate in a digital society. Consequently, FSFE believes, the freedoms to use, copy, modify and redistribute software, as described in The Free Software Definition, are necessary for equal participation in the Information Age.[7] The focus of FSFE's work is political, legal, and social, with the aim of promoting free software and the ethical, philosophical, social, political and commercial values that it implements.[8] In particular, it:
The people of the Free Software Foundation Europe (FSFE), see ourselves as Europeans from different cultures with the shared goal of co-operation across cultures and of developing a common culture of co-operation from a regional to a global level.
We form a non-profit non-governmental organisation and network that itself is part of a global network of people with common goals and visions. We are not representative for anyone but ourselves and our work. Our common work and dedication to freedom in all aspects of digital society is what defines us.
As well as being in regular contact with other FSFs — Free Software Foundation India (FSFI), Free Software Foundation Latin America (FSFLA) — FSFE has a structure of organizations which are official associates.[32] These are mostly national-level free software groups.
Free Software Foundation Latin America (FSFLA) is the Latin American sister organisation of the Free Software Foundation. It is the fourth sister organisation of FSF, after Free Software Foundation Europe and Free Software Foundation India. It was launched on November 23, 2005 in Rosario, Argentina.[2]
In 2006, Beatriz Busaniche, Enrique A. Chaparro, Federico Heinz, Juan José Ciarlante and Mario M. Bonilla left the FSFLA's Council. After that the original directives were modified to the current ones. A new position of “Observer of the Council” was created in the organisation to allow other important people in the free software community to participate, observe and advise the Council.[3]
The Free Software Foundation of India (FSFI) is the Indian sister organisation to the US-based Free Software Foundation. It was founded in Thiruvananthapuram (Trivandrum) (the capital of Kerala) in 2001[2]  as a non-profit Company.[3] The FSFI advocates to promote the use and development of free software in India. This includes educating people about free software, including how it can help the economy of a developing country like India. FSF India regards non-free software as not a solution, but a problem to be solved.  Free software is sometimes locally called swatantra software in India.
In 2003, after meeting with FSF founder Richard Stallman, the President of India Dr. Abdul Kalam[4][5] urged Indian computer scientists and professionals to use free and open-source software[6] in research and development.[7]
The left side of the FSF India logo resembles the famed Ashoka Chakra, that also appears on the Indian national flag, while its right half is a depiction of a compact disc. The combination of both symbols is intended to draw attention to the similarity between political freedom and free software, the latter of which the FSF promotes.
System software is software designed to provide a platform for other software. Examples of system software include operating systems (OS) (like macOS, Linux, Android, and Microsoft Windows).[1]
Application software is software that allows users to do user-oriented tasks such as creating text documents, playing or developing games, creating presentations, listening to music, drawing pictures, or browsing the web. Examples of such software are computational science software, game engines, search engines, industrial automation, and software as a service applications.[2]
In the late 1940s, application software was custom-written by computer users to fit their specific hardware and requirements. System software was usually supplied by the manufacturer of the computer hardware and was intended to be used by most or all users of that system.
Many operating systems come pre-packaged with basic application software. Such software is not considered system software when it can be uninstalled without affecting the functioning of other software. Examples of such software are games and simple editing tools supplied with Microsoft Windows, or software development toolchains supplied with many Linux distributions.
Some of the grayer areas between system and application software are web browsers integrated deeply into the operating system such as Internet Explorer in some versions of Microsoft Windows, or ChromeOS where the browser functions as the only user interface and the only way to run programs (and other web browser their place)[clarification needed].
The operating system (prominent examples being Microsoft Windows, macOS, Linux, and z/OS), allows the parts of a computer to work together by performing tasks like transferring data between memory and disks or rendering output onto a display device. It provides a platform (hardware abstraction layer) to run high-level system software and application software.
A kernel is the core part of the operating system that defines an application programming interface for applications programs (including some system software) and an interface to device drivers.
A user interface interacts with a computer.[3] It can either be a command-line interface (CLI) or, since the 1980s, a graphical user interface (GUI). This is the part of the operating system the user directly interacts with; it is considered an application and not system software.
Some organizations use the term systems programmer to describe a job function that is more accurately termed systems administrator. Software tools these employees use are then called system software. This utility software helps to analyze, configure, optimize and maintain the computer, such as virus protection. The term system software can also include software development tools (like a compiler, linker, or debugger).[4]
Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks.[1][2] It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.
Auxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term software development is used for this larger overall process – with the terms programming, implementation, and coding reserved for the writing and editing of code per se. Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.
Code-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.[7]
The first computer program is generally dated to 1843 when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.[8] However, Charles Babbage himself had written a program for the AE in 1837.[9][10]
These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier[16] by allowing programmers to specify calculations by entering a formula using infix notation.
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.
Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.[22] Some of these factors include:
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.
The academic field and the engineering practice of computer programming are concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into orders using Big O notation, which expresses resource use—such as execution time or memory consumption—in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.
Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic programming languages.
It is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language,[23] the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers[24] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).
Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.
Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in "high-level" languages than in "low-level" ones.
Programming languages are essential for software development. They are the building blocks for all software, from the simplest applications to the most sophisticated ones.
Learning to program has a long history related to professional standards and practices, academic initiatives and curriculum, and commercial books and materials for students, self-taught learners, hobbyists, and others who desire to create or customize software for personal use. Since the 1960s, learning to program has taken on the characteristics of a popular movement, with the rise of academic disciplines, inspirational leaders, collective identities, and strategies to grow the movement and make institutionalize change.[26] Through these social ideals and educational agendas, learning to code has become important not just for scientists and engineers, but for millions of citizens who have come to believe that creating software is beneficial to society and its members.
In 1957, there were approximately 15,000 computer programmers employed in the U.S., a figure that accounts for 80% of the world's active developers. In 2014, there were approximately 18.5 million professional programmers in the world, of which 11 million can be considered professional and 7.5 million student or hobbyists.[27] Before the rise of the commercial Internet in the mid-1990s, most programmers learned about software construction through books, magazines, user groups, and informal instruction methods, with academic coursework and corporate training playing important roles for professional workers.[28]
When high-level languages arrived, they were introduced by numerous books and materials that explained language keywords, managing program flow, working with data, and other concepts. These languages included FLOW-MATIC, COBOL, FORTRAN, ALGOL, Pascal, BASIC, and C. An example of an early programming primer from these years is Marshal H. Wrubel's A Primer of Programming for Digital Computers (1959), which included step-by-step instructions for filling out coding sheets, creating punched cards, and using the keywords in IBM's early FORTRAN system.[29] Daniel McCracken's A Guide to FORTRAN Programming (1961) presented FORTRAN to a larger audience, including students and office workers.
In 1961, Alan Perlis suggested that all university freshmen at Carnegie Technical Institute take a course in computer programming.[30] His advice was published in the popular technical journal Computers and Automation, which became a regular source of information for professional programmers.
Early and influential programmer's guides included John G. Kemeny and Thomas E. Kurtz's BASIC Programming (1967), Kathleen Jensen and Niklaus Wirth's The Pascal User Manual and Report (1971), and Brian Kernighan and Dennis Ritchie's The C Programming Language (1978). Similar books for popular audiences (but with a much lighter tone) included Bob Albrecht's My Computer Loves Me When I Speak BASIC (1972), Al Kelley and Ira Pohl's A Book on C (1984), and Dan Gookin's C for Dummies (1994).
Beyond language-specific primers, there were numerous books and academic journals that introduced professional programming practices. Many were designed for university courses in computer science, software engineering, or related disciplines. Donald Knuth's The Art of Computer Programming (1968 and later), presented hundreds of computational algorithms and their analysis. The Elements of Programming Style (1974), by Brian W. Kernighan and P. J. Plauger, concerned itself with programming style, the idea that programs should be written not only to satisfy the compiler but human readers. Jon Bentley's Programming Pearls (1986) offered practical advice about the art and craft of programming in professional and academic contexts. Texts specifically designed for students included Doug Cooper and Michael Clancy's Oh Pascal! (1982), Alfred Aho's Data Structures and Algorithms (1983), and Daniel Watt's Learning with Logo (1983).
As personal computers became mass-market products, thousands of trade books and magazines sought to teach professional, hobbyist, and casual users to write computer programs. A sample of these learning resources includes BASIC Computer Games, Microcomputer Edition (1978), by David Ahl; Programming the Z80 (1979), by Rodnay Zaks; Programmer's CP/M Handbook (1983), by Andy Johnson-Laird; C Primer Plus (1984), by Mitchell Waite and The Waite Group; The Peter Norton Programmer's Guide to the IBM PC (1985), by Peter Norton; Advanced MS-DOS (1986), by Ray Duncan; Learn BASIC Now (1989), by Michael Halvorson and David Rygymr; Programming Windows (1992 and later), by Charles Petzold; Code Complete: A Practical Handbook for Software Construction (1993), by Steve McConnell; and Tricks of the Game-Programming Gurus (1994), by André LaMothe.
The PC software industry spurred the creation of numerous book publishers that offered programming primers and tutorials, as well as books for advanced software developers.[31] These publishers included Addison-Wesley, IDG, Macmillan Inc., McGraw-Hill, Microsoft Press, O'Reilly Media, Prentice Hall, Sybex, Ventana Press, Waite Group Press, Wiley, Wrox Press, and Ziff-Davis.
Computer magazines and journals also provided learning content for professional and hobbyist programmers. A partial list of these resources includes Amiga World, Byte (magazine), Communications of the ACM, Computer (magazine), Compute!, Computer Language (magazine), Computers and Electronics, Dr. Dobb's Journal, IEEE Software, Macworld, PC Magazine, PC/Computing, and UnixWorld.
Between 2000 and 2010, computer book and magazine publishers declined significantly as providers of programming instruction, as programmers moved to Internet resources to expand their access to information. This shift brought forward new digital products and mechanisms to learn programming skills. During the transition, digital books from publishers transferred information that had traditionally been delivered in print to new and expanding audiences.[32]
Important Internet resources for learning to code included blogs, wikis, videos, online databases, subscription sites, and custom websites focused on coding skills. New commercial resources included YouTube videos, Lynda.com tutorials (later LinkedIn Learning), Khan Academy, Codecademy, GitHub, W3Schools, and numerous coding bootcamps.
Most software development systems and game engines included rich online help resources, including integrated development environments (IDEs), context-sensitive help, APIs, and other digital resources. Commercial software development kits (SDKs) also provided a collection of software development tools and documentation in one installable package.
Commercial and non-profit organizations published learning websites for developers, created blogs, and established newsfeeds and social media resources about programming. Corporations like Apple, Microsoft, Oracle, Google, and Amazon built corporate websites providing support for programmers, including resources like the Microsoft Developer Network (MSDN). Contemporary movements like Hour of Code (Code.org) show how learning to program has become associated with digital learning strategies, education agendas, and corporate philanthropy.
Although programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language.[33][34]
A computing platform, digital platform,[1] or software platform is the infrastructure on which software is executed. While the individual components of a computing platform  may be obfuscated under layers of abstraction, the summation of the required components comprise the computing platform.
Sometimes, the most relevant layer for a specific software is called a computing platform in itself to facilitate the communication, referring to the whole using only one of its attributes – i.e. using a metonymy.
For example, in a single computer system, this would be the computer's architecture, operating system (OS), and runtime libraries.[2] In the case of an application program or a computer video game, the most relevant layer is the operating system, so it can be called a platform itself (hence the term cross-platform for software that can be executed on multiple OSes, in this context). 
In a multi-computer system, such as in the case of offloading processing, it would encompass both the host computer's hardware, operating system (OS), and runtime libraries along with other computers utilized for  processing that are accessed via application programming interfaces or a web browser. As long as it is a required component for the program code to execute, it is part of the computing platform.
Software as a service (SaaS /sæs/[1]) is a cloud computing service model where the provider offers use of application software to a client and manages all needed physical and software resources.[2] SaaS is usually accessed via a web application. Unlike other software delivery models, it separates "the possession and ownership of software from its use".[3] SaaS use began around 2000, and by 2023 was the main form of software application deployment.
Unlike most self-hosted software products, only one version of the software exists[citation needed] and only one operating system and configuration is supported. SaaS products typically run on rented infrastructure as a service (IaaS) or platform as a service (PaaS) systems including hardware and sometimes operating systems and middleware, to accommodate rapid increases in usage while providing instant and continuous availability to customers. SaaS customers have the abstraction of limitless computing resources, while economy of scale drives down the cost. SaaS architectures are typically multi-tenant; usually they share resources between clients for efficiency, but sometimes they offer a siloed environment for an additional fee. Common SaaS revenue models include freemium, subscription, and usage-based fees. Unlike traditional software, it is rarely possible to buy a perpetual license for a certain version of the software.
There are no specific software development practices that distinguish SaaS from other application development, although there is often a focus on frequent testing and releases.
Infrastructure as a service (IaaS) is the most basic form of cloud computing, where infrastructure resources—such as physical computers—are not owned by the user but instead leased from a cloud provider. As a result, infrastructure resources can be increased rapidly, instead of waiting weeks for computers to ship and set up. IaaS requires time and expertise to make use of the infrastructure in the form of operating systems and applications.[4] Platform as a service (PaaS) includes the operating system and middleware, but not the applications.[5][6] SaaS providers typically use PaaS or IaaS services to run their applications.[5]
Without IaaS, it would be extremely difficult to make an SaaS product scalable for a variable number of users while providing the instant and continual availability that customers expect.[7]  Most end users consume only the SaaS product and do not have to worry about the technical complexity of the physical hardware and operating system.[8] Because cloud resources can be accessed without any human interactions, SaaS customers are provided with the abstraction of limitless computing resources, while economy of scale drives down the cost.[9] Another key feature of cloud computing is that software updates can be rolled out and made available to all customers nearly instantaneously.[10] In 2019, SaaS was estimated to make up the plurality, 43 percent, of the cloud computing market while IaaS and PaaS combined account for approximately 25 percent.[11]
Popular consumer SaaS products include all social media websites, email services like Gmail and its associated Google Docs Editors,[17] Skype, Dropbox,[18] and entertainment products like Netflix and Spotify.[19] Enterprise SaaS products include Salesforce's customer relationship management (CRM) software, SAP Cloud Platform, and Oracle Cloud Enterprise Resource Planning.[18]
The most common SaaS revenue models involve subscription and pay for usage.[24] For customers, the advantages include reduced upfront cost, increased flexibility, and lower overall cost compared to traditional software with perpetual software licenses.[25] In some cases, the steep one-time cost demanded by sellers of traditional software were out of the reach of smaller businesses, but pay-per-use SaaS models makes the software affordable.[3] Usage may be charged based on the number of users, transactions, amount of storage spaced used, or other metrics.[26] Many buyers prefer pay-per-usage because they believe that they are relatively light users of the software, and the seller benefits by reaching occasional users who would otherwise not buy the software.[26] However, it can cause revenue uncertainty for the seller and increases the overhead for billing.[27]
SaaS products are typically accessed via a web browser as a publicly available web application.[30][16] This means that customers can access the application anywhere from any device without needing to install or update it.[16][31] SaaS providers often try to minimize the difficulty of signing up for the product.[32] Many capitalize on the service-oriented structure to respond to customer feedback and evolve their product quickly to meet demands. This can enable customers to believe in the continued improvement of the product and help the SaaS provider get customers from an established traditional software company that likely can offer a deeper feature set.[33][34]
Although on-premises software is often less secure than SaaS alternatives,[35] security and privacy are among the main reasons cited by companies that do not adopt SaaS products.[36] SaaS companies have to protect their publicly available offerings from abuse, including denial-of-service attacks and hacking.[37] They often use technologies such as access control, authentication, and encryption to protect data confidentiality.[36] Nevertheless, not all companies trust SaaS providers to keep sensitive data secured.[36] The vendor is responsible for software updates, including security patches, and for protecting the customers' data.[31] SaaS systems inherently have a greater latency than software run on-premises due to the time for network packets to be delivered to the cloud facility. This can be prohibitive for some uses, such as time-sensitive industrial processes or warehousing.[38]
A challenge for SaaS providers is that demand is not known in advance. Their system must have enough slack to be able to handle all users without turning any away, but without paying for too many resources that will be unnecessary. If resources are static, they are guaranteed to be wasted during non-peak time.[42] Sometimes cheaper off-peak rates are offered to balance the load and reduce waste.[43] The expectation for continuous service is so high that outages in SaaS software are often reported in the news.[44]
There are not specific software development practices that differentiate SaaS from other application development.[45] SaaS products are often released early and often to take advantage of the flexibility of the SaaS delivery model.[46] Agile software development is commonly used to support this release schedule.[47] Many SaaS developers use test-driven development, or otherwise emphasize frequent software testing, because of the need to ensure availability of their service and rapid deployment.[48] Domain-driven design, in which business goals drive development, is popular because SaaS products must sell themselves to the customer by being useful.[49] SaaS developers do not know in advance which devices customers will try to access the product from—such as a desktop computer, tablet, or smartphone—and supporting a wide range of devices is often an important concern for the front-end development team.[50] Progressive web applications allow some functionality to be available even if the device is offline.[51]
SaaS applications predominantly offer integration protocols and application programming interfaces (APIs) that operate over a wide area network.[52]
SaaS architecture varies significantly from product to product.[53]  Nevertheless, most SaaS providers offer a multi-tenant architecture.[30] With this model, a single version of the application, with a single configuration (hardware, network, operating system), is used for all customers ("tenants").[54] This means that the company does not need to support multiple versions and configurations.[16] The architectural shift from each customer running their own version of the software on their own hardware affects many aspects of the application's design and security features.[54] In a multi-tenant architecture, many resources can be used by different tenants or shared between multiple tenants.[55]
Some SaaS products do not share any resources between tenants—called siloing. Although this negates many of the efficiency benefits of SaaS, it makes it easier to migrate legacy software to SaaS[64] and is sometimes offered as a premium offering at a higher price.[65] Pooling all resources might make it possible to achieve higher efficiency,[66] but an outage affects all customers so availability must be prioritized to a greater extent.[67] Many systems use a combination of both approaches, pooling some resources and siloing others.[68] Other companies group multiple tenants into pods and share resources between them.[69]
A system programming language is a programming language used for system programming; such languages are designed for writing system software, which usually requires different development approaches when compared with application software. Edsger Dijkstra referred to these languages as machine oriented high order languages, or mohol.[1]
General-purpose programming languages tend to focus on generic features to allow programs written in the language to use the same code on different platforms. Examples of such languages include ALGOL and Pascal. This generic quality typically comes at the cost of denying direct access to the machine's internal workings, and this often has negative effects on performance.
System languages, in contrast, are designed not for compatibility, but for performance and ease of access to the underlying hardware while still providing high-level programming concepts like structured programming. Examples include ESPOL and SPL, both of which are similar to ALGOL in syntax but tuned to their respective platforms. Others are cross-platform but designed to work close to the hardware, like BLISS, JOVIAL and BCPL.
Some languages straddle the system and application domains, bridging the gap between these uses. The canonical example is C, which is used widely for both system and application programming. Some modern languages also do this such as Rust and Swift.
In contrast with application languages, system programming languages typically offer more-direct access to the physical hardware of the machine: an archetypical system programming language in this sense was BCPL. System programming languages often lack built-in input/output (I/O) facilities because a system-software project usually develops its own I/O mechanisms or builds on basic monitor I/O or screen management facilities. The distinction between languages used for system programming and application programming became blurred over time with the widespread popularity of PL/I, C and Pascal.
The earliest system software was written in assembly language primarily because there was no alternative, but also for reasons including efficiency of object code, compilation time, and ease of debugging. Application languages such as FORTRAN were used for system programming, although they usually still required some routines to be written in assembly language.[2]
While PL360 is at the semantic level of assembly language, another kind of system programming language operates at a higher semantic level, but has specific extensions designed to make the language suitable for system programming. An early example of this kind of language is LRLTRAN,[4] which extended Fortran with features for character and bit manipulation, pointers, and directly addressed jump tables.
Subsequently, languages such as C were developed, where the combination of features was sufficient to write system software, and a compiler could be developed that generated efficient object programs on modest hardware. Such a language generally omits features that cannot be implemented efficiently, and adds a small number of machine-dependent features needed to access specific hardware abilities; inline assembly code, such as C's asm statement, is often used for this purpose. Although many such languages were developed,[1] C and C++ are the ones which survived.
A real-time operating system (RTOS) is an operating system (OS) for real-time computing applications that processes data and events that have critically defined time constraints. An RTOS is distinct from a time-sharing operating system, such as Unix, which manages the sharing of system resources with a scheduler, data buffers, or fixed task prioritization in multitasking or multiprogramming environments. All operations must verifiably complete within given time and resource constraints or else fail safe. Real-time operating systems are event-driven and preemptive, meaning the OS can monitor the relevant priority of competing tasks, and make changes to the task priority.
A key characteristic of an RTOS is the level of its consistency concerning the amount of time it takes to accept and complete an application's task; the variability is "jitter".[1] A "hard" real-time operating system (hard RTOS) has less jitter than a "soft" real-time operating system (soft RTOS); a late answer is a wrong answer in a hard RTOS while a late answer is acceptable in a soft RTOS. The chief design goal is not high throughput, but rather a guarantee of a soft or hard performance category. An RTOS that can usually or generally meet a deadline is a soft real-time OS, but if it can meet a deadline deterministically it is a hard real-time OS.[2]
An RTOS has an advanced algorithm for scheduling. Scheduler flexibility enables a wider, computer-system orchestration of process priorities, but a real-time OS is more frequently dedicated to a narrow set of applications. Key factors in a real-time OS are minimal interrupt latency and minimal thread switching latency; a real-time OS is valued more for how quickly or how predictably it can respond than for the amount of work it can perform in a given period of time.[3]
An RTOS is an operating system in which the time taken to process an input stimulus is less than the time lapsed until the next input stimulus of the same type.
Usually, the data structure of the ready list in the scheduler is designed to minimize the worst-case length of time spent in the scheduler's critical section, during which preemption is inhibited, and, in some cases, all interrupts are disabled, but the choice of data structure depends also on the maximum number of tasks that can be on the ready list.
A multitasking operating system like Unix is poor at real-time tasks. The scheduler gives the highest priority to jobs with the lowest demand on the computer, so there is no way to ensure that a time-critical job will have access to enough resources. Multitasking systems must manage sharing data and hardware resources among multiple tasks. It is usually unsafe for two tasks to access the same specific data or hardware resource simultaneously.[5] There are three common approaches to resolve this problem:
General-purpose operating systems usually do not allow user programs to mask (disable) interrupts, because the user program could control the CPU for as long as it is made to. Some modern CPUs do not allow user mode code to disable interrupts as such control is considered a key operating system resource. Many embedded systems and RTOSs, however, allow the application itself to run in kernel mode for greater system call efficiency and also to permit the application to have greater control of the operating environment without requiring OS intervention.
When the shared resource must be reserved without blocking all other tasks (such as waiting for Flash memory to be written), it is better to use mechanisms also available on general-purpose operating systems, such as a mutex and OS-supervised interprocess messaging. Such mechanisms involve system calls, and usually invoke the OS's dispatcher code on exit, so they typically take hundreds of CPU instructions to execute, while masking interrupts may take as few as one instruction on some processors.
Since an interrupt handler blocks the highest priority task from running, and since real-time operating systems are designed to keep thread latency to a minimum, interrupt handlers are typically kept as short as possible. The interrupt handler defers all interaction with the hardware if possible; typically all that is necessary is to acknowledge or disable the interrupt (so that it won't occur again when the interrupt handler returns) and notify a task that work needs to be done. This can be done by unblocking a driver task through releasing a semaphore, setting a flag or sending a message. A scheduler often provides the ability to unblock a task from interrupt handler context.
An OS maintains catalogues of objects it manages such as threads, mutexes, memory, and so on. Updates to this catalogue must be strictly controlled. For this reason, it can be problematic when an interrupt handler calls an OS function while the application is in the act of also doing so. The OS function called from an interrupt handler could find the object database to be in an inconsistent state because of the application's update. There are two major approaches to deal with this problem: the unified architecture and the segmented architecture. RTOSs implementing the unified architecture solve the problem by simply disabling interrupts while the internal catalogue is updated. The downside of this is that interrupt latency increases, potentially losing interrupts. The segmented architecture does not make direct OS calls but delegates the OS related work to a separate handler. This handler runs at a higher priority than any thread but lower than the interrupt handlers. The advantage of this architecture is that it adds very few cycles to interrupt latency. As a result, OSes which implement the segmented architecture are more predictable and can deal with higher interrupt rates compared to the unified architecture.[citation needed]
Similarly, the System Management Mode on x86 compatible hardware can take a lot of time before it returns control to the operating system.
The simple fixed-size-blocks algorithm works quite well for simple embedded systems because of its low overhead.
A high-level programming language is a programming language with strong abstraction from the details of the computer. In contrast to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable than when using a lower-level language. The amount of abstraction provided defines how "high-level" a programming language is.[1]
In the 1960s, a high-level programming language using a compiler was commonly called an autocode.[2]
Examples of autocodes are COBOL and Fortran.[3]
The first high-level programming language designed for computers was Plankalkül, created by Konrad Zuse.[4] However, it was not implemented in his time, and his original contributions were largely isolated from other developments due to World War II, aside from the language's influence on the "Superplan" language by Heinz Rutishauser and also to some degree ALGOL. The first significantly widespread high-level language was Fortran, a machine-independent development of IBM's earlier Autocode systems. The ALGOL family, with ALGOL 58 defined in 1958 and ALGOL 60 defined in 1960 by committees of European and American computer scientists, introduced recursion as well as nested functions under lexical scope. ALGOL 60 was also the first language with a clear distinction between value and name-parameters and their corresponding semantics.[5] ALGOL also introduced several structured programming concepts, such as the while-do and if-then-else constructs and its syntax was the first to be described in formal notation – Backus–Naur form (BNF). During roughly the same period, COBOL introduced records (also called structs) and Lisp introduced a fully general lambda abstraction in a programming language for the first time.
"High-level language" refers to the higher level of abstraction from machine language. Rather than dealing with registers, memory addresses, and call stacks, high-level languages deal with variables, arrays, objects, complex arithmetic or Boolean expressions, subroutines and functions, loops, threads, locks, and other abstract computer science concepts, with a focus on usability over optimal program efficiency. Unlike low-level assembly languages, high-level languages have few, if any, language elements that translate directly into a machine's native opcodes. Other features, such as string handling routines, object-oriented language features, and file input/output, may also be present. One thing to note about high-level programming languages is that these languages allow the programmer to be detached and separated from the machine. That is, unlike low-level languages like assembly or machine language, high-level programming can amplify the programmer's instructions and trigger a lot of data movements in the background without their knowledge. The responsibility and power of executing instructions have been handed over to the machine from the programmer.
High-level languages intend to provide features that standardize common tasks, permit rich debugging, and maintain architectural agnosticism; while low-level languages often produce more efficient code through optimization for a specific system architecture. Abstraction penalty is the cost that high-level programming techniques pay for being unable to optimize performance or use certain hardware because they don't take advantage of certain low-level architectural resources. High-level programming exhibits features like more generic data structures and operations, run-time interpretation, and intermediate code files; which often result in execution of far more operations than necessary, higher memory consumption, and larger binary program size.[6][7][8] For this reason, code which needs to run particularly quickly and efficiently may require the use of a lower-level language, even if a higher-level language would make the coding easier. In many cases, critical portions of a program mostly in a high-level language can be hand-coded in assembly language, leading to a much faster, more efficient, or simply reliably functioning optimised program.
However, with the growing complexity of modern microprocessor architectures, well-designed compilers for high-level languages frequently produce code comparable in efficiency to what most low-level programmers can produce by hand, and the higher abstraction may allow for more powerful techniques providing better overall results than their low-level counterparts in particular settings.[9]
High-level languages are designed independent of a specific computing system architecture. This facilitates executing a program written in such a language on any computing system with compatible support for the Interpreted or JIT program. High-level languages can be improved as their designers develop improvements. In other cases, new high-level languages evolve from one or more others with the goal of aggregating the most popular constructs with new or improved features. An example of this is Scala which maintains backward compatibility with Java, meaning that programs and libraries written in Java will continue to be usable even if a programming shop switches to Scala; this makes the transition easier and the lifespan of such high-level coding indefinite. In contrast, low-level programs rarely survive beyond the system architecture which they were written for without major revision. This is the engineering 'trade-off' for the 'Abstraction Penalty'.
Examples of high-level programming languages in active use today include Python, JavaScript, Visual Basic, Delphi, Perl, PHP, ECMAScript, Ruby, C#, Java and many others.
The terms high-level and low-level are inherently relative. Some decades ago,[timeframe?] the C language, and similar languages, were most often considered "high-level", as it supported concepts such as expression evaluation, parameterised recursive functions, and data types and structures, while assembly language was considered "low-level". Today, many programmers might refer to C as low-level, as it lacks a large runtime-system (no garbage collection, etc.), basically supports only scalar operations, and provides direct memory addressing; it therefore, readily blends with assembly language and the machine level of CPUs and microcontrollers. Also, in the introduction chapter of The C Programming Language (second edition) by Brian Kernighan and Dennis Ritchie, C is described as "not a very high level" language.[10]
Assembly language may itself be regarded as a higher level (but often still one-to-one if used without macros) representation of machine code, as it supports concepts such as constants and (limited) expressions, sometimes even variables, procedures, and data structures. Machine code, in turn, is inherently at a slightly higher level than the microcode or micro-operations used internally in many processors.[11]
Application software is any computer program that is intended for end-user use – not operating, administering or programming the computer. An application (app, application program, software application) is any program that can be categorized as application software.[1][2] Common types of applications include word processor, media player and accounting software.
The term application software refers to all applications collectively[3] and can be used to differentiate from system and utility software.
Applications may be bundled with the computer and its system software or published separately. Applications may be proprietary or open-source.[4]
The delineation between system software such as operating systems and application software is not exact, however, and is occasionally the object of controversy.[6] For example, one of the key questions in the United States v. Microsoft Corp. antitrust trial was whether Microsoft's Internet Explorer web browser was part of its Windows operating system or a separate piece of application software.  As another example, the GNU/Linux naming controversy is, in part, due to disagreement about the relationship between the Linux kernel and the operating systems built over this kernel. In some types of embedded systems, the application software and the operating system software may be indistinguishable from the user, as in the case of software used to control a VCR, DVD player, or microwave oven. The above definitions may exclude some applications that may exist on some computers in large organizations. For an alternative definition of an app:  see Application Portfolio Management.
When used as an adjective, application is not restricted to mean: of or on application software.[6] For example, concepts such as application programming interface (API), application server, application virtualization, application lifecycle management and portable application apply to all computer programs alike, not just application software.
Sometimes a new and popular application arises that only runs on one platform that results in increasing the desirability of that platform. This is called a killer application or killer app, coined in the late 1980s.[7][8] For example, VisiCalc was the first modern spreadsheet software for the Apple II and helped sell the then-new personal computers into offices. For the BlackBerry, it was its email software.
From the legal point of view, application software is mainly classified with a black-box approach, about the rights of its end-users or subscribers (with eventual intermediate and tiered subscription levels).
Software applications are also classified with respect to the programming language in which the source code is written or executed, and concerning their purpose and outputs.
Application software is usually distinguished into two main classes: closed source vs open source software applications, and free or proprietary software applications.
Proprietary software is placed under the exclusive copyright, and a software license grants limited usage rights. The open-closed principle states that software may be "open only for extension, but not for modification". Such applications can only get add-ons from third parties.
Free and open-source software (FOSS) shall be run, distributed, sold, or extended for any purpose, and -being open- shall be modified or reversed in the same way.
FOSS software applications released under a free license may be perpetual and also royalty-free. Perhaps, the owner, the holder or third-party enforcer of any right (copyright, trademark, patent, or ius in re aliena) are entitled to add exceptions, limitations, time decays or expiring dates to the license terms of use.
Public-domain software is a type of FOSS which is royalty-free and - openly or reservedly- can be run, distributed, modified, reversed, republished, or created in derivative works without any copyright attribution and therefore revocation. It can even be sold, but without transferring the public domain property to other single subjects. Public-domain SW can be released under a (un)licensing legal statement, which enforces those terms and conditions for an indefinite duration (for a lifetime, or forever).
Application software can also be seen as being either horizontal or vertical.[12][13]  Horizontal applications are more popular and widespread, because they are general purpose, for example word processors or databases. Vertical applications are niche products, designed for a particular type of industry or business, or department within an organization. Integrated suites of software will try to handle every specific aspect possible of, for example, manufacturing or banking worker, accounting, or customer service.
Applications can also be classified by computing platforms such as a desktop application for a particular operating system,[16] delivery network such as in cloud computing and Web 2.0 applications, or delivery devices such as mobile apps for mobile devices.
The operating system itself can be considered application software when performing simple calculating, measuring, rendering, and word processing tasks not used to control hardware via a command-line interface or graphical user interface. This does not include application software bundled within operating systems such as a software calculator or text editor.
In computer science, program optimization, code optimization, or software optimization is the process of modifying a software system to make some aspect of it work more efficiently or use fewer resources.[1] In general, a computer program may be optimized so that it executes more rapidly, or to make it capable of operating with less memory storage or other resources, or draw less power.
Although the term "optimization" is derived from "optimum",[2] achieving a truly optimal system is rare in practice, which is referred to as superoptimization. Optimization typically focuses on improving a system with respect to a specific quality metric rather than making it universally optimal. This often leads to trade-offs, where enhancing one metric may come at the expense of another. One popular example is space-time tradeoff, reducing a program’s execution time by increasing its memory consumption. Conversely, in scenarios where memory is limited, engineers might prioritize a slower algorithm to conserve space. There is rarely a single design that can excel in all situations, requiring engineers to prioritize attributes most relevant to the application at hand.
At the highest level, the design may be optimized to make best use of the available resources, given goals, constraints, and expected use/load. The architectural design of a system overwhelmingly affects its performance. For example, a system that is network latency-bound (where network latency is the main constraint on overall performance) would be optimized to minimize network trips, ideally making a single request (or no requests, as in a push protocol) rather than multiple roundtrips. Choice of design depends on the goals: when designing a compiler, if fast compilation is the key priority, a one-pass compiler is faster than a multi-pass compiler (assuming same work), but if speed of output code is the goal, a slower multi-pass compiler fulfills the goal better, even though it takes longer itself. Choice of platform and programming language occur at this level, and changing them frequently requires a complete rewrite, though a modular system may allow rewrite of only some component –  for example, a Python program may rewrite performance-critical sections in C. In a distributed system, choice of architecture (client-server, peer-to-peer, etc.) occurs at the design level, and may be difficult to change, particularly if all components cannot be replaced in sync (e.g., old clients).
Given an overall design, a good choice of efficient algorithms and data structures, and efficient implementation of these algorithms and data structures comes next. After design, the choice of algorithms and data structures affects efficiency more than any other aspect of the program. Generally data structures are more difficult to change than algorithms, as a data structure assumption and its performance assumptions are used throughout the program, though this can be minimized by the use of abstract data types in function definitions, and keeping the concrete data structure definitions restricted to a few places.
For algorithms, this primarily consists of ensuring that algorithms are constant O(1), logarithmic O(log n), linear O(n), or in some cases log-linear O(n log n) in the input (both in space and time). Algorithms with quadratic complexity O(n2) fail to scale, and even linear algorithms cause problems if repeatedly called, and are typically replaced with constant or logarithmic if possible.
Beyond asymptotic order of growth, the constant factors matter: an asymptotically slower algorithm may be faster or smaller (because simpler) than an asymptotically faster algorithm when they are both faced with small input, which may be the case that occurs in reality. Often a hybrid algorithm will provide the best performance, due to this tradeoff changing with size.
A general technique to improve performance is to avoid work. A good example is the use of a fast path for common cases, improving performance by avoiding unnecessary work. For example, using a simple text layout algorithm for Latin text, only switching to a complex layout algorithm for complex scripts, such as Devanagari. Another important technique is caching, particularly memoization, which avoids redundant computations. Because of the importance of caching, there are often many levels of caching in a system, which can cause problems from memory use, and correctness issues from stale caches.
Beyond general algorithms and their implementation on an abstract machine, concrete source code level choices can make a significant difference. For example, on early C compilers, while(1) was slower than for(;;) for an unconditional loop, because while(1) evaluated 1 and then had a conditional jump which tested if it was true, while for (;;) had an unconditional jump . Some optimizations (such as this one) can nowadays be performed by optimizing compilers. This depends on the source language, the target machine language, and the compiler, and can be both difficult to understand or predict and changes over time; this is a key place where understanding of compilers and machine code can improve performance. Loop-invariant code motion and return value optimization are examples of optimizations that reduce the need for auxiliary variables and can even result in faster performance by avoiding round-about optimizations.
Between the source and compile level, directives and build flags can be used to tune performance options in the source code and compiler respectively, such as using preprocessor defines to disable unneeded software features, optimizing for specific processor models or hardware capabilities, or predicting branching, for instance. Source-based software distribution systems such as BSD's Ports and Gentoo's Portage can take advantage of this form of optimization.
At the lowest level, writing code using an assembly language, designed for a particular hardware platform can produce the most efficient and compact code if the programmer takes advantage of the full repertoire of machine instructions. Many operating systems used on embedded systems have been traditionally written in assembler code for this reason. Programs (other than very small programs) are seldom written from start to finish in assembly due to the time and cost involved. Most are compiled down from a high level language to assembly and hand optimized from there. When efficiency and size are less important large parts may be written in a high-level language.
The optimization, sometimes performed automatically by an optimizing compiler, is to select a method (algorithm) that is more computationally efficient, while retaining the same functionality. See algorithmic efficiency for a discussion of some of these techniques. However, a significant improvement in performance can often be achieved by removing extraneous functionality.
In some cases, however, optimization relies on using more elaborate algorithms, making use of "special cases" and special "tricks" and performing complex trade-offs. A "fully optimized" program might be more difficult to comprehend and hence may contain more faults than unoptimized versions. Beyond eliminating obvious antipatterns, some code level optimizations decrease maintainability.
There are instances where the programmer performing the optimization must decide to make the software better for some operations but at the cost of making other operations less efficient. These trade-offs may sometimes be of a non-technical nature –  such as when a competitor has published a benchmark result that must be beaten in order to improve commercial success but comes perhaps with the burden of making normal usage of the software less efficient. Such changes are sometimes jokingly referred to as pessimizations.
In computer science, resource consumption often follows a form of power law distribution, and the Pareto principle can be applied to resource optimization by observing that 80% of the resources are typically used by 20% of the operations.[4] In software engineering, it is often a better approximation that 90% of the execution time of a computer program is spent executing 10% of the code (known as the 90/10 law in this context).
More complex algorithms and data structures perform well with many items, while simple algorithms are more suitable for small amounts of data — the setup, initialization time, and constant factors of the more complex algorithm can outweigh the benefit, and thus a hybrid algorithm or adaptive algorithm may be faster than any single algorithm.  A performance profiler can be used to narrow down decisions about which functionality fits which conditions.[5]
"In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal and I believe the same viewpoint should prevail in software engineering"[6]
In practice, it is often necessary to keep performance goals in mind when first designing software, but the programmer balances the goals of design and optimization.
Modern compilers and operating systems are so efficient that the intended performance increases often fail to materialize.  As an example, caching data at the application level that is again cached at the operating system level does not yield improvements in execution.  Even so, it is a rare case when the programmer will remove failed optimizations from production code.  It is also true that advances in hardware will more often than not obviate any potential improvements, yet the obscuring code will persist into the future long after its purpose has been negated.
In many functional programming languages, macros are implemented using parse-time substitution of parse trees/abstract syntax trees, which it is claimed makes them safer to use. Since in many cases interpretation is used, that is one way to ensure that such computations are only performed at parse-time, and sometimes the only way.
Lisp originated this style of macro,[citation needed] and such macros are often called "Lisp-like macros". A similar effect can be achieved by using template metaprogramming in C++.
In both cases, work is moved to compile-time. The difference between C macros on one side, and Lisp-like macros and C++ template metaprogramming on the other side, is that the latter tools allow performing arbitrary computations at compile-time/parse-time, while expansion of C macros does not perform any computation, and relies on the optimizer ability to perform it. Additionally, C macros do not directly support recursion or iteration, so are not Turing complete.
Optimization can be automated by compilers or performed by programmers. Gains are usually limited for local optimization, and larger for global optimizations. Usually, the most powerful optimization is to find a superior algorithm.
Optimizing a whole system is usually undertaken by programmers because it is too complex for automated optimizers. In this situation, programmers or system administrators explicitly change code so that the overall system performs better. Although it can produce better efficiency, it is far more expensive than automated optimizations. Since many parameters influence the program performance, the program optimization space is large. Meta-heuristics and machine learning are used to address the complexity of program optimization.[9]
When the bottleneck is localized, optimization usually starts with a rethinking of the algorithm used in the program. More often than not, a particular algorithm can be specifically tailored to a particular problem, yielding better performance than a generic algorithm. For example, the task of sorting a huge list of items is usually done with a quicksort routine, which is one of the most efficient generic algorithms. But if some characteristic of the items is exploitable (for example, they are already arranged in some particular order), a different method can be used, or even a custom-made sort routine.
After the programmer is reasonably sure that the best algorithm is selected, code optimization can start. Loops can be unrolled (for lower loop overhead, although this can often lead to lower speed if it overloads the CPU cache), data types as small as possible can be used, integer arithmetic can be used instead of floating-point, and so on. (See algorithmic efficiency article for these and other techniques.)
Performance bottlenecks can be due to language limitations rather than algorithms or data structures used in the program. Sometimes, a critical part of the program can be re-written in a different programming language that gives more direct access to the underlying machine. For example, it is common for very high-level languages like Python to have modules written in C for greater speed. Programs already written in C can have modules written in assembly. Programs written in D can use the inline assembler.
Today, automated optimizations are almost exclusively limited to compiler optimization. However, because compiler optimizations are usually limited to a fixed set of rather general optimizations, there is considerable demand for optimizers which can accept descriptions of problem and language-specific optimizations, allowing an engineer to specify custom optimizations. Tools that accept descriptions of optimizations are called program transformation systems and are beginning to be applied to real software systems such as C++.
Media player software is a type of application software for playing multimedia computer files like audio and video files. Media players commonly display standard media control icons known from physical devices such as tape recorders and CD players, such as play (  ), pause (  ), fastforward (⏩️), rewind (⏪), and stop (  ) buttons. In addition, they generally have progress bars (or "playback bars"), which are sliders to locate the current position in the duration of the media file.
Mainstream operating systems have at least one default media player. For example, Windows comes with Windows Media Player, Microsoft Movies & TV and Groove Music, while macOS comes with QuickTime Player and Music. Linux distributions come with different media players, such as SMPlayer, Amarok, Audacious, Banshee, MPlayer, mpv, Rhythmbox, Totem, VLC media player, and xine. Android comes with YouTube Music for audio and Google Photos for video, and smartphone vendors such as Samsung may bundle custom software.
Media player software may have the ability to adjust appearance and acoustics during playback using effects such as mirroring, rotating, cropping, cloning, adjusting colours, deinterlacing, and equalizing and visualizing audio. Easter eggs may be featured, such as a puzzle game on VLC Media Player.[4][5]
3D video players are used to play 2D video in 3D format. A high-quality three-dimensional video presentation requires that each frame of a motion picture be embedded with information on the depth of objects present in the scene. This process involves shooting the video with special equipment from two distinct perspectives or modeling and rendering each frame as a collection of objects composed of 3D vertices and textures, much like in any modern video game, to achieve special effects. Tedious and costly, this method is only used in a small fraction of movies produced worldwide, while most movies remain in the form of traditional 2D images. It is, however, possible to give an otherwise two-dimensional picture the appearance of depth. Using a technique known as anaglyph processing a "flat" picture can be transformed so as to give an illusion of depth when viewed through anaglyph glasses (usually red-cyan). An image viewed through anaglyph glasses appears to have both protruding and deeply embedded objects in it, at the expense of somewhat distorted colors. The method itself is old enough, dating back to the mid-19th century, but it is only with recent advances in computer technology that it has become possible to apply this kind of transformation to a series of frames in a motion picture reasonably fast or even in real-time, i.e. as the video is being played back. Several implementations exist in the form of 3D video players that render conventional 2D video in anaglyph 3D, as well as in the form of 3D video converters that transform video into stereoscopic anaglyph and transcode it for playback with regular software or hardware video players.[citation needed]
Well known examples of media player software include Windows Media Player, VLC media player, iTunes, Winamp, Media Player Classic, MediaMonkey, foobar2000, AIMP, MusicBee and JRiver Media Center. Most of these also include music library managers.
A home theater PC or media center computer is a convergence device that combines some or all the capabilities of a personal computer with a software application that supports video, photo, audio playback, and sometimes video recording functionality. Although computers with some of these capabilities were available from the late 1980s, the "Home Theater PC" term first appeared in mainstream press in 1996. Since 2007, other types of consumer electronics, including gaming systems and dedicated media devices have crossed over to manage video and music content. The term "media center" also refers to specialized computer programs designed to run on standard personal computers.[9]
Accounting software is a computer program that maintains account books on computers, including recording transactions and account balances. It may depend on virtual thinking. Depending on the purpose, the software can manage budgets, perform accounting tasks for multiple currencies, perform payroll and customer relationship management, and prepare financial reporting.[citation needed] Work to have accounting functions be implemented on computers goes back to the earliest days of electronic data processing.[1] Over time, accounting software has revolutionized from supporting basic accounting operations to performing real-time accounting and supporting financial processing and reporting.[2] Cloud accounting software was first introduced in 2011, and it allowed the performance of all accounting functions through the internet.
Accounting software is typically composed of various modules, with different sections dealing with particular areas of accounting. Among the most common are:
In many cases, implementation (i.e. the installation and configuration of the system at the client) can be a bigger consideration than the actual software chosen when it comes down to the total cost of ownership for the business. Most mid-market and larger applications are sold exclusively through resellers, developers, and consultants. Those organizations generally pass on a license fee to the software vendor and then charge the client for installation, customization, and support services. Clients can normally count on paying roughly 50-200% of the price of the software in implementation and consulting fees.[citation needed][3]
Other organizations sell to, consult with, and support clients directly, eliminating the reseller. Accounting software provides many benefits such as speed up the information retrieval process, bring efficiency in Bank reconciliation process, automatically prepare Value Added TAX (VAT) / Goods and Services TAX (GST), and, perhaps most importantly, provide the opportunity to see the real-time state of the company's financial position.[citation needed]
Personal accounting software is simple in design and is used mostly for individuals. Some activities that it supports are accounts payable-type accounting transactions, managing budgets, and simple account reconciliation. It is relatively inexpensive compared to the other accounting options. One of the more common uses of personal accounting software is for tax preparation. This software is used to file tax returns in a format suitable with the Internal Revenue Service. An example of such software would be TurboTax.[4]
At the low-end of the business markets, inexpensive applications software allows most general business accounting functions to be performed. Suppliers frequently serve a single national market, while larger suppliers offer separate solutions in each national market.
The mid-market covers a wide range of business software that may be capable of serving the needs of multiple national accountancy standards and allow accounting in multiple currencies.
In addition to general accounting functions, the software may include integrated or add-on management information systems, and may be oriented towards one or more markets, for example with integrated or add-on project accounting modules.
Accounting software for large organizations was typically the province of mainframe computers. Organizations could either development their own software, modeling their internal financial processes, or buy a commercial package and tailor it to their needs. By the 1970s and 1980s, some of the software companies making accounting packages for the IBM mainframe market were Management Science America, McCormack & Dodge, Walker Interactive Products, Informatics General, and Cullinet Software.[5][6]
Over time, the most complex and expensive business accounting software became frequently part of an extensive suite of software often known as enterprise resource planning (ERP) software.
These applications typically have a very long implementation period, often greater than six months. In many cases, these applications are simply a set of functions which require significant integration, configuration and customization to even begin to resemble an accounting system.
Many freeware high-end open-source accounting software are available online these days which aim to change the market dynamics[citation needed]. Most of these software solutions are web-based.
As technology improves, software vendors have been able to offer increasingly advanced software at lower prices. This software is suitable for companies at multiple stages of growth. Many of the features of mid-market and high-end software (including advanced customization and extremely scalable databases) are required even by small businesses as they open multiple locations or grow in size. Additionally, with more and more companies expanding overseas or allowing workers to home office, many smaller clients have a need to connect multiple locations. Their options are to employ software-as-a-service or another application that offers them similar accessibility from multiple locations over the internet.
With the advent of faster computers and internet connections, accounting software companies have been able to create accounting software which is paid for on a monthly recurring charge instead of a larger upfront license fee (software as a service - SaaS). The rate of adoption of this new business model has increased steadily to the point where legacy players have been forced to come out with their own online versions.
Cloud Accounting Software is where financial information can be accessed from any device connected to the Internet at any time even though the financial data itself is located at a centralized computer. This differs from more traditional accounting software as it is restricted to a certain computer or system of computers and that accounting information can not be easily accessed from other devices. Some reasons cloud accounting software is preferred by users is there is no need to worry about maintenance or hardware system upgrades, it can reduce overall costs, and that a user can gain access from multiple locations. One of the primary reasons cloud accounting software is not being used is the threat of the security of the data.[7]
Utility software is a program specifically designed to help manage and tune system or application software. It is used to support the computer infrastructure - in contrast to application software, which is aimed at directly performing tasks that benefit ordinary users. However, utilities often form part of the application systems. For example, a batch job may run user-written code to update a database and may then include a step that runs a utility to back up the database, or a job may run a utility to compress a disk before copying files.
Although a basic set of utility programs is usually distributed with an operating system (OS), and this first party utility software is often considered part of the operating system, users often install replacements or additional utilities.[1][2] Those utilities may provide additional facilities to carry out tasks that are beyond the capabilities of the operating system.
Educational software is a term used for any computer software that is made for an educational purpose. It encompasses different ranges from language learning software to classroom management software to reference software. The purpose of all this software is to make some part of education more effective and efficient.
The use of computer hardware and software in education and training dates to the early 1940s, when American researchers developed flight simulators which used analog computers to generate simulated onboard instrument data. One such system was the type19 synthetic radar trainer, built in 1943. From these early attempts in the WWII era through the mid-1970s, educational software was directly tied to the hardware, on which it ran. Pioneering educational computer systems in this era included the PLATO system (1960), developed at the University of Illinois, and TICCIT (1969).
Some programming languages from this period, such as p3 and LOGO (1967), were designed specifically for students and novice users. The PLATO IV system (1972) introduced features that later became standard in educational software, including bitmap graphics, primitive sound, and support for non-keyboard input devices, including the touchscreen.
The arrival of the personal computer, with the Altair 8800 in 1975, changed the field of software in general, with specific implications for educational software. Whereas users prior to 1975 were dependent upon university or government owned mainframe computers with timesharing, users after this shift could create and use software for computers in homes and schools, computers available for less than $2000. By the early 1980s, the availability of personal computers including the Apple II (1977), Commodore PET (1977), VIC-20 (1980), and Commodore 64 (1982) allowed for the creation of companies and nonprofits which specialized in educational software. Broderbund and The Learning Company are key companies from this period, and MECC, the Minnesota Educational Computing Consortium, a key non-profit software developer. These and other companies designed a range of titles for personal computers, with the bulk of the software initially developed for the Apple II.
"Courseware" is a term that combines the words 'course' with 'software'.[4] It was originally used to describe additional educational material intended as kits for teachers or trainers or as tutorials for students, usually packaged for use with a computer. The term's meaning and usage has expanded and can refer to the entire course and any additional material when used in reference an online or 'computer formatted' classroom. Many companies are using the term to describe the entire "package" consisting of one 'class' or 'course' bundled together with the various lessons, tests, and other material needed. The courseware itself can be in different formats: some are only available online, such as Web pages, while others can be downloaded as PDF files or other types of document. Many forms of educational technology are now covered by the term courseware. Most leading educational companies solicit or include courseware with their training packages.
Some educational software is designed for use in school classrooms. Typically such software may be projected onto a large whiteboard at the front of the class and/or run simultaneously on a network of desktop computers in a classroom. The most notable are SMART Boards that use SMART Notebook to interact with the board which allows the use of pens to digitally draw on the board. This type of software is often called classroom management software. While teachers often choose to use educational software from other categories in their IT suites (e.g. reference works, children's software), a whole category of educational software has grown up specifically intended to assist classroom teaching. 'Wordshark', for example, was first released in the mid nineties with multi-sensory games to support students learning to read and spell. Branding has been less strong in this category than in those oriented towards home users. Software titles are often very specialized and produced by various manufacturers, including many established educational book publishers.
With the impact of environmental damage and the need for institutions to become "paperless",[5] more educational institutions are seeking alternative ways of assessment and testing, which has always traditionally been known to use up vasts amount of paper. Assessment software refers to software with a primary purpose of assessing and testing students in a virtual environment.[6] Assessment software allows students to complete tests and examinations using a computer, usually networked. The software then scores each test transcript and outputs results for each student. Assessment software is available in various delivery methods, the most popular being self-hosted software, online software and hand-held voting systems. Proprietary software and open-source software systems are available. While technically falling into the Courseware category (see above), Skill evaluation lab is an example for Computer-based assessment software with PPA-2 (Plan, Prove, Assess) methodology to create and conduct computer based online examination. Moodle is an example of open-source software with an assessment component that is gaining popularity. QST Online[7] a dedicated complete open source exam/assessment solution is also growing in adoption. Other popular international assessment systems include Google Classroom, Blackboard Learn, EvaluNet and XT.
Many publishers of print dictionaries and encyclopedias have been involved in the production of educational reference software since the mid-1990s. They were joined in the reference software market by both startup companies and established software publishers, most notably Microsoft.
The first commercial reference software products were reformulations of existing content into CD-ROM editions, often supplemented with new multimedia content, including compressed video and sound. More recent products made use of internet technologies, to supplement CD-ROM products, then, more recently, to replace them entirely.
Wikipedia and its offspins (such as Wiktionary) marked a new departure in educational reference software. Previously, encyclopedias and dictionaries had compiled their contents on the basis of invited and closed teams of specialists. The Wiki concept has allowed for the development of collaborative reference works through open cooperation incorporating experts and non-experts.
Some manufacturers regarded normal personal computers as an inappropriate platform for learning software for younger children and produced custom child-friendly pieces of hardware instead. The hardware and software is generally combined into a single product, such as a child laptop-lookalike. The laptop keyboard for younger children follows an alphabetic order and the QWERTY order for the older ones.  The most well-known example are LeapFrog products. These include imaginatively designed hand-held consoles with a variety of pluggable educational game cartridges and book-like electronic devices into which a variety of electronic books can be loaded. These products are more portable than laptop computers, but have a much more limited range of purposes, concentrating on literacy.
While mainstream operating systems are designed for general usages, and are more or less customized for education only by the application sets added to them, a variety of software manufacturers, especially Linux distributions, have sought to provide integrated platforms for specifically education.
Earlier educational software for the important corporate and tertiary education markets was designed to run on a single desktop computer (or an equivalent user device). In the years immediately following 2000, planners decided to switch to server-based applications with a high degree of standardization. This means that educational software runs primarily on servers which may be hundreds or thousands of miles from the actual user. The user only receives tiny pieces of a learning module or test, fed over the internet one by one. The server software decides on what learning material to distribute, collects results and displays progress to teaching staff. Another way of expressing this change is to say that educational software morphed into an online educational service. US Governmental endorsements and approval systems ensured the rapid switch to the new way of managing and distributing learning material. McDonald's also experimented with this via the Nintendo DS software eCrew Development Program.
(remote control and monitoring software, filetransfer software, document camera and presenter, free tools,...)
Video games can be used to teach a user technology literacy or more about a subject. Some operating systems and mobile phones have these features. A notable example is Microsoft Solitaire, which was developed to familiarize users with the use of graphical user interfaces, especially the mouse and the drag-and-drop technique. Mavis Beacon Teaches Typing is a largely known program with built in mini-games to keep the user entertained while improving their typing skills.
Gamification is the use of game design elements in nongame contexts and has been shown to be effective in motivating behavior change. By seeing game elements as "motivational affordances," and formalizing the relationship between these elements and motivational affordances.[10] Classcraft is a software tool used by teachers that has gaming elements alongside an educational goal.[11] Tovertafel is a games console designed for remedial education and counter-acting the effects of dementia.
Tutor-based education software is defined as software that mimics the teacher student one on one dynamic of tutoring with software in place of a teacher. Research was conducted to see if this type of software would be effective in improving students understanding of material. It concluded that there was a positive impact which decreased the amount of time students need to study for and relative gain of understanding.[12]
A study was conducted to see the effects of education software on children with mild disabilities. The results were that the software was a positive impact assisting teaching these children social skills though team based learning and discussion, videos and games.[13]
There is a large market of educational software in use today. A team decided that they were to develop a system in which educational software should be evaluated as there is no current standard. It is called the Construction of the Comprehensive Evaluation of Electronic Learning Tools and Educational Software (CEELTES).[14] The software to be evaluated is graded on a point scale in four categories: the area of technical, technological and user attributes; area of criteria evaluating the information, content and operation of the software; the area of criteria evaluating the information in terms of educational use, learning and recognition; the area of criteria evaluating the psychological and pedagogical use of the software.[15]
In university level computer science course, learning logic is an essential part of the curriculum. There is a proposal on using two logistical education tool FOLST and LogicChess to understand First Order Logic for university students to better understand the course material and the essentials of logistical design.[16]
Open-source software (OSS) is computer software that is released under a license in which the copyright holder grants users the rights to use, study, change, and distribute the software and its source code to anyone and for any purpose.[1][2] Open-source software may be developed in a collaborative, public manner. Open-source software is a prominent example of open collaboration, meaning any capable user is able to participate online in development, making the number of possible contributors indefinite. The ability to examine the code facilitates public trust in the software.[3]
Open-source software development can bring in diverse perspectives beyond those of a single company. A 2024 estimate of the value of open-source software to firms is $8.8 trillion, as firms would need to spend 3.5 times the amount they currently do without the use of open source software.[4]
Open-source code can be used for studying and allows capable end users to adapt software to their personal needs in a similar way user scripts and custom style sheets allow for web sites, and eventually publish the modification as a fork for users with similar preferences, and directly submit possible improvements as pull requests.
The Open Source Initiative's (OSI) definition is recognized by several governments internationally[5] as the standard or de facto definition. OSI uses The Open Source Definition to determine whether it considers a software license open source. The definition was based on the Debian Free Software Guidelines, written and adapted primarily by Perens.[6][7][8] Perens did not base his writing on the "four freedoms" from the Free Software Foundation (FSF), which were only widely available later.[9]
Under Perens' definition, open source is a broad software license that makes source code available to the general public with relaxed or non-existent restrictions on the use and modification of the code. It is an explicit "feature" of open source that it puts very few restrictions on the use or distribution by any organization or user, in order to enable the rapid evolution of the software.[10]
According to Feller et al. (2005), the terms "free software" and "open-source software" should be applied to any "software products distributed under terms that allow users" to use, modify, and redistribute the software "in any manner they see fit, without requiring that they pay the author(s) of the software a royalty or fee for engaging in the listed activities."[11]
Despite initially accepting it,[12] Richard Stallman of the FSF now flatly opposes the term "Open Source" being applied to what they refer to as "free software". Although he agrees that the two terms describe "almost the same category of software", Stallman considers equating the terms incorrect and misleading.[13] Stallman also opposes the professed pragmatism of the Open Source Initiative, as he fears that the free software ideals of freedom and community are threatened by compromising on the FSF's idealistic standards for software freedom.[14] The FSF considers free software to be a subset of open-source software, and Richard Stallman explained that DRM software, for example, can be developed as open source, despite that it does not give its users freedom (it restricts them), and thus does not qualify as free software.[13]
In his 1997 essay The Cathedral and the Bazaar, open-source influential contributor Eric S. Raymond suggests a model for developing OSS known as the bazaar model.[15] Raymond likens the development of software by traditional methodologies to building a cathedral, with careful isolated work by individuals or small groups.[15] He suggests that all software should be developed using the bazaar style, with differing agendas and approaches.[15]
In the traditional model of development, which he called the cathedral model, development takes place in a centralized way.[15] Roles are clearly defined.[15] Roles include people dedicated to designing (the architects), people responsible for managing the project, and people responsible for implementation.[15] Traditional software engineering follows the cathedral model.[15]
The bazaar model, however, is different.[15] In this model, roles are not clearly defined.[15] Some proposed characteristics of software developed using the bazaar model should exhibit the following patterns:[16]
Users should be treated as co-developers: The users are treated like co-developers and so they should have access to the source code of the software.[16] Furthermore, users are encouraged to submit additions to the software, code fixes for the software, bug reports, documentation, etc. Having more co-developers increases the rate at which the software evolves.[16] Linus's law states that given enough eyeballs all bugs are shallow.[16] This means that if many users view the source code, they will eventually find all bugs and suggest how to fix them.[16] Some users have advanced programming skills, and furthermore, each user's machine provides an additional testing environment.[16] This new testing environment offers the ability to find and fix a new bug.[16]
Early releases: The first version of the software should be released as early as possible so as to increase one's chances of finding co-developers early.[16]
Several versions: There should be at least two versions of the software.[16] There should be a buggier version with more features and a more stable version with fewer features.[16] The buggy version (also called the development version) is for users who want the immediate use of the latest features and are willing to accept the risk of using code that is not yet thoroughly tested.[16] The users can then act as co-developers, reporting bugs and providing bug fixes.[16][18]
High modularization: The general structure of the software should be modular allowing for parallel development on independent components.[16]
Dynamic decision-making structure: There is a need for a decision-making structure, whether formal or informal, that makes strategic decisions depending on changing user requirements and other factors.[16] Compare with extreme programming.[16]
Moreover, lower costs of marketing and logistical services are needed for OSS.[21] OSS can be a tool to promote a company's image, including its commercial products.[22] The OSS development approach has helped produce reliable, high quality software quickly and inexpensively.[21]
Open source development offers the potential to quicken innovation and create of social value.[23] In France for instance, a policy that incentivized government to favor free open-source software increased to nearly 600,000 OSS contributions per year, generating social value by increasing the quantity and quality of open-source software.[23] This policy also led to an estimated increase of up to 18% of tech startups and a 14% increase in the number of people employed in the IT sector.[23]
OSS can be highly reliable when it has thousands of independent programmers testing and fixing bugs of the software.[16] Open source is not dependent on the company or author that originally created it.[24] Even if the company fails, the code continues to exist and be developed by its users.[24]
Moreover, free software can be developed in accordance with purely technical requirements.[26] It does not require thinking about commercial pressure that often degrades the quality of the software.[26] Commercial pressures make traditional software developers pay more attention to customers' requirements than to security requirements, since such features are somewhat invisible to the customer.[26]
In open-source software development, tools are used to support the development of the product and the development process itself.[18]
Version control systems such as Centralized Version control system (CVCS) and the distributed version control system (DVCS)  are examples of tools, often open source, that help manage the source code files and the changes to those files for a software project in order to foster collaboration.[27] CVCS are centralized with a central repository while DVCS are decentralized and have a local repository for every user.[27] concurrent versions system (CVS) and later Subversion (SVN) and Git are examples of CVCS.[27] The repositories are hosted and published on source-code-hosting facilities such as GitHub.[27]
Open-source projects use utilities such as issue trackers to organize open-source software development. Commonly used bug trackers include Bugzilla and Redmine.[18]
Some possible ways of contributing to open-source software include such roles as programming, user interface design and testing, web design, bug triage, accessibility design and testing, UX design, code testing, and security review and testing.[28] However, there are several ways of contributing to OSS projects even without coding skills.[28] For example, some less technical ways of participating are documentation writing and editing, translation, project management, event organization and coordination, marketing, release management, community management, and public relations and outreach.[28]
The adoption of open-source software by industry is increasing over time.[32] OSS is popular in several industries such as telecommunications, aerospace, healthcare, and media & entertainment due to the benefits it provides.[33] Adoption of OSS is more likely in larger organizations and is dependent on the company's IT usage, operating efficiencies, and the productivity of employees.[32]
Industries are likely to use OSS due to back-office functionality, sales support, research and development, software features, quick deployment, portability across platforms and avoidance of commercial license management.[32] Additionally, lower cost for hardware and ownership are also important benefits.[32]
Organizations that contribute to the development and expansions of free and open-source software movements exist all over the world.[28] These organizations are dedicated to goals such as teaching and spreading technology.[28] As listed by a former vice president of the Open Source Initiative, some American organizations include the Free Software Foundation, Software Freedom Conservancy, the Open Source Initiative and Software in the Public Interest.[28] Within Europe some notable organizations are Free Software Foundation Europe, open-source projects EU (OSP) and OpenForum Europe (OFE).[28] One Australian organization is Linux Australia while Asia has Open source Asia and FOSSAsia.[28] Free and open source software for Africa (FOSSFA) and OpenAfrica are African organizations and Central and South Asia has such organizations as FLISOL and GRUP de usuarios de software libre Peru.[28] Outside of these, many more organizations dedicated to the advancement of open-source software exist.[28]
FOSS products are generally licensed under two types of licenses: permissive licensing and copyleft licensing.[34] Both of these types of licenses are different than proprietary licensing in that they can allow more users access to the software and allow for the creation of derivative works as specified by the terms of the specific license, as each license has its own rules.[34] Permissive licenses allow recipients of the software to implement the author's copyright rights without having to use the same license for distribution.[34] Examples of this type of license include the BSD, MIT, and Apache licenses.[34] Copyleft licenses are different in that they require recipients to use the same license for at least some parts of the distribution of their works.[34] Strong copyleft licenses require all derivative works to use the same license while weak copyleft licenses require the use of the same license only under certain conditions.[34] Examples of this type of license include the GNU family of licenses, and the MPL and EPL licenses.[34] The similarities between these two categories of licensing include that they provide a broad grant of copyright rights, require that recipients preserve copyright notices, and that a copy of the license is provided to recipients with the code.[34]
One important legal precedent for open-source software was created in 2008, when the Jacobson v Katzer case enforced terms of the Artistic license, including attribution and identification of modifications.[34] The ruling of this case cemented enforcement under copyright law when the conditions of the license were not followed.[34] Because of the similarity of the Artistic license to other open-source software licenses, the ruling created a precedent that applied widely.[34]
Examples of free-software license / open-source licenses include Apache licenses, BSD licenses, GNU General Public Licenses, GNU Lesser General Public License, MIT License, Eclipse Public License and Mozilla Public License.[34]
Several gray areas exist within software regulation that have great impact on open-source software, such as if software is a good or service, what can be considered a modification, governance through contract vs license, ownership and right of use.[34] While there have been developments on these issues, they often lead to even more questions.[34] The existence of these uncertainties in regulation has a negative impact on industries involved in technologies as a whole.[34]
Within the legal history of software as a whole, there was much debate on whether to protect it as intellectual property under patent law, copyright law or establishing a unique regulation.[34] Ultimately, copyright law became the standard with computer programs being considered a form of literary work, with some tweaks of unique regulation.[34]
Software is generally considered source code and object code, with both being protectable, though there is legal variety in this definition.[34] Some jurisdictions attempt to expand or reduce this conceptualization for their own purposes.[34] For example, The European Court of Justice defines a computer program as not including the functionality of a program, the programing language, or the format of data files.[34] By limiting protections of the different aspects of software, the law favors an open-source approach to software use.[34] The US especially has an open approach to software, with most open-source licenses originating there.[34] However, this has increased the focus on patent rights within these licenses, which has seen backlash from the OSS community, who prefer other forms of IP protection.[34]
Another issue includes technological protection measures (TPM) and digital rights management (DRM) techniques which were internationally legally recognized and protected in the 1996 World Intellectual Property Organization (WIPO) Treaty.[34] Open source software proponents disliked these technologies as they constrained end-users potentially beyond copyright law.[34] Europe responded to such complaints by putting TPM under legal controls, representing a victory for OSS supporters.[34]
In open-source communities, instead of owning the software produced, the producer owns the development of the evolving software.[35] In this way, the future of the software is open, making ownership or intellectual property difficult within OSS.[35] Licensing and branding can prevent others from stealing it, preserving its status as a public good.[35] Open source software can be considered a public good as it is available to everyone and does not decrease in value for others when downloaded by one person.[35] Open source software is unique in that it becomes more valuable as it is used and contributed to, instead of diminishing the resource. This is explained by concepts such as investment in reputation and network effects.[35]
The economic model of open-source software can be explained as developers contribute work to projects, creating public benefits.[35] Developers choose projects based on the perceived benefits or costs, such as improved reputation or value of the project.[35] The motivations of developers can come from many different places and reasons, but the important takeaway is that money is not the only or even most important incentivization.[35]
Throughout the mid 2000s, more and more tech companies have begun to use OSS.[24] For example, Dell's move of selling computers with GNU/Linux already installed.[24] Microsoft itself has launched a Linux-based operating system despite previous animosity with the OSS movement.[24] Despite these developments, these companies tend to only use OSS for certain purposes, leading to worries that OSS is being taken advantage of by corporations and not given anything in return.[24]
While many governments are interested in implementing and promoting open-source software due to the many benefits provided, a huge issue to be considered is cybersecurity.[36] While accidental vulnerabilities are possible, so are attacks by outside agents.[36] Because of these fears, governmental interest in contributing to the governance of software has become more prominent.[36] However, these are the broad strokes of the issue, with each country having their own specific politicized interactions with open-source software and their goals for its implementation.[36] For example, the United States has focused on national security in regard to open-source software implementation due to the perceived threat of the increase of open-source software activity in countries like China and Russia, with the Department of Defense considering multiple criteria for using OSS.[36] These criteria include: if it comes from and is maintained by trusted sources, whether it will continue to be maintained, if there are dependencies on sub-components in the software, component security and integrity, and foreign governmental influence.[36]
Another issue for governments in regard to open source is their investments in technologies such as operating systems, semiconductors, cloud, and artificial intelligence.[36] These technologies all have implications for global cooperation, again opening up security issues and political consequences.[36] Many countries have to balance technological innovation with technological dependence in these partnerships.[36] For example, after China's open-source dependent company Huawei was prevented from using Google's Android system in 2019, they began to create their own alternative operating system: Harmony OS.[36]
Germany recently established a Sovereign Tech Fund, to help support the governance and maintenance of the software that they use.
In the early days of computing, such as the 1950s and into the 1960s, programmers and developers shared software to learn from each other and evolve the field of computing.[37] For example, Unix included the operating system source code for users.[37] Eventually, the commercialization of software in the years 1970–1980 began to prevent this practice.[37] However, academics still often developed software collaboratively.[37]
In response, the open-source movement was born out of the work of skilled programmer enthusiasts, widely referred to as hackers or hacker culture.[38] One of these enthusiasts, Richard Stallman, was a driving force behind the free software movement, which would later allow for the open-source movement.[17] In 1984, he resigned from MIT to create a free operating system, GNU, after the programmer culture in his lab was stifled by proprietary software preventing source code from being shared and improved upon.[17] GNU was UNIX compatible, meaning that the programmer enthusiasts would still be familiar with how it worked.[17] However, it quickly became apparent that there was some confusion with the label Stallman had chosen of free software, which he described as free as in free speech, not free beer, referring to the meaning of free as freedom rather than price.[17] He later expanded this concept of freedom to the four essential freedoms.[17] Through GNU, open-source norms of incorporating others' source code, community bug fixes and suggestions of code for new features appeared.[17] In 1985, Stallman founded the Free Software Foundation (FSF) to promote changes in software and to help write GNU.[17] In order to prevent his work from being used in proprietary software, Stallman created the concept of copyleft, which allowed the use of his work by anyone, but under specific terms.[17] To do this, he created the GNU General Public License (GNU GPL) in 1989, which was updated in 1991.[17]  In 1991, GNU was combined with the Linux kernel written by Linus Torvalds, as a kernel was missing in GNU.[39] The operating system is now usually referred to as Linux.[17] Throughout this whole period, there were many other free software projects and licenses around at the time, all with different ideas of what the concept of free software was and should be, as well as the morality of proprietary software, such as Berkeley Software Distribution, TeX, and the X Window System.[40]
As free software developed, the Free Software Foundation began to look how to bring free software ideas and perceived benefits to the commercial software industry.[40] It was concluded that FSF's social activism was not appealing to companies and they needed a way to rebrand the free software movement to emphasize the business potential of sharing and collaborating on software source code.[40] The term open source was suggested by Christine Peterson in 1998 at a meeting of supporters of free software.[17] Many in the group felt the name free software was confusing to newcomers and holding back industry interest and they readily accepted the new designation of open source, creating the Open Source Initiative (OSI) and the OSI definition of what open source software is.[17] The Open Source Initiative's (OSI) definition is now recognized by several governments internationally as the standard or de facto definition.[39] The definition was based on the Debian Free Software Guidelines, written and adapted primarily by Bruce Perens.[41] The OSI definition differed from the free software definition in that it allows the inclusion of proprietary software and allows more liberties in its licensing.[17] Some, such as Stallman, agree more with the original concept of free software as a result because it takes a strong moral stance against proprietary software, though there is much overlap between the two movements in terms of the operation of the software.[17]
While the Open Source Initiative sought to encourage the use of the new term and evangelize the principles it adhered to, commercial software vendors found themselves increasingly threatened by the concept of freely distributed software and universal access to an application's source code, with an executive of Microsoft calling open source an intellectual property destroyer in 2001.[42] However, while free and open-source software (FOSS) has historically played a role outside of mainstream private software development, companies as large as Microsoft have begun to develop official open source presences on the Internet.[42] IBM, Oracle, and State Farm are just a few of the companies with a serious public stake in today's competitive open source market, marking a significant shift in the corporate philosophy concerning the development of FOSS.[42]
The future of the open source software community, and the free software community by extension, has become successful if not confused about what it stands for.[24] For example, Android and Ubuntu are examples milestones of success in the open source software rise to prominence from the sidelines of technological innovation as it existed in the early 2000s.[24] However, some in the community consider them failures in their representation of OSS due to issues such as the downplaying of the OSS center of Android by Google and its partners, the use of an Apache license that allowed forking and resulted in a loss of opportunities for collaboration within Android, the prioritization of convenience over freedom in Ubuntu, and features within Ubuntu that track users for marketing purposes.[24]
Open source software differs from proprietary software in that it is publicly available, the license requires no fees, modifications and distributions are allowed under license specifications.[43] All of this works to prevent a monopoly on any OSS product, which is a goal of proprietary software.[43] Proprietary software limits their customers' choices to either committing to using that software, upgrading it or switching to other software, forcing customers to have their software preferences impacted by their monetary cost.[43] The ideal case scenario for the proprietary software vendor would be a lock-in, where the customer does not or cannot switch software due to these costs and continues to buy products from that vendor.[43]
Within proprietary software, bug fixes can only be provided by the vendor, moving platforms requires another purchase and the existence of the product relies on the vendor, who can discontinue it at any point.[38] Additionally, proprietary software does not provide its source code and cannot be altered by users.[17] For businesses, this can pose a security risk and source of frustration, as they cannot specialize the product to their needs, and there may be hidden threats or information leaks within the software that they cannot access or change.[17]
Under OSI's definition, open source is a broad software license that makes source code available to the general public with relaxed or non-existent restrictions on the use and modification of the code.[44] It is an explicit feature of open source that it puts very few restrictions on the use or distribution by any organization or user, in order to enable the rapid evolution of the software.[44]
Richard Stallman, leader of the Free software movement and member of the free software foundation opposes the term open source being applied to what they refer to as free software.[13] Although he agrees that the two terms describe almost the same category of software, Stallman considers equating the terms incorrect and misleading.[13]  He believes that the main difference is that by choosing one term over the other lets others know about what one's goals are: development (open source) or a social stance (free software).[45] Nevertheless, there is significant overlap between open source software and free software.[13] Stallman also opposes the professed pragmatism of the Open Source Initiative, as he fears that the free software ideals of freedom and community are threatened by compromising on the FSF's idealistic standards for software freedom.[45] The FSF considers free software to be a subset of open-source software, and Richard Stallman explained that DRM software, for example, can be developed as open source, despite how it restricts its users, and thus does not qualify as free software.[13]
The FSF said that the term open source fosters an ambiguity of a different kind such that it confuses the mere availability of the source with the freedom to use, modify, and redistribute it.[13] On the other hand, the term free software was criticized for the ambiguity of the word free, which was seen as discouraging for business adoption, and for the historical ambiguous usage of the term.[45]
Developers have used the alternative terms Free and Open Source Software (FOSS), or Free/Libre and Open Source Software (FLOSS), consequently, to describe open-source software that is also free software.[28]
Software can be distributed with source code, which is a code that is readable.[46] Software is source available when this source code is available to be seen.[46] However to be source available or FOSS, the source code does not need to be accessible to all, just the users of that software.[46] While all FOSS software is source available because this is a requirement made by the Open Source Definition, not all source available software is FOSS.[46] For example, if the software does not meet other aspects of the Open Source Definition such as permitted modification or redistribution, even if the source code is available, the software is not FOSS.[46]
A recent trend within software companies is open sourcing, or transitioning their previous proprietary software into open source software through releasing it under an open-source license.[47][48] Examples of companies who have done this are Google, Microsoft and Apple.[47] Additionally, open sourcing can refer to programming open source software or installing open source software.[48] Open sourcing can be beneficial in multiple ways, such as attracting more external contributors who bring new perspectives and problem solving capabilities.[47] The downsides of open sourcing include the work that has to be done to maintaining the new community, such as making the base code easily understandable, setting up communication channels for new developers and creating documentation to allow new developers to easily join.[47] However, a review of several open sourced projects found that although a newly open sourced project attracts many newcomers, a great amount are likely to soon leave the project and their forks are also likely to not be impactful.[47]
Other concepts that may share some similarities to open source are shareware, public domain software, freeware, and software viewers/readers that are freely available but do not provide source code.[17] However, these differ from open source software in access to source code, licensing, copyright and fees.[17]
Despite being able to collaborate internationally, open source software contributors were found to mostly be located in large clusters such as Silicon Valley that largely collaborate within themselves.[49] Possible reasons for this phenomenon may be that the OSS contributor demographic largely works in software, meaning that the OSS geographic location is closely related to that dispersion and collaborations could be encouraged through work and social networks.[49] Code acceptance can be impacted by status within these social network clusters, creating unfair predispositions in code acceptance based on location.[50] Barriers to international collaboration also include linguistic or cultural differences.[51] Furthermore, each country has been shown to have a higher acceptance rate for code from contributors within their country except India, indicating a bias for culturally similar collaborators.[51]
In 2021, the countries with the highest open source software contributions included the United States, China, Germany, India, and the UK, in that order.[49] The counties with the highest OSS developers per capita from a study in 2021 include, in order, Iceland, Switzerland, Norway, Sweden, and Finland, while in 2008 the countries with top amount of estimated contributors in SourceForge were the United States, Germany, United Kingdom, Canada and France.[49][51] Though there have been several studies done on the distribution and contributions of OSS developers, this is still an open field that can be measured in several different ways.[51] For instance, Information and communication technology participation, population, wealth and proportion of access to the internet have been shown to be correlated with OSS contributions.[51]
Although gender diversity has been found to enhance team productivity, women still face biases while contributing to open source software projects when their gender is identifiable.[52] In 2002, only 1.5% of international open-source software developers were women, while women made up 28% of tech industry roles, demonstrating their low representation in the software field.[53] Despite OSS contributions having no prerequisites, this gender bias may continue to exist due to the common belief of contributors that gender should not matter, and the quality of code should be the only consideration for code acceptance, preventing the community from addressing the systemic disparities in female representation.[38] However, a more recent figure of female OSS participation internationally calculated across 2005 to 2021 is 9.8%, with most being recent contributors, indicating that female participation may be growing.[54]
Even though programming was originally seen as a female profession, there remains a large gap in computing.[55] Social identity tends to be a large concern as women in the tech industry face insecurity about attracting unwanted male attention and harassment or being unfeminine in their technology knowledge, having a large impact on confidence.[38] Some male tech participants make clear that they believe women fitting in within the culture is impossible, furthering the insecurity for women and their place in the tech industry.[52] Additionally, even in a voluntary contribution environment like open source software, women tend to end up doing the less technical aspects of projects, such as manual testing or documentation despite women and men showing the same productivity in OSS contributions.[52] Explicit biases include longer feedback time, more scrutinization of code and lower acceptance rate of code.[52] Specifically in the open-source software community, women report that sexually offensive language is common and the women's identity as female is given more attention that as an OSS contributor[38] Bias is hard to address due to the belief that gender should not matter, with most contributors feeling that women getting special treatment is unfair and success should be dependent on skill, preventing any changes to be more inclusive.[38]
Open source software projects are built and maintained by a network of programmers, who may often be volunteers, and are widely used in free as well as commercial products.[56]
Unix: Unix is an operating system created by AT&T that began as a precursor to open source software in that the free and open-source software revolution began when developers began trying to create operating systems without Unix code.[24] Unix was created in the 1960s, before the commercialization of software and before the concept of open source software was necessary, therefore it was not considered a true open source software project.[24] It started as a research project before being commercialized in the mid 1980s.[24] Before its commercialization, it represented many of the ideals held by the Free and Open source software revolution, including the decentralized collaboration of global users, rolling releases and a community culture of distaste towards proprietary software.[24]
BSD: Berkeley Software Distribution (BSD) is an operating system that began as a variant of Unix in 1978 that mixed Unix code with code from Berkeley labs to increase functionality.[24] As BSD was focused on increasing functionality, it would publicly share its greatest innovations with the main Unix operating system.[24] This is an example of the free public code sharing that is a central characteristic of FOSS today.[24] As Unix became commercialized in the 1980s, developers or members of the community who did not support proprietary software began to focus on BSD and turning it into an operating system that did not include any of Unix's code.[24] The final version of BSD was released in 1995.[24]
GNU: GNU is a free operating system created by Richard Stallman in 1984 with its name meaning Gnu's Not Unix.[24] The idea was to create a Unix alternative operating system that would be available for anyone to use and allow programmers to share code freely between them.[24] However, the goal of GNU was not to only replace Unix, but to make a superior version that had more technological capabilities.[24] It was released before the philosophical beliefs of the Free and Open source software revolution were truly defined.[24] Because of its creation by prominent FOSS programmer Richard Stallman, GNU was heavily involved in FOSS activism, with one of the greatest achievements of GNU being the creation of the GNU General Public License or GPL, which allowed developers to release software that could be legally shared and modified.[24]
Linux: Linux is an operating system kernel that was introduced in 1991 by Linus Torvalds.[24] Linux was inspired by making a better version of the for profit operating service Minux.[24] It was radically different than what other hackers were producing at the time due to it being totally free of cost and being decentralized.[24] Later, Linux was put under the GPL license, allowing people to make money with Linux and bringing Linux into the FOSS community.[24]
While the term open source applied originally only to the source code of software, it is now being applied to many other areas such as open-source ecology, a movement to decentralize technologies so that any human can use them.[13][57] However, it is often misapplied to other areas that have different and competing principles, which overlap only partially.[38]
The same principles that underlie open-source software can be found in many other ventures, such as open source, open content, and open collaboration.[58][3]
More than 90 percent of companies use open-source software as a component of their proprietary software.[59] The decision to use open-source software, or even engage with open-source projects to improve existing open-source software, is typically a pragmatic business decision.[60][61] When proprietary software is in direct competition with an open-source alternative, research has found conflicting results on the effect of the competition on the proprietary product's price and quality.[62]
For decades, some companies have made servicing of an open-source software product for enterprise users their business model. These companies control an open-source software product, and instead of charging for licensing or use, charge for improvements, integration, and other servicing.[63] Software as a service (SaaS) products based on open-source components are increasingly common.[64]
Open-source software is preferred for scientific applications, because it increases transparency and aids in the validation and acceptance of scientific results.[65]
Since the 1970s, software copyright has been recognized in the United States. Despite the copyright being recognized, most companies prefer to sell licenses rather than copies of the software because it enables them to enforce stricter terms on redistribution. Very few purchasers read any part of the license, initially shrink-wrap contracts and now most commonly encountered as clickwrap or browsewrap. The enforceability of this kind of license is a matter of controversy and is limited in some jurisdictions. Service-level agreements are another type of software license where the vendor agrees to provide a level of service to the purchaser, often backed by financial penalties.
The source code (or compiled binaries in the form of object code)[9] of a computer program is protected by copyright law that vests the owner with the exclusive right to copy the code. The underlying ideas or algorithms are not protected by copyright law, but are often treated as a trade secret and concealed by such methods as non-disclosure agreements.[10] Software copyright has been recognized since the mid-1970s and is vested in the company that makes the software, not the employees or contractors who wrote it.[1]
The tendency to license proprietary software, rather than sell it, dates from the time period before the existence, then the scope of software copyright protection was clear. These licenses have continued in use after software copyright was recognized in the courts, and are considered to grant the company extra protection compared to copyright law.[12] According to United States federal law, a company can restrict the parties to which it sells but it cannot prevent a buyer from reselling the product. Software licensing agreements usually prohibit resale, enabling the company to maximize revenue.[13]
Traditionally, software was distributed in the form of binary object code that could not be understood or modified by the user,[9] but could be downloaded and run. The user bought a perpetual license to use a particular version of the software.[14] Software as service (SaaS) vendors—who have the majority market share in application software as of 2023[update][15]—rarely offer perpetual licenses.[16] SaaS licenses are usually temporary and charged on a pay-per-usage or subscription basis,[17] although other revenue models such as freemium are also used.[18] For customers, the advantages of temporary licenses include reduced upfront cost, increased flexibility, and lower overall cost compared to a perpetual license.[14] In some cases, the steep one-time cost demanded by sellers of traditional software were out of the reach of smaller businesses, but pay-per-use SaaS models makes the software affordable.[19]
Initially, end-user license agreements (EULAs) were printed on either the shrinkwrap packaging encasing the product (see shrink-wrap contract) or a piece of paper. The license often stipulated that a customer agreed if they did not return the product within a specified interval.[20] More recently, EULAs are most commonly found as clickwrap or browsewrap where the user's clicks or continued browsing are taken as a sign of agreement. As a result of the end of physical constraints, length increased.[21] Most EULAs have been designed so that it is very difficult to read and understand them, but easy to agree to the licensing terms without reading them.[12][20] Regardless of how easy it is to access, very few consumers read any part of the license agreement.[22][23] Most assume the terms are unobjectionable or barely notice agreeing while installing the software.[24] Companies take advantage of consumers' inattention to insert provisions into EULAs.[25]
Proprietary software is usually offered under a restrictive license that bans copying and reuse and often limits the purchaser to using the software on one computer.[5][26] Source code is rarely available. Derivative software works and reverse engineering are usually explicitly prohibited.[26] Many EULAs allow the vendor to collect information about the user and use it in unrestricted ways.[27] Some EULAs restrict the ability of users to exercise copyright over derivative work made using the software, such as creative creations in the virtual worlds of video games.[28][29]
EULAs, almost always offered on a take-it-or-leave-it basis as a non-negotiable condition for using the software,[34] are very far from the prototypical contract where both parties fully understand the terms and agree of their own free will.[35] There has been substantial debate on to what extent the agreements can be considered binding. Before 1996 in the United States, clickwrap or browsewrap licenses were not held to be binding, but since then they often have been.[36][21] Under the New Digital Content Directive effective in the European Union, EULAs are only enforceable to the extent that they do not breach reasonable consumer expectations. The gap between expectations and the content of EULAs is especially wide when it comes to restrictions on copying and transferring ownership of digital content.[37] Many EULAs contain stipulations  that are likely unenforceable  depending on the jurisdiction. Software vendors keep these unenforceable provisions in the agreements, perhaps because users rarely resort to the legal system to challenge them.[38]
Service-level agreements are often used for enterprise software and guarantee a level of service, such as software performance or time to respond to issue raised by the customer. Many stipulate financial penalties if the service falls short of the agreed standard.[39] SLAs often cover such aspects as availability, reliability, price, and security using quantifiable metrics.[40] Multi-tier SLAs are common in cloud computing because of the use of different computing services that may be managed by different companies.[41] SLAs in cloud computing are an area under active research as of 2024[update].[42]
Before the open-source movement in the 1980s, almost all software was proprietary and did not disclose its source code.[43] Open-source licensing is intended to maximize openness and minimize barriers to software use, dissemination, and follow-on innovation.[4]
Outside of software, noncommercial-only Creative Commons licenses have become popular among some artists who wish to prevent others from profiting excessively from their work.[51] However, software that is made available for noncommercial use only is not considered open source.[8] Sun Microsystems' noncommercial-only Java Research License was rejected by the open-source community, and in 2006 the company released most of Java under the GPL.[8]
Since 1989,[43] a variety of open-source licenses for software have been created.[53] Choosing an open-source software license has grown increasingly difficult due to the proliferation of licenses,[54][55] many of which are only trivially distinct.[56] Many licenses are incompatible with each other, hampering the goals of the free software movement.[57] Translation issues, ambiguity in licensing terms, and incompatibility of some licenses with the law in certain jurisdictions compounds the problem.[58]
Although downloading an open-source module is quick and easy, complying with the licensing terms can be more difficult.[59] The amount of software dependencies means that engineers working on complex projects must often rely on software license management software in order to help them achieve compliance with the licensing terms of open-source components.[60] Many open-source software files do not unambiguously state the license, increasing the difficulties of compliance.[59] When combining code bases, the original licenses can be maintained for separate components, and the larger work released under a compatible license.[61] This compatibility is often one-way. Public domain content can be used anywhere as there is no copyright claim, but code acquired under almost any set of terms cannot be waved to the public domain. Permissive licenses can be used within copyleft works, but copyleft material cannot be released under a permissive license. Some weak copyleft licenses can be used under the GPL and are said to be GPL-compatible. GPL software can only be used under the GPL or AGPL.[62]
Free and open-source software licenses have been successfully enforced in civil court since the mid-2000s.[63] Courts have found that distributing software indicates acceptance of the license's terms.[64] However, developers typically achieve compliance without lawsuits. Social pressures, such as the potential for community backlash, are often sufficient.[65] Cease and desist letters are a common method to bring companies back into compliance, especially in Germany.[66]
A long-debated subject within the FOSS community is whether open-source licenses are "bare licenses" or contracts.[67] A bare license is a set of conditions under which actions otherwise restricted by intellectual property laws are permitted.[63] Under the bare license interpretation, advocated by the Free Software Foundation (FSF), a case is brought to court by the copyright holder as copyright infringement.[63] Under the contract interpretation, a case can be brought to court by an involved party as a breach of contract.[68] United States and French courts have tried cases under both interpretations.[69]
More than 90 percent of companies use open-source software as a component of their proprietary software.[70] The decision to use open-source software, or even engage with open-source projects to improve existing open-source software, is typically a pragmatic business decision.[71][72] When proprietary software is in direct competition with an open-source alternative, research has found conflicting results on the effect of the competition on the proprietary product's price and quality.[73]
For decades, some companies have made servicing of an open-source software product for enterprise users as their business model. These companies control an open-source software product, and instead of charging for licensing or use, charge for improvements, integration, and other servicing.[74] Software as a service (SaaS) products based on open-source components are increasingly common.[75]
Open-source software is preferred for scientific applications, because it increases transparency and aids in the validation and acceptance of scientific results.[56]
Public-domain software is software that has been placed in the public domain, in other words, software for which there is absolutely no ownership such as copyright, trademark, or patent. Software in the public domain can be modified, distributed, or sold even without any attribution by anyone; this is unlike the common case of software under exclusive copyright, where licenses grant limited usage rights.
From the software culture of the 1950s to 1990s, public-domain (or PD) software were popular as original academic phenomena.  This kind of freely distributed and shared "free software" combined the present-day classes of freeware, shareware, and free and open-source software, and was created in academia, by hobbyists, and hackers.[2] As software was often written in an interpreted language such as BASIC, the source code was needed and therefore distributed to run the software. PD software was also shared and distributed as printed source code (type-in programs) in computer magazines (like Creative Computing, SoftSide, Compute!, Byte, etc.) and books, like the bestseller BASIC Computer Games.[3] Earlier on, closed-source software was uncommon until the mid-1970s to 1980s.[4][5][6]
Before 1974, when the US Commission on New Technological Uses of Copyrighted Works (CONTU) decided that "computer programs, to the extent that they embody an author's original creation, are proper subject matter of copyright",[7][8] software was not copyrightable and therefore always in the public domain. This legislation, plus court decisions such as Apple v. Franklin in 1983 for object code, clarified that the Copyright Act gave computer programs the copyright status of literary works.
In the 1980s, a common way to share public-domain software[verification needed] was by receiving them through a local user group or a company like PC-SIG of Sunnyvale, California, which maintained a mail-order catalog of more than 300 disks with an average price of US$6.[9] Public-domain software with source code was also shared on BBS networks. 
Public-domain software was commercialized sometimes by a donationware model, asking the users for a financial donation to be sent by mail.[10]
The public-domain "free sharing" and donationware commercialization models evolved in the following years to the (non-voluntary) shareware model,[11][12] and software free of charge, called freeware.[13] Additionally, due to other changes in the computer industry, the sharing of source code became less common.[6]
With the Berne Convention Implementation Act of 1988 (and the earlier Copyright Act of 1976), the legal basis for public-domain software changed drastically. Before the act, releasing software without a copyright notice was enough to dedicate it to the public domain. With the new act, software was by default copyright-protected and needed an explicit waiver statement or license from the author.[14][15]
Reference implementations of algorithms, often cryptographic meant or applied for standardization are still often released into the public domain; examples include CERN httpd[16] in 1993 and Serpent cipher in 1999. The Openwall Project maintains a list of several algorithms and their source code in the public domain.[17]
As a response of the academic software ecosystem to the change in the copyright system in the late 1980s, permissive license texts were developed, like the BSD license and its derivatives. Permissive-licensed software, which is a kind of free and open-source software, shares most characteristics of earlier public-domain software but stands on the legal basis of copyright law.
In the 1980s Richard Stallman, who for long worked in an academic environment of "public-domain"-like software sharing, noticed the emergence of proprietary software and the decline of the public-domain software ecosystem. In an effort to preserve this ecosystem he created a software license, the GPL, which encodes the public-domain rights and enforces them irrevocably on software. Paradoxically, his copyleft approach relies on the enforceability of the copyright to be effective. Copyleft free software, therefore, shares many properties with public-domain software, but does not allow relicensing or sublicensing. Unlike real public-domain software or permissive-licensed software, Stallman's copyleft license tries to enforce the free shareability of software also for the future by not allowing license changes.
To refer to free software (which is under a free software license) or to software distributed and usable free of charge (freeware) as "public-domain" is therefore incorrect. While public domain gives up the author's exclusive rights (e.g. copyright), in free software the author's copyright is still retained and used, for instance, to enforce copyleft or to hand out permissive-licensed software. Licensed software is in general not in the public domain.[18] Another distinct difference is that an executable program may be in the public domain even if its source code is not made available (making the program not feasibly modifiable), while free software always has the source code available.
With the 2000s and the emergence of peer-to-peer sharing networks and sharing in web development, a new copyright-critical generation of developers made the "license-free" public-domain software model visible again, also criticizing the FOSS license ecosystem ("Post Open Source") as stabilizing part of the copyright system.[19][20][21][22] New non-FOSS licenses and waiver texts were developed, notably the Creative Commons "CC0" (2009) and the "Unlicense" (2010), and there was a noticeable rise in the popularity of permissive software licenses. Also, the growing problem of orphaned software and digital obsolescence of software raised awareness of the relevance of again passing software into the public domain for better preservation of the digital heritage, unrestricted by copyright and digital rights management.[23][24][25][26]
Around 2004, there was debate on whether public-domain software could be considered part of the FOSS ecosystem, as argued by lawyer Lawrence Rosen in the essay "Why the public domain isn't a license",[27] a position that faced opposition by Daniel J. Bernstein and others.[28] In 2012, the status was finally resolved when Rosen changed his mind and accepted the CC0 as an open-source license, while admitting that, contrary to previous claims, copyright could be waived, as backed by a Ninth Circuit decision.[29]
Copyrighted works, like software, are meant to pass into the public domain after the copyright term, losing their copyright privilege. Due to the decades-long copyright protection granted by the Berne Convention, no software has ever passed into the public domain by leaving copyright terms. The question of how quickly works should pass into the public domain has been a matter of scientific[30][31][32] and public debates, as well as for software like video games.[24][25][26]
While real public domain makes software licenses unnecessary, as no owner/author is required to grant permission ("Permission culture"), there are licenses that grant public-domain-like rights. There is no universally agreed-upon license, but there are multiple licenses that aim to release source code into the public domain.
In 2000 the WTFPL was released as a public-domain-like license/waiver/anti-copyright notice.[34] In 2009 the Creative Commons released the CC0, which was created for compatibility with various law domains (e.g. civil law of continental Europe) where dedicating to public domain is problematic. This is achieved by a public domain waiver statement and a fallback all-permissive license, in case the waiver is not possible.[35][36] The Unlicense, published around 2010, has a focus on an anti-copyright message. The Unlicense offers a public domain waiver text with a fallback public-domain-like license inspired by permissive licenses but without attribution clause.[37][38] In 2015, GitHub reported that of the approximately 5.1 million licensed projects it hosted, almost 2% used the Unlicense.[39] Another popular option is the Zero Clause BSD license, released in 2006 and aimed at software.[40]
As result, such licensed public-domain software has all the four freedoms but is not hampered by the complexities of attribution (restriction of permissive licensed software) or license compatibility (issue with copyleft licensed software).
Public domain software in the early computer age was, for instance, shared as type-in programs in computer magazines and books like BASIC Computer Games. Explicit PD waiver statements or license files were at that time unusual. Publicly available software without a copyright notice was assumed to be, and shared as, public-domain software.
Many PD software authors kept the practices of public-domain release without having a waiver text, not knowing or caring for the changed copyright law, thus creating a legal problem. On the other hand, magazines started in the mid-1980s to claim copyright even for type-in programs that were previously seen as PD.[45][46] Only slowly did PD software authors start to include explicit relinquishment or license statement texts.
These examples of modern PD software (after the Berne Convention Implementation Act of 1988) are either under proper public domain (e.g. created by a US governmental organization), under a proper public domain like license (for instance CC0), or accompanied by a clear waiver statement from the author. Whilst not as widespread as in the pre-2000s, PD software still exists nowadays. For example, SourceForge listed 334 hosted PD projects in 2016,[47] and GitHub 102,000 under the Unlicense alone in 2015.[39] In 2016, an analysis of the Fedora Project's packages revealed PD was the seventh most popular "license".[48]
The award-winning video game developer Jason Rohrer releases his works into the PD, as do several cryptographers, such as Daniel J. Bernstein, Bruce Schneier and Douglas Crockford,[49] with reference implementations of cryptographic algorithms.
In computer software, horizontal market software is a type of application software that is useful in a wide range of industries. This is the opposite of vertical market software, which has a scope of usefulness limited to few industries. Horizontal market software is also known as "productivity software."[1]
Examples of horizontal market software include word processors, web browsers, spreadsheets, calendars, project management applications, and generic bookkeeping applications. Since horizontal market software is developed to be used by a broad audience, it generally lacks any market-specific customizations.[2][3]
Vertical market software is aimed at addressing the needs of any given business within a discernible vertical market (specific industry or market). While horizontal market software can be useful to a wide array of industries (such as word processors or spreadsheet programs), vertical market software is developed for and customized to a specific industry's needs.
Vertical market software is readily identifiable by the application specific graphical user interface which defines it. One example of vertical market software is point-of-sale software.
A software suite[1] (also known as an application suite) is a collection of computer programs (application software, or programming software) of related functionality, sharing a similar user interface and the ability to easily exchange data with each other.
Business software (or a business application) is any software or set of computer programs used by business users to perform various business functions. These business applications are used to increase productivity, measure productivity, and perform other business functions accurately.
Much business software is developed to meet the needs of a specific business, and therefore is not easily transferable to a different business environment, unless its nature and operation are identical. Due to the unique requirements of each business, off-the-shelf software is unlikely to completely address a company's needs. However, where an on-the-shelf solution is necessary, due to time or monetary considerations, some level of customization is likely to be required. Exceptions do exist, depending on the business in question, and thorough research is always required before committing to bespoke or off-the-shelf solutions.
Some business applications are built in-house and some are bought from vendors (off-the-shelf software products). These business applications are installed on either desktops or big servers. Prior to the introduction of COBOL (a universal compiler) in 1965, businesses developed their own unique machine language. RCA's language consisted of 12-position instructions. For example, to read a record into memory, the first two digits would be the instruction (action) code. The next four positions of the instruction (an 'A' address) would be the exact leftmost memory location where you want the readable character to be placed. Four positions (a 'B' address) of the instruction would note the very rightmost memory location where you want the last character of the record to be located. A two-digit 'B' address also allows the modification of any instruction. Instruction codes and memory designations excluded the use of 8's or 9's. The first RCA business application was implemented in 1962 on a 4k RCA 301. The RCA 301, mid-frame 501, and large frame 601 began their marketing in early 1960.
Technologies that previously only existed in peer-to-peer software applications, like Kazaa and Napster, are starting to appear within business applications.
Business software is designed to increase profits by cutting costs or speeding the productive cycle. In the early days of white-collar business automation, large mainframe computers were used to tackle the most tedious jobs, like bank cheque clearing and factory accounting.
Factory accounting software was among the most widely-used early business software tools and included the automation of general ledgers, fixed assets inventory ledgers, cost accounting ledgers, accounts receivable ledgers, and accounts payable ledgers (including payroll, life insurance, health insurance, federal and state insurance and retirement).
The early use of software to replace manual white-collar labor was extremely profitable and caused a radical shift in white-collar labor. One computer could in many cases replace many white-collar administrative employees, without requiring any health or retirement benefits.
Building on this success, corporate consumers demanded from IBM, Hewlett-Packard, and other early suppliers, of business software to replace the old-fashioned drafting board. Computer-aided drafting for computer-aided manufacturing (CAD-CAM) arrived in the early 1980s. Project management software was also so valued in the early 1980s that it could cost up to $500,000 per copy.
One of the most noticeable, widespread changes in business software was the word processor, whose rapid rise caused the decline of the ubiquitous IBM typewriter in the 1980s, as millions of companies switched to using Word Perfect, and later Microsoft Word. Other popular computer programs for business were mathematical spreadsheet programs such as Lotus 1-2-3, and later Microsoft Excel.
In the 1990s business shifted towards globalism, with the appearance of SAP software, which coordinates a supply-chain of vendors in order to streamline the operation of factory manufacturing. This process was triggered and vastly accelerated by the advent of the internet.
The next phase in the evolution of business software is being driven by the emergence of robotic process automation (RPA), which involves identifying and automating highly repetitive tasks and processes, with an aim to drive operational efficiency, reduce costs and limit human error.[4] Industries at the forefront of RPA adoption include the insurance industry, banking and other related financial services, the legal industry, and the healthcare industry.
Business applications are built based on the requirements of business users. Also, these business applications are built to use certain kinds of Business transactions or data items. These business applications run flawlessly until there are no new business requirements or there is no change in underlying Business transactions. Also, the business applications run flawlessly if there are no issues with computer hardware, computer networks (Internet/intranet), computer disks, power supplies, and various software components (middleware, database, computer programs, etc.).
Business applications can fail when an unexpected error occurs. This error could occur due to a data error (an unexpected data input or a wrong data input), an environment error (an in infrastructure-related error), a programming error, a human error, or a workflow error. When a business application fails one needs to fix the business application error as soon as possible so that the business users can resume their work. This work of resolving business application errors is known as business application support.
The Business User calls the business application support team phone number or sends an e-mail to the business application support team. The business application support team gets all the details of the error from the business user on the phone or from the e-mail. These details are then entered in a tracking software. The tracking software creates a request number and this request number is given to the business user. This request number is used to track the progress on the support issue. The request is assigned to a support team member.
The business application support team member collects all the necessary information about the business software error. This information is then recorded in the support request. All of the data used by the business user is also used in the investigation. The application program is reviewed for any possible programming errors.
If the business application error occurred due to programming errors, then a request is created for the application development team to correct programming errors. If the business user needs new features or functions in the business application, then the required analysis/design/programming/testing/release is planned and a new version of the business software is deployed.
Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than its individual users. Enterprise software is an integral part of a computer-based information system, handling a number of business operations, for example to enhance business and management reporting tasks, or support production operations and back office functions. Enterprise systems must process information at a relatively high speed.[1]
Services provided by enterprise software are typically business-oriented tools. As companies and other organizations have similar departments and systems, enterprise software is often available as a suite of customizable programs. Function-specific enterprise software uses include database management, customer relationship management, supply chain management and business process management.[2]
The term enterprise software is used in industry, and business research publications, but is not common in computer science. The term was widely popularized in the early 1990s by major software vendors in conjunction with licensing deals with the show Star Trek [3] In academic literature no coherent definition can be found. The computer historian Martin Campbell-Kelly contemplated in 2003 that the growth of the corporate software industry is not well understood. Enterprise application software (EAS) is recognized among academics as enterprise software components and modules which support only a particular business function. These EAS software components and modules can interoperate, so that cross-functional or inter-organizational enterprise systems can be built up. In this context the industry may speak of middleware. Software that is primarily sold to consumers, is not called enterprise software.[4]
Enterprise application software is application software that performs business functions such as order processing, procurement, production scheduling, customer information management, energy management, and accounting.
Enterprise systems (ES) are large-scale enterprise software packages which support a range of business processes, information flows, reporting, and data analytics in complex organizations. While ES are generally packaged enterprise application software (PEAS) systems,[6] they can also be bespoke, custom-developed systems created to support a specific organization's needs. Types of enterprise system include:
Although data warehousing or business intelligence systems are enterprise-wide packaged application software often sold by ES vendors, since they do not directly support execution of business processes, they are often excluded from the term.
Enterprise systems are built on software platforms, such as SAP's NetWeaver and Oracle's Fusion, and databases.
From a hardware perspective, enterprise systems are the servers, storage and associated software that large businesses use as the foundation for their IT infrastructure. These systems are designed to manage large volumes of critical data and thus are typically designed to provide high levels of transaction performance and data security.[7]
Enterprise software can be categorized by business function. Each type of enterprise application can be considered a "system" due to the integration with a firm's business processes.[9] Categories of enterprise software may overlap due to this systemic interpretation. For example, IBM's business intelligence platform (Cognos), integrates with a predictive analytics platform (SPSS) and can obtain records from its database packages (Infosphere, DB2).
Other types of software which do not fit into well-known standard categories, including backup software, billing management, and accounting software. Enterprise contract management software is used to bring all of an organisation's contractual commitments into a single system for holistic management and to avoid the variability and inefficiency inherent in manual contracting processes.[10]
Supply-chain-management software (SCMS) is the software tools or modules used in executing supply chain transactions, managing supplier relationships and controlling associated business processes. Supply chain management maximizes the efficiency of business activities that include planning and management of the entire supply chain. It helps businesses in product development, sourcing, production, and logistics by automating operations. In this way, it increases the physical flow of business as well as informative flow. The entire business benefits with higher performance, greater cost-efficiency, and thus increased supply chain efficiency.
A requirement of many SCMS often includes forecasting. Such tools often attempt to balance the disparity between supply and demand by improving business processes and using algorithms and consumption analysis to better plan future needs.[1]  SCMS also often includes integration technology that allows organizations to trade electronically with supply chain partners.[2]
SCMS adoption is growing faster than the broader enterprise application software market. The annual revenue from SCMS (on-premises and SaaS) reached $10 billion in 2014, a 12 percent increase over 2013.[3]
While premises-based software was still more widely used than SaaS solutions for SCMS in 2014, Gartner projects that about two-thirds of the growth in SCMS adoption between 2015 and 2018 will be based on the SaaS subscription model: driven by a growing realization of the benefits of cloud-based services, the SaaS-based SCMS market grew by about 24 percent in 2014 and is projected to continue to grow at a 19 percent compound annual growth rate (CAGR), reaching $4.4 billion in annual sales by 2018.[4]
Media player software is a type of application software for playing multimedia computer files like audio and video files. Media players commonly display standard media control icons known from physical devices such as tape recorders and CD players, such as play (  ), pause (  ), fastforward (⏩️), rewind (⏪), and stop (  ) buttons. In addition, they generally have progress bars (or "playback bars"), which are sliders to locate the current position in the duration of the media file.
Mainstream operating systems have at least one default media player. For example, Windows comes with Windows Media Player, Microsoft Movies & TV and Groove Music, while macOS comes with QuickTime Player and Music. Linux distributions come with different media players, such as SMPlayer, Amarok, Audacious, Banshee, MPlayer, mpv, Rhythmbox, Totem, VLC media player, and xine. Android comes with YouTube Music for audio and Google Photos for video, and smartphone vendors such as Samsung may bundle custom software.
Media player software may have the ability to adjust appearance and acoustics during playback using effects such as mirroring, rotating, cropping, cloning, adjusting colours, deinterlacing, and equalizing and visualizing audio. Easter eggs may be featured, such as a puzzle game on VLC Media Player.[4][5]
3D video players are used to play 2D video in 3D format. A high-quality three-dimensional video presentation requires that each frame of a motion picture be embedded with information on the depth of objects present in the scene. This process involves shooting the video with special equipment from two distinct perspectives or modeling and rendering each frame as a collection of objects composed of 3D vertices and textures, much like in any modern video game, to achieve special effects. Tedious and costly, this method is only used in a small fraction of movies produced worldwide, while most movies remain in the form of traditional 2D images. It is, however, possible to give an otherwise two-dimensional picture the appearance of depth. Using a technique known as anaglyph processing a "flat" picture can be transformed so as to give an illusion of depth when viewed through anaglyph glasses (usually red-cyan). An image viewed through anaglyph glasses appears to have both protruding and deeply embedded objects in it, at the expense of somewhat distorted colors. The method itself is old enough, dating back to the mid-19th century, but it is only with recent advances in computer technology that it has become possible to apply this kind of transformation to a series of frames in a motion picture reasonably fast or even in real-time, i.e. as the video is being played back. Several implementations exist in the form of 3D video players that render conventional 2D video in anaglyph 3D, as well as in the form of 3D video converters that transform video into stereoscopic anaglyph and transcode it for playback with regular software or hardware video players.[citation needed]
Well known examples of media player software include Windows Media Player, VLC media player, iTunes, Winamp, Media Player Classic, MediaMonkey, foobar2000, AIMP, MusicBee and JRiver Media Center. Most of these also include music library managers.
A home theater PC or media center computer is a convergence device that combines some or all the capabilities of a personal computer with a software application that supports video, photo, audio playback, and sometimes video recording functionality. Although computers with some of these capabilities were available from the late 1980s, the "Home Theater PC" term first appeared in mainstream press in 1996. Since 2007, other types of consumer electronics, including gaming systems and dedicated media devices have crossed over to manage video and music content. The term "media center" also refers to specialized computer programs designed to run on standard personal computers.[9]
Simulation software is based on the process of modeling a real phenomenon with a set of mathematical formulas. It is, essentially, a program that allows the user to observe an operation through simulation without actually performing that operation. Simulation software is used widely to design equipment so that the final product will be as close to design specs as possible without expensive in process modification. Simulation software with real-time response is often used in gaming, but it also has important industrial applications. When the penalty for improper operation is costly, such as airplane pilots, nuclear power plant operators, or chemical plant operators, a mock up of the actual control panel is connected to a real-time simulation of the physical response, giving valuable training experience without fear of a disastrous outcome.
The Event-Scheduling method is simpler and only has two phases so there is no Cs and Bs, this allow the program to run faster since there are no scanning for the conditional events. All these advantages also tells us something about the disadvantages of the method since there are only two phase then all events are mixed (no Bs and Cs) then the method is not parsimony, which means it is very hard to enhance (Pidd, 1998).  The Activity Scanning approach is also simpler than the Three-Phase method since it has no calendar, and it support the parsimonious modeling. However this approach is much slower than Three-Phase since it treats all activities are treated as conditional. On the other hand, the executive has two phases. Usually this approach is confused with the Three-Phase method (Pidd, 1998).  The Process- Interaction “share two common advantages first; they avoid programs that are slow to run. Second, they avoid the need to think through all possible logical consequences of an event” (Pidd, 1998). Yet, as (Pidd, 1998) claims this approach suffers from DEADLOCK problem, but this approach is very attractive for novice modelers. Although, (Schriber et al, 2003). Says “process interaction was understood only by an elite group of individuals and was beyond the reach of ordinary programmers”. In fact (Schriber et al, 2003).adds “. Multi- threaded applications were talked about in computer science classes, but rarely used in the broader community”. Which indicates that the implementation of Process-Interaction was very difficult to implement. The obvious contradiction, in the previous quote is due to the mix up between the Process Interaction approach and the Transaction-flow approach. To see the complete idea of the origins of Transaction-Flow best stated by (Schriber et al, 2003): This was the primordial soup out of which the Gordon Simulator arose. Gordon’s transaction flow world-view was a cleverly disguised form of process interaction that put the process interaction approach within the grasp of ordinary users. . Gordon did one of the great packaging jobs of all time. He devised a set of building blocks that could be put together to build a flowchart that graphically depicted the operation of a system. Under this modeling paradigm, the flow of elements through a system was readily visible, because that was the focus of the whole approach.  The Three-Phase approach allows to “simulate  parallelism, whilst avoiding deadlock” (Pidd and Cassel, 1998). Yet, Three-Phase has to scan through the schedule for bound activities, and then scans through all conditional activities which slow it down. Yet many forgo the time spent in return for solving the deadlock problem. In fact, Three-Phase is used in distributed systems whether talking about operating systems, databases, etc, under different names among them Three-Phase commit see (Tanenbaum and Steen, 2002).[3]
Electronics simulation software utilizes mathematical models to replicate the behaviour of an actual electronic device or circuit.  Essentially, it is a computer program that converts a computer into a fully functioning electronics laboratory.  Electronics simulators integrate a schematic editor, SPICE simulator and onscreen waveforms and make “what-if” scenarios easy and instant. By simulating a circuit’s behaviour before actually building it greatly improves efficiency and provides insights into the behavior and stability of electronics circuit designs. Most simulators use a SPICE engine that simulates analog, digital and mixed A/D circuits for exceptional power and accuracy. They also typically contain extensive model and device libraries.  While these simulators typically have printed circuit board (PCB) export capabilities, they are not essential for design and testing of circuits, which is the primary application of electronic circuit simulation.
The event driven algorithm provided by mixed-mode simulators is general
purpose and supports non-digital types of data. For example, elements can use real or integer values to simulate DSP functions or sampled data filters. Because the event driven algorithm is faster than the standard SPICE matrix solution simulation time is greatly reduced for circuits that use event driven models in place of analog models.[7]
In order to properly understand the operation of a programmable logic controller (PLC), it is necessary to spend considerable time programming, testing, and debugging PLC programs. PLC systems are inherently expensive, and down-time is often very costly. In addition, if a PLC is programmed incorrectly it can result in lost productivity and dangerous conditions.  PLC simulation software is a valuable tool in the understanding and learning of PLCs and to keep this knowledge refreshed and up to date.[8] PLC simulation provides users with the ability to write, edit and debug programs written using a tag-based format.  Many of the most popular PLCs use tags, which are a powerful method of programming PLCs but also more complex.  PLC simulation integrates tag-based ladder logic programs with 3D interactive animations to enhance the user’s learning experience.[9]  These interactive animations include traffic lights, batch processing, and bottling lines.[10]
Sheet metal forming simulation software utilizes mathematical models to replicate the behavior of an actual metal sheet manufacturing process.[citation needed]  Essentially, it is a computer program that converts a computer into a fully functioning metal manufacturing prediction unit.  Sheet metal forming simulation prevents metal factories from defects in their production lines and reduces testing and expensive mistakes improving efficiency in the metal forming process.[citation needed]
Metal casting simulation is currently performed by Finite Element Method simulation software designed as a defect-prediction tool for the foundry engineer, in order to correct and/or improve his/her casting process, even before prototype trials are produced. The idea is to use information to analyze and predict results in a simple and effective manner to simulate processes such as:
The interaction between the network entities is defined by various communication protocols. Network simulation software simulates behavior of networks on a protocol level. Network Protocol Simulation software can be used to develop test scenarios, understand the network behavior against certain protocol messages, compliance of new protocol stack implementation, Protocol Stack Testing. These simulators are based on telecommunications protocol architecture specifications developed by international standards body such as the ITU-T, IEEE, and so on. The output of protocol simulation software can be detailed packet traces, events logs etc.
Graphic art software[1] is a subclass of application software used for graphic design, multimedia development, stylized image development, technical illustration, general image editing, or simply to access graphic files. Art software uses either raster graphics or vector graphics reading and editing methods to create, edit, and view art.
Many artists and other creative professionals today use personal computers rather than traditional media. Using graphic art software may be more efficient than rendering using traditional media by needing less eye–hand coordination and less mental imaging skill, and using the computer's quicker (sometimes more accurate) automated rendering functions to create images. However, advanced level computer styles, effects and editing methods may need a steeper learning curve of computer technical skills than what was needed to learn traditional hand rendering and mental imaging skills. The potential of the software to enhance or hinder creativity may depend on the intuitiveness of the user interface.
Most art software includes common functions, creation tools, editing tools, filters, and automated rendering modes. Many, however, are designed to enhance a specialized skill or technique. Specialized software packages may be discontinued for various reasons such as lack of appreciation for the result, lack of expertise and training for the product, or simply not worth the time and money investment, but most likely due to obsolescence compared to newer methods or integration as a feature of newer more complete software packages.
Graphic design professionals favor general image editing software and page layout software commonly referred to as desktop publishing software. Graphic designers that are also image developers or multimedia developers may use a combination of page layout software with the following:
Multimedia development professionals favor software with audio, motion and interactivity such as software for creating and editing hypermedia, electronic presentations (more specifically slide presentations), computer simulations and games.
Image development professionals may use general graphic editors or may prefer more specialized software for rending or capturing images with style. Although images can be created from scratch with most art software, specialized software applications or advanced features of generalized applications are used for more accurate visual effects. These visual effects include:
Some generalized image editors, such as Adobe Photoshop are used for digital painting (representing real brush and canvas textures such as watercolor or burlap canvas) or handicraft textures such as mosaic or stained glass. However, unlike Adobe Photoshop, which was originally designed for photo editing, software such as Corel Painter and Photo-Paint were originally designed for rendering with digital painting effects and continue to evolve with more emphasis on hand-rendering styles that don't appear computer generated.
Unlike traditional medium effects, photorealistic effects create the illusion of a photographed image. Specialized software may contain 3D modeling and ray tracing features to make images appear photographed. Some 3D software is for general 3D object modeling, whereas other 3D software is more specialized, such as Poser for characters or Bryce for scenery. Software such as Adobe Photoshop may be used to create 3D effects from 2D (flat) images instead of 3D models. AddDepth is a discontinued software for extruding 2D shapes into 3D images with the option of beveled effects. MetaCreations Detailer and Painter 3D are discontinued software applications specifically for painting texture maps on 3D Models.
Specialized software may be used to combine traditional medium effects and photorealistic effects. 3-D modeling software may be exclusively for, include features for, or include the option of 3rd party plugins for rendering 3-D models with 2-D effects (e.g. cartoons, illustrations) for hyperrealistic effects. Other 2-D image editing software may be used to trace photographs or rotoscope animations from the film. This allows artists to rapidly apply unique styles to what would be purely photorealistic images from computer generated imagery from 3-D models or photographs. Some styles of hyperrealism may need motion visual effects (e.g., geometrically accurate rotation, accurate kinetics, simulated organic growth, lifelike motion constraints) to notice the realism of the imagery. Software may be used to bridge the gap between the imagination and the laws of physics.
Technical professionals and technical illustrators may use technical graphic software that might allow for stylized effects with more emphasis on clarity and accuracy and little or no emphasis on creative expression and aesthetics. For this reason, the results are seldom referred to as "art." For designing or technical illustration of synthetic physical objects, the software is usually referred to as CAD or CADD, Computer-Aided Design and Drafting. This software allows more precise handling of measurements and mathematical calculations, some of which simulate physics to conduct virtual testing of the models. Aside from physical objects, technical graphic software may include software for visualizing concepts, manually representing scientific data, visualizing algorithms, visual instructions, and navigation aids in the form of information graphics. Specialized software for concept maps may be used for both technical purposes and non-technical conceptualizing, which may or may not be considered technical illustration.
Ellen Mazur Thomson's exploration of technology's influence on design education in "The Literature of Graphic Design" aligns with the broader context of graphic software design. The discussion about technology's impact on design education directly correlates with the use and integration of graphic software in teaching design principles.
The insights from Thomson's book highlight how technology, specifically graphic design software, plays a pivotal role in modern design education. This software is often discussed in the context of graphic art and design and forms the practical foundation of many design courses. Thomson's exploration likely emphasizes how graphic software tools empower students to create, experiment, and visualize their design ideas in a digital format. It ties into the discussion by showcasing how these software applications are not just tools for creating art but essential components in the education and practice of modern graphic design. Understanding technology's role in design education, as discussed in the book, provides valuable context to appreciate the significance and integration of graphic software design in teaching and learning graphic design principles and techniques.
This may include software for handling specialized graphic file formats such as Fontographer software, which is dedicated to creating and editing computer fonts. Some general image editing software has unique image file handling features as well. Vector graphic editors handle vector graphic files and are able to load PostScript files natively. Some tools enable professional photographers to use nondestructive image processing for editing digital photography without permanently changing or duplicating the original, using the raw image format. Other special handling software includes software for capturing images such as 2D scanning software, 3D scanning software and screen-capturing, or software for specialized graphic format processing such as raster image processing and file format conversion. Some tools may reduce the file size of graphics for web performance optimization while maintaining the image quality as best as possible.
A software calculator is a calculator that has been implemented as a computer program, rather than as a physical hardware device.
They are among the simpler interactive software tools, and, as such, they provide operations for the user to select one at a time. They can be used to perform any process that consists of a sequence of steps each of which applies one of these operations, and have no purpose other than these processes, because the operations are the sole, or at least the primary, features of the calculator, rather than being secondary features that support other functionality that is not normally known simply as calculation.[1]
As a calculator, rather than a computer, they usually have a small set of relatively simple operations, perform short processes that are not compute intensive and do not accept large amounts of input data or produce many results, though many software calculators can emulate handheld scientific calculator and graphing calculator features such as trigonometric functions, approximations of pi, and making plots of functions.
Modern computers first emerged in the 1940s and 1950s. The software that they ran was naturally used to perform calculations, but it was specially designed for a substantial application that was not limited to simple calculations. For example, the LEO computer was designed to run business application software such as payroll.
Software specifically to perform calculations as its main purpose was first written in the 1960s, and the first software package for general calculations to obtain widespread use was released in 1978.[2] This was VisiCalc and it was called an interactive visible calculator, but it was actually a spreadsheet, and these are now not normally known simply as calculators.
Calculators have been used since ancient times and until the advent of software they were physical, hardware machines. The most recent hardware calculators are electronic hand-held devices with buttons for digits and operations, and a small display for inputs and results.
The first software calculators imitated these hardware calculators by implementing the same functionality with mouse-operated, rather than finger-operated, buttons. Such software calculators first emerged in the 1980s as part of the original Macintosh operating system (System 1) and the Windows operating system (Windows 1.0).
Some software calculators directly simulate one of the hardware calculators, by presenting an image that looks like the calculator, and by providing the same functionality.
There is now a very wide range of software calculators, and searching the Internet produces very large numbers of programs that are called calculators.
Many of the results are calculators that do not imitate or simulate hardware calculators, but that take advantage of the greater power of computer software to implement alternative types of calculators. Software calculators are provided on the Internet which are customizable to use any conceivable algebraic expression. These user-customizable software calculators can also be used in conjunction with formula or equation creation capabilities so that the software calculator can now be created to perform all possible mathematical functions. No longer limited to a set of trigonometric and simple algebraic expressions, versions of the software calculator are now tailored to any and all topical applications.
Every type of hardware calculator has been implemented in software, including conversion, financial, graphing, programmable and scientific calculators.
Not all software-based calculators take numerical data or algebraic expressions as their input. Calculators can also take in arbitrary information ranging from lifestyle information to scientific notation. Some examples of these types of software calculators include:
There are also types of software used to help solve games that are sometimes referred to as calculators, including:
There are many interactive software packages that provide user-accessible calculation features, but that are not normally called calculators, because the calculation features play only a supporting role rather than being an end in themselves. These include:
In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term "database" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.
Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.
Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.
Formally, a "database" refers to a set of related data accessed through the use of a "database management system" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.
Because of the close relationship between them, the term "database" is often used casually to refer to both a database and the DBMS used to manipulate it.
Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.[1]
Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:
Both a database and its DBMS conform to the principles of a particular database model.[5] "Database system" refers collectively to the database model, database management system, and database.[6]
Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.[citation needed]
Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.
The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational,[8] SQL/relational, and post-relational.
The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018[update] they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS.[9] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.[citation needed]
Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch, which led to the coining of the term "post-relational" and also the development of hybrid object–relational databases.
The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases. A competing "next generation" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.
The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term "data-base" in a specific technical sense.[10]
As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market.
Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications.
IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014[update].[11]
Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that were primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a "search" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[12]
In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of "tables", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each "fact" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated.
The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit.
For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.
As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic.
Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a "language" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.
In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.
Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata).
IBM started working on a prototype system loosely based on Codd's concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large "chunk". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL[citation needed] – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2).
Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979.[18]
Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).
Another data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.[citation needed]
The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation."[19] dBASE was one of the top selling software titles in the 1980s and early 1990s.
The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields.[20] The term "object–relational impedance mismatch" described the inconvenience of translating between programmed objects and database tables. Object databases and object–relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object–relational mappings (ORMs) attempt to solve the same problem.
XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.
NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.
In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.
NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.
Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).
Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.
One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.
Connolly and Begg define database management system (DBMS) as a "software system that enables users to define, create, maintain and control access to the database."[24] Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access.
The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object–relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems.
It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities.[26] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.
Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.
Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.[28]
A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.
External interaction with the database will be via an application program that interfaces with the DBMS.[29] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information.
A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.
Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:
Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, "data about the data", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database.[33] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. "storage engine". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).
Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.
Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.
Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.
Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.
With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.[34]
Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).
Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.
This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called "subschemas". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.
Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause.[35]
Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).
The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.
A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs.
After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).
When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.
After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.
Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.
Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[36] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc.
Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as "DevOps for database".[37]
The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity–relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like "can a customer also be a supplier?", or "if a product is sold with two different forms of packaging, are those the same product or different products?", or "if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.
Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.
Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).
The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary "fact" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.
The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.
Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.
A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.
While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database.
The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model.[39] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.
The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.
Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more.
The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).
Desktop publishing (DTP) is the creation of documents using dedicated software on a personal ("desktop") computer. It was first used almost exclusively for print publications, but now it also assists in the creation of various forms of online content.[1] Desktop publishing software can generate page layouts and produce text and image content comparable to the simpler forms of traditional typography and printing. This technology allows individuals, businesses, and other organizations to self-publish a wide variety of content, from menus to magazines to books, without the expense of commercial printing.[2]
Desktop publishing often requires the use of a personal computer and WYSIWYG page layout software to create documents for either large-scale publishing or small-scale local printing and distribution –  although non-WYSIWYG systems such as TeX and LaTeX are also used, especially in scientific publishing. Originally, desktop publishing methods provided more control over design, layout, and typography than word processing software but the latter has evolved to include most, if not all, capabilities previously available only with dedicated desktop publishing software.[3]
The same DTP skills and software used for common paper and book publishing are sometimes used to create graphics for point of sale displays, presentations, infographics, brochures, business cards, promotional items, trade show exhibits, retail package designs and outdoor signs.
The desktop publishing market took off in 1985 with the introduction in January of the Apple LaserWriter laser printer for the year-old Apple Macintosh personal computer.[8][9] 
This momentum was kept up with the release that July of PageMaker software from Aldus, which rapidly became the standard software application for desktop publishing.[10] With its advanced layout features, PageMaker immediately relegated word processors like Microsoft Word to the composition and editing of purely textual documents.[11] Word did not begin to acquire desktop publishing features until a decade later, and by 2003, it was regarded only as "good" and not "great" at desktop publishing tasks.[12]  The term "desktop publishing" is attributed to Aldus founder Paul Brainerd, who sought a marketing catchphrase to describe the small size and relative affordability of this suite of products, in contrast to the expensive commercial phototypesetting equipment of the day.[13]
Desktop publishing was still in its early stage in the early 1980s. Users of the PageMaker/LaserWriter/Macintosh 512K system endured frequent software crashes,[16] Mac's low-resolution 512x342 1-bit monochrome screen, the inability to control letter spacing, kerning, and other typographic features, and the discrepancies between screen display and printed output. However, it was an unheard-of combination at the time, and was received with considerable acclaim.[10]
Desktop publishing moved into the home market in 1986 with Professional Page for the Amiga, Publishing Partner (now PageStream) for the Atari ST, GST's Timeworks Publisher on the PC and Atari ST, and Calamus for the Atari TT030. Software was published even for 8-bit computers like the Apple II and Commodore 64: Home Publisher, The Newsroom, and geoPublish.
During its early years, desktop publishing acquired a bad reputation as a result of untrained users who created poorly organized, unprofessional-looking "ransom note effect" layouts.[11]  (Similar criticism was leveled again against early World Wide Web publishers a decade later.) However, some desktop publishers who mastered the programs were able to achieve near professional results. Desktop publishing skills were considered of primary importance in career advancement in the 1980s, but increased accessibility to more user-friendly DTP software has made DTP a secondary skill to art direction, graphic design, multimedia development, marketing communications, and administrative careers. DTP skill levels range from what may be learned in a couple of hours (e.g., learning how to put clip art in a word processor), to what's typically required in a college education. The discipline of DTP skills range from technical skills such as prepress production and programming, to creative skills such as communication design and graphic image development.
As of 2014[update], Apple computers remain dominant in publishing, even as the most popular software has changed from QuarkXPress – an estimated 95% market share in the 1990s – to Adobe InDesign. An Ars Technica writer said in an article: "I've heard about Windows-based publishing environments, but I've never actually seen one in my 20+ years in design and publishing".[17]
Master pages are templates used to automatically copy or link elements and graphic design styles to some or all the pages of a multipage document. Linked elements can be modified without having to change each instance of an element on pages that use the same element. Master pages can also be used to apply graphic design styles to automatic page numbering. Cascading Style Sheets can provide the same global formatting functions for web pages that master pages provide for virtual paper pages. Page layout is the process by which the elements are laid on the page orderly, aesthetically and precisely. The main types of components to be laid out on a page include text, linked images (that can only be modified as an external source), and embedded images (that may be modified with the layout application software). Some embedded images are rendered in the application software, while others can be placed from an external source image file. Text may be keyed into the layout, placed, or – with database publishing applications – linked to an external source of text which allows multiple editors to develop a document at the same time.
Graphic design styles such as color, transparency and filters may also be applied to layout elements. Typography styles may be applied to text automatically with style sheets. Some layout programs include style sheets for images in addition to text. Graphic styles for images may include border shapes, colors, transparency, filters, and a parameter designating the way text flows around the object (also known as "wraparound" or "runaround").
As desktop publishing software still provides extensive features necessary for print publishing, modern word processors now have publishing capabilities beyond those of many older DTP applications, blurring the line between word processing and desktop publishing.
In the early 1980s, the graphical user interface was still in its embryonic stage and DTP software was in a class of its own when compared to the leading word processing applications of the time. Programs such as WordPerfect and WordStar were still mainly text-based and offered little in the way of page layout, other than perhaps margins and line spacing. On the other hand, word processing software was necessary for features like indexing and spell checking – features that are common in many applications today. As computers and operating systems became more powerful, versatile, and user-friendly in the 2010s, vendors have sought to provide users with a single application that can meet almost all their publication needs.
In earlier modern-day usage, DTP usually did not include digital tools such as TeX or troff, though both can easily be used on a modern desktop system, and are standard with many Unix-like operating systems and are readily available for other systems. The key difference between digital typesetting software and DTP software is that DTP software is generally interactive and "What you see [onscreen] is what you get" (WYSIWYG) in design, while other digital typesetting software, such as TeX, LaTeX and other variants, tend to operate in "batch mode", requiring the user to enter the processing program's markup language (e.g. HTML) without immediate visualization of the finished product. This kind of workflow is less user-friendly than WYSIWYG, but more suitable for conference proceedings and scholarly articles as well as corporate newsletters or other applications where consistent, automated layout is important.
In the 2010s, interactive front-end components of TeX, such as TeXworks and LyX, have produced "what you see is what you mean" (WYSIWYM) hybrids of DTP and batch processing.[18] These hybrids are focused more on the semantics than the traditional DTP. Furthermore, with the advent of TeX editors the line between desktop publishing and markup-based typesetting is becoming increasingly narrow as well; a software which separates itself from the TeX world and develops itself in the direction of WYSIWYG markup-based typesetting is GNU TeXmacs.
In computing, a presentation program (also called presentation software) is a software package used to display information in the form of a slide show. It has three major functions:[1]
Presentation software can be viewed[by whom?] as enabling a functionally-specific category of electronic media, with its own distinct culture and practices as compared to traditional presentation media (such as blackboards, whiteboards and flip charts).
Generally in presentations, the visual material is considered[by whom?] supplemental to a strong aural presentation that accompanies the slide show, but in many cases, such as statistical graphics, it can be difficult to convey essential information other than by visual means; additionally, a well-designed infographic can be extremely effective in a way that words are not. Endemic over-reliance on slides with low information density and with a poor accompanying lecture has given presentation software a negative reputation as sometimes functioning as a crutch for the poorly informed or the poorly prepared.
The first commercial computer software specifically intended for creating WYSIWYG presentations was developed at Hewlett-Packard in 1979 and called BRUNO and later HP-Draw. The first microcomputer-based presentation software was Cromemco's Slidemaster, developed by John F. Dunn and released by Cromemco in 1981.[2][3][4] The first software displaying a presentation on a personal computer screen was VCN ExecuVision, developed in 1982. This program allowed users to choose from a library of images to accompany the text of their presentation.  Harvard Graphics was introduced for MS-DOS and Lotus Freelance Graphics was introduced for DOS and OS/2 in 1986. PowerPoint was introduced for the Macintosh computer in 1987.[5]
A presentation program is supposed to help both the speaker with an easier access to his ideas and the participants with visual information which complements the talk.
There are many different types of presentations including professional (work-related), education, entertainment, and for general communication.  Presentation programs can either supplement or replace the use of older visual-aid technology, such as pamphlets, handouts, chalkboards, flip charts, posters, slides and overhead transparencies. Text, graphics, movies, and other objects are positioned on individual pages or "slides" or "foils"[citation needed]. The "slide" analogy is a reference to the slide projector, a device that has become somewhat obsolete due to the use of presentation software. Slides can be printed, or (more usually) displayed on-screen and navigated through at the command of the presenter. An entire presentation can be saved in video format.[6] The slides can also be saved as images of any image file formats for any future reference.[7] Transitions between slides can be animated in a variety of ways, as can the emergence of elements on a slide itself. Typically a presentation has many constraints and the most important being the limited time to present consistent information.
Similar to programming extensions for an operating system or web browser, "add ons" or plugins for presentation programs can be used to enhance their capabilities.
Banking software is enterprise software that is used by the banking industry to provide and manage the financial products they provide.  Within retail banks, banking software typically refers to core banking software and all its interfaces that allow them to connect to other modular software and to the interbank networks. Within investment banking, banking software typically refers to the electronic processing application software used to access capital markets.
Commercial or retail banks use what is known as core banking software which records and manages the transactions made by the banks' customers to their accounts.  For example, it allows a customer to go to any branch of the bank and do their banking from there. In essence, it frees the customer from their home branch and enables them to do banking anywhere.
Moreover, core banking software facilitates seamless integration with other channels such as ATMs, Internet Banking, payment networks and SMS based banking.[1] This integration ensures that customers can access banking services through various channels, enhancing accessibility and the customer experience.
Banking software is used by millions of users across hundreds or thousands of branches. This means that the software must be managed on many machines even in a small bank.  The core banking system is a major investment for retail banks and maintaining and managing the system can represent a large part of the cost of running a bank. Given its critical role in daily banking operations and customer interactions, ensuring the smooth functioning and security of the core banking software is of utmost importance for financial institutions.
Investment banks use software to manage their trading desks and their client's accountants.  These systems often connect to financial markets such as securities exchanges or third-party providers such as Financial data vendors.
For example, a company such as Bloomberg is a financial software, news, and data company that offers financial software tools through the Bloomberg Terminal. Another example is Reuters whose products specialize in financial information management, purchase order management, positions and risks, and financial instrument sales.
With the help of these software companies, there is efficiency and proper management of transactions both in the front and back offices of banking firms and other financial institutions.
Financial software or financial system software is special application software that records all the financial activity within a business organization. Basic features of this system not only includes all the download of accounting software like accounts payable, accounts receivable, ledger, reporting modules and payroll but also to explore alternative investment choices and calculate statistical relationships.[1] Features of the system may vary depending on what type of business it is being used for. Primarily, the goal of the financial software is to record, categorize, analyze, compile, interpret and then present an accurate and updated financial dates for every transaction of the business.[2]
Pipeline tracking is one of the key features of an accounting system and software for asset management. This provides summarized information on all the details pertaining to the potential investments that are being monitored. The system and software will organize the pipeline and record the source, execution status, approval status, feasible investment capital and the targeted purchase price. It provides an efficient analysis of the best deals, timing and price for the utilization of the investment team. Pipeline tracking provides tracking of the source, history and status. It also provides customized classifications and categories. The system can easily execute a cash flow model and create return assumptions.[3]
Asset management is another important feature of a financial software. It uses the updated status of the investment to provide the necessary tools in creating every possible outcome, such as future payments that are distressed, conversions for debt to equity and maturity of loans. It also provides an efficient tracking of payment dates and rates. Updated financial statements are readily available, which makes it very easy to determine credit standing and as a result make proper adjustments on the projections. Asset management can modify dates of payment, conversion of floating to fixed rates, deferred payments, interest rates, maturity extensions and change schedules for repayment. It also uses various assumptions to store and run multiple cases such as downside, base and upside.[4]
Another feature of the financial software, is the data warehousing. This feature syncs the accounting system and retrieves investment transactions. It also uses a customizable category or name. This key feature allows an effortless re-categorization of the investments. The calculation of investment statistics will depend on the computation of the user at any given moment. Customized reports on the performance of the investment can be created using any of the calculations programmed in the database of the system. There are approximately more than 100 various calculations programmed in the system. If there is a need for more calculations, then it can easily be provided for the user. Several investments can be categorized into subgroups or groups and they will be used for the creation of totals. It identifies the total price of the fixed income in comparison to the equity investments.
There are many benefits of the fund management feature of the financial software. Fund management can determine which specific investments are creating returns. It also pinpoints the availability or shortage of cash. It provides an evaluation of the leverage options as well as determines the effect on the returns. The system also creates a central and systematic modelling, management and analysis of the funds.
The benefits of asset management include the creation of a platform for maintaining the projection of cash flow on investments. Excel models also eliminate the possibility of manual errors. Asset management increases the scale and adds investments without the loss of analytic perspective. Through this software managers of portfolios have a better grasp on the performance level of the investment. It also provides accurate information on whether the prediction of the analysts was close to the actual performance of the investment.[citation needed]
Workforce management provides a common set of performance-based tools and software to support corporate management, front-line supervisors, store managers and workers across manufacturing, distribution, transportation, and retail operations. It is sometimes referred to as HRM systems, Workforce asset management, or part of ERP systems.
Workforce management solutions[buzzword] can be deployed enterprise-wide and through mobile platforms. While special software is commonly used in numerous areas such as ERP (enterprise resource planning), SLM (service lifecycle management), CRM (customer relationship management) and HR (human resources) management, the management of the workforce is often still handled using spreadsheet programs or time recording. This often results in expensive overtime, non-productive idle times, high fluctuation rates, poor customer service and opportunity costs being incurred. By using a software solution[buzzword] for demand-oriented workforce management, planners can optimize staffing by creating schedules that at all times conform to the forecasted requirements.[10] At the same time, a workforce management solution[buzzword] helps users to observe all relevant legislations, local agreements and the contracts of individual employees – including work-life balance guidelines.[11]
A key aspect of workforce management is scheduling. This is achieved by establishing likely demand by analyzing historical data (such as the number and duration of customer contacts, sales figures, check-out transactions or orders to be handled). Many workforce management systems also offer manual adjustment capabilities. The calculated forecast values are then converted into actual staffing requirements by means of an algorithm that is adjusted to the particular use case. The algorithm itself is based on the work of Erlang though most modern adaptations of workforce management have shifted towards a richer state management, and optimizations to the original idea.
Depending on the sophistication of the software, it can manage estimation and planning, scheduling, cost control, budget management, resource allocation, collaboration software, communication, decision-making, quality management, time management and documentation or administration systems.
Numerous PC and browser-based project management software and contract management software products and services are available.
The first historically relevant year for the development of project management software was 1896, marked by the introduction of the Harmonogram. Polish economist Karol Adamiecki attempted to display task development in a floating chart and laid the foundation for project management software as it is today.[1] In 1912, Henry Gantt replaced the Harmonogram with the more advanced Gantt chart, a scheduling diagram that broke ship design tasks down for the purposes of Hoover Dam in early 1931.[citation needed] Today's Gantt charts are almost the same as their original counterparts and are a part of many project management systems.
In the period between 1965 and 1969, two of the leading project management associations were formed: the International Project Management Association (IPMA) in Europe, and the Project Management Institute (PMI) which trains project management professionals and issues certificates. With businesses shifting towards technology-based and paperless methods, the first project management systems started to emerge.[5] Oracle and Artemis launched their project managers in 1977, while Scitor Corporation did the same in 1979.[6][7] Many improvements followed in the upcoming decades. In 1986, Carnegie Mellon University’s Software Engineering Institute introduced capability maturity software, a five-level project management method for rapidly maturing processes, while in 1988, users were introduced to earned value management which added processes’ scope and cost to the schedule.[8] The trend continued with PRINCE2 (1996) which increased the number of processes to seven, because of which developers considered designing products for managing complex projects. In 2001, they adopted the Agile project management concept and focused on adaptive planning and flexible response to changes. In 2006, users were already able to trigger total cost management, a framework that helps control and reduce costs in project management.[9]
One of the most common project management software tool types is scheduling tools. Scheduling tools are used to sequence project activities and assign dates and resources to them. The detail and sophistication of a schedule produced by a scheduling tool can vary considerably with the project management methodology used, the features provided and the scheduling methods supported. Scheduling tools may include support for:[10]
Project planning software can be expected to provide information to various people or stakeholders, and can be used to measure and justify the level of effort required to complete the project(s). Typical requirements might include:
A collaborative system is designed to support multiple users modifying different sections of the plan at once; for example, updating the areas they personally are responsible for such that those estimates get integrated into the overall plan. Web-based tools, including extranets, generally fall into this category, but have the limitation that they can only be used when the user has live Internet access. To address this limitation, some software tools using client–server architecture provide a rich client that runs on users' desktop computer and replicates project and task information to other project team members through a central server when users connect periodically to the network. Some tools allow team members to check out their schedules (and others' as read only) to work on them while not on the network. When reconnecting to the database, all changes are synchronized with the other schedules.
A common problem in project management is a difficulty with both viewing and understanding large amounts of fluctuating project data.[11]  To tackle this, some project management software utilize information visualization, so that users can more easily find, analyze and make changes to their data. To avoid information overload,[12] the visualization mantra of “overview first, zoom and filter, then details on demand” is often followed.[13]
A digital calendar is a collaborative or personal time management software with a calendar that can be used to keep track of planned events. The calendar can also contain an appointment book, address book or contact list. Common features of digital calendars are that users can:
There are several varieties of digital calendars. Some have the ability to be connected or synchronized with other calendars across different vendors. The iCalendar 1.0 and 2.0 specifications and its associated standards have been a cornerstone of the standardization and interoperability of calendar software across different vendors. A digital calendar can be viewed as an extension of many of the features provided by time management software such as desk accessory packages and computer office automation systems.[citation needed]
Today, digital calendars are common on many desktop environments, office suites, and in collaborative software for personal computers (desktop and laptop) as well as smartphones and tablets.
Google Calendar is shipped with most Android phones and available in web.
An appointment calendar is a list of appointments and the attendees for the appointments. This software may include the capability of detecting scheduling conflicts, notifying the participants of the 
conflict, and suggesting alternate meeting times.
Some appointment software has the ability to automatically suggest shortened events by default or in cases when there are back-to-back meetings in a calendar.[4] For example, instead of a 30 minute meeting at 13:00, a calendar software may suggest that an event is shortened to 25 minutes and either starts late (for example 5 minutes late at 13:05) or ends early (for example 13:25). Some software may have the options of combining both start late and end early. Benefits of this includes minimising the risk of needing to wait for participants who otherwise might be late for the meeting startup due to being hung up in wrapping up a previous meeting, as well as alleviating further propagation of this issue to the meetings following after. Such time buffers between meetings also gives attendees a short break between meetings.[5]
Collaborative scheduling means that the software suggests meeting times when all participants are free. In some cases, it is also possible for the invitees to suggest times that suit them best, so that the organizer can choose a meeting time that works well for all the participants.
In order for the software to be able to propose meeting times for participants across different organisations, the invitees' calendars must be set up to share outside their organisation when they are free.
Calendars themselves can basically be seen as a passive reminder that the user has to check themselves, and can be seen as a form of reminder software. It is often also possible to set up active reminders. Various reminder techniques can be used, such as pop-up notifications, dialog boxes or auditory alarms 15 minutes or 1 hour before an event.
Employee scheduling software automates the process of creating and maintaining a schedule. Automating the scheduling of employees increases productivity and allows organizations with hourly workforces to re-allocate resources to non-scheduling activities. Such software will usually track vacation time, sick time, compensation time, and alert when there are conflicts.[1]  As scheduling data is accumulated over time, it may be extracted for payroll or to analyze past activity. Although employee scheduling software may or may not make optimization decisions, it does manage and coordinate the tasks.[2][3] Today's employee scheduling software often includes mobile applications. Mobile scheduling further increased scheduling productivity and eliminated inefficient scheduling steps.[2] It may also include functionality including applicant tracking and on-boarding, time and attendance, and automatic limits on overtime.[citation needed] Such functionality can help organizations with issues like employee retention, compliance with labor laws, and other workforce management challenges.
A theoretical underpinning of an employee scheduling problem can be represented as the Nurse scheduling problem, which is NP-hard. The theoretical complexity of the problem is a significant factor in the development of various software solutions. This is because systems must take into account many different forms of schedules that could be worked, and allocate employees to the correct schedule.[4] Ultimately, optimization of scheduling is to minimize costs, but also often requires a reciprocal approach from management instead of complete reliance on software.[2]
Prior to employee scheduling software companies would use physical mediums for tracking employee hours and work schedule. This then gave rise to data storage forms that later by the 80s were compatible with computer programs and software. These forms however never actually scheduled the employees, it just kept track of the employees work week, hours, and prior work schedules. This then gave way to the idea of employee scheduling software, which would be an all-inclusive system that would store and track employee work history, along with actually scheduling the employee's work week.
During the 1960s, the punched card was gradually replaced as the primary means for data storage by magnetic tape, as better, more capable computers became available. Mohawk Data Sciences introduced a magnetic tape encoder in 1965, a system marketed as a keypunch replacement which was somewhat successful, but punched cards were still commonly used for data entry and programming until the mid-1980s when the combination of lower cost magnetic disk storage, and affordable interactive terminals on less expensive minicomputers made punched cards obsolete for this role as well.[7] However, their influence lives on through many standard conventions and file formats.[8]
In the 2010s, the wide adoption of mobile devices and the rise of 3G, 4G, and 5G networks[9] worldwide has made it possible to approach the task of scheduling differently. In the last decade, many software solutions have sprung up to make the lives of business owners and managers easier and less burdensome.
The first wave of solutions helped small business owners to schedule, manage, and communicate with their employees in a more streamlined way. The newer way of solutions go a step further, leveraging machine learning and are being built on even newer cloud technologies.[citation needed] The need for automation and intelligent rostering in workforce management will continue to grow as society's heads into a gig economy.[10]
Algorithms are used within the employee scheduling software in order to determine not only who is working, but also the specific jobs and tasks required of the workers. The system still must be monitored, and any further issues with assigning of specifics is done manually.[11] Within the context of roster problems and models, there are three main factors to work out the differences: the integration of days off scheduling with line of work construction and task assignment, roster construction, and demand type.[2] These complexities thusly require that each and every workplace must optimize employee scheduling software based on their own unique set of rules, issues and needs.[11] Additionally, it is difficult to determine optimal solution that minimize costs, meet employee preferences, distribute shifts equitably among employees and satisfy all the workplace constraints. In many organizations, the people involved in developing rosters need decision support tools to help provide the right employees at the right time and the right cost while achieving a high level of employee satisfaction.[2] Due to constant change within work environments, new models and algorithms must be created in order to allow for flexibility as needs and demands arise. For example, when a large number of new employees are hired, as in the total workforce is increased, the scheduling software likely will need to be updated in order to allow for such a change.[4]
Although employee scheduling software won't necessarily improve business practices by itself, it does automate typically tedious business administration. It can also have positive effects on aspects of the business indirectly, including employee engagement, employee retention,[12] and lowered labor costs. By providing management with large amounts of data, this software can assist management in making decisions and automatically create a work schedule that fits as many constraints as possible.[13] Also, the software may be a part of an ERP package or other human resource management system.[14][15][16]
As the modern workplace becomes more complex, it is likely that rostering will need to be more flexible to cater to more individualistic preferences.[2] Artificial intelligence also looks to play a bigger role in scheduling software, requiring less oversight by management to correct issues.[1]
Software art is a work of art where the creation of software, or concepts from software, play an important role; for example software applications which were created by artists and which were intended as artworks. As an artistic discipline software art has attained growing attention since the late 1990s. It is closely related to Internet art since it often relies on the Internet, most notably the World Wide Web, for dissemination and critical discussion of the works. Art festivals such as FILE Electronic Language International Festival (São Paulo), Transmediale (Berlin), Prix Ars Electronica (Linz) and readme (Moscow, Helsinki, Aarhus, and Dortmund) have devoted considerable attention to the medium and through this have helped to bring software art to a wider audience of theorists and academics.
The demoscene (/ˈdɛmoʊˌsiːn/) is an international computer art subculture focused on producing demos: self-contained, sometimes extremely small, computer programs that produce audiovisual presentations. The purpose of a demo is to show off programming, visual art, and musical skills. Demos and other demoscene productions (graphics, music, videos, games) are shared, voted on and released online at festivals known as demoparties.
The scene started with the home computer revolution of the early 1980s, and the subsequent advent of software cracking.[1] Crackers altered the code of computer games to remove copy protection, claiming credit by adding introduction screens of their own ("cracktros"). They soon started competing for the best visual presentation of these additions.[2] Through the making of intros and stand-alone demos, a new community eventually evolved, independent of the gaming[3]: 29–30  and software sharing scenes.
Demos in the demoscene sense began as software crackers' "signatures", that is, crack screens and crack intros attached to software whose copy protection was removed. The first crack screens appeared on the Apple II in the early 1980s, and they were often nothing but plain text screens crediting the cracker or their group. Gradually, these static screens evolved into increasingly impressive-looking introductions containing animated effects and music. Eventually, many cracker groups started to release intro-like programs separately, without being attached to unlicensed software.[6] These programs were initially known by various names, such as letters or messages, but they later came to be known as demos.[citation needed]
Many European game programmers, artists, and musicians have come from the demoscene, often cultivating the learned techniques, practices and philosophies in their work. For example, the Finnish company Remedy Entertainment, known for the Max Payne series of games, was founded by the PC group Future Crew, and most of its employees are former or active Finnish demosceners.[50][51]
Sometimes demos even provide direct influence even to game developers that have no demoscene affiliation: for instance, Will Wright names demoscene as a major influence on the Maxis game Spore, which is largely based on procedural content generation.[52] Similarly, at QuakeCon in 2011, John Carmack noted that he "thinks highly" of people who do 64k intros, as an example of artificial limitations encouraging creative programming.[53] Jerry Holkins from Penny Arcade claimed to have an "abiding love" for the demoscene, and noted that it is "stuff worth knowing".[54]
This is a list of educational software that is computer software whose primary purpose is teaching or self-learning.
Reference software is software which emulates and expands upon print reference forms including the dictionary, translation dictionary, encyclopaedia, thesaurus, and atlas. Like print references, reference software can either be general or specific to a domain, and often includes maps and illustrations, as well as bibliography and statistics. Reference software may include multimedia content including animations, audio, and video, which further illustrate a concept. Well designed reference software improves upon the navigability of print references, through the use of search functionality and hyperlinks.
The opportunities brought by new media enticed new competitors into the reference software market. One of the earliest and most well-known was Microsoft Encarta,[3] first introduced on CD-ROM and then also moving online along with other major reference works. In the dictionaries market, one of the more prolific brands was Merriam-Webster, which released CD-ROM and then online versions of English dictionaries, thesauri and foreign language dictionaries.
Wikipedia and its spin-offs (such as Wiktionary) marked a new departure in educational reference software.[5] Previous encyclopedias and dictionaries had compiled their contents based on invited and closed teams of specialists. The Wiki concept allowed anyone and everyone to join in creating and editing an online set of reference works.[6][7]
AIOps (Artificial Intelligence for IT Operations) refers to the use of artificial intelligence, machine learning, and big data analytics to automate and enhance data center management. It helps organizations manage complex IT environments by detecting, diagnosing, and resolving issues more efficiently than traditional methods.[1]
AIOPs was first defined by Gartner in 2016,[2] combining "artificial intelligence" and "IT operations" to describe the application of AI and machine learning to enhance IT operations. This concept was introduced to address the increasing complexity and data volume in IT environments, aiming to automate processes such as event correlation, anomaly detection, and causality determination.
AIOps refers to the multi-layered complex technology platforms which enhance and automate IT operations by using machine learning and analytics to analyze the large amounts of data collected from various DevOps devices and tools, automatically identifying and responding to issues in real-time.[3] AIOps is used as a shift from isolated IT data to aggregated observational data (e.g., job logs and monitoring systems) and interaction data (such as ticketing, events, or incident records) within a big data platform[4] AIOps applies machine learning and analytics to this data. The result is continuous visibility, which, combined with the implementation of automation, can lead to ongoing improvements.[5] AIOps connects three IT disciplines (automation, service management, and performance management) to achieve continuous visibility and improvement. This new approach in modern, accelerated, and hyper-scaled IT environments leverages advances in machine learning and big data to overcome previous limitations.[6]
AIOps tools use big data analytics, machine learning algorithms, and predictive analytics to detect anomalies, correlate events, and provide proactive insights. This automation reduces the burden on IT teams, allowing them to focus on strategic tasks rather than routine operational issues. AIOps is widely used by IT operations teams, DevOps, network administrators, and IT service management (ITSM) teams to enhance visibility and enable quicker incident resolution in hybrid cloud environments, data centers, and other IT infrastructures.[21]
In contrast to MLOps (Machine Learning Operations), which focuses on the lifecycle management and operational aspects of machine learning models, AIOps focuses on optimizing IT operations using a variety of analytics and AI-driven techniques. While both disciplines rely on AI and data-driven methods, AIOps primarily targets IT operations, whereas MLOps is concerned with the deployment, monitoring, and maintenance of ML models.[22]
In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term "database" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.
Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.
Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.
Formally, a "database" refers to a set of related data accessed through the use of a "database management system" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.
Because of the close relationship between them, the term "database" is often used casually to refer to both a database and the DBMS used to manipulate it.
Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.[1]
Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:
Both a database and its DBMS conform to the principles of a particular database model.[5] "Database system" refers collectively to the database model, database management system, and database.[6]
Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.[citation needed]
Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.
The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational,[8] SQL/relational, and post-relational.
The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018[update] they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS.[9] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.[citation needed]
Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch, which led to the coining of the term "post-relational" and also the development of hybrid object–relational databases.
The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases. A competing "next generation" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.
The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term "data-base" in a specific technical sense.[10]
As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market.
Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications.
IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014[update].[11]
Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that were primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a "search" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[12]
In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of "tables", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each "fact" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated.
The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit.
For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.
As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic.
Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a "language" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.
In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.
Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata).
IBM started working on a prototype system loosely based on Codd's concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large "chunk". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL[citation needed] – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (IBM Db2).
Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979.[18]
Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).
Another data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.[citation needed]
The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation."[19] dBASE was one of the top selling software titles in the 1980s and early 1990s.
The 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their attributes and not to individual fields.[20] The term "object–relational impedance mismatch" described the inconvenience of translating between programmed objects and database tables. Object databases and object–relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object–relational mappings (ORMs) attempt to solve the same problem.
XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.
NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.
In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.
NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.
Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).
Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.
One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.
Connolly and Begg define database management system (DBMS) as a "software system that enables users to define, create, maintain and control access to the database."[24] Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access.
The DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object–relational model. Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems.
It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities.[26] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.
Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.
Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.[28]
A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.
External interaction with the database will be via an application program that interfaces with the DBMS.[29] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information.
A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.
Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:
Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, "data about the data", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database.[33] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. "storage engine". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).
Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.
Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.
Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.
Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.
With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.[34]
Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).
Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.
This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called "subschemas". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.
Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause.[35]
Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).
The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.
A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs.
After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).
When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.
After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.
Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.
Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[36] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc.
Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as "DevOps for database".[37]
The first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity–relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like "can a customer also be a supplier?", or "if a product is sold with two different forms of packaging, are those the same product or different products?", or "if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.
Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.
Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).
The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary "fact" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.
The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.
Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.
A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.
While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database.
The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model.[39] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.
The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.
Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more.
The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).
3D computer graphics rely on many of the same algorithms as 2D computer vector graphics in the wire-frame model and 2D computer raster graphics in the final rendered display. In computer graphics software, 2D applications may use 3D techniques to achieve effects such as lighting, and similarly, 3D may use some 2D rendering techniques.
3-D computer graphics software began appearing for home computers in the late 1970s. The earliest known example is 3D Art Graphics, a set of 3-D computer graphics effects, written by Kazumasa Mitazawa and released in June 1978 for the Apple II.[6][7]
Rendering converts a model into an image either by simulating light transport to get photo-realistic images, or by applying an art style as in non-photorealistic rendering. The two basic operations in realistic rendering are transport (how much light gets from one place to another) and scattering (how surfaces interact with light). This step is usually performed using 3-D computer graphics software or a 3-D graphics API.
Altering the scene into a suitable form for rendering also involves 3D projection, which displays a three-dimensional image in two dimensions. Although 3-D modeling and CAD software may perform 3-D rendering as well (e.g., Autodesk 3ds Max or Blender), exclusive 3-D rendering software also exists (e.g., OTOY's Octane Rendering Engine, Maxon's Redshift)
3-D computer graphics software produces computer-generated imagery (CGI) through 3D modeling and 3D rendering or produces 3-D models for analytical, scientific and industrial purposes.
There are many varieties of files supporting 3-D graphics, for example, Wavefront .obj files and .x DirectX files. Each file type generally tends to have its own unique data structure.
3-D modeling software is a class of 3-D computer graphics software used to produce 3-D models. Individual programs of this class are called modeling applications or modelers.
Computer aided design software may employ the same fundamental 3-D modeling techniques that 3-D modeling software use but their goal differs. They are used in computer-aided engineering, computer-aided manufacturing, Finite element analysis, product lifecycle management, 3D printing and computer-aided architectural design.
After producing a video, studios then edit or composite the video using programs such as Adobe Premiere Pro or Final Cut Pro at the mid-level, or Autodesk Combustion, Digital Fusion, Shake at the high-end. Match moving software is commonly used to match live video with computer-generated video, keeping the two in sync as the camera moves.
Not all computer graphics that appear 3D are based on a wireframe model. 2D computer graphics with 3D photorealistic effects are often achieved without wire-frame modeling and are sometimes indistinguishable in the final form. Some graphic art software includes filters that can be applied to 2D vector graphics or 2D raster graphics on transparent layers. Visual artists may also copy or visualize 3D effects and manually render photo-realistic effects without the use of filters.
Computer-generated animation is an umbrella term for three-dimensional (3D) animation, and 2D computer animation. These also include subcategories like asset driven, hybrid, and digital drawn animation. Creators animate using code or software instead of pencil-to-paper drawings. There are many techniques and disciplines in computer generated animation, some of which are digital representations of traditional animation - such as key frame animation - and some of which are only possible with a computer - such fluid simulation.
'CG' Animators can break physical laws by using mathematical algorithms to cheat mass, force and gravity, and more. Fundamentally, computer-generated animation is a powerful tool which can improve the quality of animation by using the power of computing to unleash the animator's imagination. This is because Computer Generated Animation allows for things like onion skinning which allows 2D animators to see the flow of their work all at once, and interpolation which allows 3D animators to automate the process of inbetweening.
Before becoming a final product, 3D computer animations only exist as a series of moving shapes and systems within 3d software, and must be rendered. This can happen as a separate process for animations developed for movies and short films, or it can be done in real-time when animated for videogames. After an animation is rendered, it can be composited into a final product.
3D animation software interpolates between keyframes by generating a spline between keys plotted on a graph which represents the animation. Additionally, these splines can follow Bézier curves to control how the spline curves relative to the keyframes. Using interpolation allows 3D animators to dynamically change animations without having to redo all the in-between animation. This also allows the creation of complex movements such as ellipses with only a few keyframes. Lastly, interpolation allows the animator to change the framerate, timing, and even scale of the movements at any point in the animation process.
Another way to automate 3D animation is to use procedural tools such as 4D noise. Noise is any algorithm that plots pseudo-random values within a dimensional space.[10] 4D noise can be used to do things like move a swarm of bees around; the first three dimensions correspond to the position of the bees in space, and the fourth is used to change the bee's position over time. Noise can also be used as a cheap replacement for simulation. For example, smoke and clouds can be animated using noise.
There are many different disciplines of 3D animation, some of which include entirely separate artforms. For example, hair simulation for computer animated characters in and of itself is a career path which involves separate workflows,[11] and different software and tools. The combination of all or some 3D computer animation disciplines is commonly referred to within the animation industry as the 3D animation pipeline.[12]
Computer-assisted animation is usually classed as two-dimensional (2D) animation and is also known as digital ink and paint. Drawings are either hand drawn (pencil to paper) or interactively drawn (on the computer) using different assisting appliances and are positioned into specific software packages. Within the software package, the creator places drawings into different key frames which fundamentally create an outline of the most important movements.[17] The computer then fills in the "in-between frames", a process commonly known as Tweening.[18] Computer-assisted animation employs new technologies to produce content faster than is possible with traditional animation, while still retaining the stylistic elements of traditionally drawn characters or objects.[19]
Computer animation can be created with a computer and an animation software. Some impressive animation can be achieved even with basic programs; however, the rendering can require much time on an ordinary home computer.[56] Professional animators of movies, television and video games could make photorealistic animation with high detail. This level of quality for movie animation would take hundreds of years to create on a home computer. Instead, many powerful workstation computers are used;[57] Silicon Graphics said in 1989 that the animation industry's needs typically caused graphical innovations in workstations.[58] Graphics workstation computers use two to four processors, and they are a lot more powerful than an actual home computer and are specialized for rendering. Many workstations (known as a "render farm") are networked together to effectively act as a giant computer,[59] resulting in a computer-animated movie that can be completed in about one to five years (however, this process is not composed solely of rendering). A workstation typically costs $2,000 to $16,000 with the more expensive stations being able to render much faster due to the more technologically advanced hardware that they contain. Professionals also use digital movie cameras, motion/performance capture, bluescreens, film editing software, props, and other tools used for movie animation. Programs like Blender allow for people who can not afford expensive animation and rendering software to be able to work in a similar manner to those who use the commercial grade equipment.[60]
The popularity of websites that allow members to upload their own movies for others to view has created a growing community of independent and amateur computer animators.[77] With utilities and programs often included free with modern operating systems, many users can make their own animated movies and shorts. Several free and open-source animation software applications exist as well. The ease at which these animations can be distributed has attracted professional animation talent also. Companies such as PowToon and Vyond attempt to bridge the gap by giving amateurs access to professional animations as clip art.
The oldest (most backward compatible) web-based animations are in the animated GIF format, which can be uploaded and seen on the web easily.[78] However, the raster graphics format of GIF animations slows the download and frame rate, especially with larger screen sizes. The growing demand for higher quality web-based animations was met by a vector graphics alternative that relied on the use of a plugin. For decades, Flash animations were a common format, until the web development community abandoned support for the Flash Player plugin. Web browsers on mobile devices and mobile operating systems never fully supported the Flash plugin.
Audio editing software is any software or computer program which allows editing and generating audio data.[1] Audio editing software can be implemented completely or partly as a library, as a computer application, as a web application, or as a loadable kernel module. Wave editors are digital audio editors. There are many sources of software available to perform this function. Most can edit music, apply effects and filters, and adjust stereo channels.
A digital audio workstation (DAW) is software-based and typically comprises multiple software suite components, all accessible through a unified graphical user interface. DAWs are used for recording or producing  music, sound effects and more.[2]
Audio plug-ins are small software programs that can be "plugged in" to use inside the main workstation. Plug-ins are used in DAWs to allow more capabilities when it comes to audio editing.[3]  There are several different types of plug-ins. For example, stock plug-ins are the plug-ins that come already installed with a DAW, and Virtual Studio Technology (VST) plug-ins. Invented by Steinberg, VST plug-ins allow producers to apply simple or advanced effects such as filters, limiting, compression, reverb, flanging, panning, noise reduction, and equalizers.[3]
In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name "compiler" is primarily used for programs that translate source code from a high-level programming language to a low-level programming language (e.g. assembly language, object code, or machine code) to create an executable program.[1][2]: p1 [3]
There are many different types of compilers which produce output in different useful forms. A cross-compiler  produces code for a different CPU or operating system than the one on which the cross-compiler itself runs.  A bootstrap compiler is often a temporary compiler, used for compiling a more permanent or better optimised compiler for a language.
Related software include decompilers, programs that translate from low-level languages to higher level ones; programs that translate between high-level languages, usually called source-to-source compilers or transpilers; language rewriters, usually programs that translate the form of expressions without a change of language; and compiler-compilers, compilers that produce compilers (or parts of them), often in a generic and reusable way so as to be able to produce many differing compilers.
Low-level programming languages, such as assembly and C, are typically compiled, especially when speed is a significant concern, rather than cross-platform support. For such languages, there are more one-to-one correspondences between the source code and the resulting machine code, making it easier for programmers to control the use of hardware.
In theory, a programming language can be used via either a compiler or an interpreter, but in practice, each language tends to be used with only one or the other. None-the-less, it is possible to write a compiler for a languages that is commonly interpreted. For example, Common Lisp can be compiled to Java bytecode (then interpreted by the Java virtual machine), C code (then compiled to native machine code), or directly to native code.
Backus–Naur form (BNF) describes the syntax of "sentences" of a language. It was developed by John Backus and used for the syntax of Algol 60.[7] The ideas derive from the context-free grammar concepts by linguist Noam Chomsky.[8] "BNF and its extensions have become standard tools for describing the syntax of programming notations. In many cases, parts of compilers are generated automatically from a BNF description."[9]
Between 1942 and 1945, Konrad Zuse designed the first (algorithmic) programming language for computers called Plankalkül ("Plan Calculus").  Zuse also envisioned a Planfertigungsgerät ("Plan assembly device") to automatically translate the mathematical formulation of a program into machine-readable punched film stock.[10] While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s.[11] APL is a language for mathematical computations.
High-level language design during the formative years of digital computing provided useful programming tools for a variety of applications:
Early operating systems and software were written in assembly language. In the 1960s and early 1970s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C.
BCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool.[30] Several compilers have been implemented, Richards' book provides insights to the language and its compiler.[31] BCPL was not only an influential systems programming language that is still used in research[32] but also provided a basis for the design of B and C languages.
BLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W. A. Wulf's Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970.
Multics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbató from MIT.[33] Multics was written in the PL/I language developed by IBM and IBM User Group.[34] IBM's goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented.[35] For the first few years of the Multics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs.[36] EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.[37]
Bell Labs left the Multics project in 1969, and developed a system programming language B based on BCPL concepts, written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix.
Object-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science.[41] Bell Labs became interested in OOP with the development of C++.[42] C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983.[43] The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew.
In many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex.
PQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure.[45] The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation.[46] Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the (since 1995, object-oriented) programming language Ada.
Other Ada compiler efforts got underway in Britain at the University of York and in Germany at the University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation.[48] There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment.
High-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.[49]
"When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security."[50] The "Compiler Research: The Next 50 Years" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets.
A compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.
The front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.
Interprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.
Compiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification.[58] Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.
Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language – for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.
A native or hosted compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.
The lower level language that is the target of a compiler may itself be a high-level programming language. C, viewed by some as a sort of portable assembly language, is frequently the target language of such compilers. For example, Cfront, the original compiler for C++, used C as its target language. The C code generated by such a compiler is usually not intended to be readable and maintained by humans, so indent style and creating pretty C intermediate code are ignored. Some of the features of C that make it a good target language include the #line directive, which can be generated by the compiler to support debugging of the original source, and the wide platform support available with C compilers.
Database security  concerns the use of a broad range of information security controls to protect databases  against compromises of their confidentiality, integrity and availability.[1] It involves various types or categories of controls, such as technical, procedural or administrative, and physical.
Ross J. Anderson has often said that by their nature large databases will never be free of abuse by breaches of security; if a large system is designed for ease of access it becomes insecure; if made watertight it becomes impossible to use. This is sometimes known as Anderson's Rule.[2]
Databases have been largely secured against hackers through network security measures such as firewalls, and network-based intrusion detection systems. While network security controls remain valuable in this regard, securing the database systems themselves, and the programs/functions and data within them, has arguably become more critical as networks are increasingly opened to wider access, in particular access from the Internet. Furthermore, system, program, function and data access controls, along with the associated user identification, authentication and rights management functions, have always been important to limit and in some cases log the activities of authorized users and administrators. In other words, these are complementary approaches to database security, working from both the outside-in and the inside-out as it were.
Many organizations develop their own "baseline" security standards and designs detailing basic security control measures for their database systems. These may reflect general information security requirements or obligations imposed by corporate information security policies and applicable laws and regulations (e.g. concerning privacy, financial management and reporting systems), along with generally accepted good database security practices (such as appropriate hardening of the underlying systems) and perhaps security recommendations from the relevant database system and software vendors. The security designs for specific database systems typically specify further security administration and management functions (such as administration and reporting of user access rights, log management and analysis, database replication/synchronization and backups) along with various business-driven information security controls within the database programs and functions (e.g. data entry validation and audit trails). Furthermore, various security-related activities (manual controls) are normally incorporated into the procedures, guidelines etc. relating to the design, development, configuration, use, management and maintenance of databases.
Two types of privileges are important relating to database security within the database environment: system privileges and object privileges.
Object privileges allow for the use of certain operations on database objects as authorized by another user. Examples include: usage, select, insert, update, and references.[3]
Databases that fall under internal controls (that is, data used for public reporting, annual reports, etc.) are subject to the separation of duties, meaning there must be segregation of tasks between development, and production. Each task has to be validated (via code walk-through/fresh eyes) by a third person who is not writing the actual code. The database developer should not be able to execute anything in production without an independent review of the documentation/code for the work that is being performed. Typically, the role of the developer is to pass code to a DBA; however, given the cutbacks that have resulted from the economic downturn, a DBA might not be readily available. If a DBA is not involved, it is important, at minimum, for a peer to conduct a code review. This ensures that the role of the developer is clearly separate.[citation needed]
One technique for evaluating database security involves performing vulnerability assessments or penetration tests against the database. Testers attempt to find security vulnerabilities that could be used to defeat or bypass security controls, break into the database, compromise the system etc. Database administrators or information security administrators may for example use automated vulnerability scans to search out misconfiguration of controls (often referred to as 'drift') within the layers mentioned above along with known vulnerabilities within the database software. The results of such scans are used to harden the database (improve security) and close off the specific vulnerabilities identified, but other vulnerabilities often remain unrecognized and unaddressed.
In database environments where security is critical, continual monitoring for compliance with standards improves security. Security compliance requires, amongst other procedures, patch management and the review and management of permissions (especially public) granted to objects within the database. Database objects may include table or other objects listed in the Table link. The permissions granted for SQL language commands on objects are considered in this process. Compliance monitoring is similar to vulnerability assessment, except that the results of vulnerability assessments generally drive the security standards that lead to the continuous monitoring program. Essentially, vulnerability assessment is a preliminary procedure to determine risk where a compliance program is the process of on-going risk assessment.
The compliance program should take into consideration any dependencies at the application software level as changes at the database level may have effects on the application software or the application server.
Application level authentication and authorization mechanisms may be effective means of providing abstraction from the database layer. The primary benefit of abstraction is that of a single sign-on capability across multiple databases and platforms. A single sign-on system stores the database user's credentials and authenticates to the database on behalf of the user. Abstraction is the idea of making complex ideas easier to understand.
Another security layer of a more sophisticated nature includes real-time database activity monitoring, either by analyzing protocol traffic (SQL) over the network, or by observing local database activity on each server using software agents, or both. Use of agents or native logging is required to capture activities executed on the database server, which typically include the activities of the database administrator. Agents allow this information to be captured in a fashion that can not be disabled by the database administrator, who has the ability to disable or modify native audit logs.
Analysis can be performed to identify known exploits or policy breaches, or baselines can be captured over time to build a normal pattern used for detection of anomalous activity that could be indicative of intrusion. These systems can provide a comprehensive database audit trail in addition to the intrusion detection mechanisms, and some systems can also provide protection by terminating user sessions and/or quarantining users demonstrating suspicious behavior. Some systems are designed to support separation of duties (SOD), which is a typical requirement of auditors. SOD requires that the database administrators who are typically monitored as part of the DAM, not be able to disable or alter the DAM functionality. This requires the DAM audit trail to be securely stored in a separate system not administered by the database administration group.
In addition to using external tools for monitoring or auditing, native database audit capabilities are also available for many database platforms. The native audit trails are extracted on a regular basis and transferred to a designated security system where the database administrators do/should not have access. This ensures a certain level of segregation of duties that may provide evidence the native audit trails were not modified by authenticated administrators, and should be conducted by a security-oriented senior DBA group with read rights into production. Turning on native impacts the performance of the server. Generally, the native audit trails of databases do not provide sufficient controls to enforce separation of duties; therefore, the network and/or kernel module level host based monitoring capabilities provides a higher degree of confidence for forensics and preservation of evidence.
A good database security program includes the regular review of privileges granted to user accounts and accounts used by immediate processes. For individual accounts a two-factor authentication system improves security but adds complexity and cost. Accounts used by automated processes require appropriate controls around password storage such as sufficient encryption and access controls to reduce the risk of compromise.
In conjunction with a sound database security program, an appropriate disaster recovery program can ensure that service is not interrupted during a security incident, or any incident that results in an outage of the primary database environment. An example is that of replication for the primary databases to sites located in different geographical regions.[5]
After an incident occurs, database forensics can be employed to determine the scope of the breach, and to identify appropriate changes to systems and processes.
A database model is a type of data model that determines the logical structure of a database. It fundamentally determines in which manner data can be stored, organized and manipulated. The most popular example of a database model is the relational model, which uses a table-based format.
A given database management system may provide one or more models. The optimal structure depends on the natural organization of the application's data, and on the application's requirements, which include transaction rate (speed), reliability, maintainability, scalability, and cost.  Most database management systems are built around one particular data model, although it is possible for products to offer support for more than one model.
Various physical data models can implement any given logical model. Most database software will offer the user some level of control in tuning the physical implementation, since the choices that are made have a significant effect on performance.
The flat (or table) model consists of a single, two-dimensional array of data elements, where all members of a given column are assumed to be similar values, and all members of a row are assumed to be related to one another. For instance, columns for name and password that might be used as a part of a system security database. Each row would have the specific password associated with an individual user. Columns of the table often have a type associated with them, defining them as character data, date or time information, integers, or floating point numbers.  This tabular format is a precursor to the relational model.
In a hierarchical model, data is organized into a tree-like structure, implying a single parent for each record. A sort field keeps sibling records in a particular order. Hierarchical structures were widely used in the early mainframe database management systems, such as the Information Management System (IMS) by IBM, and now describe the structure of XML documents. This structure allows one-to-many relationship between two types of data. This structure is very efficient to describe many relationships in the real world; recipes, table of contents, ordering of paragraphs/verses, any nested and sorted information.
This hierarchy is used as the physical order of records in storage. Record access is done by navigating downward through the data structure using pointers combined with sequential accessing.  Because of this, the hierarchical structure is inefficient for certain database operations when a full path (as opposed to upward link and sort field) is not also included for each record.  Such limitations have been compensated for in later IMS versions by additional logical hierarchies imposed on the base physical hierarchy.
The network model organizes data using two fundamental concepts, called records and sets. Records contain fields (which may be organized hierarchically, as in the programming language COBOL). Sets (not to be confused with mathematical sets) define one-to-many relationships between records: one owner, many members. A record may be an owner in any number of sets, and a member in any number of sets.
Although it is not an essential feature of the model, network databases generally implement the set relationships by means of pointers that directly address the location of a record on disk. This gives excellent retrieval performance, at the expense of operations such as database loading and reorganization.
Most object databases (invented in the 1990s) use the navigational concept to provide fast navigation across networks of objects, generally using object identifiers as "smart" pointers to related objects. Objectivity/DB, for instance, implements named one-to-one, one-to-many, many-to-one, and many-to-many named relationships that can cross databases. Many object databases also support SQL, combining the strengths of both models.
In an inverted file or inverted index, the  contents of the data are used as keys in a lookup table, and the values in the table are pointers to the location of each instance of a given content item.  This is also the logical structure of contemporary database indexes, which might only use the contents from a particular columns in the lookup table.  The inverted file data model can put indexes in a set of files next to existing flat database files, in order to efficiently directly access needed records in these files.
Notable for using this data model is the ADABAS DBMS of Software AG, introduced in 1970. ADABAS has gained considerable customer base and exists and supported until today. In the 1980s it has adopted the relational model and SQL in addition to its original tools and languages.
Document-oriented database Clusterpoint uses inverted indexing model to provide fast full-text search for XML or JSON data objects for example.
The relational model was introduced by E.F. Codd in 1970[2] as a way to make database management systems more independent of any particular application. It is a mathematical model defined in terms of predicate logic and set theory, and implementations of it have been used by mainframe, midrange and microcomputer systems.
The products that are generally referred to as relational databases in fact implement a model that is only an approximation to the mathematical model defined by Codd. Three key terms are used extensively in relational database models: relations, attributes, and domains. A relation is a table with columns and rows. The named columns of the relation are called attributes, and the domain is the set of values the attributes are allowed to take.
The basic data structure of the relational model is the table, where information about a particular entity (say, an employee) is represented in rows (also called tuples) and columns. Thus, the "relation" in "relational database" refers to the various tables in the database; a relation is a set of tuples. The columns enumerate the various attributes of the entity (the employee's name, address or phone number, for example), and a row is an actual instance of the entity (a specific employee) that is represented by the relation. As a result, each tuple of the employee table represents various attributes of a single employee.
All relations (and, thus, tables) in a relational database have to adhere to some basic rules to qualify as relations. First, the ordering of columns is immaterial in a table. Second, there can not be identical tuples or rows in a table. And third, each tuple will contain a single value for each of its attributes.
A relational database contains multiple tables, each similar to the one in the "flat" database model. One of the strengths of the relational model is that, in principle, any value occurring in two different records (belonging to the same table or to different tables), implies a relationship among those two records.  Yet, in order to enforce explicit integrity constraints,  relationships between records in tables can also be defined explicitly, by identifying or non-identifying parent-child relationships characterized by assigning cardinality (1:1, (0)1:M, M:M). Tables can also have a designated single attribute or a set of attributes that can act as a "key", which can be used to uniquely identify each tuple in the table.
A key that has an external, real-world meaning (such as a person's name, a book's ISBN, or a car's serial number) is sometimes called a "natural" key. If no natural key is suitable (think of the many people named Brown), an arbitrary or surrogate key can be assigned (such as by giving employees ID numbers). In practice, most databases have both generated and natural keys, because generated keys can be used internally to create links between rows that cannot break, while natural keys can be used, less reliably, for searches and for integration with other databases. (For example, records in two independently developed databases could be matched up by social security number, except when the social security numbers are incorrect, missing, or have changed.)
The dimensional model is a specialized adaptation of the relational model used to represent data in data warehouses in a way that data can be easily summarized using online analytical processing, or OLAP queries. In the dimensional model, a database schema consists of a single large table of facts that are described using dimensions and measures. A dimension provides the context of a fact (such as who participated, when and where it happened, and its type) and is used in queries to group related facts together. Dimensions tend to be discrete and are often hierarchical; for example, the location might include the building, state, and country. A measure is a quantity describing the fact, such as revenue. It is important that measures can be meaningfully aggregated—for example, the revenue from different locations can be added together.
Products offering a more general data model than the relational model are sometimes classified as post-relational.[3] Alternate terms include "hybrid database", "Object-enhanced RDBMS" and others. The data model in such products incorporates relations but is not constrained by E.F. Codd's Information Principle, which requires that
all information in the database must be cast explicitly in terms of values in relations and in no other way
Graph databases allow even more general structure than a network database; any node may be connected to any other node.
Multivalue databases are "lumpy" data, in that they can store exactly the same way as relational databases, but they also permit a level of depth which the relational model can only approximate using sub-tables. This is nearly identical to the way XML expresses data, where a given field/attribute can have multiple right answers at the same time. Multivalue can be thought of as a compressed form of XML.
In the 1990s, the object-oriented programming paradigm was applied to database technology, creating a new database model known as object databases.  This aims to avoid the object–relational impedance mismatch – the overhead of converting information between its representation in the database (for example as rows in tables) and its representation in the application program (typically as objects).  Even further, the type system used in a particular application can be defined directly in the database, allowing the database to enforce the same data integrity invariants.  Object databases also introduce the key ideas of object programming, such as encapsulation and polymorphism, into the world of databases.
A variety of these ways have been tried for storing objects in a database. Some[which?] products have approached the problem from the application programming end, by making the objects manipulated by the program persistent. This typically requires the addition of some kind of query language, since conventional programming languages do not have the ability to find objects based on their information content. Others[which?] have attacked the problem from the database end, by defining an object-oriented data model for the database, and defining a database programming language that allows full programming capabilities as well as traditional query facilities.
Object databases suffered because of a lack of standardization: although standards were defined by ODMG, they were never implemented well enough to ensure interoperability between products. Nevertheless, object databases have been used successfully in many applications: usually specialized applications such as engineering databases or molecular biology databases rather than mainstream commercial data processing. However, object database ideas were picked up by the relational vendors and influenced extensions made to these products and indeed to the SQL language.
An alternative to translating between objects and relational databases is to use an object–relational mapping (ORM) library.
In a relational database, a row or "record" or "tuple", represents a single, implicitly structured data item in a table. A database table can be thought of as consisting of rows and columns.[1] Each row in a table represents a set of related data, and every row in the table has the same structure.
In a relational database, a column is a set of data values of a particular type, one value for each row of a table.[1] A column may contain text values, numbers, or even pointers to files in the operating system.[2] Columns typically contain simple types, though some relational database systems allow columns to contain more complex data types, such as whole documents, images, or even video clips.[3][better source needed] A column can also be called an attribute.
Each row would provide a data value for each column and would then be understood as a single structured data value. For example, a database that represents company contact information might have the following columns: ID, Company Name, Address Line 1, Address Line 2, City, and Postal Code. More formally, a row is a tuple containing a specific value for each column,[4] for example: (1234, 'Big Company Inc.', '123 East Example Street', '456 West Example Drive', 'Big City', 98765).
The word 'field' is normally used interchangeably with 'column'.[5] However, database perfectionists tend to favor using 'field' to signify a specific cell of a given row. [citation needed] This is to enable accuracy in communicating with other developers. Columns (really column names) being referred to as field names (common for each row/record in the table). Then a field refers to a single storage location in a specific record (like a cell) to store one value (the field value). The terms record and field come from the more practical field of database usage and traditional DBMS system usage (This was linked into business like terms used in manual databases e.g. filing cabinet storage with records for each customer). The terms row and column come from the more theoretical study of relational theory.
Another distinction between the terms 'column' and 'field' is that the term 'column' does not apply to certain databases, for instance key-value stores, that do not conform to the traditional relational database structure.[6]
In a database, a table is a collection of related data organized in table format; consisting of columns and rows.
In relational databases, and flat file databases, a table is a set of data elements (values) using a model of vertical columns (identifiable by name) and horizontal rows, the cell being the unit where a row and column intersect.[1] A table has a specified number of columns, but can have any number of rows.[2] Each row is identified by one or more values appearing in a particular column subset. A specific choice of columns which uniquely identify rows is called the primary key.
The data in a table does not have to be physically stored in the database. Views also function as relational tables, but their data are calculated at query time. External tables (in Informix[3]
or Oracle,[4][5]
for example) can also be thought of as views.
In terms of the relational model of databases, a table can be considered a convenient representation of a relation, but the two are not strictly equivalent. For instance, a SQL table can potentially contain duplicate rows, whereas a true relation cannot contain duplicate rows that we call tuples. Similarly, representation as a table implies a particular ordering to the rows and columns, whereas a relation is explicitly unordered.  However, the database system does not guarantee any ordering of the rows unless an ORDER BY clause is specified in the SELECT  statement that queries the table.
In non-relational systems, hierarchical databases, the distant counterpart of a table is a structured file, representing the rows of a table in each row of the file and each column in a row. This structure implies that a row can have repeating information, generally in the child data segments. Data are stored in sequence of physical records.
A database application is a computer program whose primary purpose is retrieving information from a computerized database. From here, information can be inserted, modified or deleted which is subsequently conveyed back into the database. Early examples of database applications were accounting systems and airline reservations systems, such as SABRE, developed starting in 1957.
A characteristic of modern database applications is that they facilitate simultaneous updates and queries from multiple users. Systems in the 1970s might have accomplished this by having each user in front of a 3270 terminal to a mainframe computer. By the mid-1980s it was becoming more common to give each user a personal computer and have a program running on that PC that is connected to a database server. Information would be pulled from the database, transmitted over a network, and then arranged, graphed, or otherwise formatted by the program running on the PC. Starting in the mid-1990s it became more common to build database applications with a Web interface. Rather than develop custom software to run on a user's PC, the user would use the same Web browser program for every application. A database application with a Web interface had the advantage that it could be used on devices of different sizes, with different hardware, and with different operating systems. Examples of early database applications with Web interfaces include amazon.com, which used the Oracle relational database management system, the photo.net online community, whose implementation on top of Oracle was described in the book Database-Backed Web Sites (Ziff-Davis Press; May 1997), and eBay, also running Oracle.[1]
Electronic medical records are referred to on emrexperts.com,[2] in December 2010, as "a software database application". A 2005 O'Reilly book uses the term in its title: Database Applications and the Web.
Some of the most complex database applications remain accounting systems, such as SAP, which may contain thousands of tables in only a single module.[3] Many of today's most widely used computer systems are database applications, for example, Facebook, which was built on top of MySQL.[4]
The etymology of the phrase "database application" comes from the practice of dividing computer software into systems programs, such as the operating system, compilers, the file system, and tools such as the database management system, and application programs, such as a payroll check processor. On a standard PC running Microsoft Windows, for example, the Windows operating system contains all of the systems programs while games, word processors, spreadsheet programs, photo editing programs, etc. would be application programs. As "application" is short for "application program", "database application" is short for "database application program".
Not every program that uses a database would typically be considered a "database application". For example, many physics experiments, e.g., the Large Hadron Collider,[5] generate massive data sets that programs subsequently analyze. The data sets constitute a "database", though they are not typically managed with a standard relational database management system. The computer programs that analyze the data are primarily developed to answer hypotheses, not to put information back into the database and therefore the overall program would not be called a "database application".
A navigational database is a type of database in which records or objects are found primarily by following references from other objects. The term was popularized by the title of Charles Bachman's 1973 Turing Award paper, The Programmer as Navigator.[1] This paper emphasized the fact that the new disk-based database systems allowed the programmer to choose arbitrary navigational routes following relationships from record to record, contrasting this with the constraints of earlier magnetic-tape and punched card systems where data access was strictly sequential.
One of the earliest navigational databases was Integrated Data Store (IDS), which was developed by Bachman for General Electric in the 1960s. IDS became the basis for the CODASYL database model in 1969.
Navigational database programming thus came to be seen as intrinsically procedural; and moreover to depend on the maintenance of an implicit set of global variables (currency indicators) holding the current state. As such, the approach was seen as diametrically opposed to the declarative programming style used by the relational model. The declarative nature of relational languages such as SQL offered better programmer productivity and a higher level of data independence (that is, the ability of programs to continue working as the database structure evolves.) Navigational interfaces, as a result, were gradually eclipsed during the 1980s by declarative query languages.
During the 1990s it started becoming clear that for certain applications handling complex data (for example, spatial databases and engineering databases), the relational calculus had limitations. At that time, a reappraisal of the entire database market began, with several companies describing the new systems using the marketing term NoSQL. Many of these systems introduced data manipulation languages which, while far removed from the CODASYL DML with its currency indicators, could be understood as implementing Bachman's "navigational" vision. Some of these languages are procedural; others (such as XPath) are entirely declarative. Offshoots of the navigational concept, such as the graph database, found new uses in modern transaction processing workloads.
Navigational access is traditionally associated with the network model and hierarchical model of database, and conventionally describes data manipulation APIs in which records (or objects) are processed one at a time, iteratively. The essential characteristic as described by Bachman, however, is finding records by virtue of their relationship to other records: so an interface can still be navigational if it has set-oriented features.[3] From this viewpoint, the key difference between navigational data manipulation languages and relational languages is the use of explicit named relationships rather than value-based joins: for department with name="Sales", find all employees in set department-employees versus find employees, departments where employee.department-code = department.code and department.name="Sales".
On this viewpoint, the key difference between navigational APIs and the relational model (implemented in relational databases) is that relational APIs use "declarative" or logic programming techniques that ask the system what to fetch, while navigational APIs instruct the system in a sequence of steps how to reach the required records.
For many years the primary defence of navigational APIs was performance. Database systems that support navigational APIs often use internal storage structures that contain physical links or pointers from one record to another. While such structures may allow very efficient navigation, they have disadvantages because it becomes difficult to reorganize the physical placement of data. It is quite possible to implement navigational APIs without low-level pointer chasing (Bachman's paper envisaged logical relationships being implemented just as in relational systems, using primary keys and foreign keys), so the two ideas should not be conflated. But without the performance benefits of low-level pointers, navigational APIs become harder to justify.
As relational systems came to prominence in the 1980s, navigational APIs (and in particular, procedural APIs) were criticized and fell out of favour. The 1990s, however, brought a new wave of object-oriented databases that often provided both declarative and procedural interfaces. One explanation for this is that they were often used to represent graph-structured information (for example spatial data and engineering data) where access is inherently recursive: the mathematics underpinning SQL (specifically, first-order predicate calculus) does not have sufficient power to support recursive queries, even those as simple as a transitive closure.
A current example of a popular navigational API can be found in the Document Object Model (DOM) often used in web browsers and closely associated with JavaScript. The DOM is essentially an in-memory hierarchical database with an API that is both procedural and navigational. By contrast, the same data (XML or HTML) can be accessed using XPath, which can be categorized as declarative and navigational: data is accessed by following relationships, but the calling program does not issue a sequence of instructions to be followed in order. Languages such as SPARQL used to retrieve Linked Data from the Semantic Web are also simultaneously declarative and navigational.
A hierarchical database model is a data model in which the data is organized into a tree-like structure. The data are stored as records which is a collection of one or more fields. Each field contains a single value, and the collection of fields in a record defines its type. One type of field is the link, which connects a given record to associated records. Using links, records link to other records, and to other records, forming a tree. An example is a "customer" record that has links to that customer's "orders", which in turn link to "line_items".
The hierarchical database model mandates that each child record has only one parent, whereas each parent record can have zero or more child records. The network model extends the hierarchical by allowing multiple parents and children. In order to retrieve data from these databases, the whole tree needs to be traversed starting from the root node. Both models were well suited to data that was normally stored on tape drives, which had to move the tape from end to end in order to retrieve data.
When the relational database model emerged, one criticism of hierarchical database models was their close dependence on application-specific implementation. This limitation, along with the relational model's ease of use, contributed to the popularity of relational databases, despite their initially lower performance in comparison with the existing network and hierarchical models.[1]
The hierarchical structure was developed by IBM in the 1960s and used in early mainframe DBMS.  Records' relationships form a treelike model.  This structure is simple but inflexible because the relationship is confined to a one-to-many relationship.  The IBM Information Management System (IMS) and RDM Mobile are examples of a hierarchical database system with multiple hierarchies over the same data.
The hierarchical data model lost traction as Codd's relational model became the de facto standard used by virtually all mainstream database management systems. A relational-database implementation of a hierarchical model was first discussed in published form in 1992[2] (see also nested set model). Hierarchical data organization schemes resurfaced with the advent of XML in the late 1990s[3] (see also XML database).  The hierarchical structure is used primarily today for storing geographic information and file systems.[citation needed]
Currently, hierarchical databases are still widely used especially in applications that require very high performance and availability such as banking, health care, and telecommunications. One of the most widely used commercial hierarchical databases is IMS.[4]
Another example of the use of hierarchical databases is Windows Registry in the Microsoft Windows operating systems.[5]
In this model, the employee data table represents the "parent" part of the hierarchy, while the computer table represents the "child" part of the hierarchy.
In contrast to tree structures usually found in computer software algorithms, in this model the children point to the parents.
As shown, each employee may possess several pieces of computer equipment, but each individual piece of computer equipment may have only one employee owner.
In this, the "child" is the same type as the "parent". The hierarchy stating EmpNo 10 is boss of 20, and 30 and 40 each report to 20 is represented by the "ReportsTo" column. In Relational database terms, the ReportsTo column is a foreign key referencing the EmpNo column. If the "child" data type were different, it would be in a different table, but there would still be a foreign key referencing the EmpNo column of the employees table.
Oracle Database (commonly referred to as Oracle DBMS, Oracle Autonomous Database, or simply as Oracle) is a proprietary multi-model[4] database management system produced and marketed by Oracle Corporation.
It is a database commonly used for running online transaction processing (OLTP), data warehousing (DW) and mixed (OLTP & DW) database workloads. Oracle Database is available by several service providers on-premises, on-cloud, or as a hybrid cloud installation.  It may be run on third party servers as well as on Oracle hardware (Exadata on-premises, on Oracle Cloud or at Cloud at Customer).[5]
Larry Ellison and his two friends and former co-workers, Bob Miner and Ed Oates, started a consultancy called Software Development Laboratories (SDL) in 1977. SDL developed the original version of the Oracle software. The name Oracle comes from the code-name of a CIA-funded project Ellison had worked on while formerly employed by Ampex.[7]
Oracle products follow a custom release-numbering and -naming convention. The "ai" in the current release, Oracle Database 23ai, stands for "Artificial Intelligence". Previous releases (e.g. Oracle Database 19c, 10g, and Oracle9i Database) have used suffixes of "c", "g", and "i" which stand for "Cloud", "Grid", and "Internet" respectively. Prior to the release of Oracle8i Database, no suffixes featured in Oracle Database naming conventions. There was no v1 of Oracle Database, as co-founder Larry Ellison "knew no one would want to buy version 1".[8] For some database releases, Oracle also provides an Express Edition (XE) that is free to use.[9]
The Introduction to Oracle Database includes a brief history on some of the key innovations introduced with each major release of Oracle Database.
See My Oracle Support (MOS) note Release Schedule of Current Database Releases (Doc ID 742060.1) for the current Oracle Database releases and their patching end dates.
Prior to Oracle Database 18c, Oracle Corporation released Critical Patch Updates (CPUs) and Security Patch Updates (SPUs)[33] and Security Alerts to close security vulnerabilities. These releases are issued quarterly; some of these releases have updates issued prior to the next quarterly release.
Starting with Oracle Database 18c, Oracle Corporation releases Release Updates (RUs) and Release Update Revisions (RURs).[34] RUs usually contain security, regression (bug), optimizer, and functional fixes which may include feature extensions as well. RURs include all fixes from their corresponding RU but only add new security and regression fixes. However, no new optimizer or functional fixes are included.
In the market for relational databases, Oracle Database competes against commercial products such as IBM Db2 and Microsoft SQL Server. Oracle and IBM tend to battle for the mid-range database market on Unix and Linux platforms, while Microsoft dominates the mid-range database market on Microsoft Windows platforms. However, since they share many of the same customers, Oracle and IBM tend to support each other's products in many middleware and application categories (for example: WebSphere, PeopleSoft, and Siebel Systems CRM), and IBM's hardware divisions work closely[citation needed] with Oracle on performance-optimizing server-technologies (for example, Linux on IBM Z). Niche commercial competitors include Teradata (in data warehousing and business intelligence), Software AG's ADABAS, Sybase, and IBM's Informix, among many others.
In the cloud, Oracle Database competes against the database services of AWS, Microsoft Azure, and Google Cloud Platform.
Increasingly, the Oracle database products compete against open-source software relational and non-relational database systems such as PostgreSQL, MongoDB, Couchbase, Neo4j, ArangoDB and others. Oracle acquired Innobase, supplier of the InnoDB codebase to MySQL, in part to compete better against open source alternatives, and acquired Sun Microsystems, owner of MySQL, in 2010.  Database products licensed as open-source are, by the legal terms of the Open Source Definition, free to distribute and free of royalty or other licensing fees.
An object–relational database (ORD), or object–relational database management system (ORDBMS), is a database management system (DBMS) similar to a relational database, but with an object-oriented database model: objects, classes and inheritance are directly supported in database schemas and in the query language. Also, as with pure relational systems, it supports extension of the data model with custom data types and methods.
An object–relational database can be said to provide a middle ground between relational databases and object-oriented databases. In object–relational databases, the approach is essentially that of relational databases: the data resides in the database and is manipulated collectively with queries in a query language; at the other extreme are OODBMSes in which the database is essentially a persistent object store for software written in an object-oriented programming language, with an application programming interface API for storing and retrieving objects, and little or no specific support for querying.
The basic need of object–relational database arises from the fact that both Relational and Object database have their individual advantages and drawbacks. The isomorphism of the relational database system with a mathematical relation allows it to exploit many useful techniques and theorems from set theory. But these types of databases are not optimal for certain kinds of applications. An object oriented database model allows containers like sets and lists, arbitrary user-defined datatypes as well as nested objects. This brings commonality between the application type systems and database type systems which removes any issue of impedance mismatch. But object databases, unlike relational do not provide any mathematical base for their deep analysis.[2][3]
The basic goal for the object–relational database is to bridge the gap between relational databases and the object-oriented modeling techniques used in programming languages such as Java, C++, Visual Basic (.NET) or C#. However, a more popular alternative for achieving such a bridge is to use a standard relational database systems with some form of object–relational mapping (ORM) software. Whereas traditional RDBMS or SQL-DBMS products focused on the efficient management of data drawn from a limited set of data-types (defined by the relevant language standards), an object–relational DBMS allows software developers to integrate their own types and the methods that apply to them into the DBMS.
The ORDBMS (like ODBMS or OODBMS) is integrated with an object-oriented programming language. The characteristic properties of ORDBMS are 1) complex data, 2) type inheritance, and 3) object behavior. Complex data creation in most SQL ORDBMSs is based on preliminary schema definition via the user-defined type (UDT). Hierarchy within structured complex data offers an added property, type inheritance. That is, a structured type can have subtypes that reuse all of its attributes and contain additional attributes specific to the subtype. Another advantage, the object behavior, is related with access to the program objects. Such program objects must be storable and transportable for database processing, therefore they usually are named as persistent objects. Inside a database, all the relations with a persistent program object are relations with its object identifier (OID). All of these points can be addressed in a proper relational system, although the SQL standard and its implementations impose arbitrary restrictions and additional complexity[4][page needed]
In object-oriented programming (OOP), object behavior is described through the methods (object functions). The methods denoted by one name are distinguished by the type of their parameters and type of objects for which they attached (method signature). The OOP languages call this the polymorphism principle, which briefly is defined as "one interface, many implementations". Other OOP principles, inheritance and encapsulation, are related both to methods and attributes. Method inheritance is included in type inheritance. Encapsulation in OOP is a visibility degree declared, for example, through the public, private and protected access modifiers.
Object–relational database management systems grew out of research that occurred in the early 1990s. That research extended existing relational database concepts by adding object concepts. The researchers aimed to retain a declarative query-language based on predicate calculus as a central component of the architecture. Probably the most notable research project, Postgres (UC Berkeley), spawned two products tracing their lineage to that research: Illustra and PostgreSQL.
In the mid-1990s, early commercial products appeared. These included Illustra[5] (Illustra Information Systems, acquired by Informix Software, which was in turn acquired by International Business Machines (IBM), Omniscience (Omniscience Corporation, acquired by Oracle Corporation and became the original Oracle Lite), and UniSQL (UniSQL, Inc., acquired by KCOM Group). Ukrainian developer Ruslan Zasukhin, founder of Paradigma Software, Inc., developed and shipped the first version of Valentina database in the mid-1990s as a C++ software development kit (SDK). By the next decade, PostgreSQL had become a commercially viable database, and is the basis for several current products that maintain its ORDBMS features.
Computer scientists came to refer to these products as "object–relational database management systems" or ORDBMSs.[6]
Many of the ideas of early object–relational database efforts have largely become incorporated into SQL:1999 via structured types. In fact, any product that adheres to the object-oriented aspects of SQL:1999 could be described as an object–relational database management product. For example, IBM Db2, Oracle Database, and Microsoft SQL Server, make claims to support this technology and do so with varying degrees of success.
Most current[update] SQL databases allow the crafting of custom functions, which would allow the query to appear as:
In an object–relational database, one might see something like this, with user-defined data-types and expressions such as BirthDay():
The object–relational model can offer another advantage in that the database can make use of the relationships between data to easily collect related records. In an address book application, an additional table would be added to the ones above to hold zero or more addresses for each customer. Using a traditional RDBMS, collecting information for both the user and their address requires a "join":
A document-oriented database, or document store, is a computer program and data storage system designed for storing, retrieving and managing document-oriented information, also known as semi-structured data.[1]
Document-oriented databases are one of the main categories of NoSQL databases, and the popularity of the term "document-oriented database" has grown[2] with the use of the term NoSQL itself. XML databases are a subclass of document-oriented databases that are optimized to work with XML documents. Graph databases are similar, but add another layer, the relationship, which allows them to link documents for rapid traversal.
Document-oriented databases are inherently a subclass of the key-value store, another NoSQL database concept. The difference[contradictory] lies in the way the data is processed; in a key-value store, the data is considered to be inherently opaque to the database, whereas a document-oriented system relies on internal structure in the document in order to extract metadata that the database engine uses for further optimization. Although the difference is often negligible due to tools in the systems,[a] conceptually the document-store is designed to offer a richer experience with modern programming techniques.
Document databases[b] contrast strongly with the traditional relational database (RDB). Relational databases generally store data in separate tables that are defined by the programmer, and a single object may be spread across several tables. Document databases store all information for a given object in a single instance in the database, and every stored object can be different from every other. This eliminates the need for object-relational mapping while loading data into the database.
The central concept of a document-oriented database is the notion of a document. While each document-oriented database implementation differs on the details of this definition, in general, they all assume documents encapsulate and encode data (or information) in some standard format or encoding. Encodings in use include XML, YAML, JSON, as well as binary forms like BSON.
Documents in a document store are roughly equivalent to the programming concept of an object. They are not required to adhere to a standard schema, nor will they have all the same sections, slots, parts or keys. Generally, programs using objects have many different types of objects, and those objects often have many optional fields. Every object, even those of the same class, can look very different. Document stores are similar in that they allow different types of documents in a single store, allow the fields within them to be optional, and often allow them to be encoded using different encoding systems. For example, the following is a document, encoded in JSON:
These two documents share some structural elements with one another, but each also has unique elements. The structure and text and other data inside the document are usually referred to as the document's content and may be referenced via retrieval or editing methods, (see below). Unlike a relational database where every record contains the same fields, leaving unused fields empty; there are no empty 'fields' in either document (record) in the above example. This approach allows new information to be added to some records without requiring that every other record in the database share the same structure.
Document databases typically provide for additional metadata to be associated with and stored along with the document content. That metadata may be related to facilities the datastore provides for organizing documents, providing security, or other implementation specific features.
The core operations that a document-oriented database supports for documents are similar to other databases, and while the terminology is not perfectly standardized, most practitioners will recognize them as CRUD:
Documents are addressed in the database via a unique key that represents that document. This key is a simple identifier (or ID), typically a string, a URI, or a path. The key can be used to retrieve the document from the database. Typically the database retains an index on the key to speed up document retrieval, and in some cases the key is required to create or insert the document into the database.
Another defining characteristic of a document-oriented database is that, beyond the simple key-to-document lookup that can be used to retrieve a document, the database offers an API or query language that allows the user to retrieve documents based on content (or metadata). For example, you may want a query that retrieves all the documents with a certain field set to a certain value.  The set of query APIs or query language features available, as well as the expected performance of the queries, varies significantly from one implementation to another. Likewise, the specific set of indexing options and configuration that are available vary greatly by implementation.
Document databases typically provide some mechanism for updating or editing the content (or metadata) of a document, either by allowing for replacement of the entire document, or individual structural pieces of the document.
Document database implementations offer a variety of ways of organizing documents, including notions of
A document-oriented database is a specialized key-value store, which itself is another NoSQL database category.  In a simple key-value store, the document content is opaque. A document-oriented database provides APIs or a query/update language that exposes the ability to query or update based on the internal structure in the document. This difference may be minor for users that do not need richer query, retrieval, or editing APIs that are typically provided by document databases. Modern key-value stores often include features for working with metadata, blurring the lines between document stores.
Some search engine (aka information retrieval) systems like Apache Solr and Elasticsearch provide enough of the core operations on documents to fit the definition of a document-oriented database.
In a relational database, data is first categorized into a number of predefined types, and tables are created to hold individual entries, or records, of each type. The tables define the data within each record's fields, meaning that every record in the table has the same overall form. The administrator also defines the relationships between the tables, and selects certain fields that they believe will be most commonly used for searching and defines indexes on them. A key concept in the relational design is that any data that may be repeated is normally placed in its own table, and if these instances are related to each other, a column is selected to group them together, the foreign key. This design is known as database normalization.[3]
For example, an address book application will generally need to store the contact name, an optional image, one or more phone numbers, one or more mailing addresses, and one or more email addresses. In a canonical relational database, tables would be created for each of these rows with predefined fields for each bit of data: the CONTACT table might include FIRST_NAME, LAST_NAME and IMAGE columns, while the PHONE_NUMBER table might include COUNTRY_CODE, AREA_CODE, PHONE_NUMBER and TYPE (home, work, etc.). The PHONE_NUMBER table also contains a foreign key column, "CONTACT_ID", which holds the unique ID number assigned to the contact when it was created. In order to recreate the original contact, the database engine uses the foreign keys to look for the related items across the group of tables and reconstruct the original data.
In contrast, in a document-oriented database there may be no internal structure that maps directly onto the concept of a table, and the fields and relationships generally don't exist as predefined concepts. Instead, all of the data for an object is placed in a single document, and stored in the database as a single entry. In the address book example, the document would contain the contact's name, image, and any contact info, all in a single record. That entry is accessed through its key, which allows the database to retrieve and return the document to the application. No additional work is needed to retrieve the related data; all of this is returned in a single object.
A key difference between the document-oriented and relational models is that the data formats are not predefined in the document case. In most cases, any sort of document can be stored in any database, and those documents can change in type and form at any time. If one wishes to add a COUNTRY_FLAG to a CONTACT, this field can be added to new documents as they are inserted, this will have no effect on the database or the existing documents already stored. To aid retrieval of information from the database, document-oriented systems generally allow the administrator to provide hints to the database to look for certain types of information. These work in a similar fashion to indexes in the relational case. Most also offer the ability to add additional metadata outside of the content of the document itself, for instance, tagging entries as being part of an address book, which allows the programmer to retrieve related types of information, like "all the address book entries". This provides functionality similar to a table, but separates the concept (categories of data) from its physical implementation (tables).
In the classic normalized relational model, objects in the database are represented as separate rows of data with no inherent structure beyond that given to them as they are retrieved. This leads to problems when trying to translate programming objects to and from their associated database rows, a problem known as object-relational impedance mismatch.[4] Document stores more closely, or in some cases directly, map programming objects into the store. These are often marketed using the term NoSQL.
The Data Base Task Group (DBTG) was a working group founded in 1965 by the Cobol Committee, formerly Programming Language Committee, of the Conference on Data Systems Language (CODASYL). It was initially named the List Processing Task Force and later renamed to DBTG in 1967. The DBTG was chaired by William Olle of RCA.[1]
In April 1971, the DBTG published a report containing specifications of a Data Manipulation Language (DML) and a Data Definition Language (DDL) for standardization of network database model. The first DBTG proposals had already been published in 1969. The specification was subsequently modified and developed in various committees and published by other reports in 1973 and 1978. The specification is often referred to as the DBTG database model or the CODASYL database model. As well as the data model, many basic concepts of database terminology were introduced by this group, notably the concepts of schema and subschema.
A hierarchical database model is a data model in which the data is organized into a tree-like structure. The data are stored as records which is a collection of one or more fields. Each field contains a single value, and the collection of fields in a record defines its type. One type of field is the link, which connects a given record to associated records. Using links, records link to other records, and to other records, forming a tree. An example is a "customer" record that has links to that customer's "orders", which in turn link to "line_items".
The hierarchical database model mandates that each child record has only one parent, whereas each parent record can have zero or more child records. The network model extends the hierarchical by allowing multiple parents and children. In order to retrieve data from these databases, the whole tree needs to be traversed starting from the root node. Both models were well suited to data that was normally stored on tape drives, which had to move the tape from end to end in order to retrieve data.
When the relational database model emerged, one criticism of hierarchical database models was their close dependence on application-specific implementation. This limitation, along with the relational model's ease of use, contributed to the popularity of relational databases, despite their initially lower performance in comparison with the existing network and hierarchical models.[1]
The hierarchical structure was developed by IBM in the 1960s and used in early mainframe DBMS.  Records' relationships form a treelike model.  This structure is simple but inflexible because the relationship is confined to a one-to-many relationship.  The IBM Information Management System (IMS) and RDM Mobile are examples of a hierarchical database system with multiple hierarchies over the same data.
The hierarchical data model lost traction as Codd's relational model became the de facto standard used by virtually all mainstream database management systems. A relational-database implementation of a hierarchical model was first discussed in published form in 1992[2] (see also nested set model). Hierarchical data organization schemes resurfaced with the advent of XML in the late 1990s[3] (see also XML database).  The hierarchical structure is used primarily today for storing geographic information and file systems.[citation needed]
Currently, hierarchical databases are still widely used especially in applications that require very high performance and availability such as banking, health care, and telecommunications. One of the most widely used commercial hierarchical databases is IMS.[4]
Another example of the use of hierarchical databases is Windows Registry in the Microsoft Windows operating systems.[5]
In this model, the employee data table represents the "parent" part of the hierarchy, while the computer table represents the "child" part of the hierarchy.
In contrast to tree structures usually found in computer software algorithms, in this model the children point to the parents.
As shown, each employee may possess several pieces of computer equipment, but each individual piece of computer equipment may have only one employee owner.
In this, the "child" is the same type as the "parent". The hierarchy stating EmpNo 10 is boss of 20, and 30 and 40 each report to 20 is represented by the "ReportsTo" column. In Relational database terms, the ReportsTo column is a foreign key referencing the EmpNo column. If the "child" data type were different, it would be in a different table, but there would still be a foreign key referencing the EmpNo column of the employees table.
The MICRO Relational Database Management System was the first large-scale set-theoretic database management system to be used in production.[1] Though MICRO was initially considered to be an "Information Management System", it was eventually recognized to provide all the capabilities of an RDBMS.  MICRO's major underpinnings and algorithms were based on the Set-Theoretic Data Structure (STDS) model developed by D. L. Childs of the University of Michigan's CONCOMP (Conversational Use of Computers) Project.[2][3][4] MICRO featured a natural language interface which allowed non-programmers to use the system.[5][6]
Implementation of MICRO began in 1970 as part of the Labor Market Information System (LMIS) project at the University of Michigan's Institute of Labor and Industrial Relations (ILIR). Dr. Malcolm S. Cohen was Director of the LMIS Project and was the principal innovator and designer of the original MICRO Retrieval System.[7] Carol Easthope and Jack Guskin were the principal programmers. D.L. Childs, Vice President of Set Theoretic Information Systems (STIS) Corporation, provided continuing guidance in the use of Set-Theoretic Data Structure (STDS) data access software for MICRO. Funding came from the Office of Manpower Administration within the U.S. Department of Labor.[5] MICRO was first used for the study of large social science data bases referred to as micro data; hence the name. Organizations such as the US Department of Labor, the US Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, Wayne State University, the University of Newcastle upon Tyne, and Durham University used MICRO to manage very large scale databases until 1998.
MICRO runs under the Michigan Terminal System (MTS), the interactive time-sharing system developed at the University of Michigan that runs on IBM System/360 Model 67, System/370, and compatible mainframe computers.[8] MICRO provides a query language, a database directory, and a data dictionary to create an interface between the user and the very efficient proprietary Set-Theoretic Data Structure (STDS) software developed by the Set-Theoretic Information Systems Corporation (STIS) of Ann Arbor, Michigan. The lower level routines from STIS treat the data bases as sets and perform set operations on them, e.g., union, intersection, restrictions, etc. Although the underlying STDS model is based on set theory, the MICRO user interface is similar to those subsequently used in relational database management systems.[2][9] MICRO's data representation can be thought of as a matrix or table in which the rows represent different records or "cases", and the columns contain individual data items for each record; however, the actual data representation is in set-theoretic form. In labor market applications the rows typically represent job applicants or employees and columns represent fields such as age, sex, and income or type of industry, number of employees, and payroll.[1]
MICRO permits users with little programming experience to define, enter, interrogate, manipulate, and update collections of data in a relatively unstructured and unconstrained environment.[5] An interactive system, MICRO is powerful in terms of the complexity of requests which can be made by users without prior programming language experience.[10] MICRO includes basic statistical computations such as mean, variance, frequency, median, etc. If more rigorous statistical analysis are desired, the data from a MICRO database can be exported to the Michigan Interactive Data Analysis System (MIDAS),[11] a statistical analysis package available under the Michigan Terminal System.[12]
A database machines or back end processor is a computer or special hardware that stores and retrieves data from a database. It is specially designed for database access and is tightly coupled to the main (front-end) computer(s) by a high-speed channel, whereas a database server is a general-purpose computer that holds a database and it's loosely coupled via a local area network to its clients.
Database machines can retrieve large amount of data using hundreds to thousands of microprocessors with database software. The front end processor asks the back end (typically sending a query expressed in a query language) the data and further processes it. The back end processor on the other hand analyzes and stores the data from the front end processor. Back end processors result in higher performance, increasing host main memory, increasing database recovery and security, and decreasing cost to manufacture.
Britton-Lee (IDM), Tandem (Non-Stop System), and Teradata (DBC) all offered early commercial specialized database machines.[1] A more recent example was Oracle Exadata.
"Finally, back in 1983 Boral predicted the demise of the Database Machine (DBM) and he was right to an extent [5].[3] DBM architectures based on specialised hardware or tightly coupled to specific specialised machines were always going to be problematic. However as componentisation dissolves the DBMSs architecture into components and that this is integrated, without boundaries, with the operating system (which in turn only activated the components that are required by the DB function, thus tailoring the architecture down to the metal), means that at that instant the system becomes effectively a Database Machine but potentially without the problems of standardisation and portability of the past."
Oracle Database (commonly referred to as Oracle DBMS, Oracle Autonomous Database, or simply as Oracle) is a proprietary multi-model[4] database management system produced and marketed by Oracle Corporation.
It is a database commonly used for running online transaction processing (OLTP), data warehousing (DW) and mixed (OLTP & DW) database workloads. Oracle Database is available by several service providers on-premises, on-cloud, or as a hybrid cloud installation.  It may be run on third party servers as well as on Oracle hardware (Exadata on-premises, on Oracle Cloud or at Cloud at Customer).[5]
Larry Ellison and his two friends and former co-workers, Bob Miner and Ed Oates, started a consultancy called Software Development Laboratories (SDL) in 1977. SDL developed the original version of the Oracle software. The name Oracle comes from the code-name of a CIA-funded project Ellison had worked on while formerly employed by Ampex.[7]
Oracle products follow a custom release-numbering and -naming convention. The "ai" in the current release, Oracle Database 23ai, stands for "Artificial Intelligence". Previous releases (e.g. Oracle Database 19c, 10g, and Oracle9i Database) have used suffixes of "c", "g", and "i" which stand for "Cloud", "Grid", and "Internet" respectively. Prior to the release of Oracle8i Database, no suffixes featured in Oracle Database naming conventions. There was no v1 of Oracle Database, as co-founder Larry Ellison "knew no one would want to buy version 1".[8] For some database releases, Oracle also provides an Express Edition (XE) that is free to use.[9]
The Introduction to Oracle Database includes a brief history on some of the key innovations introduced with each major release of Oracle Database.
See My Oracle Support (MOS) note Release Schedule of Current Database Releases (Doc ID 742060.1) for the current Oracle Database releases and their patching end dates.
Prior to Oracle Database 18c, Oracle Corporation released Critical Patch Updates (CPUs) and Security Patch Updates (SPUs)[33] and Security Alerts to close security vulnerabilities. These releases are issued quarterly; some of these releases have updates issued prior to the next quarterly release.
Starting with Oracle Database 18c, Oracle Corporation releases Release Updates (RUs) and Release Update Revisions (RURs).[34] RUs usually contain security, regression (bug), optimizer, and functional fixes which may include feature extensions as well. RURs include all fixes from their corresponding RU but only add new security and regression fixes. However, no new optimizer or functional fixes are included.
In the market for relational databases, Oracle Database competes against commercial products such as IBM Db2 and Microsoft SQL Server. Oracle and IBM tend to battle for the mid-range database market on Unix and Linux platforms, while Microsoft dominates the mid-range database market on Microsoft Windows platforms. However, since they share many of the same customers, Oracle and IBM tend to support each other's products in many middleware and application categories (for example: WebSphere, PeopleSoft, and Siebel Systems CRM), and IBM's hardware divisions work closely[citation needed] with Oracle on performance-optimizing server-technologies (for example, Linux on IBM Z). Niche commercial competitors include Teradata (in data warehousing and business intelligence), Software AG's ADABAS, Sybase, and IBM's Informix, among many others.
In the cloud, Oracle Database competes against the database services of AWS, Microsoft Azure, and Google Cloud Platform.
Increasingly, the Oracle database products compete against open-source software relational and non-relational database systems such as PostgreSQL, MongoDB, Couchbase, Neo4j, ArangoDB and others. Oracle acquired Innobase, supplier of the InnoDB codebase to MySQL, in part to compete better against open source alternatives, and acquired Sun Microsystems, owner of MySQL, in 2010.  Database products licensed as open-source are, by the legal terms of the Open Source Definition, free to distribute and free of royalty or other licensing fees.
An XML database is a data persistence software system that allows data to be specified, and stored, in XML format. This data can be queried, transformed, exported and returned to a calling system. XML databases are a flavor of document-oriented databases which are in turn a category of NoSQL database.
Steve O'Connell gives one reason for the use of XML in databases: the increasingly common use of XML for data transport, which has meant that "data is extracted from databases and put into XML documents and vice-versa".[4][needs update] It may prove more efficient in terms of conversion costs,[citation needed] and easier to store the data in XML format.  In content-based applications, the ability of the native XML database also minimizes the need for extraction or entry of metadata to support searching and navigation.
XML-enabled databases typically offer one or more of the following approaches to storing XML within the traditional relational structure:
Typically an XML-enabled database is best suited where the majority of data are non-XML. For datasets where the majority of data are XML, a native XML database is better suited.
XML databases are often used in combination with relational databases to manage and store hierarchical data. A significant challenge in such integrations is extracting XML documents from relational databases, which requires specialized techniques and tools. These techniques often include:
For example, extracting hierarchical data from relational databases and converting it into XML is a common approach when generating XML feeds, exchanging data between systems, or implementing XML-based configurations.
Native XML databases are especially tailored for working with XML data. As managing XML as large strings would be inefficient, and due to the hierarchical nature of XML, custom optimized data structures are used for storage and querying. This usually increases performance both in terms of read-only queries and updates.[12] XML nodes and documents are the fundamental unit of (logical) storage, just as a relational database has fields and rows.
The standard for querying XML data per W3C recommendation is XQuery; the latest version is XQuery 3.1.[13] XQuery includes XPath as a sub-language and XML itself is a valid sub-syntax of XQuery. In addition to XPath, some XML databases support XSLT as a method of transforming documents or query results retrieved from the database.
For data-centric XML datasets, the unique and distinct keyword search method, namely, XDMA[14] for XML databases is designed and developed based on dual indexing and mutual summation.
A bibliographic database is a database of bibliographic records. This is an organised online collection of references to published written works  like journal and newspaper articles, conference proceedings, reports, government and legal publications, patents and books. In contrast to library catalogue entries, a majority of the records in bibliographic databases describe articles and conference papers rather than complete monographs, and they generally contain very rich subject descriptions in the form of keywords, subject classification terms, or abstracts.[1]
A bibliographic database may cover a wide range of topics or one academic field like computer science.[2] A significant number of bibliographic databases are marketed under a trade name by licensing agreement from vendors, or directly from their makers: the indexing and abstracting services.[3]
Many bibliographic databases have evolved into digital libraries, providing the full text of the organised contents:[citation needed]for instance CORE also organises and mirrors scholarly articles and OurResearch develops a search engine for open access content in Unpaywall.[4] Others merge with non-bibliographic and scholarly databases to create more complete disciplinary search engine systems, such as Chemical Abstracts or Entrez.
During the early 1960s computers were used to digitize text for the first time; the purpose was to reduce the cost and time required to publish two American abstracting journals, the Index Medicus of the National Library of Medicine and the Scientific and Technical Aerospace Reports of the National Aeronautics and Space Administration (NASA). By the late 1960s, such bodies of digitized alphanumeric information, known as bibliographic and numeric databases, constituted a new type of information resource.[5] Online interactive retrieval became commercially viable in the early 1970s over private telecommunications networks. The first services offered a few databases of indexes and abstracts of scholarly literature. These databases contained bibliographic descriptions of journal articles that were searchable by keywords in author and title, and sometimes by journal name or subject heading. The user interfaces were crude, the access was expensive, and searching was done by librarians on behalf of "end users".[6]
An in-memory database (IMDb, or main memory database system (MMDB) or memory resident database) is a database management system that primarily relies on main memory for computer data storage. It is contrasted with database management systems that employ a disk storage mechanism. In-memory databases are faster than disk-optimized databases because disk access is slower than memory access and the internal optimization algorithms are simpler and execute fewer CPU instructions. Accessing data in memory eliminates seek time when querying the data, which provides faster and more predictable performance than disk.[1][2]
Applications where response time is critical, such as those running telecommunications network equipment and mobile advertising networks, often use main-memory databases.[3] IMDBs have gained much traction, especially in the data analytics space, starting in the  mid-2000s – mainly due to multi-core processors that can address large memory and due to less expensive RAM.[4][5]
A potential technical hurdle with in-memory data storage is the volatility of RAM. Specifically in the event of a power loss, intentional or otherwise, data stored in volatile RAM is lost.[6] With the introduction of non-volatile random-access memory technology, in-memory databases will be able to run at full speed and maintain data in the event of power failure.[7][8][9]
In its simplest form, main memory databases store data on volatile memory devices. These devices lose all stored information when the device loses power or is reset. In this case, IMDBs can be said to lack support for the "durability" portion of the ACID (atomicity, consistency, isolation, durability) properties. Volatile memory-based IMDBs can, and often do, support the other three ACID properties of atomicity, consistency and isolation.
Some IMDBs allow the database schema to specify different durability requirements for selected areas of the database –  thus, faster-changing data that can easily be regenerated or that has no meaning after a system shut-down would not need to be journaled for durability (though it would have to be replicated for high availability), whereas configuration information would be flagged as needing preservation.
Manufacturing efficiency provides another reason for selecting a combined in-memory/on-disk database system. Some device product lines, especially in consumer electronics, include some units with permanent storage, and others that rely on memory for storage (set-top boxes, for example). If such devices require a database system, a manufacturer can adopt a hybrid database system at lower and upper cost, and with less customization of code, rather than using separate in-memory and on-disk databases, respectively, for its disk-less and disk-based products.
The first database engine to support both in-memory and on-disk tables in a single database, WebDNA, was released in 1995.
Another variation involves large amounts of nonvolatile memory in the server, for example, flash memory chips as addressable memory rather than structured as disk arrays. A database in this form of memory combines very fast access speed with persistence over reboots and power losses.[12]
In computing, an active database is a database that includes an event-driven architecture (often in the form of ECA rules) that can respond to conditions both inside and outside the database.  Possible uses include security monitoring, alerting, statistics gathering and authorization.[1]
A database trigger is procedural code that is automatically executed in response to certain events on a particular table or view in a database. The trigger is mostly used for maintaining the integrity of the information on the database. For example, when a new record (representing a new worker) is added to the employees table, new records should also be created in the tables of the taxes, vacations and salaries. Triggers can also be used to log historical data, for example to keep track of employees' previous salaries.
The scope of DDL triggers can be a database (CREATE TRIGGER name ON DATABASE ...) or the entire SQL Server instance (CREATE TRIGGER name ON ALL SERVER). When you use the entire instance, you can capture all events executed on commands that have server-level scope as well as any commands that have database-level scope in the SQL instance.
Database-level triggers can help enforce multi-table constraints, or emulate materialized views. If an exception is raised in a TRANSACTION COMMIT trigger, the changes made by the trigger so far are rolled back and the client application is notified, but the transaction remains active as if COMMIT had never been requested; the client application can continue to make changes and re-request COMMIT.
Before trigger is for checking data and deciding if operation should be permitted. If exception is thrown from before trigger then operation is aborted and no data are changed. In DB2 before triggers are read only — you can't modify data in before triggers. After triggers are designed for post processing after requested change was performed. After triggers can write data into tables and unlike some[which?] other databases you can write into any table including table on which trigger operates. Instead of triggers are for making views writeable.
An example of implementation of triggers in non-relational database can be Sedna, that provides support for triggers based on XQuery. Triggers in Sedna were designed to be analogous to SQL:2003 triggers, but natively base on XML query and update languages (XPath, XQuery and XML update language).
A trigger in Sedna is set on any nodes of an XML document stored in database. When these nodes are updated, the trigger automatically executes XQuery queries and updates specified in its body. For example, the following trigger cancels person node deletion if there are any open auctions referenced by this person:
Using the BEFORE and AFTER options[7]  determine when the trigger is called. Suppose you have a trigger that is called on an INSERT to a certain table. If your trigger is using the BEFORE option, the code within the trigger will be executed before the INSERT into the table occurs. A common use of the BEFORE trigger is to verify the input values of the INSERT, or modify the values accordingly. Now let's say we have a trigger that uses AFTER instead. The code within the trigger is executed after the INSERT happens to the table. An example use of this trigger is creating an audit history of who has made inserts into the database, keeping track of the changes made. When using these options you need to keep a few things in mind. The BEFORE option does not allow you to modify tables, that is why input validation is a practical use. Using AFTER triggers allows you to modify tables such as inserting into an audit history table.
The following is an Oracle syntax example of a row level trigger that is called AFTER an update FOR EACH ROW affected. This trigger is called on an update to a phone book database. When the trigger is called it adds an entry into a separate table named phone_book_audit. Also take note of triggers being able to take advantage of schema objects like sequences,[8] in this example audit_id_sequence.nexVal is used to generate unique primary keys in the phone_book_audit table.
Notice that the phone_number_audit table is now populated with two entries. This is due to the database having two entries with the 
last name of 'Jones'. Since the update modified two separate row values, the created trigger was called twice; once after each modification.
Using a BEFORE statement trigger is particularly useful when enforcing database restrictions.[10] This example demonstrate how to enforce a restriction upon someone named "SOMEUSER" on the table phone_book.
A cloud database is a database that typically runs on a cloud computing platform and access to the database is provided as-a-service. There are two common deployment models: users can run databases on the cloud independently, using a virtual machine image, or they can purchase access to a database service, maintained by a cloud database provider. Of the databases available on the cloud, some are SQL-based and some use a NoSQL data model.
Database services take care of scalability and high availability of the database. Database services make the underlying software-stack transparent to the user.[1]
The design and development of typical systems utilize data management and relational databases as their key building blocks.  Advanced queries expressed in SQL work well with the strict relationships that are imposed on information by relational databases.  However, relational database technology was not initially designed or developed for use over distributed systems.  This issue has been addressed with the addition of clustering enhancements to the relational databases, although some basic tasks require complex and expensive protocols, such as with data synchronization.[5]
Modern relational databases have shown poor performance on data-intensive systems, therefore, the idea of NoSQL has been utilized within database management systems for cloud based systems.[6]  Within NoSQL implemented storage, there are no requirements for fixed table schemas, and the use of join operations is avoided.  "The  NoSQL  databases  have  proven  to  provide  efficient  horizontal  scalability,  good  performance, and ease of assembly into cloud applications."[7] Data models relying on simplified relay algorithms have also been employed in data-intensive cloud mapping applications unique to virtual frameworks.[8]
It is also important to differentiate between cloud databases which are relational as opposed to non-relational or NoSQL:[9]
The following table lists notable database vendors with a cloud database offering, classified by their deployment model – machine image vs. database as a service – and data model, SQL vs. NoSQL.
A deductive database is a database system that can make deductions (i.e. conclude additional facts) based on rules and facts stored in its database. Datalog is the language typically used to specify facts, rules and queries in deductive databases. Deductive databases have grown out of the desire to combine logic programming with relational databases to construct systems that support a powerful formalism and are still fast and able to deal with very large datasets. Deductive databases are more expressive than relational databases but less expressive than logic programming systems such as Prolog. In recent years, deductive databases have found new application in data integration, information extraction, networking, program analysis, security, and cloud computing.[1]
Deductive databases reuse many concepts from logic programming; rules and facts specified in Datalog look very similar to those written in Prolog,[2] but there are some important differences:
A distributed database is a database in which data is stored across different physical locations.[1] It may be stored in multiple computers located in the same physical location (e.g. a data centre); or maybe dispersed over a network of interconnected computers. Unlike parallel systems, in which the processors are tightly coupled and constitute a single database system, a distributed database system consists of loosely coupled sites that share no physical components.
System administrators can distribute collections of data (e.g. in a database) across multiple physical locations. A distributed database can reside on organised network servers or decentralised independent computers on the Internet, on corporate intranets or extranets, or on other organisation networks. Because distributed databases store data across multiple computers, distributed databases may improve performance at end-user worksites by allowing transactions to be processed on many machines, instead of being limited to one.[2]
Two processes ensure that the distributed databases remain up-to-date and current: replication[3] and duplication.
Besides distributed database replication and fragmentation, there are many other distributed database design technologies. For example, local autonomy, synchronous, and asynchronous distributed database technologies. The implementation of these technologies can and do depend on the needs of the business and the sensitivity/confidentiality of the data stored in the database and the price the business is willing to spend on ensuring data security, consistency and integrity.
When discussing access to distributed databases, Microsoft favors the term distributed query, which it defines in protocol-specific manner as "[a]ny SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources".[4]
Oracle provides a more language-centric view in which distributed queries and distributed transactions form part of distributed SQL.[5]
In practice, different layers of the database can have different architectures. It is now common to have a compute layer with a shared nothing architecture, and a storage layer with a shared disk architecture. This is for instance the case of Snowflake[7] and AWS Aurora.[8]
An embedded database system is a database management system (DBMS) which is tightly integrated with an application software; it is embedded in the application (instead of coming as a standalone application). It is a broad technology category that includes:[1]
Note: The term “embedded” can sometimes be used to refer to the use on embedded devices (as opposed to the definition given above). However, only a tiny subset of embedded database products are used in real-time embedded systems such as telecommunications switches and consumer electronics.[2] (See mobile database for small-footprint databases that could be used on embedded devices.)
Sybase's Advantage Database Server (ADS) is an embedded database management system. It provides both Indexed Sequential Access Method (ISAM) and relational data access and is compatible with multiple platforms including Windows, Linux, and Netware. It is available as a royalty-free local file-server database or a full client-server version.  ADS is highly scalable, with no administration, and has support for a variety of IDEs including .NET Framework (.NET), Object Pascal (Delphi), Visual FoxPro (FoxPro), PHP, Visual Basic (VB), Visual Objects (VO), Vulcan, Clipper, Perl, Java, xHarbour, etc.
Empress Software, Inc., developer of the Empress Embedded Database, is a privately held company founded in 1979. Empress Embedded Database is a full-function, relational database that has been embedded into applications by organizations small to large, with deployment environments including medical systems, network routers, nuclear power plant monitors, satellite management systems, and other embedded system applications that require reliability and power.[3] Empress is an ACID compliant, SQL database engine with C, C++, Java, JDBC, ODBC, SQL, ADO.NET and kernel level APIs. Applications developed using these APIs may be run in standalone and/or server modes.  Empress Embedded Database runs on Linux, Unix, Microsoft Windows and real-time operating systems.
ESE is an ISAM data storage technology from Microsoft, a core of Microsoft Exchange Server and Active Directory. Its purpose is to allow applications to store and retrieve data via indexed and sequential access. Windows Mail and Desktop Search in the Windows Vista operating system also make use of ESE to store indexes and property information respectively.
McObject LLC launched eXtremeDB as the first in-memory embedded database designed from scratch for real-time embedded systems. The initial product was soon joined by eXtremeDB High Availability (HA) for fault tolerant applications. The product family now includes 64-bit and transaction logging editions, and the hybrid eXtremeDB Fusion, which combines in-memory and on-disk data storage. In 2008, McObject introduced eXtremeDB Kernel Mode, the first embedded DBMS designed to run in an operating system kernel.[4] Today, eXtremeDB is used in millions of real-time and embedded systems worldwide. McObject also offers Perst, an open source, object-oriented embedded database for Java, Java ME, .NET, .NET Compact Framework and Silverlight.
Firebird Embedded is a relational database engine. As an open-source fork of InterBase, it is ACID compliant, supports triggers and stored procedures, and is available on Linux, OSX and Windows systems. It has the same features as the classic and superserver version of Firebird; two or more threads (and applications) can access the same database at the same time starting with Firebird 2.5. Therefore, Firebird Embedded acts as a local server for one threaded client accessing its databases (that means it works properly for ASP.NET web applications, because there, each user has its own thread, which means two users could access the same database at the same time, but they would not be in the same thread, because ASP.NET opens a new thread for each user). It exports the standard Firebird API entry points. The main advantage of Firebird Embedded databases is, that unlike SQLite or Access databases, they can be plugged into a full Firebird server without any modifications at all also is multiplatform (runs on Linux, OS X with full ASP.NET Mono support)
Firebird is not truly embedded since it cannot be statically linked
Written in Java Open source database engine. Embedded and Server mode, Clustering support, can run inside the Google App Engine. Supports encrypted database files (AES or XTEA). The development of H2 was started in May 2004, but it was first published on December 14, 2005. H2 is dual licensed and available under a modified version of the MPL 1.1 (Mozilla Public License) or under the (unmodified) EPL 1.0 (Eclipse Public License).
HailDB is a standalone, embeddable form of the InnoDB Storage Engine.  Given that HailDB is based on the same code base as the InnoDB Storage Engine, it contains many of the same features, including high-performance and scalability, multiversion concurrency control (MVCC), row-level locking, deadlock detection, fault tolerance and automatic crash recovery. Because the embedded engine is completely independent from MySQL, it lacks server components such as networking, object-level permissions, etc.  By eliminating the MySQL server overhead, InnoDB has a small footprint and is well-suited for embedding in applications which require high-performance and concurrency. As with most embedded database systems, HailDB is designed to be accessed primarily with an ISAM-like C API rather than SQL (though an extremely rudimentary SQL variant is supported).[5]
HSQLDB is an opensource relational database management system with a BSD-like license that runs in the same Java Virtual Machine as the embedded application. HSQLDB supports a variety of in-memory and disk-based table modes, Unicode, and SQL:2016.
InfinityDB Embedded Java DBMS is a sorted hierarchical key/value store. It now has an Encrypted edition and a Client/Server edition. The multi-core speed is patent-applied-for. InfinityDB is secure, transactional, compressing, and robust, in a single file for instant installation and zero administration. APIs include the simple fast 'ItemSpace', a ConcurrentNavigableMap view, and JSON. A RemoteItemSpace can transparently redirect the embedded APIs to other db instances. Client/Server includes a light-weight Servlet server, web admin and database browsing, and REST for python.
Informix Dynamic Server (IDS) is characterized as an enterprise class embeddable database server, combining embeddable features such as low footprint, programmable and autonomic capabilities with enterprise class database features such as high availability and flexible replication features.[7] IDS is used in deeply embedded scenarios such as IP telephony call-processing systems, point of sale applications and financial transaction processing systems.
InterBase is an IoT Award-winning cross-platform, Unicode enabled SQL database platform able to be embedded within turn-key applications. Out of the box SMP support and on disk AES strength 256bit encryption, SQL 92 & ACID compliance and support for Windows, Macintosh, Linux, Solaris, iOS and Android platforms.  Ideal for both small-to-medium and large enterprises supporting hundreds of users and mobile application development. InterBase Light is a free version that can be used on any mobile device and is ideal for mobile applications. Enterprises can switch to a paid version as requirements for change management and security increase. InterBase has high adoption in defense, airspace, oil and gas, and manufacturing industries.
Kùzu is an embeddable graph database management system that supports the Cypher (query language). It implements several existing and novel state-of-art storage, indexing, and query processing techniques[8] to help users manage and query very large graphs. Kùzu achieves its performance largely through novel join algorithms that combine binary and worst-case optimal joins,[9] factorization[10] and vectorized query execution on a columnar storage layer,[10] as well as numerous compression and parallelization techniques common in modern database systems. Kùzu is built and maintained by Kùzu Inc., a startup based in Waterloo, Ontario, Canada, and is available open-source under an MIT license.
Lightning Memory-Mapped Database (LMDB) is a memory-mapped key-value database for the OpenLDAP Project. It is written in C and the API is modeled after the Berkeley DB API, though much simplified. The library is extremely compact, compiling down to under 40KB of x86 object code, being usually faster than similar libraries like Berkeley DB, LevelDB, etc. The library implements B+trees with multiversion concurrency control (MVCC), single-level store, Copy on write and provides full ACID transactions with no deadlocks. The library is optimized for high read concurrency; readers need no locks at all. Readers don't block writers and writers don't block readers, so read performance scales perfectly linearly across arbitrarily many threads and CPUs. Third-party wrappers exist for C++, Erlang and Python. LMDB is distributed by the OpenLDAP Project under the OpenLDAP Public License. As of 2013 the OpenLDAP Project is deprecating the use of Berkeley DB, in favor of LMDB.
An embedded zero maintenance version of the proprietary Mimer SQL relational database server is available. It has a small footprint due to its modular design, full support for the SQL standard, and with ports to Windows, Linux, Automotive Grade Linux, Android, QNX, INTEGRITY, among others.
The Embedded MySQL Server Library provides most of the features of regular MySQL as a linkable library that can be run in the context of a client process. After initialization, clients can use the same C API calls as when talking to a separate MySQL server but with less communication overhead and with no need for a separate database process.
NexusDB is the commercial successor to the FlashFiler database which is now open source. They can both be embedded in Delphi applications to create stand-alone executables with full database functionality.
ObjectDB is an object database for Java, which can be used in either client-server mode or embedded (in process) mode.
As the name implies, Oracle's embedded database is actually Berkeley DB, which Oracle acquired from Sleepycat Software. It was originally developed at the University of California.[11] Berkeley DB is a fast, open-source embedded database and is used in several well-known open-source products, including the Linux and BSD Unix operating systems, Apache Web server, OpenOffice productivity suite. Nonetheless, over recent years many well-known projects switched to using LMDB, because it outperform Berkeley DB in key scenarios on the ground of "less is more" design, as well due to the license changing.[12]
RocksDB, created at Facebook, began as a fork of LevelDB.[13]  It focuses on performance, especially on SSDs.  It adds many features, including transactions,[14] backups,[15] snapshots,[16] bloom filters,[17] column families,[18] expiry,[19] custom merge operators,[20] more tunable compaction,[21] statistics collection,[22] and geospatial indexing.[23]  It is used as a storage engine inside of several other databases, including ArangoDB,[24] Ceph,[25] CockroachDB,[26] MongoRocks,[27] MyRocks,[28] Rocksandra,[29] TiKV.[30][31] and YugabyteDB.[32]
solid DB is a hybrid on-disk/in-memory, relational database and is often used as an embedded system database in telecommunications equipment, network software, and similar systems. In-memory database technology is used to achieve throughput of tens of thousands of transactions per second with response times measured in microseconds. High availability option maintains two copies of the data synchronized at all times. In case of system failure, applications can recover access to solid DB in less than a second without loss of data.
SQLite is a software library that implements a self-contained, server-less, zero-configuration, transactional SQL database engine. SQLite is the most widely deployed SQL database engine in the world. The source code, chiefly C, for SQLite is in the public domain. It includes both a native C library and a simple command line client for its database. It's included in several operating systems; among them are Android, FreeBSD, iOS, OS X and Windows 10.[33] It's also used by Chromium web browser and derivatives.[34]
SQL Server Compact is an embedded database by Microsoft with wide variety of features like multi-process connections, T-SQL, ADO.NET Sync Services to sync with any back-end database, Merge Replication with SQL Server, Programming API: LINQ to SQL, LINQ to Entities, ADO.NET. The product runs on both Desktop and Mobile Windows platforms. It has been in the market for long time, used by many enterprises in production software (Case Studies). The product went through multiple re-brandings and was known with multiple names like: SQL CE, SQL Server CE, SQL Server Mobile, SQL Mobile.
A federated database system (FDBS) is a type of meta-database management system (DBMS), which transparently maps multiple autonomous database systems into a single federated database. The constituent databases are interconnected via a computer network and may be geographically decentralized. Since the constituent database systems remain autonomous, a federated database system is a contrastable alternative to the (sometimes daunting) task of merging several disparate databases. A federated database, or virtual database, is a composite of all constituent databases in a federated database system.  There is no actual data integration in the constituent disparate databases as a result of data federation.
Through data abstraction, federated database systems can provide a uniform user interface, enabling users and clients to store and retrieve data from multiple noncontiguous databases with a single query—even if the constituent databases are heterogeneous. To this end, a federated database system must be able to decompose the query into subqueries for submission to the relevant constituent DBMSs, after which the system must composite the result sets of the subqueries. Because various database management systems employ different query languages, federated database systems can apply wrappers to the subqueries to translate them into the appropriate query languages.
McLeod and Heimbigner[1] were among the first to define a federated database system in the mid-1980s.
A FDBS is one which "define[s] the architecture and interconnect[s] databases that minimize central authority yet support partial sharing and coordination among database systems".[1] This description might not accurately reflect the McLeod/Heimbigner[1] definition of a federated database.  Rather, this description fits what McLeod/Heimbigner called a composite database.  McLeod/Heimbigner's federated database is a collection of autonomous components that make their data available to other members of the federation through the publication of an export schema and access operations; there is no unified, central schema that encompasses the information available from the members of the federation.
Among other surveys,[2] practitioners define a Federated Database as a collection of cooperating component systems which are autonomous and are possibly heterogeneous.
A DBMS can be classified as either centralized or distributed. A centralized system manages a single database while distributed manages multiple databases. A component DBS in a DBMS may be centralized or distributed. A multiple DBS (MDBS) can be classified into two types depending on the autonomy of the component DBS as federated and non federated. A nonfederated database system is an integration of component DBMS that are not autonomous.
A federated database system consists of component DBS that are autonomous yet participate in a federation to allow partial and controlled sharing of their data.
Federated architectures differ based on levels of integration with the component database systems and the extent of services offered by the federation. A FDBS can be categorized as loosely or tightly coupled systems.
Multiple DBS of which FDBS are a specific type can be characterized along three dimensions: Distribution, Heterogeneity and Autonomy. Another characterization could be based on the dimension of networking, for example single databases or multiple databases in a LAN or WAN.
Distribution of data in an FDBS is due to the existence of a multiple DBS before an FDBS is built. Data can be distributed among multiple databases which could be stored in a single computer or multiple computers. These computers could be geographically located in different places but interconnected by a network. The benefits of data distribution help in increased availability and reliability as well as improved access times.
Heterogeneities in databases arise due to factors such as differences in structures, semantics of data, the constraints supported or query language. Differences in structure occur when two data models provide different primitives such as object oriented (OO) models that support specialization and inheritance and relational models that do not. Differences due to constraints occur when two models support two different constraints. For example, the set type in CODASYL schema may be partially modeled as a referential integrity constraint in a relationship schema. CODASYL supports insertion and retention that are not captured by referential integrity alone. The query language supported by one DBMS can also contribute to heterogeneity between other component DBMSs. For example, differences in query languages with the same data models or different versions of query languages could contribute to heterogeneity.
Dealing with incompatible data types or query syntax is not the only obstacle to a concrete implementation of an FDBS. In systems that are not planned top-down, a generic problem lies in matching semantically equivalent, but differently named parts from different schemas (=data models) (tables, attributes). A pairwise mapping between n attributes would result in 




n
(
n
−
1
)

2


{\displaystyle n(n-1) \over 2}

 mapping rules (given equivalence mappings) - a number that quickly gets too large for practical purposes. A common way out is to provide a global schema that comprises the relevant parts of all member schemas and provide mappings in the form of database views. Two principal approaches depend on the direction of the mapping:
Fundamental to the difference between an MDBS and an FDBS is the concept of autonomy.  It is important to understand the aspects of autonomy for component databases and how they can be addressed when a component DBS participates in an FDBS.
There are four kinds of autonomies addressed:
The ANSI/X3/SPARC Study Group outlined a three level data description architecture, the components of which are the conceptual schema, internal schema and external schema of databases.  The three level architecture is however inadequate to describing the architectures of an FDBS.  It was therefore extended to support the three dimensions of the FDBS namely Distribution, Autonomy and Heterogeneity.  The five level schema architecture is explained below.
A heterogeneous database system is an automated (or semi-automated) system for the integration of heterogeneous, disparate database management systems to present a user with a single, unified query interface.
Heterogeneous database systems (HDBs) are computational models and software implementations that provide heterogeneous database integration.[1][2]
This article does not contain details of distributed database management systems (sometimes known as federated database systems).
Data across constituent databases may be related but different. Perhaps a database system must be able to integrate genomic and proteomic data. They are related—a gene may have several protein products—but the data are different (nucleotide sequences and amino acid sequences, or hydrophilic or -phobic amino acid sequence and positively or negatively charged amino acids). There may be many ways of looking at semantically similar, but distinct, datasets.
A graph database (GDB) is a database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data.[1] A key concept of the system is the graph (or edge or relationship). The graph relates the data items in the store to a collection of nodes and edges, the edges representing the relationships between the nodes. The relationships allow data in the store to be linked together directly and, in many cases, retrieved with one operation. Graph databases hold the relationships between data as a priority. Querying relationships is fast because they are perpetually stored in the database. Relationships can be intuitively visualized using graph databases, making them useful for heavily inter-connected data.[2]
Graph databases are commonly referred to as a NoSQL database. Graph databases are similar to 1970s network model databases in that both represent general graphs, but network-model databases operate at a lower level of abstraction[3] and lack easy traversal over a chain of edges.[4]
The underlying storage mechanism of graph databases can vary. Relationships are first-class citizens in a graph database and can be labelled, directed, and given properties. Some depend on a relational engine and store the graph data in a table (although a table is a logical element, therefore this approach imposes a level of abstraction between the graph database management system and physical storage devices). Others use a key–value store or document-oriented database for storage, making them inherently NoSQL structures.
As of 2021[update], no graph query language has been universally adopted in the same way as SQL was for relational databases, and there are a wide variety of systems, many of which are tightly tied to one product. Some early standardization efforts led to multi-vendor query languages like Gremlin, SPARQL, and Cypher. In September 2019 a proposal for a project to create a new standard graph query language (ISO/IEC 39075 Information Technology — Database Languages — GQL) was approved by members of ISO/IEC Joint Technical Committee 1(ISO/IEC JTC 1). GQL is intended to be a declarative database query language, like SQL. In addition to having query language interfaces, some graph databases are accessed through application programming interfaces (APIs).
Graph databases differ from graph compute engines. Graph databases are technologies that are translations of the relational online transaction processing (OLTP) databases. On the other hand, graph compute engines are used in online analytical processing (OLAP) for bulk analysis.[5] Graph databases attracted considerable attention in the 2000s, due to the successes of major technology corporations in using proprietary graph databases,[6] along with the introduction of open-source graph databases.
In the mid-1960s, navigational databases such as IBM's IMS supported tree-like structures in its hierarchical model, but the strict tree structure could be circumvented with virtual records.[8][9]
Graph structures could be represented in network model databases from the late 1960s. CODASYL, which had defined COBOL in 1959, defined the Network Database Language in 1969.
Labeled graphs could be represented in graph databases from the mid-1980s, such as the Logical Data Model.[10][11]
Commercial object databases (ODBMSs) emerged in the early 1990s. In 2000, the Object Data Management Group published a standard language for defining object and relationship (graph) structures in their ODMG'93 publication.
Several improvements to graph databases appeared in the early 1990s, accelerating in the late 1990s with endeavors to index web pages.
In the mid-to-late 2000s, commercial graph databases with ACID guarantees such as Neo4j and Oracle Spatial and Graph became available.
In the 2010s, commercial ACID graph databases that could be scaled horizontally became available. Further, SAP HANA brought in-memory and columnar technologies to graph databases.[12] Also in the 2010s, multi-model databases that supported graph models (and other models such as relational database or document-oriented database) became available, such as OrientDB, ArangoDB, and MarkLogic (starting with its 7.0 version). During this time, graph databases of various types have become especially popular with social network analysis with the advent of social media companies. Also during the decade, cloud-based graph databases such as Amazon Neptune and Neo4j AuraDB became available.
Graph databases portray the data as it is viewed conceptually. This is accomplished by transferring the data into nodes and its relationships into edges.
A graph database is a database that is based on graph theory. It consists of a set of objects, which can be a node or an edge.
Graph databases are a powerful tool for graph-like queries. For example, computing the shortest path between two nodes in the graph. Other graph-like queries can be performed over a graph database in a natural way (for example graph's diameter computations or community detection).
Graphs are flexible, meaning it allows the user to insert new data into the existing graph without loss of application functionality. There is no need for the designer of the database to plan out extensive details of the database's future use cases.
The underlying storage mechanism of graph databases can vary. Some depend on a relational engine and "store" the graph data in a table (although a table is a logical element, therefore this approach imposes another level of abstraction between the graph database, the graph database management system and the physical devices where the data is actually stored). Others use a key–value store or document-oriented database for storage, making them inherently NoSQL structures. A node would be represented as any other document store, but edges that link two different nodes hold special attributes inside its document; a _from and _to attributes.
Data lookup performance is dependent on the access speed from one particular node to another. Because index-free adjacency enforces the nodes to have direct physical RAM addresses and physically point to other adjacent nodes, it results in a fast retrieval. A native graph system with index-free adjacency does not have to move through any other type of data structures to find links between the nodes. Directly related nodes in a graph are stored in the cache once one of the nodes are retrieved, making the data lookup even faster than the first time a user fetches a node. However, such advantage comes at a cost. Index-free adjacency sacrifices the efficiency of queries that do not use graph traversals. Native graph databases use index-free adjacency to process CRUD operations on the stored data.
Since Edgar F. Codd's 1970 paper on the relational model,[18] relational databases have been the de facto industry standard for large-scale data storage systems. Relational models require a strict schema and data normalization which separates data into many tables and removes any duplicate data within the database. Data is normalized in order to preserve data consistency and support ACID transactions. However this imposes limitations on how relationships can be queried.
Compared with relational databases, graph databases are often faster for associative data sets[citation needed] and map more directly to the structure of object-oriented applications. They can scale more naturally[citation needed] to large datasets as they do not typically need join operations, which can often be expensive. As they depend less on a rigid schema, they are marketed as more suitable to manage ad hoc and changing data with evolving schemas.
Conversely, relational database management systems are typically faster at performing the same operation on large numbers of data elements, permitting the manipulation of the data in its natural structure. Despite the graph databases' advantages and recent popularity over [citation needed] relational databases, it is recommended the graph model itself should not be the sole reason to replace an existing relational database. A graph database may become relevant if there is an evidence for performance improvement by orders of magnitude and lower latency.[19]
The relational model gathers data together using information in the data. For example, one might look for all the "users" whose phone number contains the area code "311". This would be done by searching selected datastores, or tables, looking in the selected phone number fields for the string "311". This can be a time-consuming process in large tables, so relational databases offer indexes, which allow data to be stored in a smaller sub-table, containing only the selected data and a unique key (or primary key) of the record. If the phone numbers are indexed, the same search would occur in the smaller index table, gathering the keys of matching records, and then looking in the main data table for the records with those keys. Usually, a table is stored in a way that allows a lookup via a key to be very fast.[20]
Relational databases do not inherently contain the idea of fixed relationships between records. Instead, related data is linked to each other by storing one record's unique key in another record's data. For example, a table containing email addresses for users might hold a data item called userpk, which contains the primary key of the user record it is associated with. In order to link users and their email addresses, the system first looks up the selected user records primary keys, looks for those keys in the userpk column in the email table (or, more likely, an index of them), extracts the email data, and then links the user and email records to make composite records containing all the selected data. This operation, termed a join, can be computationally expensive. Depending on the complexity of the query, the number of joins, and indexing various keys, the system may have to search through multiple tables and indexes and then sort it all to match it together.[20]
In contrast, graph databases directly store the relationships between records. Instead of an email address being found by looking up its user's key in the userpk column, the user record contains a pointer that directly refers to the email address record. That is, having selected a user, the pointer can be followed directly to the email records, there is no need to search the email table to find the matching records. This can eliminate the costly join operations. For example, if one searches for all of the email addresses for users in area code "311", the engine would first perform a conventional search to find the users in "311", but then retrieve the email addresses by following the links found in those records. A relational database would first find all the users in "311", extract a list of the primary keys, perform another search for any records in the email table with those primary keys, and link the matching records together. For these types of common operations, graph databases would theoretically be faster.[20]
The true value of the graph approach becomes evident when one performs searches that are more than one level deep. For example, consider a search for users who have "subscribers" (a table linking users to other users) in the "311" area code. In this case a relational database has to first search for all the users with an area code in "311", then search the subscribers table for any of those users, and then finally search the users table to retrieve the matching users. In contrast, a graph database would search for all the users in "311", then follow the backlinks through the subscriber relationship to find the subscriber users. This avoids several searches, look-ups, and the memory usage involved in holding all of the temporary data from multiple records needed to construct the output. In terms of big O notation, this query would be 



O
(
log
⁡
n
)
+
O
(
1
)


{\displaystyle O(\log n)+O(1)}

 time – i.e., proportional to the logarithm of the size of the data. In contrast, the relational version would be multiple 



O
(
log
⁡
n
)


{\displaystyle O(\log n)}

 lookups, plus the 



O
(
n
)


{\displaystyle O(n)}

 time needed to join all of the data records.[20]
The relative advantage of graph retrieval grows with the complexity of a query. For example, one might want to know "that movie about submarines with the actor who was in that movie with that other actor that played the lead in Gone With the Wind". This first requires the system to find the actors in Gone With the Wind, find all the movies they were in, find all the actors in all of those movies who were not the lead in Gone With the Wind, and then find all of the movies they were in, finally filtering that list to those with descriptions containing "submarine". In a relational database, this would require several separate searches through the movies and actors tables, doing another search on submarine movies, finding all the actors in those movies, and then comparing the (large) collected results. In contrast, the graph database would walk from Gone With the Wind to Clark Gable, gather the links to the movies he has been in, gather the links out of those movies to other actors, and then follow the links out of those actors back to the list of movies. The resulting list of movies can then be searched for "submarine". All of this can be done via one search.[21]
Properties add another layer of abstraction to this structure that also improves many common queries. Properties are essentially labels that can be applied to any record, or in some cases, edges as well. For example, one might label Clark Gable as "actor", which would then allow the system to quickly find all the records that are actors, as opposed to director or camera operator. If labels on edges are allowed, one could also label the relationship between Gone With the Wind and Clark Gable as "lead", and by performing a search on people that are "lead" "actor" in the movie Gone With the Wind, the database would produce Vivien Leigh, Olivia de Havilland and Clark Gable. The equivalent SQL query would have to rely on added data in the table linking people and movies, adding more complexity to the query syntax. These sorts of labels may improve search performance under certain circumstances, but are generally more useful in providing added semantic data for end users.[21]
Relational databases are very well suited to flat data layouts, where relationships between data are only one or two levels deep. For example, an accounting database might need to look up all the line items for all the invoices for a given customer, a three-join query. Graph databases are aimed at datasets that contain many more links. They are especially well suited to social networking systems, where the "friends" relationship is essentially unbounded. These properties make graph databases naturally suited to types of searches that are increasingly common in online systems, and in big data environments. For this reason, graph databases are becoming very popular for large online systems like Facebook, Google, Twitter, and similar systems with deep links between records.
The above examples are a simple illustration of a basic relationship query. They condense the idea of relational models' query complexity that increases with the total amount of data. In comparison, a graph database query is easily able to sort through the relationship graph to present the results.
There are also results that indicate simple, condensed, and declarative queries of the graph databases do not necessarily provide good performance in comparison to the relational databases. While graph databases offer an intuitive representation of data, relational databases offer better results when set operations are needed.[15]
In computing, the network model is a database model conceived as a flexible way of representing objects and their relationships. Its distinguishing feature is that the schema, viewed as a graph in which object types are nodes and relationship types are arcs, is not restricted to being a hierarchy or lattice.
The network model was adopted by the CODASYL Data Base Task Group in 1969 and underwent a major update in 1971. It is sometimes known as the CODASYL model for this reason. A number of network database systems became popular on mainframe and minicomputers through the 1970s before being widely replaced by relational databases in the 1980s.
While the hierarchical database model structures data as a tree of records, with each record having one parent record and many children, the network model allows each record to have multiple parent and child records, forming a generalized graph structure. This property applies at two levels: the schema is a generalized graph of record types connected by relationship types (called "set types" in CODASYL), and the database itself is a generalized graph of record occurrences connected by relationships (CODASYL "sets"). Cycles are permitted at both levels.
The chief argument in favour of the network model, in comparison to the hierarchical model, was that it allowed a more natural modeling of relationships between entities. Although the model was widely implemented and used, it failed to become dominant for two main reasons. Firstly, IBM chose to stick to the hierarchical model with semi-network extensions in their established products such as IMS and DL/I. Secondly, it was eventually displaced by the relational model, which offered a higher-level, more declarative interface. Until the early 1980s the performance benefits of the low-level navigational interfaces offered by hierarchical and network databases were persuasive for many large-scale applications, but as hardware became faster, the extra productivity and flexibility of the relational model led to the gradual obsolescence of the network model in corporate enterprise usage.
Bachman's influence is recognized in the term Bachman diagram, a diagrammatic notation that represents a database schema expressed using the network model. In a Bachman diagram, named rectangles represent record types, and arrows represent one-to-many relationship types between records (CODASYL set types).
David M, k., 1997. Fundamentals, Design, and Implementation. database processing ed. s.l.:Prentice-Hall.
