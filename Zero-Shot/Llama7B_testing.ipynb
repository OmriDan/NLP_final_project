{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iKy422v9nQ_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the pickle file\n",
        "file_path = '/content/drive/MyDrive/NLP_PROJECT/DS_tests_with_difficulty.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Filter only open questions\n",
        "open_df = df[df[\"question_type\"] == \"Open\"].reset_index(drop=True)\n",
        "open_df.fillna(\"\", inplace=True)"
      ],
      "metadata": {
        "id": "W5-xBMXLndwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OMRI Experiment with LLaMA 2 7B in Google Colab\n",
        "\n",
        "# 1. Install Required Libraries\n",
        "!pip install -q transformers accelerate datasets scikit-learn matplotlib\n",
        "\n",
        "# 2. Load Dataset (Assumes a CSV file is uploaded to Colab)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "jhd7vHmjnLXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "Dm5IR9vonQZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load LLaMA 2 7B model (via HuggingFace Hub)\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n"
      ],
      "metadata": {
        "id": "81XBYN66naR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert in question difficulty estimation, in the field of data structures.\n",
        "Estimate the difficulty of the new question based on a semantic and technical analysis of the question and its' answer.\n",
        "Please estimate the difficulty of the question on a scale from 0 (very easy) to 1 (very hard), rounded to 3 decimal points.\n",
        "\n",
        "New Question:\n",
        "\"{question}\"\n",
        "New Answer:\n",
        "\"{answer}\"\n",
        "\n",
        "Estimated Difficulty:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3b1B8NZgn3mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_estimated_difficulty(text):\n",
        "    match = re.search(r\"Estimated Difficulty:\\s*([0-9.]+)\", text)\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    return None"
      ],
      "metadata": {
        "id": "6gfYu8BpszPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "true_difficulties = []\n",
        "estimated_difficulties = []\n",
        "\n",
        "# Iterate through all rows (or a sample if you want)\n",
        "sample_df = open_df.reset_index(drop=True)\n",
        "\n",
        "for idx, row in sample_df.iterrows():\n",
        "    question = row[\"question_translated_to_English\"]\n",
        "    answer = row[\"answer_translated_to_English\"]\n",
        "    true_diff = row[\"Difficulty\"]\n",
        "    # Build a simple prompt without any external database\n",
        "    prompt = PROMPT_TEMPLATE.format(question=question, answer=answer)\n",
        "    print(\"i\")\n",
        "    # Send to LLaMA\n",
        "    response = llama_pipeline(prompt)[0]['generated_text']\n",
        "\n",
        "    est_diff = extract_estimated_difficulty(response)\n",
        "\n",
        "\n",
        "    true_difficulties.append(true_diff)\n",
        "    estimated_difficulties.append(est_diff)\n"
      ],
      "metadata": {
        "id": "kfKO1Aj1oIA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "\n",
        "true_vals = np.array(true_difficulties)\n",
        "est_vals = np.array([v if v is not None else 0 for v in estimated_difficulties])\n",
        "\n",
        "mse = mean_squared_error(true_vals, est_vals)\n",
        "mae = mean_absolute_error(true_vals, est_vals)\n",
        "rmse = sqrt(mse)\n",
        "\n",
        "print(\"\\nEvaluation Metrics for QDE with LLaMA 2 7B:\")\n",
        "print(f\"  MSE  = {round(mse, 4)}\")\n",
        "print(f\"  RMSE = {round(rmse, 4)}\")\n",
        "print(f\"  MAE  = {round(mae, 4)}\")"
      ],
      "metadata": {
        "id": "uV96moXcvUdT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}